I0826 05:16:40.539763      20 e2e.go:126] Starting e2e run "e53ca283-6d70-4e16-83be-49a5afe9d1bc" on Ginkgo node 1
Aug 26 05:16:40.557: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1693027000 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 26 05:16:40.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:16:40.706: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0826 05:16:40.707017      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Aug 26 05:16:40.733: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 26 05:16:40.793: INFO: The status of Pod etcd-backup-with-interval-28217100-65lkw is Succeeded, skipping waiting
Aug 26 05:16:40.793: INFO: The status of Pod etcd-backup-with-interval-28217110-q6dz4 is Succeeded, skipping waiting
Aug 26 05:16:40.793: INFO: 21 / 23 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 26 05:16:40.793: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Aug 26 05:16:40.793: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 26 05:16:40.807: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 26 05:16:40.807: INFO: e2e test version: v1.26.7
Aug 26 05:16:40.809: INFO: kube-apiserver version: v1.26.7
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 26 05:16:40.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:16:40.814: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.109 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 26 05:16:40.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:16:40.706: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0826 05:16:40.707017      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Aug 26 05:16:40.733: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Aug 26 05:16:40.793: INFO: The status of Pod etcd-backup-with-interval-28217100-65lkw is Succeeded, skipping waiting
    Aug 26 05:16:40.793: INFO: The status of Pod etcd-backup-with-interval-28217110-q6dz4 is Succeeded, skipping waiting
    Aug 26 05:16:40.793: INFO: 21 / 23 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Aug 26 05:16:40.793: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
    Aug 26 05:16:40.793: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Aug 26 05:16:40.807: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Aug 26 05:16:40.807: INFO: e2e test version: v1.26.7
    Aug 26 05:16:40.809: INFO: kube-apiserver version: v1.26.7
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 26 05:16:40.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:16:40.814: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:16:40.864
Aug 26 05:16:40.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename dns 08/26/23 05:16:40.865
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:16:40.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:16:40.894
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 08/26/23 05:16:40.897
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6948 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6948;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6948 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6948;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6948.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6948.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6948.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6948.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6948.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6948.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6948.svc;check="$$(dig +notcp +noall +answer +search 173.74.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.74.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.74.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.74.173_tcp@PTR;sleep 1; done
 08/26/23 05:16:40.94
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6948 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6948;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6948 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6948;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6948.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6948.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6948.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6948.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6948.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6948.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6948.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6948.svc;check="$$(dig +notcp +noall +answer +search 173.74.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.74.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.74.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.74.173_tcp@PTR;sleep 1; done
 08/26/23 05:16:40.94
STEP: creating a pod to probe DNS 08/26/23 05:16:40.94
STEP: submitting the pod to kubernetes 08/26/23 05:16:40.942
Aug 26 05:16:40.959: INFO: Waiting up to 15m0s for pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1" in namespace "dns-6948" to be "running"
Aug 26 05:16:40.964: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.800299ms
Aug 26 05:16:42.974: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015372527s
Aug 26 05:16:44.970: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011584074s
Aug 26 05:16:46.971: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011891416s
Aug 26 05:16:48.971: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011745576s
Aug 26 05:16:50.976: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Running", Reason="", readiness=true. Elapsed: 10.01695765s
Aug 26 05:16:50.976: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1" satisfied condition "running"
STEP: retrieving the pod 08/26/23 05:16:50.976
STEP: looking for the results for each expected name from probers 08/26/23 05:16:50.983
Aug 26 05:16:50.990: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:50.996: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.002: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.008: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.013: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.019: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.033: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.061: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.076: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.081: INFO: Unable to read jessie_udp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.086: INFO: Unable to read jessie_tcp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.093: INFO: Unable to read jessie_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.100: INFO: Unable to read jessie_tcp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.110: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.115: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:51.140: INFO: Lookups using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6948 wheezy_tcp@dns-test-service.dns-6948 wheezy_udp@dns-test-service.dns-6948.svc wheezy_tcp@dns-test-service.dns-6948.svc wheezy_udp@_http._tcp.dns-test-service.dns-6948.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6948 jessie_tcp@dns-test-service.dns-6948 jessie_udp@dns-test-service.dns-6948.svc jessie_tcp@dns-test-service.dns-6948.svc jessie_udp@_http._tcp.dns-test-service.dns-6948.svc jessie_tcp@_http._tcp.dns-test-service.dns-6948.svc]

Aug 26 05:16:56.146: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:56.150: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:56.156: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:56.164: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:56.171: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:56.195: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:16:56.306: INFO: Lookups using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6948 wheezy_tcp@dns-test-service.dns-6948 wheezy_udp@dns-test-service.dns-6948.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc]

Aug 26 05:17:01.146: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:17:01.153: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:17:01.158: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:17:01.164: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:17:01.170: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:17:01.190: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:17:01.280: INFO: Lookups using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6948 wheezy_tcp@dns-test-service.dns-6948 wheezy_udp@dns-test-service.dns-6948.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc]

Aug 26 05:17:06.160: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:17:06.177: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:17:06.183: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:17:06.283: INFO: Lookups using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-6948 wheezy_udp@dns-test-service.dns-6948.svc]

Aug 26 05:17:11.184: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
Aug 26 05:17:11.307: INFO: Lookups using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 failed for: [wheezy_udp@dns-test-service.dns-6948.svc]

Aug 26 05:17:16.267: INFO: DNS probes using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 succeeded

STEP: deleting the pod 08/26/23 05:17:16.267
STEP: deleting the test service 08/26/23 05:17:16.284
STEP: deleting the test headless service 08/26/23 05:17:16.337
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 26 05:17:16.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6948" for this suite. 08/26/23 05:17:16.36
------------------------------
• [SLOW TEST] [35.502 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:16:40.864
    Aug 26 05:16:40.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename dns 08/26/23 05:16:40.865
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:16:40.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:16:40.894
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 08/26/23 05:16:40.897
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6948 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6948;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6948 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6948;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6948.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6948.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6948.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6948.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6948.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6948.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6948.svc;check="$$(dig +notcp +noall +answer +search 173.74.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.74.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.74.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.74.173_tcp@PTR;sleep 1; done
     08/26/23 05:16:40.94
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6948 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6948;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6948 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6948;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6948.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6948.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6948.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6948.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6948.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6948.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6948.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6948.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6948.svc;check="$$(dig +notcp +noall +answer +search 173.74.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.74.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.74.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.74.173_tcp@PTR;sleep 1; done
     08/26/23 05:16:40.94
    STEP: creating a pod to probe DNS 08/26/23 05:16:40.94
    STEP: submitting the pod to kubernetes 08/26/23 05:16:40.942
    Aug 26 05:16:40.959: INFO: Waiting up to 15m0s for pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1" in namespace "dns-6948" to be "running"
    Aug 26 05:16:40.964: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.800299ms
    Aug 26 05:16:42.974: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015372527s
    Aug 26 05:16:44.970: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011584074s
    Aug 26 05:16:46.971: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011891416s
    Aug 26 05:16:48.971: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011745576s
    Aug 26 05:16:50.976: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1": Phase="Running", Reason="", readiness=true. Elapsed: 10.01695765s
    Aug 26 05:16:50.976: INFO: Pod "dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1" satisfied condition "running"
    STEP: retrieving the pod 08/26/23 05:16:50.976
    STEP: looking for the results for each expected name from probers 08/26/23 05:16:50.983
    Aug 26 05:16:50.990: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:50.996: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.002: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.008: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.013: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.019: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.033: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.061: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.076: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.081: INFO: Unable to read jessie_udp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.086: INFO: Unable to read jessie_tcp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.093: INFO: Unable to read jessie_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.100: INFO: Unable to read jessie_tcp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.110: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.115: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:51.140: INFO: Lookups using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6948 wheezy_tcp@dns-test-service.dns-6948 wheezy_udp@dns-test-service.dns-6948.svc wheezy_tcp@dns-test-service.dns-6948.svc wheezy_udp@_http._tcp.dns-test-service.dns-6948.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6948 jessie_tcp@dns-test-service.dns-6948 jessie_udp@dns-test-service.dns-6948.svc jessie_tcp@dns-test-service.dns-6948.svc jessie_udp@_http._tcp.dns-test-service.dns-6948.svc jessie_tcp@_http._tcp.dns-test-service.dns-6948.svc]

    Aug 26 05:16:56.146: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:56.150: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:56.156: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:56.164: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:56.171: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:56.195: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:16:56.306: INFO: Lookups using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6948 wheezy_tcp@dns-test-service.dns-6948 wheezy_udp@dns-test-service.dns-6948.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc]

    Aug 26 05:17:01.146: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:17:01.153: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:17:01.158: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:17:01.164: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:17:01.170: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:17:01.190: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:17:01.280: INFO: Lookups using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6948 wheezy_tcp@dns-test-service.dns-6948 wheezy_udp@dns-test-service.dns-6948.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6948.svc]

    Aug 26 05:17:06.160: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:17:06.177: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6948 from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:17:06.183: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:17:06.283: INFO: Lookups using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-6948 wheezy_udp@dns-test-service.dns-6948.svc]

    Aug 26 05:17:11.184: INFO: Unable to read wheezy_udp@dns-test-service.dns-6948.svc from pod dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1: the server could not find the requested resource (get pods dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1)
    Aug 26 05:17:11.307: INFO: Lookups using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 failed for: [wheezy_udp@dns-test-service.dns-6948.svc]

    Aug 26 05:17:16.267: INFO: DNS probes using dns-6948/dns-test-8871ba8b-d9e9-4dd1-bd03-3ca0c6982cd1 succeeded

    STEP: deleting the pod 08/26/23 05:17:16.267
    STEP: deleting the test service 08/26/23 05:17:16.284
    STEP: deleting the test headless service 08/26/23 05:17:16.337
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:17:16.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6948" for this suite. 08/26/23 05:17:16.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:17:16.369
Aug 26 05:17:16.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename namespaces 08/26/23 05:17:16.371
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:16.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:17:16.402
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 08/26/23 05:17:16.408
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:16.43
STEP: Creating a pod in the namespace 08/26/23 05:17:16.433
STEP: Waiting for the pod to have running status 08/26/23 05:17:16.442
Aug 26 05:17:16.443: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2609" to be "running"
Aug 26 05:17:16.446: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.628834ms
Aug 26 05:17:18.451: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008884942s
Aug 26 05:17:20.451: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008731284s
Aug 26 05:17:20.451: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 08/26/23 05:17:20.451
STEP: Waiting for the namespace to be removed. 08/26/23 05:17:20.463
STEP: Recreating the namespace 08/26/23 05:17:31.468
STEP: Verifying there are no pods in the namespace 08/26/23 05:17:31.491
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:17:31.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1451" for this suite. 08/26/23 05:17:31.507
STEP: Destroying namespace "nsdeletetest-2609" for this suite. 08/26/23 05:17:31.515
Aug 26 05:17:31.523: INFO: Namespace nsdeletetest-2609 was already deleted
STEP: Destroying namespace "nsdeletetest-1136" for this suite. 08/26/23 05:17:31.523
------------------------------
• [SLOW TEST] [15.162 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:17:16.369
    Aug 26 05:17:16.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename namespaces 08/26/23 05:17:16.371
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:16.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:17:16.402
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 08/26/23 05:17:16.408
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:16.43
    STEP: Creating a pod in the namespace 08/26/23 05:17:16.433
    STEP: Waiting for the pod to have running status 08/26/23 05:17:16.442
    Aug 26 05:17:16.443: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2609" to be "running"
    Aug 26 05:17:16.446: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.628834ms
    Aug 26 05:17:18.451: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008884942s
    Aug 26 05:17:20.451: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008731284s
    Aug 26 05:17:20.451: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 08/26/23 05:17:20.451
    STEP: Waiting for the namespace to be removed. 08/26/23 05:17:20.463
    STEP: Recreating the namespace 08/26/23 05:17:31.468
    STEP: Verifying there are no pods in the namespace 08/26/23 05:17:31.491
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:17:31.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1451" for this suite. 08/26/23 05:17:31.507
    STEP: Destroying namespace "nsdeletetest-2609" for this suite. 08/26/23 05:17:31.515
    Aug 26 05:17:31.523: INFO: Namespace nsdeletetest-2609 was already deleted
    STEP: Destroying namespace "nsdeletetest-1136" for this suite. 08/26/23 05:17:31.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:17:31.536
Aug 26 05:17:31.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:17:31.539
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:31.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:17:31.56
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:17:31.564
Aug 26 05:17:31.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e" in namespace "projected-6389" to be "Succeeded or Failed"
Aug 26 05:17:31.578: INFO: Pod "downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18486ms
Aug 26 05:17:33.585: INFO: Pod "downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011424401s
Aug 26 05:17:35.583: INFO: Pod "downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00926093s
STEP: Saw pod success 08/26/23 05:17:35.583
Aug 26 05:17:35.583: INFO: Pod "downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e" satisfied condition "Succeeded or Failed"
Aug 26 05:17:35.589: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e container client-container: <nil>
STEP: delete the pod 08/26/23 05:17:35.613
Aug 26 05:17:35.630: INFO: Waiting for pod downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e to disappear
Aug 26 05:17:35.634: INFO: Pod downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 05:17:35.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6389" for this suite. 08/26/23 05:17:35.642
------------------------------
• [4.113 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:17:31.536
    Aug 26 05:17:31.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:17:31.539
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:31.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:17:31.56
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:17:31.564
    Aug 26 05:17:31.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e" in namespace "projected-6389" to be "Succeeded or Failed"
    Aug 26 05:17:31.578: INFO: Pod "downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18486ms
    Aug 26 05:17:33.585: INFO: Pod "downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011424401s
    Aug 26 05:17:35.583: INFO: Pod "downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00926093s
    STEP: Saw pod success 08/26/23 05:17:35.583
    Aug 26 05:17:35.583: INFO: Pod "downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e" satisfied condition "Succeeded or Failed"
    Aug 26 05:17:35.589: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e container client-container: <nil>
    STEP: delete the pod 08/26/23 05:17:35.613
    Aug 26 05:17:35.630: INFO: Waiting for pod downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e to disappear
    Aug 26 05:17:35.634: INFO: Pod downwardapi-volume-4bafd990-1a60-4978-82c4-cde06fcc281e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:17:35.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6389" for this suite. 08/26/23 05:17:35.642
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:17:35.65
Aug 26 05:17:35.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename svcaccounts 08/26/23 05:17:35.651
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:35.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:17:35.674
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-bpgpx"  08/26/23 05:17:35.677
Aug 26 05:17:35.682: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-bpgpx"  08/26/23 05:17:35.682
Aug 26 05:17:35.694: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 26 05:17:35.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8152" for this suite. 08/26/23 05:17:35.702
------------------------------
• [0.060 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:17:35.65
    Aug 26 05:17:35.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename svcaccounts 08/26/23 05:17:35.651
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:35.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:17:35.674
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-bpgpx"  08/26/23 05:17:35.677
    Aug 26 05:17:35.682: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-bpgpx"  08/26/23 05:17:35.682
    Aug 26 05:17:35.694: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:17:35.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8152" for this suite. 08/26/23 05:17:35.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:17:35.711
Aug 26 05:17:35.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 05:17:35.713
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:35.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:17:35.74
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 08/26/23 05:17:35.744
Aug 26 05:17:35.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9814 api-versions'
Aug 26 05:17:35.821: INFO: stderr: ""
Aug 26 05:17:35.821: INFO: stdout: "admissionregistration.k8s.io/v1\nagent.pf9.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 05:17:35.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9814" for this suite. 08/26/23 05:17:35.83
------------------------------
• [0.126 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:17:35.711
    Aug 26 05:17:35.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 05:17:35.713
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:35.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:17:35.74
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 08/26/23 05:17:35.744
    Aug 26 05:17:35.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9814 api-versions'
    Aug 26 05:17:35.821: INFO: stderr: ""
    Aug 26 05:17:35.821: INFO: stdout: "admissionregistration.k8s.io/v1\nagent.pf9.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:17:35.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9814" for this suite. 08/26/23 05:17:35.83
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:17:35.838
Aug 26 05:17:35.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename statefulset 08/26/23 05:17:35.839
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:35.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:17:35.858
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4474 08/26/23 05:17:35.866
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-4474 08/26/23 05:17:35.877
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4474 08/26/23 05:17:35.891
Aug 26 05:17:35.899: INFO: Found 0 stateful pods, waiting for 1
Aug 26 05:17:45.906: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/26/23 05:17:45.906
Aug 26 05:17:45.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 26 05:17:46.144: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 26 05:17:46.144: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 26 05:17:46.144: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 26 05:17:46.149: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 26 05:17:56.154: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 05:17:56.154: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 05:17:56.182: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Aug 26 05:17:56.182: INFO: ss-0  ip-10-0-1-101.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:35 +0000 UTC  }]
Aug 26 05:17:56.182: INFO: 
Aug 26 05:17:56.182: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 26 05:17:57.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989624421s
Aug 26 05:17:58.196: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982503434s
Aug 26 05:17:59.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975549768s
Aug 26 05:18:00.208: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969567975s
Aug 26 05:18:01.214: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963271953s
Aug 26 05:18:02.220: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957492214s
Aug 26 05:18:03.227: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950809235s
Aug 26 05:18:04.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.945249133s
Aug 26 05:18:05.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.040992ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4474 08/26/23 05:18:06.238
Aug 26 05:18:06.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 26 05:18:06.459: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 26 05:18:06.459: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 26 05:18:06.459: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 26 05:18:06.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 26 05:18:06.680: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 26 05:18:06.680: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 26 05:18:06.680: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 26 05:18:06.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 26 05:18:06.878: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 26 05:18:06.878: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 26 05:18:06.878: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 26 05:18:06.884: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 26 05:18:16.890: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 05:18:16.890: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 05:18:16.890: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 08/26/23 05:18:16.89
Aug 26 05:18:16.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 26 05:18:17.148: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 26 05:18:17.148: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 26 05:18:17.148: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 26 05:18:17.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 26 05:18:17.313: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 26 05:18:17.313: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 26 05:18:17.313: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 26 05:18:17.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 26 05:18:17.498: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 26 05:18:17.498: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 26 05:18:17.498: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 26 05:18:17.498: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 05:18:17.502: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 26 05:18:27.520: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 05:18:27.520: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 05:18:27.520: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 05:18:27.552: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Aug 26 05:18:27.552: INFO: ss-0  ip-10-0-1-101.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:35 +0000 UTC  }]
Aug 26 05:18:27.552: INFO: ss-1  ip-10-0-1-31.us-west-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  }]
Aug 26 05:18:27.552: INFO: ss-2  ip-10-0-1-5.us-west-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  }]
Aug 26 05:18:27.552: INFO: 
Aug 26 05:18:27.552: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 26 05:18:28.557: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Aug 26 05:18:28.557: INFO: ss-2  ip-10-0-1-5.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  }]
Aug 26 05:18:28.558: INFO: 
Aug 26 05:18:28.558: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 26 05:18:29.562: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.974334257s
Aug 26 05:18:30.567: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.969999678s
Aug 26 05:18:31.572: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.965325602s
Aug 26 05:18:32.576: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.960734218s
Aug 26 05:18:33.580: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.956774469s
Aug 26 05:18:34.584: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.952204252s
Aug 26 05:18:35.589: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.947281362s
Aug 26 05:18:36.595: INFO: Verifying statefulset ss doesn't scale past 0 for another 942.320735ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4474 08/26/23 05:18:37.595
Aug 26 05:18:37.600: INFO: Scaling statefulset ss to 0
Aug 26 05:18:37.617: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 26 05:18:37.622: INFO: Deleting all statefulset in ns statefulset-4474
Aug 26 05:18:37.625: INFO: Scaling statefulset ss to 0
Aug 26 05:18:37.641: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 05:18:37.645: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:18:37.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4474" for this suite. 08/26/23 05:18:37.686
------------------------------
• [SLOW TEST] [61.859 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:17:35.838
    Aug 26 05:17:35.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename statefulset 08/26/23 05:17:35.839
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:17:35.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:17:35.858
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4474 08/26/23 05:17:35.866
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-4474 08/26/23 05:17:35.877
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4474 08/26/23 05:17:35.891
    Aug 26 05:17:35.899: INFO: Found 0 stateful pods, waiting for 1
    Aug 26 05:17:45.906: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/26/23 05:17:45.906
    Aug 26 05:17:45.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 26 05:17:46.144: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 26 05:17:46.144: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 26 05:17:46.144: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 26 05:17:46.149: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 26 05:17:56.154: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 26 05:17:56.154: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 05:17:56.182: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
    Aug 26 05:17:56.182: INFO: ss-0  ip-10-0-1-101.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:35 +0000 UTC  }]
    Aug 26 05:17:56.182: INFO: 
    Aug 26 05:17:56.182: INFO: StatefulSet ss has not reached scale 3, at 1
    Aug 26 05:17:57.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989624421s
    Aug 26 05:17:58.196: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982503434s
    Aug 26 05:17:59.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975549768s
    Aug 26 05:18:00.208: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969567975s
    Aug 26 05:18:01.214: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963271953s
    Aug 26 05:18:02.220: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957492214s
    Aug 26 05:18:03.227: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950809235s
    Aug 26 05:18:04.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.945249133s
    Aug 26 05:18:05.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.040992ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4474 08/26/23 05:18:06.238
    Aug 26 05:18:06.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 26 05:18:06.459: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 26 05:18:06.459: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 26 05:18:06.459: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 26 05:18:06.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 26 05:18:06.680: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 26 05:18:06.680: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 26 05:18:06.680: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 26 05:18:06.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 26 05:18:06.878: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 26 05:18:06.878: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 26 05:18:06.878: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 26 05:18:06.884: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Aug 26 05:18:16.890: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 05:18:16.890: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 05:18:16.890: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 08/26/23 05:18:16.89
    Aug 26 05:18:16.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 26 05:18:17.148: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 26 05:18:17.148: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 26 05:18:17.148: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 26 05:18:17.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 26 05:18:17.313: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 26 05:18:17.313: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 26 05:18:17.313: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 26 05:18:17.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-4474 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 26 05:18:17.498: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 26 05:18:17.498: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 26 05:18:17.498: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 26 05:18:17.498: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 05:18:17.502: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 26 05:18:27.520: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 26 05:18:27.520: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 26 05:18:27.520: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 26 05:18:27.552: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
    Aug 26 05:18:27.552: INFO: ss-0  ip-10-0-1-101.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:35 +0000 UTC  }]
    Aug 26 05:18:27.552: INFO: ss-1  ip-10-0-1-31.us-west-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  }]
    Aug 26 05:18:27.552: INFO: ss-2  ip-10-0-1-5.us-west-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  }]
    Aug 26 05:18:27.552: INFO: 
    Aug 26 05:18:27.552: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 26 05:18:28.557: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Aug 26 05:18:28.557: INFO: ss-2  ip-10-0-1-5.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:18:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:17:56 +0000 UTC  }]
    Aug 26 05:18:28.558: INFO: 
    Aug 26 05:18:28.558: INFO: StatefulSet ss has not reached scale 0, at 1
    Aug 26 05:18:29.562: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.974334257s
    Aug 26 05:18:30.567: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.969999678s
    Aug 26 05:18:31.572: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.965325602s
    Aug 26 05:18:32.576: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.960734218s
    Aug 26 05:18:33.580: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.956774469s
    Aug 26 05:18:34.584: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.952204252s
    Aug 26 05:18:35.589: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.947281362s
    Aug 26 05:18:36.595: INFO: Verifying statefulset ss doesn't scale past 0 for another 942.320735ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4474 08/26/23 05:18:37.595
    Aug 26 05:18:37.600: INFO: Scaling statefulset ss to 0
    Aug 26 05:18:37.617: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 26 05:18:37.622: INFO: Deleting all statefulset in ns statefulset-4474
    Aug 26 05:18:37.625: INFO: Scaling statefulset ss to 0
    Aug 26 05:18:37.641: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 05:18:37.645: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:18:37.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4474" for this suite. 08/26/23 05:18:37.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:18:37.698
Aug 26 05:18:37.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubelet-test 08/26/23 05:18:37.701
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:18:37.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:18:37.729
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Aug 26 05:18:37.749: INFO: Waiting up to 5m0s for pod "busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd" in namespace "kubelet-test-799" to be "running and ready"
Aug 26 05:18:37.760: INFO: Pod "busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.16404ms
Aug 26 05:18:37.760: INFO: The phase of Pod busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:18:39.764: INFO: Pod "busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014550824s
Aug 26 05:18:39.764: INFO: The phase of Pod busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:18:41.765: INFO: Pod "busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd": Phase="Running", Reason="", readiness=true. Elapsed: 4.015376978s
Aug 26 05:18:41.765: INFO: The phase of Pod busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd is Running (Ready = true)
Aug 26 05:18:41.765: INFO: Pod "busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:18:41.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-799" for this suite. 08/26/23 05:18:41.792
------------------------------
• [4.102 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:18:37.698
    Aug 26 05:18:37.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubelet-test 08/26/23 05:18:37.701
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:18:37.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:18:37.729
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Aug 26 05:18:37.749: INFO: Waiting up to 5m0s for pod "busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd" in namespace "kubelet-test-799" to be "running and ready"
    Aug 26 05:18:37.760: INFO: Pod "busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.16404ms
    Aug 26 05:18:37.760: INFO: The phase of Pod busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:18:39.764: INFO: Pod "busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014550824s
    Aug 26 05:18:39.764: INFO: The phase of Pod busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:18:41.765: INFO: Pod "busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd": Phase="Running", Reason="", readiness=true. Elapsed: 4.015376978s
    Aug 26 05:18:41.765: INFO: The phase of Pod busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd is Running (Ready = true)
    Aug 26 05:18:41.765: INFO: Pod "busybox-readonly-fscc84b52a-52d3-4ca3-9b94-455f2106bfdd" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:18:41.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-799" for this suite. 08/26/23 05:18:41.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:18:41.8
Aug 26 05:18:41.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename job 08/26/23 05:18:41.801
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:18:41.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:18:41.821
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 08/26/23 05:18:41.825
STEP: Ensuring active pods == parallelism 08/26/23 05:18:41.834
STEP: Orphaning one of the Job's Pods 08/26/23 05:18:43.84
Aug 26 05:18:44.364: INFO: Successfully updated pod "adopt-release-g9lc8"
STEP: Checking that the Job readopts the Pod 08/26/23 05:18:44.364
Aug 26 05:18:44.365: INFO: Waiting up to 15m0s for pod "adopt-release-g9lc8" in namespace "job-6120" to be "adopted"
Aug 26 05:18:44.374: INFO: Pod "adopt-release-g9lc8": Phase="Running", Reason="", readiness=true. Elapsed: 9.209011ms
Aug 26 05:18:46.380: INFO: Pod "adopt-release-g9lc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015247695s
Aug 26 05:18:46.380: INFO: Pod "adopt-release-g9lc8" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 08/26/23 05:18:46.38
Aug 26 05:18:46.894: INFO: Successfully updated pod "adopt-release-g9lc8"
STEP: Checking that the Job releases the Pod 08/26/23 05:18:46.894
Aug 26 05:18:46.894: INFO: Waiting up to 15m0s for pod "adopt-release-g9lc8" in namespace "job-6120" to be "released"
Aug 26 05:18:46.898: INFO: Pod "adopt-release-g9lc8": Phase="Running", Reason="", readiness=true. Elapsed: 3.967332ms
Aug 26 05:18:48.904: INFO: Pod "adopt-release-g9lc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009723154s
Aug 26 05:18:48.904: INFO: Pod "adopt-release-g9lc8" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 26 05:18:48.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6120" for this suite. 08/26/23 05:18:48.911
------------------------------
• [SLOW TEST] [7.118 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:18:41.8
    Aug 26 05:18:41.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename job 08/26/23 05:18:41.801
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:18:41.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:18:41.821
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 08/26/23 05:18:41.825
    STEP: Ensuring active pods == parallelism 08/26/23 05:18:41.834
    STEP: Orphaning one of the Job's Pods 08/26/23 05:18:43.84
    Aug 26 05:18:44.364: INFO: Successfully updated pod "adopt-release-g9lc8"
    STEP: Checking that the Job readopts the Pod 08/26/23 05:18:44.364
    Aug 26 05:18:44.365: INFO: Waiting up to 15m0s for pod "adopt-release-g9lc8" in namespace "job-6120" to be "adopted"
    Aug 26 05:18:44.374: INFO: Pod "adopt-release-g9lc8": Phase="Running", Reason="", readiness=true. Elapsed: 9.209011ms
    Aug 26 05:18:46.380: INFO: Pod "adopt-release-g9lc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015247695s
    Aug 26 05:18:46.380: INFO: Pod "adopt-release-g9lc8" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 08/26/23 05:18:46.38
    Aug 26 05:18:46.894: INFO: Successfully updated pod "adopt-release-g9lc8"
    STEP: Checking that the Job releases the Pod 08/26/23 05:18:46.894
    Aug 26 05:18:46.894: INFO: Waiting up to 15m0s for pod "adopt-release-g9lc8" in namespace "job-6120" to be "released"
    Aug 26 05:18:46.898: INFO: Pod "adopt-release-g9lc8": Phase="Running", Reason="", readiness=true. Elapsed: 3.967332ms
    Aug 26 05:18:48.904: INFO: Pod "adopt-release-g9lc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009723154s
    Aug 26 05:18:48.904: INFO: Pod "adopt-release-g9lc8" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:18:48.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6120" for this suite. 08/26/23 05:18:48.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:18:48.919
Aug 26 05:18:48.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename statefulset 08/26/23 05:18:48.92
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:18:48.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:18:48.942
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6256 08/26/23 05:18:48.945
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-6256 08/26/23 05:18:48.953
Aug 26 05:18:48.967: INFO: Found 0 stateful pods, waiting for 1
Aug 26 05:18:58.976: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 08/26/23 05:18:58.986
STEP: updating a scale subresource 08/26/23 05:18:58.99
STEP: verifying the statefulset Spec.Replicas was modified 08/26/23 05:18:59.007
STEP: Patch a scale subresource 08/26/23 05:18:59.011
STEP: verifying the statefulset Spec.Replicas was modified 08/26/23 05:18:59.03
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 26 05:18:59.037: INFO: Deleting all statefulset in ns statefulset-6256
Aug 26 05:18:59.041: INFO: Scaling statefulset ss to 0
Aug 26 05:19:09.086: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 05:19:09.089: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:09.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6256" for this suite. 08/26/23 05:19:09.111
------------------------------
• [SLOW TEST] [20.203 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:18:48.919
    Aug 26 05:18:48.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename statefulset 08/26/23 05:18:48.92
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:18:48.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:18:48.942
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6256 08/26/23 05:18:48.945
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-6256 08/26/23 05:18:48.953
    Aug 26 05:18:48.967: INFO: Found 0 stateful pods, waiting for 1
    Aug 26 05:18:58.976: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 08/26/23 05:18:58.986
    STEP: updating a scale subresource 08/26/23 05:18:58.99
    STEP: verifying the statefulset Spec.Replicas was modified 08/26/23 05:18:59.007
    STEP: Patch a scale subresource 08/26/23 05:18:59.011
    STEP: verifying the statefulset Spec.Replicas was modified 08/26/23 05:18:59.03
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 26 05:18:59.037: INFO: Deleting all statefulset in ns statefulset-6256
    Aug 26 05:18:59.041: INFO: Scaling statefulset ss to 0
    Aug 26 05:19:09.086: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 05:19:09.089: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:09.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6256" for this suite. 08/26/23 05:19:09.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:09.123
Aug 26 05:19:09.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 05:19:09.124
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:09.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:09.151
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 08/26/23 05:19:09.155
Aug 26 05:19:09.169: INFO: Waiting up to 5m0s for pod "pod-eee30360-0229-4e42-ae74-471a5ea2f06d" in namespace "emptydir-5937" to be "Succeeded or Failed"
Aug 26 05:19:09.176: INFO: Pod "pod-eee30360-0229-4e42-ae74-471a5ea2f06d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.159685ms
Aug 26 05:19:11.181: INFO: Pod "pod-eee30360-0229-4e42-ae74-471a5ea2f06d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012119669s
Aug 26 05:19:13.181: INFO: Pod "pod-eee30360-0229-4e42-ae74-471a5ea2f06d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012834103s
STEP: Saw pod success 08/26/23 05:19:13.181
Aug 26 05:19:13.181: INFO: Pod "pod-eee30360-0229-4e42-ae74-471a5ea2f06d" satisfied condition "Succeeded or Failed"
Aug 26 05:19:13.186: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-eee30360-0229-4e42-ae74-471a5ea2f06d container test-container: <nil>
STEP: delete the pod 08/26/23 05:19:13.207
Aug 26 05:19:13.220: INFO: Waiting for pod pod-eee30360-0229-4e42-ae74-471a5ea2f06d to disappear
Aug 26 05:19:13.224: INFO: Pod pod-eee30360-0229-4e42-ae74-471a5ea2f06d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:13.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5937" for this suite. 08/26/23 05:19:13.231
------------------------------
• [4.114 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:09.123
    Aug 26 05:19:09.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 05:19:09.124
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:09.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:09.151
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 08/26/23 05:19:09.155
    Aug 26 05:19:09.169: INFO: Waiting up to 5m0s for pod "pod-eee30360-0229-4e42-ae74-471a5ea2f06d" in namespace "emptydir-5937" to be "Succeeded or Failed"
    Aug 26 05:19:09.176: INFO: Pod "pod-eee30360-0229-4e42-ae74-471a5ea2f06d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.159685ms
    Aug 26 05:19:11.181: INFO: Pod "pod-eee30360-0229-4e42-ae74-471a5ea2f06d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012119669s
    Aug 26 05:19:13.181: INFO: Pod "pod-eee30360-0229-4e42-ae74-471a5ea2f06d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012834103s
    STEP: Saw pod success 08/26/23 05:19:13.181
    Aug 26 05:19:13.181: INFO: Pod "pod-eee30360-0229-4e42-ae74-471a5ea2f06d" satisfied condition "Succeeded or Failed"
    Aug 26 05:19:13.186: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-eee30360-0229-4e42-ae74-471a5ea2f06d container test-container: <nil>
    STEP: delete the pod 08/26/23 05:19:13.207
    Aug 26 05:19:13.220: INFO: Waiting for pod pod-eee30360-0229-4e42-ae74-471a5ea2f06d to disappear
    Aug 26 05:19:13.224: INFO: Pod pod-eee30360-0229-4e42-ae74-471a5ea2f06d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:13.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5937" for this suite. 08/26/23 05:19:13.231
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:13.238
Aug 26 05:19:13.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename svcaccounts 08/26/23 05:19:13.239
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:13.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:13.258
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Aug 26 05:19:13.283: INFO: created pod pod-service-account-defaultsa
Aug 26 05:19:13.283: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 26 05:19:13.292: INFO: created pod pod-service-account-mountsa
Aug 26 05:19:13.292: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 26 05:19:13.299: INFO: created pod pod-service-account-nomountsa
Aug 26 05:19:13.299: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 26 05:19:13.309: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 26 05:19:13.309: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 26 05:19:13.315: INFO: created pod pod-service-account-mountsa-mountspec
Aug 26 05:19:13.315: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 26 05:19:13.322: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 26 05:19:13.322: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 26 05:19:13.339: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 26 05:19:13.339: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 26 05:19:13.347: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 26 05:19:13.347: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 26 05:19:13.365: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 26 05:19:13.365: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:13.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6078" for this suite. 08/26/23 05:19:13.385
------------------------------
• [0.158 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:13.238
    Aug 26 05:19:13.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename svcaccounts 08/26/23 05:19:13.239
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:13.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:13.258
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Aug 26 05:19:13.283: INFO: created pod pod-service-account-defaultsa
    Aug 26 05:19:13.283: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Aug 26 05:19:13.292: INFO: created pod pod-service-account-mountsa
    Aug 26 05:19:13.292: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Aug 26 05:19:13.299: INFO: created pod pod-service-account-nomountsa
    Aug 26 05:19:13.299: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Aug 26 05:19:13.309: INFO: created pod pod-service-account-defaultsa-mountspec
    Aug 26 05:19:13.309: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Aug 26 05:19:13.315: INFO: created pod pod-service-account-mountsa-mountspec
    Aug 26 05:19:13.315: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Aug 26 05:19:13.322: INFO: created pod pod-service-account-nomountsa-mountspec
    Aug 26 05:19:13.322: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Aug 26 05:19:13.339: INFO: created pod pod-service-account-defaultsa-nomountspec
    Aug 26 05:19:13.339: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Aug 26 05:19:13.347: INFO: created pod pod-service-account-mountsa-nomountspec
    Aug 26 05:19:13.347: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Aug 26 05:19:13.365: INFO: created pod pod-service-account-nomountsa-nomountspec
    Aug 26 05:19:13.365: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:13.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6078" for this suite. 08/26/23 05:19:13.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:13.399
Aug 26 05:19:13.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 05:19:13.402
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:13.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:13.423
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 08/26/23 05:19:13.426
Aug 26 05:19:13.437: INFO: Waiting up to 5m0s for pod "pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f" in namespace "pods-9297" to be "running and ready"
Aug 26 05:19:13.442: INFO: Pod "pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.856403ms
Aug 26 05:19:13.442: INFO: The phase of Pod pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:19:15.446: INFO: Pod "pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009010335s
Aug 26 05:19:15.446: INFO: The phase of Pod pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:19:17.448: INFO: Pod "pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f": Phase="Running", Reason="", readiness=true. Elapsed: 4.0104305s
Aug 26 05:19:17.448: INFO: The phase of Pod pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f is Running (Ready = true)
Aug 26 05:19:17.448: INFO: Pod "pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f" satisfied condition "running and ready"
Aug 26 05:19:17.458: INFO: Pod pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f has hostIP: 10.0.1.31
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:17.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9297" for this suite. 08/26/23 05:19:17.48
------------------------------
• [4.088 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:13.399
    Aug 26 05:19:13.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 05:19:13.402
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:13.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:13.423
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 08/26/23 05:19:13.426
    Aug 26 05:19:13.437: INFO: Waiting up to 5m0s for pod "pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f" in namespace "pods-9297" to be "running and ready"
    Aug 26 05:19:13.442: INFO: Pod "pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.856403ms
    Aug 26 05:19:13.442: INFO: The phase of Pod pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:19:15.446: INFO: Pod "pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009010335s
    Aug 26 05:19:15.446: INFO: The phase of Pod pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:19:17.448: INFO: Pod "pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f": Phase="Running", Reason="", readiness=true. Elapsed: 4.0104305s
    Aug 26 05:19:17.448: INFO: The phase of Pod pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f is Running (Ready = true)
    Aug 26 05:19:17.448: INFO: Pod "pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f" satisfied condition "running and ready"
    Aug 26 05:19:17.458: INFO: Pod pod-hostip-6775ae2e-40cd-4935-b46a-0fd55d98299f has hostIP: 10.0.1.31
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:17.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9297" for this suite. 08/26/23 05:19:17.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:17.488
Aug 26 05:19:17.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:19:17.49
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:17.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:17.512
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:19:17.537
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:19:18.236
STEP: Deploying the webhook pod 08/26/23 05:19:18.252
STEP: Wait for the deployment to be ready 08/26/23 05:19:18.275
Aug 26 05:19:18.290: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:19:20.306
STEP: Verifying the service has paired with the endpoint 08/26/23 05:19:20.332
Aug 26 05:19:21.332: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 08/26/23 05:19:21.436
STEP: Creating a configMap that should be mutated 08/26/23 05:19:21.461
STEP: Deleting the collection of validation webhooks 08/26/23 05:19:21.497
STEP: Creating a configMap that should not be mutated 08/26/23 05:19:21.565
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:21.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2562" for this suite. 08/26/23 05:19:21.644
STEP: Destroying namespace "webhook-2562-markers" for this suite. 08/26/23 05:19:21.659
------------------------------
• [4.179 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:17.488
    Aug 26 05:19:17.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:19:17.49
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:17.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:17.512
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:19:17.537
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:19:18.236
    STEP: Deploying the webhook pod 08/26/23 05:19:18.252
    STEP: Wait for the deployment to be ready 08/26/23 05:19:18.275
    Aug 26 05:19:18.290: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:19:20.306
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:19:20.332
    Aug 26 05:19:21.332: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 08/26/23 05:19:21.436
    STEP: Creating a configMap that should be mutated 08/26/23 05:19:21.461
    STEP: Deleting the collection of validation webhooks 08/26/23 05:19:21.497
    STEP: Creating a configMap that should not be mutated 08/26/23 05:19:21.565
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:21.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2562" for this suite. 08/26/23 05:19:21.644
    STEP: Destroying namespace "webhook-2562-markers" for this suite. 08/26/23 05:19:21.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:21.672
Aug 26 05:19:21.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 05:19:21.673
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:21.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:21.703
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 08/26/23 05:19:21.706
Aug 26 05:19:21.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3619 create -f -'
Aug 26 05:19:22.824: INFO: stderr: ""
Aug 26 05:19:22.824: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 08/26/23 05:19:22.824
Aug 26 05:19:22.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3619 diff -f -'
Aug 26 05:19:23.887: INFO: rc: 1
Aug 26 05:19:23.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3619 delete -f -'
Aug 26 05:19:23.985: INFO: stderr: ""
Aug 26 05:19:23.985: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:23.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3619" for this suite. 08/26/23 05:19:23.995
------------------------------
• [2.332 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:21.672
    Aug 26 05:19:21.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 05:19:21.673
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:21.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:21.703
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 08/26/23 05:19:21.706
    Aug 26 05:19:21.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3619 create -f -'
    Aug 26 05:19:22.824: INFO: stderr: ""
    Aug 26 05:19:22.824: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 08/26/23 05:19:22.824
    Aug 26 05:19:22.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3619 diff -f -'
    Aug 26 05:19:23.887: INFO: rc: 1
    Aug 26 05:19:23.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3619 delete -f -'
    Aug 26 05:19:23.985: INFO: stderr: ""
    Aug 26 05:19:23.985: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:23.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3619" for this suite. 08/26/23 05:19:23.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:24.004
Aug 26 05:19:24.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubelet-test 08/26/23 05:19:24.005
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:24.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:24.031
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 08/26/23 05:19:24.053
Aug 26 05:19:24.053: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases62fb5418-b419-45cb-97c2-70d5a6498f12" in namespace "kubelet-test-3680" to be "completed"
Aug 26 05:19:24.056: INFO: Pod "agnhost-host-aliases62fb5418-b419-45cb-97c2-70d5a6498f12": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534016ms
Aug 26 05:19:26.062: INFO: Pod "agnhost-host-aliases62fb5418-b419-45cb-97c2-70d5a6498f12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009403044s
Aug 26 05:19:28.062: INFO: Pod "agnhost-host-aliases62fb5418-b419-45cb-97c2-70d5a6498f12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009005198s
Aug 26 05:19:28.062: INFO: Pod "agnhost-host-aliases62fb5418-b419-45cb-97c2-70d5a6498f12" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:28.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3680" for this suite. 08/26/23 05:19:28.085
------------------------------
• [4.104 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:24.004
    Aug 26 05:19:24.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubelet-test 08/26/23 05:19:24.005
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:24.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:24.031
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 08/26/23 05:19:24.053
    Aug 26 05:19:24.053: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases62fb5418-b419-45cb-97c2-70d5a6498f12" in namespace "kubelet-test-3680" to be "completed"
    Aug 26 05:19:24.056: INFO: Pod "agnhost-host-aliases62fb5418-b419-45cb-97c2-70d5a6498f12": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534016ms
    Aug 26 05:19:26.062: INFO: Pod "agnhost-host-aliases62fb5418-b419-45cb-97c2-70d5a6498f12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009403044s
    Aug 26 05:19:28.062: INFO: Pod "agnhost-host-aliases62fb5418-b419-45cb-97c2-70d5a6498f12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009005198s
    Aug 26 05:19:28.062: INFO: Pod "agnhost-host-aliases62fb5418-b419-45cb-97c2-70d5a6498f12" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:28.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3680" for this suite. 08/26/23 05:19:28.085
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:28.108
Aug 26 05:19:28.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-webhook 08/26/23 05:19:28.109
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:28.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:28.133
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/26/23 05:19:28.135
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/26/23 05:19:28.446
STEP: Deploying the custom resource conversion webhook pod 08/26/23 05:19:28.454
STEP: Wait for the deployment to be ready 08/26/23 05:19:28.479
Aug 26 05:19:28.491: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/26/23 05:19:30.512
STEP: Verifying the service has paired with the endpoint 08/26/23 05:19:30.542
Aug 26 05:19:31.543: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Aug 26 05:19:31.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Creating a v1 custom resource 08/26/23 05:19:34.179
STEP: v2 custom resource should be converted 08/26/23 05:19:34.186
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:34.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8147" for this suite. 08/26/23 05:19:34.795
------------------------------
• [SLOW TEST] [6.703 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:28.108
    Aug 26 05:19:28.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-webhook 08/26/23 05:19:28.109
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:28.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:28.133
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/26/23 05:19:28.135
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/26/23 05:19:28.446
    STEP: Deploying the custom resource conversion webhook pod 08/26/23 05:19:28.454
    STEP: Wait for the deployment to be ready 08/26/23 05:19:28.479
    Aug 26 05:19:28.491: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/26/23 05:19:30.512
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:19:30.542
    Aug 26 05:19:31.543: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Aug 26 05:19:31.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Creating a v1 custom resource 08/26/23 05:19:34.179
    STEP: v2 custom resource should be converted 08/26/23 05:19:34.186
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:34.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8147" for this suite. 08/26/23 05:19:34.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:34.812
Aug 26 05:19:34.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 05:19:34.813
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:34.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:34.842
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Aug 26 05:19:34.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 create -f -'
Aug 26 05:19:35.890: INFO: stderr: ""
Aug 26 05:19:35.890: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 26 05:19:35.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 create -f -'
Aug 26 05:19:37.008: INFO: stderr: ""
Aug 26 05:19:37.008: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/26/23 05:19:37.008
Aug 26 05:19:38.013: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 26 05:19:38.013: INFO: Found 1 / 1
Aug 26 05:19:38.013: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 26 05:19:38.017: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 26 05:19:38.018: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 26 05:19:38.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 describe pod agnhost-primary-rzqx6'
Aug 26 05:19:38.106: INFO: stderr: ""
Aug 26 05:19:38.106: INFO: stdout: "Name:             agnhost-primary-rzqx6\nNamespace:        kubectl-4359\nPriority:         0\nService Account:  default\nNode:             ip-10-0-1-5.us-west-2.compute.internal/10.0.1.5\nStart Time:       Sat, 26 Aug 2023 05:19:35 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 494338669ef1cc962b4c110d0cb8c0da7df09eae1272b9b517865b9b2418f746\n                  cni.projectcalico.org/podIP: 10.20.199.75/32\n                  cni.projectcalico.org/podIPs: 10.20.199.75/32\nStatus:           Running\nIP:               10.20.199.75\nIPs:\n  IP:           10.20.199.75\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://a3c61ecafe7fd1ace85294b2a12ad2f06e38ae52e33853c74cb632ef2d3fda52\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 26 Aug 2023 05:19:36 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qk54t (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qk54t:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-4359/agnhost-primary-rzqx6 to ip-10-0-1-5.us-west-2.compute.internal\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Aug 26 05:19:38.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 describe rc agnhost-primary'
Aug 26 05:19:38.237: INFO: stderr: ""
Aug 26 05:19:38.237: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4359\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-rzqx6\n"
Aug 26 05:19:38.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 describe service agnhost-primary'
Aug 26 05:19:38.375: INFO: stderr: ""
Aug 26 05:19:38.375: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4359\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.21.86.192\nIPs:               10.21.86.192\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.20.199.75:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 26 05:19:38.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 describe node ip-10-0-1-101.us-west-2.compute.internal'
Aug 26 05:19:38.598: INFO: stderr: ""
Aug 26 05:19:38.598: INFO: stdout: "Name:               ip-10-0-1-101.us-west-2.compute.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-1-101.us-west-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=t3.xlarge\n                    topology.kubernetes.io/region=us-west-2\n                    topology.kubernetes.io/zone=us-west-2b\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.1.101/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.20.50.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 26 Aug 2023 04:58:45 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-1-101.us-west-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 26 Aug 2023 05:19:37 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 26 Aug 2023 04:59:12 +0000   Sat, 26 Aug 2023 04:59:12 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 26 Aug 2023 05:18:11 +0000   Sat, 26 Aug 2023 04:58:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 26 Aug 2023 05:18:11 +0000   Sat, 26 Aug 2023 04:58:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 26 Aug 2023 05:18:11 +0000   Sat, 26 Aug 2023 04:58:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 26 Aug 2023 05:18:11 +0000   Sat, 26 Aug 2023 04:59:16 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.0.1.101\n  ExternalIP:   54.202.229.59\n  Hostname:     ip-10-0-1-101.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-1-101.us-west-2.compute.internal\n  ExternalDNS:  ec2-54-202-229-59.us-west-2.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           50758604Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16197976Ki\n  pods:                        200\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           46779129369\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16095576Ki\n  pods:                        200\nSystem Info:\n  Machine ID:                 ec27b025ce9ca21bd3723ed1de6124be\n  System UUID:                ec27b025-ce9c-a21b-d372-3ed1de6124be\n  Boot ID:                    7b5a320c-f12e-485d-a01a-998bb89d959a\n  Kernel Version:             5.11.0-1028-aws\n  OS Image:                   Ubuntu 20.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.6\n  Kubelet Version:            v1.26.7\n  Kube-Proxy Version:         v1.26.7\nPodCIDR:                      10.20.6.0/24\nPodCIDRs:                     10.20.6.0/24\nProviderID:                   aws:///us-west-2b/i-04a91b7d3b13f8586\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-hgm7c                                          250m (6%)     1 (25%)     70Mi (0%)        1000Mi (6%)    20m\n  kube-system                 calico-typha-6dd9648c8f-85hjp                              10m (0%)      500m (12%)  70Mi (0%)        1Gi (6%)       15m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m19s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m16s\n  svcaccounts-6078            pod-service-account-defaultsa                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         25s\n  svcaccounts-6078            pod-service-account-mountsa-mountspec                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         25s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         260m (6%)   1500m (37%)\n  memory                      140Mi (0%)  2024Mi (12%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 22m                kube-proxy       \n  Warning  InvalidDiskCapacity      24m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeNotSchedulable       24m                kubelet          Node ip-10-0-1-101.us-west-2.compute.internal status is now: NodeNotSchedulable\n  Normal   NodeAllocatableEnforced  24m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  23m (x7 over 24m)  kubelet          Node ip-10-0-1-101.us-west-2.compute.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    23m (x7 over 24m)  kubelet          Node ip-10-0-1-101.us-west-2.compute.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     23m (x7 over 24m)  kubelet          Node ip-10-0-1-101.us-west-2.compute.internal status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           20m                node-controller  Node ip-10-0-1-101.us-west-2.compute.internal event: Registered Node ip-10-0-1-101.us-west-2.compute.internal in Controller\n"
Aug 26 05:19:38.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 describe namespace kubectl-4359'
Aug 26 05:19:38.705: INFO: stderr: ""
Aug 26 05:19:38.705: INFO: stdout: "Name:         kubectl-4359\nLabels:       e2e-framework=kubectl\n              e2e-run=e53ca283-6d70-4e16-83be-49a5afe9d1bc\n              kubernetes.io/metadata.name=kubectl-4359\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:38.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4359" for this suite. 08/26/23 05:19:38.713
------------------------------
• [3.912 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:34.812
    Aug 26 05:19:34.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 05:19:34.813
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:34.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:34.842
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Aug 26 05:19:34.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 create -f -'
    Aug 26 05:19:35.890: INFO: stderr: ""
    Aug 26 05:19:35.890: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Aug 26 05:19:35.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 create -f -'
    Aug 26 05:19:37.008: INFO: stderr: ""
    Aug 26 05:19:37.008: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/26/23 05:19:37.008
    Aug 26 05:19:38.013: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 26 05:19:38.013: INFO: Found 1 / 1
    Aug 26 05:19:38.013: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 26 05:19:38.017: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 26 05:19:38.018: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 26 05:19:38.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 describe pod agnhost-primary-rzqx6'
    Aug 26 05:19:38.106: INFO: stderr: ""
    Aug 26 05:19:38.106: INFO: stdout: "Name:             agnhost-primary-rzqx6\nNamespace:        kubectl-4359\nPriority:         0\nService Account:  default\nNode:             ip-10-0-1-5.us-west-2.compute.internal/10.0.1.5\nStart Time:       Sat, 26 Aug 2023 05:19:35 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 494338669ef1cc962b4c110d0cb8c0da7df09eae1272b9b517865b9b2418f746\n                  cni.projectcalico.org/podIP: 10.20.199.75/32\n                  cni.projectcalico.org/podIPs: 10.20.199.75/32\nStatus:           Running\nIP:               10.20.199.75\nIPs:\n  IP:           10.20.199.75\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://a3c61ecafe7fd1ace85294b2a12ad2f06e38ae52e33853c74cb632ef2d3fda52\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 26 Aug 2023 05:19:36 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qk54t (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qk54t:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-4359/agnhost-primary-rzqx6 to ip-10-0-1-5.us-west-2.compute.internal\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Aug 26 05:19:38.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 describe rc agnhost-primary'
    Aug 26 05:19:38.237: INFO: stderr: ""
    Aug 26 05:19:38.237: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4359\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-rzqx6\n"
    Aug 26 05:19:38.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 describe service agnhost-primary'
    Aug 26 05:19:38.375: INFO: stderr: ""
    Aug 26 05:19:38.375: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4359\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.21.86.192\nIPs:               10.21.86.192\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.20.199.75:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Aug 26 05:19:38.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 describe node ip-10-0-1-101.us-west-2.compute.internal'
    Aug 26 05:19:38.598: INFO: stderr: ""
    Aug 26 05:19:38.598: INFO: stdout: "Name:               ip-10-0-1-101.us-west-2.compute.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-1-101.us-west-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=t3.xlarge\n                    topology.kubernetes.io/region=us-west-2\n                    topology.kubernetes.io/zone=us-west-2b\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.1.101/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.20.50.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 26 Aug 2023 04:58:45 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-1-101.us-west-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 26 Aug 2023 05:19:37 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 26 Aug 2023 04:59:12 +0000   Sat, 26 Aug 2023 04:59:12 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 26 Aug 2023 05:18:11 +0000   Sat, 26 Aug 2023 04:58:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 26 Aug 2023 05:18:11 +0000   Sat, 26 Aug 2023 04:58:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 26 Aug 2023 05:18:11 +0000   Sat, 26 Aug 2023 04:58:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 26 Aug 2023 05:18:11 +0000   Sat, 26 Aug 2023 04:59:16 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.0.1.101\n  ExternalIP:   54.202.229.59\n  Hostname:     ip-10-0-1-101.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-1-101.us-west-2.compute.internal\n  ExternalDNS:  ec2-54-202-229-59.us-west-2.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           50758604Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16197976Ki\n  pods:                        200\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           46779129369\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16095576Ki\n  pods:                        200\nSystem Info:\n  Machine ID:                 ec27b025ce9ca21bd3723ed1de6124be\n  System UUID:                ec27b025-ce9c-a21b-d372-3ed1de6124be\n  Boot ID:                    7b5a320c-f12e-485d-a01a-998bb89d959a\n  Kernel Version:             5.11.0-1028-aws\n  OS Image:                   Ubuntu 20.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.6\n  Kubelet Version:            v1.26.7\n  Kube-Proxy Version:         v1.26.7\nPodCIDR:                      10.20.6.0/24\nPodCIDRs:                     10.20.6.0/24\nProviderID:                   aws:///us-west-2b/i-04a91b7d3b13f8586\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-hgm7c                                          250m (6%)     1 (25%)     70Mi (0%)        1000Mi (6%)    20m\n  kube-system                 calico-typha-6dd9648c8f-85hjp                              10m (0%)      500m (12%)  70Mi (0%)        1Gi (6%)       15m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m19s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m16s\n  svcaccounts-6078            pod-service-account-defaultsa                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         25s\n  svcaccounts-6078            pod-service-account-mountsa-mountspec                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         25s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         260m (6%)   1500m (37%)\n  memory                      140Mi (0%)  2024Mi (12%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 22m                kube-proxy       \n  Warning  InvalidDiskCapacity      24m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeNotSchedulable       24m                kubelet          Node ip-10-0-1-101.us-west-2.compute.internal status is now: NodeNotSchedulable\n  Normal   NodeAllocatableEnforced  24m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  23m (x7 over 24m)  kubelet          Node ip-10-0-1-101.us-west-2.compute.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    23m (x7 over 24m)  kubelet          Node ip-10-0-1-101.us-west-2.compute.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     23m (x7 over 24m)  kubelet          Node ip-10-0-1-101.us-west-2.compute.internal status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           20m                node-controller  Node ip-10-0-1-101.us-west-2.compute.internal event: Registered Node ip-10-0-1-101.us-west-2.compute.internal in Controller\n"
    Aug 26 05:19:38.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4359 describe namespace kubectl-4359'
    Aug 26 05:19:38.705: INFO: stderr: ""
    Aug 26 05:19:38.705: INFO: stdout: "Name:         kubectl-4359\nLabels:       e2e-framework=kubectl\n              e2e-run=e53ca283-6d70-4e16-83be-49a5afe9d1bc\n              kubernetes.io/metadata.name=kubectl-4359\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:38.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4359" for this suite. 08/26/23 05:19:38.713
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:38.724
Aug 26 05:19:38.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:19:38.725
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:38.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:38.752
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-bf16cb3d-b1c7-4ec0-a919-e881c27c1818 08/26/23 05:19:38.755
STEP: Creating secret with name secret-projected-all-test-volume-23473bd6-61da-42a4-813c-eae24e27e6cd 08/26/23 05:19:38.764
STEP: Creating a pod to test Check all projections for projected volume plugin 08/26/23 05:19:38.773
Aug 26 05:19:38.795: INFO: Waiting up to 5m0s for pod "projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f" in namespace "projected-1920" to be "Succeeded or Failed"
Aug 26 05:19:38.802: INFO: Pod "projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.308316ms
Aug 26 05:19:40.840: INFO: Pod "projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f": Phase="Running", Reason="", readiness=false. Elapsed: 2.04540543s
Aug 26 05:19:42.807: INFO: Pod "projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012011434s
STEP: Saw pod success 08/26/23 05:19:42.807
Aug 26 05:19:42.807: INFO: Pod "projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f" satisfied condition "Succeeded or Failed"
Aug 26 05:19:42.812: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f container projected-all-volume-test: <nil>
STEP: delete the pod 08/26/23 05:19:42.832
Aug 26 05:19:42.853: INFO: Waiting for pod projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f to disappear
Aug 26 05:19:42.860: INFO: Pod projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:42.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1920" for this suite. 08/26/23 05:19:42.871
------------------------------
• [4.156 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:38.724
    Aug 26 05:19:38.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:19:38.725
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:38.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:38.752
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-bf16cb3d-b1c7-4ec0-a919-e881c27c1818 08/26/23 05:19:38.755
    STEP: Creating secret with name secret-projected-all-test-volume-23473bd6-61da-42a4-813c-eae24e27e6cd 08/26/23 05:19:38.764
    STEP: Creating a pod to test Check all projections for projected volume plugin 08/26/23 05:19:38.773
    Aug 26 05:19:38.795: INFO: Waiting up to 5m0s for pod "projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f" in namespace "projected-1920" to be "Succeeded or Failed"
    Aug 26 05:19:38.802: INFO: Pod "projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.308316ms
    Aug 26 05:19:40.840: INFO: Pod "projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f": Phase="Running", Reason="", readiness=false. Elapsed: 2.04540543s
    Aug 26 05:19:42.807: INFO: Pod "projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012011434s
    STEP: Saw pod success 08/26/23 05:19:42.807
    Aug 26 05:19:42.807: INFO: Pod "projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f" satisfied condition "Succeeded or Failed"
    Aug 26 05:19:42.812: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f container projected-all-volume-test: <nil>
    STEP: delete the pod 08/26/23 05:19:42.832
    Aug 26 05:19:42.853: INFO: Waiting for pod projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f to disappear
    Aug 26 05:19:42.860: INFO: Pod projected-volume-a2ecc76e-6d82-431b-99e1-a855c000c14f no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:42.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1920" for this suite. 08/26/23 05:19:42.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:42.881
Aug 26 05:19:42.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 05:19:42.882
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:42.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:42.903
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 08/26/23 05:19:42.907
STEP: Ensuring ResourceQuota status is calculated 08/26/23 05:19:42.914
STEP: Creating a ResourceQuota with not terminating scope 08/26/23 05:19:44.919
STEP: Ensuring ResourceQuota status is calculated 08/26/23 05:19:44.925
STEP: Creating a long running pod 08/26/23 05:19:46.929
STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/26/23 05:19:46.96
STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/26/23 05:19:48.965
STEP: Deleting the pod 08/26/23 05:19:50.97
STEP: Ensuring resource quota status released the pod usage 08/26/23 05:19:50.986
STEP: Creating a terminating pod 08/26/23 05:19:52.992
STEP: Ensuring resource quota with terminating scope captures the pod usage 08/26/23 05:19:53.007
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/26/23 05:19:55.012
STEP: Deleting the pod 08/26/23 05:19:57.017
STEP: Ensuring resource quota status released the pod usage 08/26/23 05:19:57.056
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 05:19:59.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5309" for this suite. 08/26/23 05:19:59.07
------------------------------
• [SLOW TEST] [16.199 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:42.881
    Aug 26 05:19:42.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 05:19:42.882
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:42.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:42.903
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 08/26/23 05:19:42.907
    STEP: Ensuring ResourceQuota status is calculated 08/26/23 05:19:42.914
    STEP: Creating a ResourceQuota with not terminating scope 08/26/23 05:19:44.919
    STEP: Ensuring ResourceQuota status is calculated 08/26/23 05:19:44.925
    STEP: Creating a long running pod 08/26/23 05:19:46.929
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/26/23 05:19:46.96
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/26/23 05:19:48.965
    STEP: Deleting the pod 08/26/23 05:19:50.97
    STEP: Ensuring resource quota status released the pod usage 08/26/23 05:19:50.986
    STEP: Creating a terminating pod 08/26/23 05:19:52.992
    STEP: Ensuring resource quota with terminating scope captures the pod usage 08/26/23 05:19:53.007
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/26/23 05:19:55.012
    STEP: Deleting the pod 08/26/23 05:19:57.017
    STEP: Ensuring resource quota status released the pod usage 08/26/23 05:19:57.056
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:19:59.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5309" for this suite. 08/26/23 05:19:59.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:19:59.081
Aug 26 05:19:59.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replication-controller 08/26/23 05:19:59.082
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:59.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:59.109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 08/26/23 05:19:59.114
STEP: When the matched label of one of its pods change 08/26/23 05:19:59.121
Aug 26 05:19:59.125: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 26 05:20:04.130: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 08/26/23 05:20:04.144
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 26 05:20:05.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6983" for this suite. 08/26/23 05:20:05.16
------------------------------
• [SLOW TEST] [6.085 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:19:59.081
    Aug 26 05:19:59.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replication-controller 08/26/23 05:19:59.082
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:19:59.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:19:59.109
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 08/26/23 05:19:59.114
    STEP: When the matched label of one of its pods change 08/26/23 05:19:59.121
    Aug 26 05:19:59.125: INFO: Pod name pod-release: Found 0 pods out of 1
    Aug 26 05:20:04.130: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/26/23 05:20:04.144
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:20:05.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6983" for this suite. 08/26/23 05:20:05.16
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:20:05.167
Aug 26 05:20:05.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:20:05.168
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:05.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:05.197
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-8ad241d5-c089-446a-b339-069040c35aa8 08/26/23 05:20:05.2
STEP: Creating a pod to test consume configMaps 08/26/23 05:20:05.207
Aug 26 05:20:05.218: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26" in namespace "projected-3708" to be "Succeeded or Failed"
Aug 26 05:20:05.224: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493715ms
Aug 26 05:20:07.230: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011718288s
Aug 26 05:20:09.231: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012634411s
Aug 26 05:20:11.230: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011950772s
Aug 26 05:20:13.232: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014342306s
STEP: Saw pod success 08/26/23 05:20:13.232
Aug 26 05:20:13.232: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26" satisfied condition "Succeeded or Failed"
Aug 26 05:20:13.241: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26 container agnhost-container: <nil>
STEP: delete the pod 08/26/23 05:20:13.25
Aug 26 05:20:13.266: INFO: Waiting for pod pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26 to disappear
Aug 26 05:20:13.270: INFO: Pod pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:20:13.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3708" for this suite. 08/26/23 05:20:13.276
------------------------------
• [SLOW TEST] [8.117 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:20:05.167
    Aug 26 05:20:05.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:20:05.168
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:05.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:05.197
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-8ad241d5-c089-446a-b339-069040c35aa8 08/26/23 05:20:05.2
    STEP: Creating a pod to test consume configMaps 08/26/23 05:20:05.207
    Aug 26 05:20:05.218: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26" in namespace "projected-3708" to be "Succeeded or Failed"
    Aug 26 05:20:05.224: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493715ms
    Aug 26 05:20:07.230: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011718288s
    Aug 26 05:20:09.231: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012634411s
    Aug 26 05:20:11.230: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011950772s
    Aug 26 05:20:13.232: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014342306s
    STEP: Saw pod success 08/26/23 05:20:13.232
    Aug 26 05:20:13.232: INFO: Pod "pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26" satisfied condition "Succeeded or Failed"
    Aug 26 05:20:13.241: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26 container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 05:20:13.25
    Aug 26 05:20:13.266: INFO: Waiting for pod pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26 to disappear
    Aug 26 05:20:13.270: INFO: Pod pod-projected-configmaps-38d71dcd-abe4-43c4-89b8-fa58b0b8df26 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:20:13.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3708" for this suite. 08/26/23 05:20:13.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:20:13.284
Aug 26 05:20:13.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 05:20:13.285
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:13.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:13.308
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 08/26/23 05:20:13.31
STEP: Creating a ResourceQuota 08/26/23 05:20:18.314
STEP: Ensuring resource quota status is calculated 08/26/23 05:20:18.321
STEP: Creating a Pod that fits quota 08/26/23 05:20:20.326
STEP: Ensuring ResourceQuota status captures the pod usage 08/26/23 05:20:20.349
STEP: Not allowing a pod to be created that exceeds remaining quota 08/26/23 05:20:22.354
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/26/23 05:20:22.357
STEP: Ensuring a pod cannot update its resource requirements 08/26/23 05:20:22.359
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/26/23 05:20:22.366
STEP: Deleting the pod 08/26/23 05:20:24.371
STEP: Ensuring resource quota status released the pod usage 08/26/23 05:20:24.383
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 05:20:26.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-195" for this suite. 08/26/23 05:20:26.398
------------------------------
• [SLOW TEST] [13.123 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:20:13.284
    Aug 26 05:20:13.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 05:20:13.285
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:13.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:13.308
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 08/26/23 05:20:13.31
    STEP: Creating a ResourceQuota 08/26/23 05:20:18.314
    STEP: Ensuring resource quota status is calculated 08/26/23 05:20:18.321
    STEP: Creating a Pod that fits quota 08/26/23 05:20:20.326
    STEP: Ensuring ResourceQuota status captures the pod usage 08/26/23 05:20:20.349
    STEP: Not allowing a pod to be created that exceeds remaining quota 08/26/23 05:20:22.354
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/26/23 05:20:22.357
    STEP: Ensuring a pod cannot update its resource requirements 08/26/23 05:20:22.359
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/26/23 05:20:22.366
    STEP: Deleting the pod 08/26/23 05:20:24.371
    STEP: Ensuring resource quota status released the pod usage 08/26/23 05:20:24.383
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:20:26.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-195" for this suite. 08/26/23 05:20:26.398
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:20:26.408
Aug 26 05:20:26.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 05:20:26.409
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:26.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:26.435
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 08/26/23 05:20:26.438
Aug 26 05:20:26.462: INFO: Waiting up to 5m0s for pod "pod-5ad71217-14fc-4bd8-af60-d2f10982c94b" in namespace "emptydir-9704" to be "Succeeded or Failed"
Aug 26 05:20:26.467: INFO: Pod "pod-5ad71217-14fc-4bd8-af60-d2f10982c94b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.170082ms
Aug 26 05:20:28.472: INFO: Pod "pod-5ad71217-14fc-4bd8-af60-d2f10982c94b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010291698s
Aug 26 05:20:30.473: INFO: Pod "pod-5ad71217-14fc-4bd8-af60-d2f10982c94b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010710576s
STEP: Saw pod success 08/26/23 05:20:30.473
Aug 26 05:20:30.473: INFO: Pod "pod-5ad71217-14fc-4bd8-af60-d2f10982c94b" satisfied condition "Succeeded or Failed"
Aug 26 05:20:30.477: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-5ad71217-14fc-4bd8-af60-d2f10982c94b container test-container: <nil>
STEP: delete the pod 08/26/23 05:20:30.486
Aug 26 05:20:30.501: INFO: Waiting for pod pod-5ad71217-14fc-4bd8-af60-d2f10982c94b to disappear
Aug 26 05:20:30.505: INFO: Pod pod-5ad71217-14fc-4bd8-af60-d2f10982c94b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 05:20:30.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9704" for this suite. 08/26/23 05:20:30.513
------------------------------
• [4.113 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:20:26.408
    Aug 26 05:20:26.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 05:20:26.409
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:26.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:26.435
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/26/23 05:20:26.438
    Aug 26 05:20:26.462: INFO: Waiting up to 5m0s for pod "pod-5ad71217-14fc-4bd8-af60-d2f10982c94b" in namespace "emptydir-9704" to be "Succeeded or Failed"
    Aug 26 05:20:26.467: INFO: Pod "pod-5ad71217-14fc-4bd8-af60-d2f10982c94b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.170082ms
    Aug 26 05:20:28.472: INFO: Pod "pod-5ad71217-14fc-4bd8-af60-d2f10982c94b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010291698s
    Aug 26 05:20:30.473: INFO: Pod "pod-5ad71217-14fc-4bd8-af60-d2f10982c94b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010710576s
    STEP: Saw pod success 08/26/23 05:20:30.473
    Aug 26 05:20:30.473: INFO: Pod "pod-5ad71217-14fc-4bd8-af60-d2f10982c94b" satisfied condition "Succeeded or Failed"
    Aug 26 05:20:30.477: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-5ad71217-14fc-4bd8-af60-d2f10982c94b container test-container: <nil>
    STEP: delete the pod 08/26/23 05:20:30.486
    Aug 26 05:20:30.501: INFO: Waiting for pod pod-5ad71217-14fc-4bd8-af60-d2f10982c94b to disappear
    Aug 26 05:20:30.505: INFO: Pod pod-5ad71217-14fc-4bd8-af60-d2f10982c94b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:20:30.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9704" for this suite. 08/26/23 05:20:30.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:20:30.523
Aug 26 05:20:30.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename conformance-tests 08/26/23 05:20:30.526
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:30.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:30.55
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 08/26/23 05:20:30.552
Aug 26 05:20:30.552: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Aug 26 05:20:30.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-3134" for this suite. 08/26/23 05:20:30.577
------------------------------
• [0.063 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:20:30.523
    Aug 26 05:20:30.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename conformance-tests 08/26/23 05:20:30.526
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:30.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:30.55
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 08/26/23 05:20:30.552
    Aug 26 05:20:30.552: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:20:30.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-3134" for this suite. 08/26/23 05:20:30.577
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:20:30.586
Aug 26 05:20:30.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:20:30.587
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:30.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:30.609
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-fec032d5-2e18-46ca-b1cb-1e5954eebd61 08/26/23 05:20:30.611
STEP: Creating a pod to test consume secrets 08/26/23 05:20:30.617
Aug 26 05:20:30.627: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5" in namespace "projected-942" to be "Succeeded or Failed"
Aug 26 05:20:30.632: INFO: Pod "pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.206484ms
Aug 26 05:20:32.637: INFO: Pod "pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009946071s
Aug 26 05:20:34.638: INFO: Pod "pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011337711s
STEP: Saw pod success 08/26/23 05:20:34.638
Aug 26 05:20:34.638: INFO: Pod "pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5" satisfied condition "Succeeded or Failed"
Aug 26 05:20:34.644: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5 container secret-volume-test: <nil>
STEP: delete the pod 08/26/23 05:20:34.652
Aug 26 05:20:34.667: INFO: Waiting for pod pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5 to disappear
Aug 26 05:20:34.671: INFO: Pod pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 26 05:20:34.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-942" for this suite. 08/26/23 05:20:34.678
------------------------------
• [4.099 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:20:30.586
    Aug 26 05:20:30.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:20:30.587
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:30.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:30.609
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-fec032d5-2e18-46ca-b1cb-1e5954eebd61 08/26/23 05:20:30.611
    STEP: Creating a pod to test consume secrets 08/26/23 05:20:30.617
    Aug 26 05:20:30.627: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5" in namespace "projected-942" to be "Succeeded or Failed"
    Aug 26 05:20:30.632: INFO: Pod "pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.206484ms
    Aug 26 05:20:32.637: INFO: Pod "pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009946071s
    Aug 26 05:20:34.638: INFO: Pod "pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011337711s
    STEP: Saw pod success 08/26/23 05:20:34.638
    Aug 26 05:20:34.638: INFO: Pod "pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5" satisfied condition "Succeeded or Failed"
    Aug 26 05:20:34.644: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5 container secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 05:20:34.652
    Aug 26 05:20:34.667: INFO: Waiting for pod pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5 to disappear
    Aug 26 05:20:34.671: INFO: Pod pod-projected-secrets-8b5f4fe1-3a71-403a-9df7-808e50e58de5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:20:34.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-942" for this suite. 08/26/23 05:20:34.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:20:34.685
Aug 26 05:20:34.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 05:20:34.686
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:34.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:34.71
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-0416b926-40b3-4717-9c22-95ce3adea26f 08/26/23 05:20:34.713
STEP: Creating a pod to test consume configMaps 08/26/23 05:20:34.723
Aug 26 05:20:34.736: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90" in namespace "configmap-8030" to be "Succeeded or Failed"
Aug 26 05:20:34.742: INFO: Pod "pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523757ms
Aug 26 05:20:36.747: INFO: Pod "pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010825947s
Aug 26 05:20:38.751: INFO: Pod "pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014761422s
STEP: Saw pod success 08/26/23 05:20:38.751
Aug 26 05:20:38.751: INFO: Pod "pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90" satisfied condition "Succeeded or Failed"
Aug 26 05:20:38.756: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90 container agnhost-container: <nil>
STEP: delete the pod 08/26/23 05:20:38.765
Aug 26 05:20:38.792: INFO: Waiting for pod pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90 to disappear
Aug 26 05:20:38.796: INFO: Pod pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:20:38.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8030" for this suite. 08/26/23 05:20:38.809
------------------------------
• [4.132 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:20:34.685
    Aug 26 05:20:34.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 05:20:34.686
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:34.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:34.71
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-0416b926-40b3-4717-9c22-95ce3adea26f 08/26/23 05:20:34.713
    STEP: Creating a pod to test consume configMaps 08/26/23 05:20:34.723
    Aug 26 05:20:34.736: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90" in namespace "configmap-8030" to be "Succeeded or Failed"
    Aug 26 05:20:34.742: INFO: Pod "pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523757ms
    Aug 26 05:20:36.747: INFO: Pod "pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010825947s
    Aug 26 05:20:38.751: INFO: Pod "pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014761422s
    STEP: Saw pod success 08/26/23 05:20:38.751
    Aug 26 05:20:38.751: INFO: Pod "pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90" satisfied condition "Succeeded or Failed"
    Aug 26 05:20:38.756: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90 container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 05:20:38.765
    Aug 26 05:20:38.792: INFO: Waiting for pod pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90 to disappear
    Aug 26 05:20:38.796: INFO: Pod pod-configmaps-cc291e13-3d52-4538-8ea5-fd1f2010ab90 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:20:38.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8030" for this suite. 08/26/23 05:20:38.809
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:20:38.818
Aug 26 05:20:38.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename svcaccounts 08/26/23 05:20:38.819
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:38.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:38.851
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Aug 26 05:20:38.878: INFO: Waiting up to 5m0s for pod "pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0" in namespace "svcaccounts-2147" to be "running"
Aug 26 05:20:38.881: INFO: Pod "pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.25348ms
Aug 26 05:20:40.890: INFO: Pod "pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.01256086s
Aug 26 05:20:40.890: INFO: Pod "pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0" satisfied condition "running"
STEP: reading a file in the container 08/26/23 05:20:40.89
Aug 26 05:20:40.891: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2147 pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 08/26/23 05:20:41.085
Aug 26 05:20:41.085: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2147 pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 08/26/23 05:20:41.26
Aug 26 05:20:41.260: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2147 pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 26 05:20:41.459: INFO: Got root ca configmap in namespace "svcaccounts-2147"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 26 05:20:41.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2147" for this suite. 08/26/23 05:20:41.476
------------------------------
• [2.665 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:20:38.818
    Aug 26 05:20:38.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename svcaccounts 08/26/23 05:20:38.819
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:38.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:38.851
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Aug 26 05:20:38.878: INFO: Waiting up to 5m0s for pod "pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0" in namespace "svcaccounts-2147" to be "running"
    Aug 26 05:20:38.881: INFO: Pod "pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.25348ms
    Aug 26 05:20:40.890: INFO: Pod "pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.01256086s
    Aug 26 05:20:40.890: INFO: Pod "pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0" satisfied condition "running"
    STEP: reading a file in the container 08/26/23 05:20:40.89
    Aug 26 05:20:40.891: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2147 pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 08/26/23 05:20:41.085
    Aug 26 05:20:41.085: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2147 pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 08/26/23 05:20:41.26
    Aug 26 05:20:41.260: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2147 pod-service-account-c551b210-3d68-46c9-9914-9b9aa56762d0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Aug 26 05:20:41.459: INFO: Got root ca configmap in namespace "svcaccounts-2147"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:20:41.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2147" for this suite. 08/26/23 05:20:41.476
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:20:41.483
Aug 26 05:20:41.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pod-network-test 08/26/23 05:20:41.484
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:41.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:41.507
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-3043 08/26/23 05:20:41.512
STEP: creating a selector 08/26/23 05:20:41.512
STEP: Creating the service pods in kubernetes 08/26/23 05:20:41.512
Aug 26 05:20:41.512: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 26 05:20:41.734: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3043" to be "running and ready"
Aug 26 05:20:41.742: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.874044ms
Aug 26 05:20:41.742: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:20:43.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012895698s
Aug 26 05:20:43.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:20:45.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013495653s
Aug 26 05:20:45.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:20:47.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012709667s
Aug 26 05:20:47.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:20:49.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013224326s
Aug 26 05:20:49.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:20:51.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01362269s
Aug 26 05:20:51.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:20:53.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013619772s
Aug 26 05:20:53.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:20:55.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013531017s
Aug 26 05:20:55.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:20:57.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013703803s
Aug 26 05:20:57.748: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:20:59.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013021176s
Aug 26 05:20:59.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:21:01.748: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014585536s
Aug 26 05:21:01.748: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:21:03.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013165071s
Aug 26 05:21:03.747: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 26 05:21:03.747: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 26 05:21:03.751: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3043" to be "running and ready"
Aug 26 05:21:03.755: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.778873ms
Aug 26 05:21:03.755: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 26 05:21:03.755: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 26 05:21:03.759: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3043" to be "running and ready"
Aug 26 05:21:03.764: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.770381ms
Aug 26 05:21:03.764: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 26 05:21:03.764: INFO: Pod "netserver-2" satisfied condition "running and ready"
Aug 26 05:21:03.769: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-3043" to be "running and ready"
Aug 26 05:21:03.775: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.993039ms
Aug 26 05:21:03.775: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Aug 26 05:21:03.775: INFO: Pod "netserver-3" satisfied condition "running and ready"
Aug 26 05:21:03.781: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-3043" to be "running and ready"
Aug 26 05:21:03.786: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.766249ms
Aug 26 05:21:03.786: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Aug 26 05:21:03.786: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 08/26/23 05:21:03.79
Aug 26 05:21:03.798: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3043" to be "running"
Aug 26 05:21:03.803: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.323541ms
Aug 26 05:21:05.808: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010678356s
Aug 26 05:21:05.808: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 26 05:21:05.813: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Aug 26 05:21:05.813: INFO: Breadth first check of 10.20.50.202 on host 10.0.1.101...
Aug 26 05:21:05.817: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.205:9080/dial?request=hostname&protocol=http&host=10.20.50.202&port=8083&tries=1'] Namespace:pod-network-test-3043 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:21:05.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:21:05.818: INFO: ExecWithOptions: Clientset creation
Aug 26 05:21:05.818: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-3043/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.205%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.50.202%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 26 05:21:05.907: INFO: Waiting for responses: map[]
Aug 26 05:21:05.907: INFO: reached 10.20.50.202 after 0/1 tries
Aug 26 05:21:05.907: INFO: Breadth first check of 10.20.193.198 on host 10.0.1.126...
Aug 26 05:21:05.911: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.205:9080/dial?request=hostname&protocol=http&host=10.20.193.198&port=8083&tries=1'] Namespace:pod-network-test-3043 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:21:05.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:21:05.912: INFO: ExecWithOptions: Clientset creation
Aug 26 05:21:05.912: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-3043/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.205%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.193.198%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 26 05:21:06.013: INFO: Waiting for responses: map[]
Aug 26 05:21:06.013: INFO: reached 10.20.193.198 after 0/1 tries
Aug 26 05:21:06.013: INFO: Breadth first check of 10.20.62.137 on host 10.0.1.23...
Aug 26 05:21:06.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.205:9080/dial?request=hostname&protocol=http&host=10.20.62.137&port=8083&tries=1'] Namespace:pod-network-test-3043 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:21:06.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:21:06.020: INFO: ExecWithOptions: Clientset creation
Aug 26 05:21:06.020: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-3043/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.205%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.62.137%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 26 05:21:06.111: INFO: Waiting for responses: map[]
Aug 26 05:21:06.112: INFO: reached 10.20.62.137 after 0/1 tries
Aug 26 05:21:06.112: INFO: Breadth first check of 10.20.8.204 on host 10.0.1.31...
Aug 26 05:21:06.117: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.205:9080/dial?request=hostname&protocol=http&host=10.20.8.204&port=8083&tries=1'] Namespace:pod-network-test-3043 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:21:06.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:21:06.118: INFO: ExecWithOptions: Clientset creation
Aug 26 05:21:06.118: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-3043/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.205%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.8.204%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 26 05:21:06.206: INFO: Waiting for responses: map[]
Aug 26 05:21:06.206: INFO: reached 10.20.8.204 after 0/1 tries
Aug 26 05:21:06.206: INFO: Breadth first check of 10.20.199.80 on host 10.0.1.5...
Aug 26 05:21:06.213: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.205:9080/dial?request=hostname&protocol=http&host=10.20.199.80&port=8083&tries=1'] Namespace:pod-network-test-3043 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:21:06.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:21:06.214: INFO: ExecWithOptions: Clientset creation
Aug 26 05:21:06.214: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-3043/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.205%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.199.80%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 26 05:21:06.303: INFO: Waiting for responses: map[]
Aug 26 05:21:06.303: INFO: reached 10.20.199.80 after 0/1 tries
Aug 26 05:21:06.303: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 26 05:21:06.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3043" for this suite. 08/26/23 05:21:06.312
------------------------------
• [SLOW TEST] [24.842 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:20:41.483
    Aug 26 05:20:41.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pod-network-test 08/26/23 05:20:41.484
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:20:41.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:20:41.507
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-3043 08/26/23 05:20:41.512
    STEP: creating a selector 08/26/23 05:20:41.512
    STEP: Creating the service pods in kubernetes 08/26/23 05:20:41.512
    Aug 26 05:20:41.512: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 26 05:20:41.734: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3043" to be "running and ready"
    Aug 26 05:20:41.742: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.874044ms
    Aug 26 05:20:41.742: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:20:43.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012895698s
    Aug 26 05:20:43.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:20:45.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013495653s
    Aug 26 05:20:45.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:20:47.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012709667s
    Aug 26 05:20:47.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:20:49.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013224326s
    Aug 26 05:20:49.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:20:51.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01362269s
    Aug 26 05:20:51.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:20:53.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013619772s
    Aug 26 05:20:53.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:20:55.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013531017s
    Aug 26 05:20:55.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:20:57.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013703803s
    Aug 26 05:20:57.748: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:20:59.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013021176s
    Aug 26 05:20:59.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:21:01.748: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014585536s
    Aug 26 05:21:01.748: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:21:03.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013165071s
    Aug 26 05:21:03.747: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 26 05:21:03.747: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 26 05:21:03.751: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3043" to be "running and ready"
    Aug 26 05:21:03.755: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.778873ms
    Aug 26 05:21:03.755: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 26 05:21:03.755: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 26 05:21:03.759: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3043" to be "running and ready"
    Aug 26 05:21:03.764: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.770381ms
    Aug 26 05:21:03.764: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 26 05:21:03.764: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Aug 26 05:21:03.769: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-3043" to be "running and ready"
    Aug 26 05:21:03.775: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.993039ms
    Aug 26 05:21:03.775: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Aug 26 05:21:03.775: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Aug 26 05:21:03.781: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-3043" to be "running and ready"
    Aug 26 05:21:03.786: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.766249ms
    Aug 26 05:21:03.786: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Aug 26 05:21:03.786: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 08/26/23 05:21:03.79
    Aug 26 05:21:03.798: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3043" to be "running"
    Aug 26 05:21:03.803: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.323541ms
    Aug 26 05:21:05.808: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010678356s
    Aug 26 05:21:05.808: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 26 05:21:05.813: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Aug 26 05:21:05.813: INFO: Breadth first check of 10.20.50.202 on host 10.0.1.101...
    Aug 26 05:21:05.817: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.205:9080/dial?request=hostname&protocol=http&host=10.20.50.202&port=8083&tries=1'] Namespace:pod-network-test-3043 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:21:05.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:21:05.818: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:21:05.818: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-3043/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.205%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.50.202%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 26 05:21:05.907: INFO: Waiting for responses: map[]
    Aug 26 05:21:05.907: INFO: reached 10.20.50.202 after 0/1 tries
    Aug 26 05:21:05.907: INFO: Breadth first check of 10.20.193.198 on host 10.0.1.126...
    Aug 26 05:21:05.911: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.205:9080/dial?request=hostname&protocol=http&host=10.20.193.198&port=8083&tries=1'] Namespace:pod-network-test-3043 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:21:05.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:21:05.912: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:21:05.912: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-3043/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.205%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.193.198%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 26 05:21:06.013: INFO: Waiting for responses: map[]
    Aug 26 05:21:06.013: INFO: reached 10.20.193.198 after 0/1 tries
    Aug 26 05:21:06.013: INFO: Breadth first check of 10.20.62.137 on host 10.0.1.23...
    Aug 26 05:21:06.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.205:9080/dial?request=hostname&protocol=http&host=10.20.62.137&port=8083&tries=1'] Namespace:pod-network-test-3043 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:21:06.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:21:06.020: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:21:06.020: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-3043/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.205%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.62.137%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 26 05:21:06.111: INFO: Waiting for responses: map[]
    Aug 26 05:21:06.112: INFO: reached 10.20.62.137 after 0/1 tries
    Aug 26 05:21:06.112: INFO: Breadth first check of 10.20.8.204 on host 10.0.1.31...
    Aug 26 05:21:06.117: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.205:9080/dial?request=hostname&protocol=http&host=10.20.8.204&port=8083&tries=1'] Namespace:pod-network-test-3043 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:21:06.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:21:06.118: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:21:06.118: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-3043/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.205%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.8.204%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 26 05:21:06.206: INFO: Waiting for responses: map[]
    Aug 26 05:21:06.206: INFO: reached 10.20.8.204 after 0/1 tries
    Aug 26 05:21:06.206: INFO: Breadth first check of 10.20.199.80 on host 10.0.1.5...
    Aug 26 05:21:06.213: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.205:9080/dial?request=hostname&protocol=http&host=10.20.199.80&port=8083&tries=1'] Namespace:pod-network-test-3043 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:21:06.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:21:06.214: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:21:06.214: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-3043/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.205%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.20.199.80%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 26 05:21:06.303: INFO: Waiting for responses: map[]
    Aug 26 05:21:06.303: INFO: reached 10.20.199.80 after 0/1 tries
    Aug 26 05:21:06.303: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:21:06.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3043" for this suite. 08/26/23 05:21:06.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:21:06.327
Aug 26 05:21:06.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename namespaces 08/26/23 05:21:06.328
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:06.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:06.355
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-8k98x" 08/26/23 05:21:06.358
Aug 26 05:21:06.385: INFO: Namespace "e2e-ns-8k98x-4350" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-8k98x-4350" 08/26/23 05:21:06.385
Aug 26 05:21:06.401: INFO: Namespace "e2e-ns-8k98x-4350" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-8k98x-4350" 08/26/23 05:21:06.401
Aug 26 05:21:06.422: INFO: Namespace "e2e-ns-8k98x-4350" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:21:06.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4257" for this suite. 08/26/23 05:21:06.438
STEP: Destroying namespace "e2e-ns-8k98x-4350" for this suite. 08/26/23 05:21:06.448
------------------------------
• [0.134 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:21:06.327
    Aug 26 05:21:06.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename namespaces 08/26/23 05:21:06.328
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:06.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:06.355
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-8k98x" 08/26/23 05:21:06.358
    Aug 26 05:21:06.385: INFO: Namespace "e2e-ns-8k98x-4350" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-8k98x-4350" 08/26/23 05:21:06.385
    Aug 26 05:21:06.401: INFO: Namespace "e2e-ns-8k98x-4350" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-8k98x-4350" 08/26/23 05:21:06.401
    Aug 26 05:21:06.422: INFO: Namespace "e2e-ns-8k98x-4350" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:21:06.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4257" for this suite. 08/26/23 05:21:06.438
    STEP: Destroying namespace "e2e-ns-8k98x-4350" for this suite. 08/26/23 05:21:06.448
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:21:06.462
Aug 26 05:21:06.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubelet-test 08/26/23 05:21:06.463
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:06.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:06.493
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:21:06.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4270" for this suite. 08/26/23 05:21:06.531
------------------------------
• [0.077 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:21:06.462
    Aug 26 05:21:06.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubelet-test 08/26/23 05:21:06.463
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:06.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:06.493
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:21:06.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4270" for this suite. 08/26/23 05:21:06.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:21:06.54
Aug 26 05:21:06.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 05:21:06.546
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:06.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:06.567
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 08/26/23 05:21:06.57
STEP: submitting the pod to kubernetes 08/26/23 05:21:06.57
Aug 26 05:21:06.586: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18" in namespace "pods-7793" to be "running and ready"
Aug 26 05:21:06.589: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Pending", Reason="", readiness=false. Elapsed: 3.642413ms
Aug 26 05:21:06.589: INFO: The phase of Pod pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:21:08.594: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Running", Reason="", readiness=true. Elapsed: 2.008766095s
Aug 26 05:21:08.594: INFO: The phase of Pod pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18 is Running (Ready = true)
Aug 26 05:21:08.594: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/26/23 05:21:08.598
STEP: updating the pod 08/26/23 05:21:08.602
Aug 26 05:21:09.117: INFO: Successfully updated pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18"
Aug 26 05:21:09.117: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18" in namespace "pods-7793" to be "terminated with reason DeadlineExceeded"
Aug 26 05:21:09.120: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Running", Reason="", readiness=true. Elapsed: 3.628031ms
Aug 26 05:21:11.125: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Running", Reason="", readiness=true. Elapsed: 2.008425489s
Aug 26 05:21:13.128: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Running", Reason="", readiness=false. Elapsed: 4.01084471s
Aug 26 05:21:15.130: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.012743254s
Aug 26 05:21:15.130: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 26 05:21:15.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7793" for this suite. 08/26/23 05:21:15.138
------------------------------
• [SLOW TEST] [8.608 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:21:06.54
    Aug 26 05:21:06.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 05:21:06.546
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:06.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:06.567
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 08/26/23 05:21:06.57
    STEP: submitting the pod to kubernetes 08/26/23 05:21:06.57
    Aug 26 05:21:06.586: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18" in namespace "pods-7793" to be "running and ready"
    Aug 26 05:21:06.589: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Pending", Reason="", readiness=false. Elapsed: 3.642413ms
    Aug 26 05:21:06.589: INFO: The phase of Pod pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:21:08.594: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Running", Reason="", readiness=true. Elapsed: 2.008766095s
    Aug 26 05:21:08.594: INFO: The phase of Pod pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18 is Running (Ready = true)
    Aug 26 05:21:08.594: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/26/23 05:21:08.598
    STEP: updating the pod 08/26/23 05:21:08.602
    Aug 26 05:21:09.117: INFO: Successfully updated pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18"
    Aug 26 05:21:09.117: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18" in namespace "pods-7793" to be "terminated with reason DeadlineExceeded"
    Aug 26 05:21:09.120: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Running", Reason="", readiness=true. Elapsed: 3.628031ms
    Aug 26 05:21:11.125: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Running", Reason="", readiness=true. Elapsed: 2.008425489s
    Aug 26 05:21:13.128: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Running", Reason="", readiness=false. Elapsed: 4.01084471s
    Aug 26 05:21:15.130: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.012743254s
    Aug 26 05:21:15.130: INFO: Pod "pod-update-activedeadlineseconds-74e36930-bc05-4e1b-9dee-f9eae2308d18" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:21:15.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7793" for this suite. 08/26/23 05:21:15.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:21:15.149
Aug 26 05:21:15.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 05:21:15.151
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:15.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:15.189
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/26/23 05:21:15.194
Aug 26 05:21:15.207: INFO: Waiting up to 5m0s for pod "pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e" in namespace "emptydir-6686" to be "Succeeded or Failed"
Aug 26 05:21:15.211: INFO: Pod "pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.218485ms
Aug 26 05:21:17.216: INFO: Pod "pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009696824s
Aug 26 05:21:19.217: INFO: Pod "pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010195038s
STEP: Saw pod success 08/26/23 05:21:19.217
Aug 26 05:21:19.217: INFO: Pod "pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e" satisfied condition "Succeeded or Failed"
Aug 26 05:21:19.221: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e container test-container: <nil>
STEP: delete the pod 08/26/23 05:21:19.23
Aug 26 05:21:19.246: INFO: Waiting for pod pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e to disappear
Aug 26 05:21:19.251: INFO: Pod pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 05:21:19.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6686" for this suite. 08/26/23 05:21:19.26
------------------------------
• [4.122 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:21:15.149
    Aug 26 05:21:15.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 05:21:15.151
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:15.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:15.189
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/26/23 05:21:15.194
    Aug 26 05:21:15.207: INFO: Waiting up to 5m0s for pod "pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e" in namespace "emptydir-6686" to be "Succeeded or Failed"
    Aug 26 05:21:15.211: INFO: Pod "pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.218485ms
    Aug 26 05:21:17.216: INFO: Pod "pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009696824s
    Aug 26 05:21:19.217: INFO: Pod "pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010195038s
    STEP: Saw pod success 08/26/23 05:21:19.217
    Aug 26 05:21:19.217: INFO: Pod "pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e" satisfied condition "Succeeded or Failed"
    Aug 26 05:21:19.221: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e container test-container: <nil>
    STEP: delete the pod 08/26/23 05:21:19.23
    Aug 26 05:21:19.246: INFO: Waiting for pod pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e to disappear
    Aug 26 05:21:19.251: INFO: Pod pod-187570fe-2b1b-45d8-aeac-bb6a0b998d2e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:21:19.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6686" for this suite. 08/26/23 05:21:19.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:21:19.273
Aug 26 05:21:19.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 05:21:19.274
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:19.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:19.298
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 08/26/23 05:21:19.304
Aug 26 05:21:19.304: INFO: namespace kubectl-1443
Aug 26 05:21:19.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1443 create -f -'
Aug 26 05:21:20.393: INFO: stderr: ""
Aug 26 05:21:20.393: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/26/23 05:21:20.393
Aug 26 05:21:21.401: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 26 05:21:21.401: INFO: Found 0 / 1
Aug 26 05:21:22.399: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 26 05:21:22.399: INFO: Found 1 / 1
Aug 26 05:21:22.399: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 26 05:21:22.404: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 26 05:21:22.404: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 26 05:21:22.404: INFO: wait on agnhost-primary startup in kubectl-1443 
Aug 26 05:21:22.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1443 logs agnhost-primary-lvftc agnhost-primary'
Aug 26 05:21:22.542: INFO: stderr: ""
Aug 26 05:21:22.542: INFO: stdout: "Paused\n"
STEP: exposing RC 08/26/23 05:21:22.542
Aug 26 05:21:22.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1443 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 26 05:21:22.677: INFO: stderr: ""
Aug 26 05:21:22.677: INFO: stdout: "service/rm2 exposed\n"
Aug 26 05:21:22.683: INFO: Service rm2 in namespace kubectl-1443 found.
STEP: exposing service 08/26/23 05:21:24.692
Aug 26 05:21:24.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1443 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 26 05:21:24.819: INFO: stderr: ""
Aug 26 05:21:24.819: INFO: stdout: "service/rm3 exposed\n"
Aug 26 05:21:24.826: INFO: Service rm3 in namespace kubectl-1443 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 05:21:26.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1443" for this suite. 08/26/23 05:21:26.844
------------------------------
• [SLOW TEST] [7.578 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:21:19.273
    Aug 26 05:21:19.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 05:21:19.274
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:19.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:19.298
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 08/26/23 05:21:19.304
    Aug 26 05:21:19.304: INFO: namespace kubectl-1443
    Aug 26 05:21:19.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1443 create -f -'
    Aug 26 05:21:20.393: INFO: stderr: ""
    Aug 26 05:21:20.393: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/26/23 05:21:20.393
    Aug 26 05:21:21.401: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 26 05:21:21.401: INFO: Found 0 / 1
    Aug 26 05:21:22.399: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 26 05:21:22.399: INFO: Found 1 / 1
    Aug 26 05:21:22.399: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 26 05:21:22.404: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 26 05:21:22.404: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 26 05:21:22.404: INFO: wait on agnhost-primary startup in kubectl-1443 
    Aug 26 05:21:22.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1443 logs agnhost-primary-lvftc agnhost-primary'
    Aug 26 05:21:22.542: INFO: stderr: ""
    Aug 26 05:21:22.542: INFO: stdout: "Paused\n"
    STEP: exposing RC 08/26/23 05:21:22.542
    Aug 26 05:21:22.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1443 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Aug 26 05:21:22.677: INFO: stderr: ""
    Aug 26 05:21:22.677: INFO: stdout: "service/rm2 exposed\n"
    Aug 26 05:21:22.683: INFO: Service rm2 in namespace kubectl-1443 found.
    STEP: exposing service 08/26/23 05:21:24.692
    Aug 26 05:21:24.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1443 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Aug 26 05:21:24.819: INFO: stderr: ""
    Aug 26 05:21:24.819: INFO: stdout: "service/rm3 exposed\n"
    Aug 26 05:21:24.826: INFO: Service rm3 in namespace kubectl-1443 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:21:26.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1443" for this suite. 08/26/23 05:21:26.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:21:26.852
Aug 26 05:21:26.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename job 08/26/23 05:21:26.853
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:26.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:26.896
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 08/26/23 05:21:26.906
STEP: Patching the Job 08/26/23 05:21:26.916
STEP: Watching for Job to be patched 08/26/23 05:21:26.943
Aug 26 05:21:26.944: INFO: Event ADDED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 26 05:21:26.944: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 26 05:21:26.945: INFO: Event MODIFIED found for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 08/26/23 05:21:26.945
STEP: Watching for Job to be updated 08/26/23 05:21:26.957
Aug 26 05:21:26.959: INFO: Event MODIFIED found for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 26 05:21:26.959: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 08/26/23 05:21:26.959
Aug 26 05:21:26.966: INFO: Job: e2e-f4c6r as labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r]
STEP: Waiting for job to complete 08/26/23 05:21:26.966
STEP: Delete a job collection with a labelselector 08/26/23 05:21:36.971
STEP: Watching for Job to be deleted 08/26/23 05:21:36.981
Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 26 05:21:36.983: INFO: Event DELETED found for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 08/26/23 05:21:36.983
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 26 05:21:36.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-675" for this suite. 08/26/23 05:21:37
------------------------------
• [SLOW TEST] [10.162 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:21:26.852
    Aug 26 05:21:26.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename job 08/26/23 05:21:26.853
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:26.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:26.896
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 08/26/23 05:21:26.906
    STEP: Patching the Job 08/26/23 05:21:26.916
    STEP: Watching for Job to be patched 08/26/23 05:21:26.943
    Aug 26 05:21:26.944: INFO: Event ADDED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 26 05:21:26.944: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 26 05:21:26.945: INFO: Event MODIFIED found for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 08/26/23 05:21:26.945
    STEP: Watching for Job to be updated 08/26/23 05:21:26.957
    Aug 26 05:21:26.959: INFO: Event MODIFIED found for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 26 05:21:26.959: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 08/26/23 05:21:26.959
    Aug 26 05:21:26.966: INFO: Job: e2e-f4c6r as labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r]
    STEP: Waiting for job to complete 08/26/23 05:21:26.966
    STEP: Delete a job collection with a labelselector 08/26/23 05:21:36.971
    STEP: Watching for Job to be deleted 08/26/23 05:21:36.981
    Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 26 05:21:36.983: INFO: Event MODIFIED observed for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 26 05:21:36.983: INFO: Event DELETED found for Job e2e-f4c6r in namespace job-675 with labels: map[e2e-f4c6r:patched e2e-job-label:e2e-f4c6r] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 08/26/23 05:21:36.983
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:21:36.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-675" for this suite. 08/26/23 05:21:37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:21:37.015
Aug 26 05:21:37.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename aggregator 08/26/23 05:21:37.016
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:37.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:37.108
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Aug 26 05:21:37.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 08/26/23 05:21:37.111
Aug 26 05:21:37.749: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Aug 26 05:21:39.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:21:41.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:21:43.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:21:45.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:21:47.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:21:49.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:21:51.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:21:53.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:21:55.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:21:57.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:21:59.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:22:01.982: INFO: Waited 140.642425ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 08/26/23 05:22:02.059
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/26/23 05:22:02.066
STEP: List APIServices 08/26/23 05:22:02.074
Aug 26 05:22:02.083: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Aug 26 05:22:02.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-5206" for this suite. 08/26/23 05:22:02.291
------------------------------
• [SLOW TEST] [25.307 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:21:37.015
    Aug 26 05:21:37.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename aggregator 08/26/23 05:21:37.016
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:21:37.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:21:37.108
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Aug 26 05:21:37.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 08/26/23 05:21:37.111
    Aug 26 05:21:37.749: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Aug 26 05:21:39.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:21:41.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:21:43.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:21:45.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:21:47.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:21:49.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:21:51.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:21:53.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:21:55.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:21:57.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:21:59.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:22:01.982: INFO: Waited 140.642425ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 08/26/23 05:22:02.059
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/26/23 05:22:02.066
    STEP: List APIServices 08/26/23 05:22:02.074
    Aug 26 05:22:02.083: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:22:02.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-5206" for this suite. 08/26/23 05:22:02.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:22:02.322
Aug 26 05:22:02.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pod-network-test 08/26/23 05:22:02.323
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:22:02.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:22:02.354
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2129 08/26/23 05:22:02.357
STEP: creating a selector 08/26/23 05:22:02.357
STEP: Creating the service pods in kubernetes 08/26/23 05:22:02.357
Aug 26 05:22:02.357: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 26 05:22:02.434: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2129" to be "running and ready"
Aug 26 05:22:02.441: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.007356ms
Aug 26 05:22:02.442: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:22:04.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012388179s
Aug 26 05:22:04.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:22:06.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012089058s
Aug 26 05:22:06.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:22:08.452: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017609165s
Aug 26 05:22:08.452: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:22:10.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012595768s
Aug 26 05:22:10.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:22:12.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01314798s
Aug 26 05:22:12.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:22:14.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.012206022s
Aug 26 05:22:14.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:22:16.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011969903s
Aug 26 05:22:16.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:22:18.449: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014278244s
Aug 26 05:22:18.449: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:22:20.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012624501s
Aug 26 05:22:20.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:22:22.449: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01434328s
Aug 26 05:22:22.449: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:22:24.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015243517s
Aug 26 05:22:24.450: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 26 05:22:24.450: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 26 05:22:24.454: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2129" to be "running and ready"
Aug 26 05:22:24.457: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.491315ms
Aug 26 05:22:24.457: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 26 05:22:24.457: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 26 05:22:24.463: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2129" to be "running and ready"
Aug 26 05:22:24.466: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.483019ms
Aug 26 05:22:24.466: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 26 05:22:24.466: INFO: Pod "netserver-2" satisfied condition "running and ready"
Aug 26 05:22:24.470: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2129" to be "running and ready"
Aug 26 05:22:24.473: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.193563ms
Aug 26 05:22:24.473: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Aug 26 05:22:24.473: INFO: Pod "netserver-3" satisfied condition "running and ready"
Aug 26 05:22:24.477: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2129" to be "running and ready"
Aug 26 05:22:24.480: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.656766ms
Aug 26 05:22:24.480: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Aug 26 05:22:24.480: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 08/26/23 05:22:24.484
Aug 26 05:22:24.490: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2129" to be "running"
Aug 26 05:22:24.493: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.168265ms
Aug 26 05:22:26.498: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008405272s
Aug 26 05:22:26.498: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 26 05:22:26.505: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Aug 26 05:22:26.505: INFO: Breadth first check of 10.20.50.205 on host 10.0.1.101...
Aug 26 05:22:26.508: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.211:9080/dial?request=hostname&protocol=udp&host=10.20.50.205&port=8081&tries=1'] Namespace:pod-network-test-2129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:22:26.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:22:26.509: INFO: ExecWithOptions: Clientset creation
Aug 26 05:22:26.509: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.211%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.50.205%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 26 05:22:26.601: INFO: Waiting for responses: map[]
Aug 26 05:22:26.601: INFO: reached 10.20.50.205 after 0/1 tries
Aug 26 05:22:26.601: INFO: Breadth first check of 10.20.193.199 on host 10.0.1.126...
Aug 26 05:22:26.605: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.211:9080/dial?request=hostname&protocol=udp&host=10.20.193.199&port=8081&tries=1'] Namespace:pod-network-test-2129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:22:26.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:22:26.606: INFO: ExecWithOptions: Clientset creation
Aug 26 05:22:26.606: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.211%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.193.199%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 26 05:22:26.694: INFO: Waiting for responses: map[]
Aug 26 05:22:26.694: INFO: reached 10.20.193.199 after 0/1 tries
Aug 26 05:22:26.694: INFO: Breadth first check of 10.20.62.138 on host 10.0.1.23...
Aug 26 05:22:26.699: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.211:9080/dial?request=hostname&protocol=udp&host=10.20.62.138&port=8081&tries=1'] Namespace:pod-network-test-2129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:22:26.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:22:26.700: INFO: ExecWithOptions: Clientset creation
Aug 26 05:22:26.700: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.211%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.62.138%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 26 05:22:26.792: INFO: Waiting for responses: map[]
Aug 26 05:22:26.792: INFO: reached 10.20.62.138 after 0/1 tries
Aug 26 05:22:26.792: INFO: Breadth first check of 10.20.8.210 on host 10.0.1.31...
Aug 26 05:22:26.796: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.211:9080/dial?request=hostname&protocol=udp&host=10.20.8.210&port=8081&tries=1'] Namespace:pod-network-test-2129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:22:26.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:22:26.797: INFO: ExecWithOptions: Clientset creation
Aug 26 05:22:26.797: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.211%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.8.210%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 26 05:22:26.894: INFO: Waiting for responses: map[]
Aug 26 05:22:26.894: INFO: reached 10.20.8.210 after 0/1 tries
Aug 26 05:22:26.894: INFO: Breadth first check of 10.20.199.83 on host 10.0.1.5...
Aug 26 05:22:26.899: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.211:9080/dial?request=hostname&protocol=udp&host=10.20.199.83&port=8081&tries=1'] Namespace:pod-network-test-2129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:22:26.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:22:26.900: INFO: ExecWithOptions: Clientset creation
Aug 26 05:22:26.900: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.211%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.199.83%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 26 05:22:26.984: INFO: Waiting for responses: map[]
Aug 26 05:22:26.984: INFO: reached 10.20.199.83 after 0/1 tries
Aug 26 05:22:26.984: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 26 05:22:26.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2129" for this suite. 08/26/23 05:22:26.999
------------------------------
• [SLOW TEST] [24.687 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:22:02.322
    Aug 26 05:22:02.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pod-network-test 08/26/23 05:22:02.323
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:22:02.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:22:02.354
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2129 08/26/23 05:22:02.357
    STEP: creating a selector 08/26/23 05:22:02.357
    STEP: Creating the service pods in kubernetes 08/26/23 05:22:02.357
    Aug 26 05:22:02.357: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 26 05:22:02.434: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2129" to be "running and ready"
    Aug 26 05:22:02.441: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.007356ms
    Aug 26 05:22:02.442: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:22:04.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012388179s
    Aug 26 05:22:04.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:22:06.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012089058s
    Aug 26 05:22:06.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:22:08.452: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017609165s
    Aug 26 05:22:08.452: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:22:10.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012595768s
    Aug 26 05:22:10.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:22:12.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01314798s
    Aug 26 05:22:12.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:22:14.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.012206022s
    Aug 26 05:22:14.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:22:16.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011969903s
    Aug 26 05:22:16.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:22:18.449: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014278244s
    Aug 26 05:22:18.449: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:22:20.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012624501s
    Aug 26 05:22:20.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:22:22.449: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01434328s
    Aug 26 05:22:22.449: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:22:24.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015243517s
    Aug 26 05:22:24.450: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 26 05:22:24.450: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 26 05:22:24.454: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2129" to be "running and ready"
    Aug 26 05:22:24.457: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.491315ms
    Aug 26 05:22:24.457: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 26 05:22:24.457: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 26 05:22:24.463: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2129" to be "running and ready"
    Aug 26 05:22:24.466: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.483019ms
    Aug 26 05:22:24.466: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 26 05:22:24.466: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Aug 26 05:22:24.470: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2129" to be "running and ready"
    Aug 26 05:22:24.473: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.193563ms
    Aug 26 05:22:24.473: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Aug 26 05:22:24.473: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Aug 26 05:22:24.477: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2129" to be "running and ready"
    Aug 26 05:22:24.480: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.656766ms
    Aug 26 05:22:24.480: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Aug 26 05:22:24.480: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 08/26/23 05:22:24.484
    Aug 26 05:22:24.490: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2129" to be "running"
    Aug 26 05:22:24.493: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.168265ms
    Aug 26 05:22:26.498: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008405272s
    Aug 26 05:22:26.498: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 26 05:22:26.505: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Aug 26 05:22:26.505: INFO: Breadth first check of 10.20.50.205 on host 10.0.1.101...
    Aug 26 05:22:26.508: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.211:9080/dial?request=hostname&protocol=udp&host=10.20.50.205&port=8081&tries=1'] Namespace:pod-network-test-2129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:22:26.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:22:26.509: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:22:26.509: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.211%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.50.205%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 26 05:22:26.601: INFO: Waiting for responses: map[]
    Aug 26 05:22:26.601: INFO: reached 10.20.50.205 after 0/1 tries
    Aug 26 05:22:26.601: INFO: Breadth first check of 10.20.193.199 on host 10.0.1.126...
    Aug 26 05:22:26.605: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.211:9080/dial?request=hostname&protocol=udp&host=10.20.193.199&port=8081&tries=1'] Namespace:pod-network-test-2129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:22:26.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:22:26.606: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:22:26.606: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.211%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.193.199%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 26 05:22:26.694: INFO: Waiting for responses: map[]
    Aug 26 05:22:26.694: INFO: reached 10.20.193.199 after 0/1 tries
    Aug 26 05:22:26.694: INFO: Breadth first check of 10.20.62.138 on host 10.0.1.23...
    Aug 26 05:22:26.699: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.211:9080/dial?request=hostname&protocol=udp&host=10.20.62.138&port=8081&tries=1'] Namespace:pod-network-test-2129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:22:26.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:22:26.700: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:22:26.700: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.211%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.62.138%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 26 05:22:26.792: INFO: Waiting for responses: map[]
    Aug 26 05:22:26.792: INFO: reached 10.20.62.138 after 0/1 tries
    Aug 26 05:22:26.792: INFO: Breadth first check of 10.20.8.210 on host 10.0.1.31...
    Aug 26 05:22:26.796: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.211:9080/dial?request=hostname&protocol=udp&host=10.20.8.210&port=8081&tries=1'] Namespace:pod-network-test-2129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:22:26.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:22:26.797: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:22:26.797: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.211%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.8.210%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 26 05:22:26.894: INFO: Waiting for responses: map[]
    Aug 26 05:22:26.894: INFO: reached 10.20.8.210 after 0/1 tries
    Aug 26 05:22:26.894: INFO: Breadth first check of 10.20.199.83 on host 10.0.1.5...
    Aug 26 05:22:26.899: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.8.211:9080/dial?request=hostname&protocol=udp&host=10.20.199.83&port=8081&tries=1'] Namespace:pod-network-test-2129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:22:26.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:22:26.900: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:22:26.900: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.20.8.211%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.20.199.83%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 26 05:22:26.984: INFO: Waiting for responses: map[]
    Aug 26 05:22:26.984: INFO: reached 10.20.199.83 after 0/1 tries
    Aug 26 05:22:26.984: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:22:26.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2129" for this suite. 08/26/23 05:22:26.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:22:27.009
Aug 26 05:22:27.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:22:27.011
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:22:27.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:22:27.032
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-643666e4-de9e-4ae7-be91-85c597c040f2 08/26/23 05:22:27.035
STEP: Creating a pod to test consume configMaps 08/26/23 05:22:27.042
Aug 26 05:22:27.052: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba" in namespace "projected-533" to be "Succeeded or Failed"
Aug 26 05:22:27.068: INFO: Pod "pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba": Phase="Pending", Reason="", readiness=false. Elapsed: 15.36657ms
Aug 26 05:22:29.074: INFO: Pod "pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021120836s
Aug 26 05:22:31.074: INFO: Pod "pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021476084s
STEP: Saw pod success 08/26/23 05:22:31.074
Aug 26 05:22:31.074: INFO: Pod "pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba" satisfied condition "Succeeded or Failed"
Aug 26 05:22:31.078: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba container agnhost-container: <nil>
STEP: delete the pod 08/26/23 05:22:31.099
Aug 26 05:22:31.117: INFO: Waiting for pod pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba to disappear
Aug 26 05:22:31.121: INFO: Pod pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:22:31.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-533" for this suite. 08/26/23 05:22:31.134
------------------------------
• [4.132 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:22:27.009
    Aug 26 05:22:27.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:22:27.011
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:22:27.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:22:27.032
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-643666e4-de9e-4ae7-be91-85c597c040f2 08/26/23 05:22:27.035
    STEP: Creating a pod to test consume configMaps 08/26/23 05:22:27.042
    Aug 26 05:22:27.052: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba" in namespace "projected-533" to be "Succeeded or Failed"
    Aug 26 05:22:27.068: INFO: Pod "pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba": Phase="Pending", Reason="", readiness=false. Elapsed: 15.36657ms
    Aug 26 05:22:29.074: INFO: Pod "pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021120836s
    Aug 26 05:22:31.074: INFO: Pod "pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021476084s
    STEP: Saw pod success 08/26/23 05:22:31.074
    Aug 26 05:22:31.074: INFO: Pod "pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba" satisfied condition "Succeeded or Failed"
    Aug 26 05:22:31.078: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 05:22:31.099
    Aug 26 05:22:31.117: INFO: Waiting for pod pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba to disappear
    Aug 26 05:22:31.121: INFO: Pod pod-projected-configmaps-67eea85b-7688-4046-b850-e869c56d81ba no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:22:31.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-533" for this suite. 08/26/23 05:22:31.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:22:31.142
Aug 26 05:22:31.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:22:31.144
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:22:31.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:22:31.173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-9ba74f05-ca2c-4861-b9a8-5199aa9d4a06 08/26/23 05:22:31.176
STEP: Creating a pod to test consume configMaps 08/26/23 05:22:31.184
Aug 26 05:22:31.193: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a" in namespace "projected-5704" to be "Succeeded or Failed"
Aug 26 05:22:31.198: INFO: Pod "pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.586385ms
Aug 26 05:22:33.282: INFO: Pod "pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089176554s
Aug 26 05:22:35.202: INFO: Pod "pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009098668s
STEP: Saw pod success 08/26/23 05:22:35.202
Aug 26 05:22:35.202: INFO: Pod "pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a" satisfied condition "Succeeded or Failed"
Aug 26 05:22:35.206: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a container agnhost-container: <nil>
STEP: delete the pod 08/26/23 05:22:35.214
Aug 26 05:22:35.230: INFO: Waiting for pod pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a to disappear
Aug 26 05:22:35.233: INFO: Pod pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:22:35.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5704" for this suite. 08/26/23 05:22:35.241
------------------------------
• [4.106 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:22:31.142
    Aug 26 05:22:31.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:22:31.144
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:22:31.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:22:31.173
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-9ba74f05-ca2c-4861-b9a8-5199aa9d4a06 08/26/23 05:22:31.176
    STEP: Creating a pod to test consume configMaps 08/26/23 05:22:31.184
    Aug 26 05:22:31.193: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a" in namespace "projected-5704" to be "Succeeded or Failed"
    Aug 26 05:22:31.198: INFO: Pod "pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.586385ms
    Aug 26 05:22:33.282: INFO: Pod "pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089176554s
    Aug 26 05:22:35.202: INFO: Pod "pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009098668s
    STEP: Saw pod success 08/26/23 05:22:35.202
    Aug 26 05:22:35.202: INFO: Pod "pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a" satisfied condition "Succeeded or Failed"
    Aug 26 05:22:35.206: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 05:22:35.214
    Aug 26 05:22:35.230: INFO: Waiting for pod pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a to disappear
    Aug 26 05:22:35.233: INFO: Pod pod-projected-configmaps-7292c5b8-469b-4d2f-bec1-fb52660b614a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:22:35.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5704" for this suite. 08/26/23 05:22:35.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:22:35.249
Aug 26 05:22:35.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename daemonsets 08/26/23 05:22:35.25
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:22:35.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:22:35.273
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Aug 26 05:22:35.316: INFO: Create a RollingUpdate DaemonSet
Aug 26 05:22:35.323: INFO: Check that daemon pods launch on every node of the cluster
Aug 26 05:22:35.330: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:35.330: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:35.330: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:35.334: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 05:22:35.334: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 05:22:36.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:36.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:36.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:36.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 05:22:36.349: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 05:22:37.345: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:37.346: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:37.346: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:37.352: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 26 05:22:37.353: INFO: Node ip-10-0-1-126.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 05:22:38.345: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:38.345: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:38.345: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:38.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 26 05:22:38.355: INFO: Node ip-10-0-1-126.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 05:22:39.343: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:39.343: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:39.343: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:39.348: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 26 05:22:39.348: INFO: Node ip-10-0-1-126.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 05:22:40.343: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:40.344: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:40.344: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:40.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 26 05:22:40.353: INFO: Node ip-10-0-1-126.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 05:22:41.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:41.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:41.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:41.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 26 05:22:41.347: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
Aug 26 05:22:41.347: INFO: Update the DaemonSet to trigger a rollout
Aug 26 05:22:41.363: INFO: Updating DaemonSet daemon-set
Aug 26 05:22:43.396: INFO: Roll back the DaemonSet before rollout is complete
Aug 26 05:22:43.411: INFO: Updating DaemonSet daemon-set
Aug 26 05:22:43.411: INFO: Make sure DaemonSet rollback is complete
Aug 26 05:22:43.424: INFO: Wrong image for pod: daemon-set-bw8sx. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Aug 26 05:22:43.424: INFO: Pod daemon-set-bw8sx is not available
Aug 26 05:22:43.436: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:43.437: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:43.437: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:44.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:44.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:44.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:45.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:45.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:45.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:46.443: INFO: Pod daemon-set-x26kj is not available
Aug 26 05:22:46.450: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:46.450: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:22:46.450: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/26/23 05:22:46.46
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3380, will wait for the garbage collector to delete the pods 08/26/23 05:22:46.46
Aug 26 05:22:46.525: INFO: Deleting DaemonSet.extensions daemon-set took: 9.186304ms
Aug 26 05:22:46.625: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.686498ms
Aug 26 05:22:48.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 05:22:48.730: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 26 05:22:48.737: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8209"},"items":null}

Aug 26 05:22:48.742: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8209"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:22:48.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3380" for this suite. 08/26/23 05:22:48.801
------------------------------
• [SLOW TEST] [13.563 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:22:35.249
    Aug 26 05:22:35.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename daemonsets 08/26/23 05:22:35.25
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:22:35.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:22:35.273
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Aug 26 05:22:35.316: INFO: Create a RollingUpdate DaemonSet
    Aug 26 05:22:35.323: INFO: Check that daemon pods launch on every node of the cluster
    Aug 26 05:22:35.330: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:35.330: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:35.330: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:35.334: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 05:22:35.334: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 05:22:36.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:36.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:36.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:36.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 05:22:36.349: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 05:22:37.345: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:37.346: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:37.346: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:37.352: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 26 05:22:37.353: INFO: Node ip-10-0-1-126.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 05:22:38.345: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:38.345: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:38.345: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:38.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 26 05:22:38.355: INFO: Node ip-10-0-1-126.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 05:22:39.343: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:39.343: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:39.343: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:39.348: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 26 05:22:39.348: INFO: Node ip-10-0-1-126.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 05:22:40.343: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:40.344: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:40.344: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:40.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 26 05:22:40.353: INFO: Node ip-10-0-1-126.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 05:22:41.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:41.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:41.342: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:41.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 26 05:22:41.347: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    Aug 26 05:22:41.347: INFO: Update the DaemonSet to trigger a rollout
    Aug 26 05:22:41.363: INFO: Updating DaemonSet daemon-set
    Aug 26 05:22:43.396: INFO: Roll back the DaemonSet before rollout is complete
    Aug 26 05:22:43.411: INFO: Updating DaemonSet daemon-set
    Aug 26 05:22:43.411: INFO: Make sure DaemonSet rollback is complete
    Aug 26 05:22:43.424: INFO: Wrong image for pod: daemon-set-bw8sx. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Aug 26 05:22:43.424: INFO: Pod daemon-set-bw8sx is not available
    Aug 26 05:22:43.436: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:43.437: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:43.437: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:44.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:44.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:44.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:45.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:45.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:45.452: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:46.443: INFO: Pod daemon-set-x26kj is not available
    Aug 26 05:22:46.450: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:46.450: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:22:46.450: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/26/23 05:22:46.46
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3380, will wait for the garbage collector to delete the pods 08/26/23 05:22:46.46
    Aug 26 05:22:46.525: INFO: Deleting DaemonSet.extensions daemon-set took: 9.186304ms
    Aug 26 05:22:46.625: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.686498ms
    Aug 26 05:22:48.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 05:22:48.730: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 26 05:22:48.737: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8209"},"items":null}

    Aug 26 05:22:48.742: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8209"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:22:48.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3380" for this suite. 08/26/23 05:22:48.801
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:22:48.815
Aug 26 05:22:48.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename statefulset 08/26/23 05:22:48.821
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:22:48.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:22:48.853
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6379 08/26/23 05:22:48.86
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 08/26/23 05:22:48.872
Aug 26 05:22:48.888: INFO: Found 0 stateful pods, waiting for 3
Aug 26 05:22:58.894: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 05:22:58.894: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 05:22:58.894: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 05:22:58.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-6379 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 26 05:22:59.097: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 26 05:22:59.097: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 26 05:22:59.097: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/26/23 05:23:09.122
Aug 26 05:23:09.143: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/26/23 05:23:09.143
STEP: Updating Pods in reverse ordinal order 08/26/23 05:23:19.171
Aug 26 05:23:19.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-6379 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 26 05:23:19.364: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 26 05:23:19.364: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 26 05:23:19.364: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 08/26/23 05:23:39.397
Aug 26 05:23:39.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-6379 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 26 05:23:39.630: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 26 05:23:39.630: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 26 05:23:39.630: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 26 05:23:49.684: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 08/26/23 05:23:59.71
Aug 26 05:23:59.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-6379 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 26 05:23:59.895: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 26 05:23:59.895: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 26 05:23:59.895: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 26 05:24:09.927: INFO: Deleting all statefulset in ns statefulset-6379
Aug 26 05:24:09.933: INFO: Scaling statefulset ss2 to 0
Aug 26 05:24:19.954: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 05:24:19.957: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:19.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6379" for this suite. 08/26/23 05:24:19.988
------------------------------
• [SLOW TEST] [91.181 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:22:48.815
    Aug 26 05:22:48.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename statefulset 08/26/23 05:22:48.821
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:22:48.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:22:48.853
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6379 08/26/23 05:22:48.86
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 08/26/23 05:22:48.872
    Aug 26 05:22:48.888: INFO: Found 0 stateful pods, waiting for 3
    Aug 26 05:22:58.894: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 05:22:58.894: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 05:22:58.894: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 05:22:58.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-6379 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 26 05:22:59.097: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 26 05:22:59.097: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 26 05:22:59.097: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/26/23 05:23:09.122
    Aug 26 05:23:09.143: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/26/23 05:23:09.143
    STEP: Updating Pods in reverse ordinal order 08/26/23 05:23:19.171
    Aug 26 05:23:19.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-6379 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 26 05:23:19.364: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 26 05:23:19.364: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 26 05:23:19.364: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 08/26/23 05:23:39.397
    Aug 26 05:23:39.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-6379 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 26 05:23:39.630: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 26 05:23:39.630: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 26 05:23:39.630: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 26 05:23:49.684: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 08/26/23 05:23:59.71
    Aug 26 05:23:59.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-6379 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 26 05:23:59.895: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 26 05:23:59.895: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 26 05:23:59.895: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 26 05:24:09.927: INFO: Deleting all statefulset in ns statefulset-6379
    Aug 26 05:24:09.933: INFO: Scaling statefulset ss2 to 0
    Aug 26 05:24:19.954: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 05:24:19.957: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:19.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6379" for this suite. 08/26/23 05:24:19.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:19.997
Aug 26 05:24:19.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-webhook 08/26/23 05:24:19.998
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:20.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:20.023
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/26/23 05:24:20.028
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/26/23 05:24:20.281
STEP: Deploying the custom resource conversion webhook pod 08/26/23 05:24:20.29
STEP: Wait for the deployment to be ready 08/26/23 05:24:20.315
Aug 26 05:24:20.325: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:24:22.338
STEP: Verifying the service has paired with the endpoint 08/26/23 05:24:22.353
Aug 26 05:24:23.353: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Aug 26 05:24:23.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Creating a v1 custom resource 08/26/23 05:24:25.951
STEP: Create a v2 custom resource 08/26/23 05:24:25.972
STEP: List CRs in v1 08/26/23 05:24:26.039
STEP: List CRs in v2 08/26/23 05:24:26.044
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:26.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-5724" for this suite. 08/26/23 05:24:26.678
------------------------------
• [SLOW TEST] [6.702 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:19.997
    Aug 26 05:24:19.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-webhook 08/26/23 05:24:19.998
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:20.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:20.023
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/26/23 05:24:20.028
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/26/23 05:24:20.281
    STEP: Deploying the custom resource conversion webhook pod 08/26/23 05:24:20.29
    STEP: Wait for the deployment to be ready 08/26/23 05:24:20.315
    Aug 26 05:24:20.325: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:24:22.338
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:24:22.353
    Aug 26 05:24:23.353: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Aug 26 05:24:23.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Creating a v1 custom resource 08/26/23 05:24:25.951
    STEP: Create a v2 custom resource 08/26/23 05:24:25.972
    STEP: List CRs in v1 08/26/23 05:24:26.039
    STEP: List CRs in v2 08/26/23 05:24:26.044
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:26.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-5724" for this suite. 08/26/23 05:24:26.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:26.701
Aug 26 05:24:26.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 05:24:26.702
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:26.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:26.803
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-d614671c-0a61-42de-bd7f-604ba58ed50d 08/26/23 05:24:26.806
STEP: Creating a pod to test consume configMaps 08/26/23 05:24:26.812
Aug 26 05:24:26.822: INFO: Waiting up to 5m0s for pod "pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2" in namespace "configmap-1078" to be "Succeeded or Failed"
Aug 26 05:24:26.827: INFO: Pod "pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.883359ms
Aug 26 05:24:28.832: INFO: Pod "pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009936656s
Aug 26 05:24:30.832: INFO: Pod "pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010560679s
STEP: Saw pod success 08/26/23 05:24:30.832
Aug 26 05:24:30.833: INFO: Pod "pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2" satisfied condition "Succeeded or Failed"
Aug 26 05:24:30.843: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2 container agnhost-container: <nil>
STEP: delete the pod 08/26/23 05:24:30.862
Aug 26 05:24:30.883: INFO: Waiting for pod pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2 to disappear
Aug 26 05:24:30.886: INFO: Pod pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:30.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1078" for this suite. 08/26/23 05:24:30.895
------------------------------
• [4.211 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:26.701
    Aug 26 05:24:26.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 05:24:26.702
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:26.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:26.803
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-d614671c-0a61-42de-bd7f-604ba58ed50d 08/26/23 05:24:26.806
    STEP: Creating a pod to test consume configMaps 08/26/23 05:24:26.812
    Aug 26 05:24:26.822: INFO: Waiting up to 5m0s for pod "pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2" in namespace "configmap-1078" to be "Succeeded or Failed"
    Aug 26 05:24:26.827: INFO: Pod "pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.883359ms
    Aug 26 05:24:28.832: INFO: Pod "pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009936656s
    Aug 26 05:24:30.832: INFO: Pod "pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010560679s
    STEP: Saw pod success 08/26/23 05:24:30.832
    Aug 26 05:24:30.833: INFO: Pod "pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2" satisfied condition "Succeeded or Failed"
    Aug 26 05:24:30.843: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2 container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 05:24:30.862
    Aug 26 05:24:30.883: INFO: Waiting for pod pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2 to disappear
    Aug 26 05:24:30.886: INFO: Pod pod-configmaps-a1bbfa2a-f478-454b-9561-074cd35090d2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:30.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1078" for this suite. 08/26/23 05:24:30.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:30.913
Aug 26 05:24:30.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:24:30.914
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:30.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:30.945
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:24:30.947
Aug 26 05:24:30.962: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b" in namespace "downward-api-3804" to be "Succeeded or Failed"
Aug 26 05:24:30.966: INFO: Pod "downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02911ms
Aug 26 05:24:33.056: INFO: Pod "downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b": Phase="Running", Reason="", readiness=false. Elapsed: 2.094865999s
Aug 26 05:24:34.982: INFO: Pod "downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020003526s
STEP: Saw pod success 08/26/23 05:24:34.982
Aug 26 05:24:34.982: INFO: Pod "downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b" satisfied condition "Succeeded or Failed"
Aug 26 05:24:34.986: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b container client-container: <nil>
STEP: delete the pod 08/26/23 05:24:34.997
Aug 26 05:24:35.015: INFO: Waiting for pod downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b to disappear
Aug 26 05:24:35.019: INFO: Pod downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:35.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3804" for this suite. 08/26/23 05:24:35.027
------------------------------
• [4.125 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:30.913
    Aug 26 05:24:30.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:24:30.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:30.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:30.945
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:24:30.947
    Aug 26 05:24:30.962: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b" in namespace "downward-api-3804" to be "Succeeded or Failed"
    Aug 26 05:24:30.966: INFO: Pod "downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02911ms
    Aug 26 05:24:33.056: INFO: Pod "downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b": Phase="Running", Reason="", readiness=false. Elapsed: 2.094865999s
    Aug 26 05:24:34.982: INFO: Pod "downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020003526s
    STEP: Saw pod success 08/26/23 05:24:34.982
    Aug 26 05:24:34.982: INFO: Pod "downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b" satisfied condition "Succeeded or Failed"
    Aug 26 05:24:34.986: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b container client-container: <nil>
    STEP: delete the pod 08/26/23 05:24:34.997
    Aug 26 05:24:35.015: INFO: Waiting for pod downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b to disappear
    Aug 26 05:24:35.019: INFO: Pod downwardapi-volume-c8361e88-d4aa-4a52-a7ec-a92b3502380b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:35.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3804" for this suite. 08/26/23 05:24:35.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:35.039
Aug 26 05:24:35.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename podtemplate 08/26/23 05:24:35.04
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:35.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:35.068
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 08/26/23 05:24:35.071
STEP: Replace a pod template 08/26/23 05:24:35.077
Aug 26 05:24:35.090: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:35.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4631" for this suite. 08/26/23 05:24:35.119
------------------------------
• [0.092 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:35.039
    Aug 26 05:24:35.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename podtemplate 08/26/23 05:24:35.04
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:35.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:35.068
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 08/26/23 05:24:35.071
    STEP: Replace a pod template 08/26/23 05:24:35.077
    Aug 26 05:24:35.090: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:35.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4631" for this suite. 08/26/23 05:24:35.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:35.132
Aug 26 05:24:35.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename watch 08/26/23 05:24:35.133
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:35.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:35.165
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 08/26/23 05:24:35.167
STEP: modifying the configmap once 08/26/23 05:24:35.193
STEP: modifying the configmap a second time 08/26/23 05:24:35.224
STEP: deleting the configmap 08/26/23 05:24:35.247
STEP: creating a watch on configmaps from the resource version returned by the first update 08/26/23 05:24:35.257
STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/26/23 05:24:35.259
Aug 26 05:24:35.259: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7688  0729b582-45ac-4c6e-963a-aca2c954c92c 9205 0 2023-08-26 05:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-26 05:24:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 05:24:35.259: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7688  0729b582-45ac-4c6e-963a-aca2c954c92c 9206 0 2023-08-26 05:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-26 05:24:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:35.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7688" for this suite. 08/26/23 05:24:35.267
------------------------------
• [0.145 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:35.132
    Aug 26 05:24:35.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename watch 08/26/23 05:24:35.133
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:35.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:35.165
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 08/26/23 05:24:35.167
    STEP: modifying the configmap once 08/26/23 05:24:35.193
    STEP: modifying the configmap a second time 08/26/23 05:24:35.224
    STEP: deleting the configmap 08/26/23 05:24:35.247
    STEP: creating a watch on configmaps from the resource version returned by the first update 08/26/23 05:24:35.257
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/26/23 05:24:35.259
    Aug 26 05:24:35.259: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7688  0729b582-45ac-4c6e-963a-aca2c954c92c 9205 0 2023-08-26 05:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-26 05:24:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 05:24:35.259: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7688  0729b582-45ac-4c6e-963a-aca2c954c92c 9206 0 2023-08-26 05:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-26 05:24:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:35.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7688" for this suite. 08/26/23 05:24:35.267
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:35.277
Aug 26 05:24:35.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:24:35.278
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:35.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:35.325
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 08/26/23 05:24:35.329
Aug 26 05:24:35.340: INFO: Waiting up to 5m0s for pod "labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1" in namespace "downward-api-9859" to be "running and ready"
Aug 26 05:24:35.343: INFO: Pod "labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.651466ms
Aug 26 05:24:35.343: INFO: The phase of Pod labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:24:37.349: INFO: Pod "labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009711227s
Aug 26 05:24:37.350: INFO: The phase of Pod labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1 is Running (Ready = true)
Aug 26 05:24:37.350: INFO: Pod "labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1" satisfied condition "running and ready"
Aug 26 05:24:37.885: INFO: Successfully updated pod "labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:39.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9859" for this suite. 08/26/23 05:24:39.92
------------------------------
• [4.653 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:35.277
    Aug 26 05:24:35.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:24:35.278
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:35.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:35.325
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 08/26/23 05:24:35.329
    Aug 26 05:24:35.340: INFO: Waiting up to 5m0s for pod "labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1" in namespace "downward-api-9859" to be "running and ready"
    Aug 26 05:24:35.343: INFO: Pod "labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.651466ms
    Aug 26 05:24:35.343: INFO: The phase of Pod labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:24:37.349: INFO: Pod "labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009711227s
    Aug 26 05:24:37.350: INFO: The phase of Pod labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1 is Running (Ready = true)
    Aug 26 05:24:37.350: INFO: Pod "labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1" satisfied condition "running and ready"
    Aug 26 05:24:37.885: INFO: Successfully updated pod "labelsupdate88364ddb-1a94-4258-91f7-e6ca0c2930d1"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:39.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9859" for this suite. 08/26/23 05:24:39.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:39.931
Aug 26 05:24:39.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 05:24:39.933
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:39.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:39.96
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 08/26/23 05:24:39.963
Aug 26 05:24:39.978: INFO: Waiting up to 5m0s for pod "pod-vl7qt" in namespace "pods-4689" to be "running"
Aug 26 05:24:39.988: INFO: Pod "pod-vl7qt": Phase="Pending", Reason="", readiness=false. Elapsed: 10.324791ms
Aug 26 05:24:41.993: INFO: Pod "pod-vl7qt": Phase="Running", Reason="", readiness=true. Elapsed: 2.015501331s
Aug 26 05:24:41.993: INFO: Pod "pod-vl7qt" satisfied condition "running"
STEP: patching /status 08/26/23 05:24:41.994
Aug 26 05:24:42.003: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:42.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4689" for this suite. 08/26/23 05:24:42.01
------------------------------
• [2.088 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:39.931
    Aug 26 05:24:39.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 05:24:39.933
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:39.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:39.96
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 08/26/23 05:24:39.963
    Aug 26 05:24:39.978: INFO: Waiting up to 5m0s for pod "pod-vl7qt" in namespace "pods-4689" to be "running"
    Aug 26 05:24:39.988: INFO: Pod "pod-vl7qt": Phase="Pending", Reason="", readiness=false. Elapsed: 10.324791ms
    Aug 26 05:24:41.993: INFO: Pod "pod-vl7qt": Phase="Running", Reason="", readiness=true. Elapsed: 2.015501331s
    Aug 26 05:24:41.993: INFO: Pod "pod-vl7qt" satisfied condition "running"
    STEP: patching /status 08/26/23 05:24:41.994
    Aug 26 05:24:42.003: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:42.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4689" for this suite. 08/26/23 05:24:42.01
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:42.019
Aug 26 05:24:42.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename ingressclass 08/26/23 05:24:42.021
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:42.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:42.043
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 08/26/23 05:24:42.052
STEP: getting /apis/networking.k8s.io 08/26/23 05:24:42.057
STEP: getting /apis/networking.k8s.iov1 08/26/23 05:24:42.06
STEP: creating 08/26/23 05:24:42.063
STEP: getting 08/26/23 05:24:42.084
STEP: listing 08/26/23 05:24:42.092
STEP: watching 08/26/23 05:24:42.103
Aug 26 05:24:42.103: INFO: starting watch
STEP: patching 08/26/23 05:24:42.104
STEP: updating 08/26/23 05:24:42.111
Aug 26 05:24:42.122: INFO: waiting for watch events with expected annotations
Aug 26 05:24:42.122: INFO: saw patched and updated annotations
STEP: deleting 08/26/23 05:24:42.123
STEP: deleting a collection 08/26/23 05:24:42.163
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:42.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-1166" for this suite. 08/26/23 05:24:42.216
------------------------------
• [0.206 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:42.019
    Aug 26 05:24:42.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename ingressclass 08/26/23 05:24:42.021
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:42.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:42.043
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 08/26/23 05:24:42.052
    STEP: getting /apis/networking.k8s.io 08/26/23 05:24:42.057
    STEP: getting /apis/networking.k8s.iov1 08/26/23 05:24:42.06
    STEP: creating 08/26/23 05:24:42.063
    STEP: getting 08/26/23 05:24:42.084
    STEP: listing 08/26/23 05:24:42.092
    STEP: watching 08/26/23 05:24:42.103
    Aug 26 05:24:42.103: INFO: starting watch
    STEP: patching 08/26/23 05:24:42.104
    STEP: updating 08/26/23 05:24:42.111
    Aug 26 05:24:42.122: INFO: waiting for watch events with expected annotations
    Aug 26 05:24:42.122: INFO: saw patched and updated annotations
    STEP: deleting 08/26/23 05:24:42.123
    STEP: deleting a collection 08/26/23 05:24:42.163
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:42.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-1166" for this suite. 08/26/23 05:24:42.216
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:42.226
Aug 26 05:24:42.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:24:42.227
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:42.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:42.248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:24:42.251
Aug 26 05:24:42.295: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9" in namespace "projected-9219" to be "Succeeded or Failed"
Aug 26 05:24:42.303: INFO: Pod "downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.622819ms
Aug 26 05:24:44.308: INFO: Pod "downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013471019s
Aug 26 05:24:46.308: INFO: Pod "downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01367258s
STEP: Saw pod success 08/26/23 05:24:46.308
Aug 26 05:24:46.308: INFO: Pod "downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9" satisfied condition "Succeeded or Failed"
Aug 26 05:24:46.315: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9 container client-container: <nil>
STEP: delete the pod 08/26/23 05:24:46.325
Aug 26 05:24:46.340: INFO: Waiting for pod downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9 to disappear
Aug 26 05:24:46.344: INFO: Pod downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:46.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9219" for this suite. 08/26/23 05:24:46.355
------------------------------
• [4.142 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:42.226
    Aug 26 05:24:42.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:24:42.227
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:42.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:42.248
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:24:42.251
    Aug 26 05:24:42.295: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9" in namespace "projected-9219" to be "Succeeded or Failed"
    Aug 26 05:24:42.303: INFO: Pod "downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.622819ms
    Aug 26 05:24:44.308: INFO: Pod "downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013471019s
    Aug 26 05:24:46.308: INFO: Pod "downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01367258s
    STEP: Saw pod success 08/26/23 05:24:46.308
    Aug 26 05:24:46.308: INFO: Pod "downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9" satisfied condition "Succeeded or Failed"
    Aug 26 05:24:46.315: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9 container client-container: <nil>
    STEP: delete the pod 08/26/23 05:24:46.325
    Aug 26 05:24:46.340: INFO: Waiting for pod downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9 to disappear
    Aug 26 05:24:46.344: INFO: Pod downwardapi-volume-61a73448-f04a-4e0b-9601-0b6f504fdea9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:46.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9219" for this suite. 08/26/23 05:24:46.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:46.37
Aug 26 05:24:46.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 05:24:46.371
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:46.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:46.41
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 08/26/23 05:24:46.413
STEP: Getting a ResourceQuota 08/26/23 05:24:46.421
STEP: Listing all ResourceQuotas with LabelSelector 08/26/23 05:24:46.428
STEP: Patching the ResourceQuota 08/26/23 05:24:46.433
STEP: Deleting a Collection of ResourceQuotas 08/26/23 05:24:46.442
STEP: Verifying the deleted ResourceQuota 08/26/23 05:24:46.454
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:46.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5637" for this suite. 08/26/23 05:24:46.471
------------------------------
• [0.111 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:46.37
    Aug 26 05:24:46.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 05:24:46.371
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:46.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:46.41
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 08/26/23 05:24:46.413
    STEP: Getting a ResourceQuota 08/26/23 05:24:46.421
    STEP: Listing all ResourceQuotas with LabelSelector 08/26/23 05:24:46.428
    STEP: Patching the ResourceQuota 08/26/23 05:24:46.433
    STEP: Deleting a Collection of ResourceQuotas 08/26/23 05:24:46.442
    STEP: Verifying the deleted ResourceQuota 08/26/23 05:24:46.454
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:46.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5637" for this suite. 08/26/23 05:24:46.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:46.482
Aug 26 05:24:46.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 05:24:46.483
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:46.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:46.505
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 08/26/23 05:24:46.516
STEP: waiting for available Endpoint 08/26/23 05:24:46.528
STEP: listing all Endpoints 08/26/23 05:24:46.53
STEP: updating the Endpoint 08/26/23 05:24:46.537
STEP: fetching the Endpoint 08/26/23 05:24:46.544
STEP: patching the Endpoint 08/26/23 05:24:46.548
STEP: fetching the Endpoint 08/26/23 05:24:46.556
STEP: deleting the Endpoint by Collection 08/26/23 05:24:46.56
STEP: waiting for Endpoint deletion 08/26/23 05:24:46.57
STEP: fetching the Endpoint 08/26/23 05:24:46.572
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:46.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5184" for this suite. 08/26/23 05:24:46.586
------------------------------
• [0.113 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:46.482
    Aug 26 05:24:46.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 05:24:46.483
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:46.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:46.505
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 08/26/23 05:24:46.516
    STEP: waiting for available Endpoint 08/26/23 05:24:46.528
    STEP: listing all Endpoints 08/26/23 05:24:46.53
    STEP: updating the Endpoint 08/26/23 05:24:46.537
    STEP: fetching the Endpoint 08/26/23 05:24:46.544
    STEP: patching the Endpoint 08/26/23 05:24:46.548
    STEP: fetching the Endpoint 08/26/23 05:24:46.556
    STEP: deleting the Endpoint by Collection 08/26/23 05:24:46.56
    STEP: waiting for Endpoint deletion 08/26/23 05:24:46.57
    STEP: fetching the Endpoint 08/26/23 05:24:46.572
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:46.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5184" for this suite. 08/26/23 05:24:46.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:46.597
Aug 26 05:24:46.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:24:46.601
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:46.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:46.636
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-d4ae9fb9-8d8c-4cfc-b856-7d082cfdb6c4 08/26/23 05:24:46.651
STEP: Creating secret with name s-test-opt-upd-03ad73a0-9307-4100-8f8b-73cea9a8bfa1 08/26/23 05:24:46.658
STEP: Creating the pod 08/26/23 05:24:46.663
Aug 26 05:24:46.676: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604" in namespace "projected-665" to be "running and ready"
Aug 26 05:24:46.681: INFO: Pod "pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604": Phase="Pending", Reason="", readiness=false. Elapsed: 4.801191ms
Aug 26 05:24:46.681: INFO: The phase of Pod pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:24:48.686: INFO: Pod "pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604": Phase="Running", Reason="", readiness=true. Elapsed: 2.01009991s
Aug 26 05:24:48.686: INFO: The phase of Pod pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604 is Running (Ready = true)
Aug 26 05:24:48.686: INFO: Pod "pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d4ae9fb9-8d8c-4cfc-b856-7d082cfdb6c4 08/26/23 05:24:48.734
STEP: Updating secret s-test-opt-upd-03ad73a0-9307-4100-8f8b-73cea9a8bfa1 08/26/23 05:24:48.749
STEP: Creating secret with name s-test-opt-create-98e14dca-f165-46fc-805e-6b7a504d7c6b 08/26/23 05:24:48.756
STEP: waiting to observe update in volume 08/26/23 05:24:48.762
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:52.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-665" for this suite. 08/26/23 05:24:52.83
------------------------------
• [SLOW TEST] [6.243 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:46.597
    Aug 26 05:24:46.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:24:46.601
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:46.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:46.636
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-d4ae9fb9-8d8c-4cfc-b856-7d082cfdb6c4 08/26/23 05:24:46.651
    STEP: Creating secret with name s-test-opt-upd-03ad73a0-9307-4100-8f8b-73cea9a8bfa1 08/26/23 05:24:46.658
    STEP: Creating the pod 08/26/23 05:24:46.663
    Aug 26 05:24:46.676: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604" in namespace "projected-665" to be "running and ready"
    Aug 26 05:24:46.681: INFO: Pod "pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604": Phase="Pending", Reason="", readiness=false. Elapsed: 4.801191ms
    Aug 26 05:24:46.681: INFO: The phase of Pod pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:24:48.686: INFO: Pod "pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604": Phase="Running", Reason="", readiness=true. Elapsed: 2.01009991s
    Aug 26 05:24:48.686: INFO: The phase of Pod pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604 is Running (Ready = true)
    Aug 26 05:24:48.686: INFO: Pod "pod-projected-secrets-af59c110-0545-40db-8101-cc6f371ec604" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d4ae9fb9-8d8c-4cfc-b856-7d082cfdb6c4 08/26/23 05:24:48.734
    STEP: Updating secret s-test-opt-upd-03ad73a0-9307-4100-8f8b-73cea9a8bfa1 08/26/23 05:24:48.749
    STEP: Creating secret with name s-test-opt-create-98e14dca-f165-46fc-805e-6b7a504d7c6b 08/26/23 05:24:48.756
    STEP: waiting to observe update in volume 08/26/23 05:24:48.762
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:52.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-665" for this suite. 08/26/23 05:24:52.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:52.844
Aug 26 05:24:52.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 05:24:52.845
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:52.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:52.873
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-0b04ca1c-1d84-4c24-9877-b070b9d58799 08/26/23 05:24:52.906
STEP: Creating a pod to test consume secrets 08/26/23 05:24:52.913
Aug 26 05:24:52.935: INFO: Waiting up to 5m0s for pod "pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c" in namespace "secrets-6732" to be "Succeeded or Failed"
Aug 26 05:24:52.944: INFO: Pod "pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.35702ms
Aug 26 05:24:54.950: INFO: Pod "pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014810221s
Aug 26 05:24:56.950: INFO: Pod "pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01480813s
STEP: Saw pod success 08/26/23 05:24:56.95
Aug 26 05:24:56.950: INFO: Pod "pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c" satisfied condition "Succeeded or Failed"
Aug 26 05:24:56.955: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c container secret-volume-test: <nil>
STEP: delete the pod 08/26/23 05:24:56.98
Aug 26 05:24:57.007: INFO: Waiting for pod pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c to disappear
Aug 26 05:24:57.010: INFO: Pod pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 05:24:57.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6732" for this suite. 08/26/23 05:24:57.032
STEP: Destroying namespace "secret-namespace-942" for this suite. 08/26/23 05:24:57.047
------------------------------
• [4.216 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:52.844
    Aug 26 05:24:52.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 05:24:52.845
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:52.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:52.873
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-0b04ca1c-1d84-4c24-9877-b070b9d58799 08/26/23 05:24:52.906
    STEP: Creating a pod to test consume secrets 08/26/23 05:24:52.913
    Aug 26 05:24:52.935: INFO: Waiting up to 5m0s for pod "pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c" in namespace "secrets-6732" to be "Succeeded or Failed"
    Aug 26 05:24:52.944: INFO: Pod "pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.35702ms
    Aug 26 05:24:54.950: INFO: Pod "pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014810221s
    Aug 26 05:24:56.950: INFO: Pod "pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01480813s
    STEP: Saw pod success 08/26/23 05:24:56.95
    Aug 26 05:24:56.950: INFO: Pod "pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c" satisfied condition "Succeeded or Failed"
    Aug 26 05:24:56.955: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c container secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 05:24:56.98
    Aug 26 05:24:57.007: INFO: Waiting for pod pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c to disappear
    Aug 26 05:24:57.010: INFO: Pod pod-secrets-31966e2f-d405-4023-9ee5-d41968c3222c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:24:57.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6732" for this suite. 08/26/23 05:24:57.032
    STEP: Destroying namespace "secret-namespace-942" for this suite. 08/26/23 05:24:57.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:24:57.06
Aug 26 05:24:57.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 05:24:57.061
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:57.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:57.088
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 08/26/23 05:24:57.093
Aug 26 05:24:57.120: INFO: Waiting up to 5m0s for pod "pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9" in namespace "emptydir-584" to be "Succeeded or Failed"
Aug 26 05:24:57.127: INFO: Pod "pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.490128ms
Aug 26 05:24:59.132: INFO: Pod "pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011448548s
Aug 26 05:25:01.132: INFO: Pod "pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011358428s
STEP: Saw pod success 08/26/23 05:25:01.132
Aug 26 05:25:01.132: INFO: Pod "pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9" satisfied condition "Succeeded or Failed"
Aug 26 05:25:01.136: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9 container test-container: <nil>
STEP: delete the pod 08/26/23 05:25:01.144
Aug 26 05:25:01.170: INFO: Waiting for pod pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9 to disappear
Aug 26 05:25:01.178: INFO: Pod pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:01.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-584" for this suite. 08/26/23 05:25:01.19
------------------------------
• [4.143 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:24:57.06
    Aug 26 05:24:57.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 05:24:57.061
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:24:57.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:24:57.088
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/26/23 05:24:57.093
    Aug 26 05:24:57.120: INFO: Waiting up to 5m0s for pod "pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9" in namespace "emptydir-584" to be "Succeeded or Failed"
    Aug 26 05:24:57.127: INFO: Pod "pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.490128ms
    Aug 26 05:24:59.132: INFO: Pod "pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011448548s
    Aug 26 05:25:01.132: INFO: Pod "pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011358428s
    STEP: Saw pod success 08/26/23 05:25:01.132
    Aug 26 05:25:01.132: INFO: Pod "pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9" satisfied condition "Succeeded or Failed"
    Aug 26 05:25:01.136: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9 container test-container: <nil>
    STEP: delete the pod 08/26/23 05:25:01.144
    Aug 26 05:25:01.170: INFO: Waiting for pod pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9 to disappear
    Aug 26 05:25:01.178: INFO: Pod pod-39c65af6-68f1-47f2-8a0a-40ce325c08e9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:01.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-584" for this suite. 08/26/23 05:25:01.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:01.206
Aug 26 05:25:01.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename namespaces 08/26/23 05:25:01.207
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:01.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:01.23
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 08/26/23 05:25:01.234
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:01.255
STEP: Creating a service in the namespace 08/26/23 05:25:01.262
STEP: Deleting the namespace 08/26/23 05:25:01.286
STEP: Waiting for the namespace to be removed. 08/26/23 05:25:01.306
STEP: Recreating the namespace 08/26/23 05:25:07.313
STEP: Verifying there is no service in the namespace 08/26/23 05:25:07.329
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:07.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7825" for this suite. 08/26/23 05:25:07.342
STEP: Destroying namespace "nsdeletetest-9493" for this suite. 08/26/23 05:25:07.353
Aug 26 05:25:07.360: INFO: Namespace nsdeletetest-9493 was already deleted
STEP: Destroying namespace "nsdeletetest-7493" for this suite. 08/26/23 05:25:07.36
------------------------------
• [SLOW TEST] [6.163 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:01.206
    Aug 26 05:25:01.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename namespaces 08/26/23 05:25:01.207
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:01.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:01.23
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 08/26/23 05:25:01.234
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:01.255
    STEP: Creating a service in the namespace 08/26/23 05:25:01.262
    STEP: Deleting the namespace 08/26/23 05:25:01.286
    STEP: Waiting for the namespace to be removed. 08/26/23 05:25:01.306
    STEP: Recreating the namespace 08/26/23 05:25:07.313
    STEP: Verifying there is no service in the namespace 08/26/23 05:25:07.329
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:07.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7825" for this suite. 08/26/23 05:25:07.342
    STEP: Destroying namespace "nsdeletetest-9493" for this suite. 08/26/23 05:25:07.353
    Aug 26 05:25:07.360: INFO: Namespace nsdeletetest-9493 was already deleted
    STEP: Destroying namespace "nsdeletetest-7493" for this suite. 08/26/23 05:25:07.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:07.369
Aug 26 05:25:07.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename dns 08/26/23 05:25:07.371
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:07.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:07.399
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 08/26/23 05:25:07.403
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2667.svc.cluster.local;sleep 1; done
 08/26/23 05:25:07.412
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2667.svc.cluster.local;sleep 1; done
 08/26/23 05:25:07.412
STEP: creating a pod to probe DNS 08/26/23 05:25:07.412
STEP: submitting the pod to kubernetes 08/26/23 05:25:07.412
Aug 26 05:25:07.430: INFO: Waiting up to 15m0s for pod "dns-test-9b78e125-efe9-466a-8421-3c20fc459c24" in namespace "dns-2667" to be "running"
Aug 26 05:25:07.438: INFO: Pod "dns-test-9b78e125-efe9-466a-8421-3c20fc459c24": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124474ms
Aug 26 05:25:09.444: INFO: Pod "dns-test-9b78e125-efe9-466a-8421-3c20fc459c24": Phase="Running", Reason="", readiness=true. Elapsed: 2.013120092s
Aug 26 05:25:09.444: INFO: Pod "dns-test-9b78e125-efe9-466a-8421-3c20fc459c24" satisfied condition "running"
STEP: retrieving the pod 08/26/23 05:25:09.444
STEP: looking for the results for each expected name from probers 08/26/23 05:25:09.448
Aug 26 05:25:09.452: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
Aug 26 05:25:09.457: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
Aug 26 05:25:09.461: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
Aug 26 05:25:09.470: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
Aug 26 05:25:09.474: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
Aug 26 05:25:09.478: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
Aug 26 05:25:09.482: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
Aug 26 05:25:09.488: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
Aug 26 05:25:09.488: INFO: Lookups using dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2667.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2667.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local jessie_udp@dns-test-service-2.dns-2667.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2667.svc.cluster.local]

Aug 26 05:25:14.531: INFO: DNS probes using dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24 succeeded

STEP: deleting the pod 08/26/23 05:25:14.532
STEP: deleting the test headless service 08/26/23 05:25:14.551
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:14.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2667" for this suite. 08/26/23 05:25:14.583
------------------------------
• [SLOW TEST] [7.228 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:07.369
    Aug 26 05:25:07.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename dns 08/26/23 05:25:07.371
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:07.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:07.399
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 08/26/23 05:25:07.403
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2667.svc.cluster.local;sleep 1; done
     08/26/23 05:25:07.412
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2667.svc.cluster.local;sleep 1; done
     08/26/23 05:25:07.412
    STEP: creating a pod to probe DNS 08/26/23 05:25:07.412
    STEP: submitting the pod to kubernetes 08/26/23 05:25:07.412
    Aug 26 05:25:07.430: INFO: Waiting up to 15m0s for pod "dns-test-9b78e125-efe9-466a-8421-3c20fc459c24" in namespace "dns-2667" to be "running"
    Aug 26 05:25:07.438: INFO: Pod "dns-test-9b78e125-efe9-466a-8421-3c20fc459c24": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124474ms
    Aug 26 05:25:09.444: INFO: Pod "dns-test-9b78e125-efe9-466a-8421-3c20fc459c24": Phase="Running", Reason="", readiness=true. Elapsed: 2.013120092s
    Aug 26 05:25:09.444: INFO: Pod "dns-test-9b78e125-efe9-466a-8421-3c20fc459c24" satisfied condition "running"
    STEP: retrieving the pod 08/26/23 05:25:09.444
    STEP: looking for the results for each expected name from probers 08/26/23 05:25:09.448
    Aug 26 05:25:09.452: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
    Aug 26 05:25:09.457: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
    Aug 26 05:25:09.461: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
    Aug 26 05:25:09.470: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
    Aug 26 05:25:09.474: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
    Aug 26 05:25:09.478: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
    Aug 26 05:25:09.482: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
    Aug 26 05:25:09.488: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2667.svc.cluster.local from pod dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24: the server could not find the requested resource (get pods dns-test-9b78e125-efe9-466a-8421-3c20fc459c24)
    Aug 26 05:25:09.488: INFO: Lookups using dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2667.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2667.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2667.svc.cluster.local jessie_udp@dns-test-service-2.dns-2667.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2667.svc.cluster.local]

    Aug 26 05:25:14.531: INFO: DNS probes using dns-2667/dns-test-9b78e125-efe9-466a-8421-3c20fc459c24 succeeded

    STEP: deleting the pod 08/26/23 05:25:14.532
    STEP: deleting the test headless service 08/26/23 05:25:14.551
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:14.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2667" for this suite. 08/26/23 05:25:14.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:14.6
Aug 26 05:25:14.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:25:14.601
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:14.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:14.63
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:25:14.65
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:25:14.962
STEP: Deploying the webhook pod 08/26/23 05:25:14.973
STEP: Wait for the deployment to be ready 08/26/23 05:25:14.99
Aug 26 05:25:15.018: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:25:17.03
STEP: Verifying the service has paired with the endpoint 08/26/23 05:25:17.042
Aug 26 05:25:18.042: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 08/26/23 05:25:18.048
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/26/23 05:25:18.05
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/26/23 05:25:18.05
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/26/23 05:25:18.05
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/26/23 05:25:18.053
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/26/23 05:25:18.053
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/26/23 05:25:18.054
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:18.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5990" for this suite. 08/26/23 05:25:18.131
STEP: Destroying namespace "webhook-5990-markers" for this suite. 08/26/23 05:25:18.141
------------------------------
• [3.554 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:14.6
    Aug 26 05:25:14.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:25:14.601
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:14.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:14.63
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:25:14.65
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:25:14.962
    STEP: Deploying the webhook pod 08/26/23 05:25:14.973
    STEP: Wait for the deployment to be ready 08/26/23 05:25:14.99
    Aug 26 05:25:15.018: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:25:17.03
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:25:17.042
    Aug 26 05:25:18.042: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 08/26/23 05:25:18.048
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/26/23 05:25:18.05
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/26/23 05:25:18.05
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/26/23 05:25:18.05
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/26/23 05:25:18.053
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/26/23 05:25:18.053
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/26/23 05:25:18.054
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:18.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5990" for this suite. 08/26/23 05:25:18.131
    STEP: Destroying namespace "webhook-5990-markers" for this suite. 08/26/23 05:25:18.141
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:18.154
Aug 26 05:25:18.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replicaset 08/26/23 05:25:18.155
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:18.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:18.199
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Aug 26 05:25:18.219: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 26 05:25:23.224: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/26/23 05:25:23.224
STEP: Scaling up "test-rs" replicaset  08/26/23 05:25:23.225
Aug 26 05:25:23.241: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 08/26/23 05:25:23.241
W0826 05:25:23.254990      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 26 05:25:23.256: INFO: observed ReplicaSet test-rs in namespace replicaset-9293 with ReadyReplicas 1, AvailableReplicas 1
Aug 26 05:25:23.286: INFO: observed ReplicaSet test-rs in namespace replicaset-9293 with ReadyReplicas 1, AvailableReplicas 1
Aug 26 05:25:23.324: INFO: observed ReplicaSet test-rs in namespace replicaset-9293 with ReadyReplicas 1, AvailableReplicas 1
Aug 26 05:25:23.334: INFO: observed ReplicaSet test-rs in namespace replicaset-9293 with ReadyReplicas 1, AvailableReplicas 1
Aug 26 05:25:24.583: INFO: observed ReplicaSet test-rs in namespace replicaset-9293 with ReadyReplicas 2, AvailableReplicas 2
Aug 26 05:25:25.124: INFO: observed Replicaset test-rs in namespace replicaset-9293 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:25.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9293" for this suite. 08/26/23 05:25:25.139
------------------------------
• [SLOW TEST] [6.994 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:18.154
    Aug 26 05:25:18.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replicaset 08/26/23 05:25:18.155
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:18.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:18.199
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Aug 26 05:25:18.219: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 26 05:25:23.224: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/26/23 05:25:23.224
    STEP: Scaling up "test-rs" replicaset  08/26/23 05:25:23.225
    Aug 26 05:25:23.241: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 08/26/23 05:25:23.241
    W0826 05:25:23.254990      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 26 05:25:23.256: INFO: observed ReplicaSet test-rs in namespace replicaset-9293 with ReadyReplicas 1, AvailableReplicas 1
    Aug 26 05:25:23.286: INFO: observed ReplicaSet test-rs in namespace replicaset-9293 with ReadyReplicas 1, AvailableReplicas 1
    Aug 26 05:25:23.324: INFO: observed ReplicaSet test-rs in namespace replicaset-9293 with ReadyReplicas 1, AvailableReplicas 1
    Aug 26 05:25:23.334: INFO: observed ReplicaSet test-rs in namespace replicaset-9293 with ReadyReplicas 1, AvailableReplicas 1
    Aug 26 05:25:24.583: INFO: observed ReplicaSet test-rs in namespace replicaset-9293 with ReadyReplicas 2, AvailableReplicas 2
    Aug 26 05:25:25.124: INFO: observed Replicaset test-rs in namespace replicaset-9293 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:25.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9293" for this suite. 08/26/23 05:25:25.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:25.149
Aug 26 05:25:25.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 05:25:25.15
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:25.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:25.175
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-bfb459c0-a658-42ce-8cb4-0ac3f06af912 08/26/23 05:25:25.178
STEP: Creating a pod to test consume secrets 08/26/23 05:25:25.191
Aug 26 05:25:25.208: INFO: Waiting up to 5m0s for pod "pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853" in namespace "secrets-7907" to be "Succeeded or Failed"
Aug 26 05:25:25.211: INFO: Pod "pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853": Phase="Pending", Reason="", readiness=false. Elapsed: 3.810964ms
Aug 26 05:25:27.223: INFO: Pod "pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015229461s
Aug 26 05:25:29.216: INFO: Pod "pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008780996s
STEP: Saw pod success 08/26/23 05:25:29.216
Aug 26 05:25:29.217: INFO: Pod "pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853" satisfied condition "Succeeded or Failed"
Aug 26 05:25:29.221: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853 container secret-volume-test: <nil>
STEP: delete the pod 08/26/23 05:25:29.229
Aug 26 05:25:29.246: INFO: Waiting for pod pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853 to disappear
Aug 26 05:25:29.250: INFO: Pod pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:29.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7907" for this suite. 08/26/23 05:25:29.257
------------------------------
• [4.115 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:25.149
    Aug 26 05:25:25.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 05:25:25.15
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:25.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:25.175
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-bfb459c0-a658-42ce-8cb4-0ac3f06af912 08/26/23 05:25:25.178
    STEP: Creating a pod to test consume secrets 08/26/23 05:25:25.191
    Aug 26 05:25:25.208: INFO: Waiting up to 5m0s for pod "pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853" in namespace "secrets-7907" to be "Succeeded or Failed"
    Aug 26 05:25:25.211: INFO: Pod "pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853": Phase="Pending", Reason="", readiness=false. Elapsed: 3.810964ms
    Aug 26 05:25:27.223: INFO: Pod "pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015229461s
    Aug 26 05:25:29.216: INFO: Pod "pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008780996s
    STEP: Saw pod success 08/26/23 05:25:29.216
    Aug 26 05:25:29.217: INFO: Pod "pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853" satisfied condition "Succeeded or Failed"
    Aug 26 05:25:29.221: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853 container secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 05:25:29.229
    Aug 26 05:25:29.246: INFO: Waiting for pod pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853 to disappear
    Aug 26 05:25:29.250: INFO: Pod pod-secrets-72735b06-4c4f-4267-9ee9-4ebb438bc853 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:29.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7907" for this suite. 08/26/23 05:25:29.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:29.264
Aug 26 05:25:29.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 05:25:29.265
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:29.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:29.292
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-cdec8f24-86ad-4a9c-a666-1f4508a5e659 08/26/23 05:25:29.295
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:29.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3190" for this suite. 08/26/23 05:25:29.314
------------------------------
• [0.058 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:29.264
    Aug 26 05:25:29.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 05:25:29.265
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:29.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:29.292
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-cdec8f24-86ad-4a9c-a666-1f4508a5e659 08/26/23 05:25:29.295
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:29.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3190" for this suite. 08/26/23 05:25:29.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:29.326
Aug 26 05:25:29.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename csiinlinevolumes 08/26/23 05:25:29.327
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:29.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:29.359
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 08/26/23 05:25:29.362
STEP: getting 08/26/23 05:25:29.385
STEP: listing 08/26/23 05:25:29.392
STEP: deleting 08/26/23 05:25:29.396
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:29.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2296" for this suite. 08/26/23 05:25:29.424
------------------------------
• [0.118 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:29.326
    Aug 26 05:25:29.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename csiinlinevolumes 08/26/23 05:25:29.327
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:29.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:29.359
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 08/26/23 05:25:29.362
    STEP: getting 08/26/23 05:25:29.385
    STEP: listing 08/26/23 05:25:29.392
    STEP: deleting 08/26/23 05:25:29.396
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:29.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2296" for this suite. 08/26/23 05:25:29.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:29.451
Aug 26 05:25:29.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 05:25:29.454
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:29.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:29.48
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3678 08/26/23 05:25:29.483
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/26/23 05:25:29.497
STEP: creating service externalsvc in namespace services-3678 08/26/23 05:25:29.497
STEP: creating replication controller externalsvc in namespace services-3678 08/26/23 05:25:29.524
I0826 05:25:29.536032      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3678, replica count: 2
I0826 05:25:32.587910      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 08/26/23 05:25:32.592
Aug 26 05:25:32.610: INFO: Creating new exec pod
Aug 26 05:25:32.639: INFO: Waiting up to 5m0s for pod "execpodpbp6p" in namespace "services-3678" to be "running"
Aug 26 05:25:32.648: INFO: Pod "execpodpbp6p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.414159ms
Aug 26 05:25:34.653: INFO: Pod "execpodpbp6p": Phase="Running", Reason="", readiness=true. Elapsed: 2.014026522s
Aug 26 05:25:34.653: INFO: Pod "execpodpbp6p" satisfied condition "running"
Aug 26 05:25:34.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-3678 exec execpodpbp6p -- /bin/sh -x -c nslookup clusterip-service.services-3678.svc.cluster.local'
Aug 26 05:25:34.854: INFO: stderr: "+ nslookup clusterip-service.services-3678.svc.cluster.local\n"
Aug 26 05:25:34.854: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nclusterip-service.services-3678.svc.cluster.local\tcanonical name = externalsvc.services-3678.svc.cluster.local.\nName:\texternalsvc.services-3678.svc.cluster.local\nAddress: 10.21.177.160\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3678, will wait for the garbage collector to delete the pods 08/26/23 05:25:34.854
Aug 26 05:25:34.920: INFO: Deleting ReplicationController externalsvc took: 6.901991ms
Aug 26 05:25:35.021: INFO: Terminating ReplicationController externalsvc pods took: 100.291875ms
Aug 26 05:25:37.259: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:37.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3678" for this suite. 08/26/23 05:25:37.289
------------------------------
• [SLOW TEST] [7.846 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:29.451
    Aug 26 05:25:29.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 05:25:29.454
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:29.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:29.48
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3678 08/26/23 05:25:29.483
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/26/23 05:25:29.497
    STEP: creating service externalsvc in namespace services-3678 08/26/23 05:25:29.497
    STEP: creating replication controller externalsvc in namespace services-3678 08/26/23 05:25:29.524
    I0826 05:25:29.536032      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3678, replica count: 2
    I0826 05:25:32.587910      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 08/26/23 05:25:32.592
    Aug 26 05:25:32.610: INFO: Creating new exec pod
    Aug 26 05:25:32.639: INFO: Waiting up to 5m0s for pod "execpodpbp6p" in namespace "services-3678" to be "running"
    Aug 26 05:25:32.648: INFO: Pod "execpodpbp6p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.414159ms
    Aug 26 05:25:34.653: INFO: Pod "execpodpbp6p": Phase="Running", Reason="", readiness=true. Elapsed: 2.014026522s
    Aug 26 05:25:34.653: INFO: Pod "execpodpbp6p" satisfied condition "running"
    Aug 26 05:25:34.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-3678 exec execpodpbp6p -- /bin/sh -x -c nslookup clusterip-service.services-3678.svc.cluster.local'
    Aug 26 05:25:34.854: INFO: stderr: "+ nslookup clusterip-service.services-3678.svc.cluster.local\n"
    Aug 26 05:25:34.854: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nclusterip-service.services-3678.svc.cluster.local\tcanonical name = externalsvc.services-3678.svc.cluster.local.\nName:\texternalsvc.services-3678.svc.cluster.local\nAddress: 10.21.177.160\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3678, will wait for the garbage collector to delete the pods 08/26/23 05:25:34.854
    Aug 26 05:25:34.920: INFO: Deleting ReplicationController externalsvc took: 6.901991ms
    Aug 26 05:25:35.021: INFO: Terminating ReplicationController externalsvc pods took: 100.291875ms
    Aug 26 05:25:37.259: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:37.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3678" for this suite. 08/26/23 05:25:37.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:37.298
Aug 26 05:25:37.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replication-controller 08/26/23 05:25:37.299
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:37.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:37.325
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-sdlwl" 08/26/23 05:25:37.328
Aug 26 05:25:37.337: INFO: Get Replication Controller "e2e-rc-sdlwl" to confirm replicas
Aug 26 05:25:38.341: INFO: Get Replication Controller "e2e-rc-sdlwl" to confirm replicas
Aug 26 05:25:38.348: INFO: Found 1 replicas for "e2e-rc-sdlwl" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-sdlwl" 08/26/23 05:25:38.348
STEP: Updating a scale subresource 08/26/23 05:25:38.352
STEP: Verifying replicas where modified for replication controller "e2e-rc-sdlwl" 08/26/23 05:25:38.36
Aug 26 05:25:38.361: INFO: Get Replication Controller "e2e-rc-sdlwl" to confirm replicas
Aug 26 05:25:39.366: INFO: Get Replication Controller "e2e-rc-sdlwl" to confirm replicas
Aug 26 05:25:39.370: INFO: Found 2 replicas for "e2e-rc-sdlwl" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:39.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9617" for this suite. 08/26/23 05:25:39.378
------------------------------
• [2.087 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:37.298
    Aug 26 05:25:37.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replication-controller 08/26/23 05:25:37.299
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:37.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:37.325
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-sdlwl" 08/26/23 05:25:37.328
    Aug 26 05:25:37.337: INFO: Get Replication Controller "e2e-rc-sdlwl" to confirm replicas
    Aug 26 05:25:38.341: INFO: Get Replication Controller "e2e-rc-sdlwl" to confirm replicas
    Aug 26 05:25:38.348: INFO: Found 1 replicas for "e2e-rc-sdlwl" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-sdlwl" 08/26/23 05:25:38.348
    STEP: Updating a scale subresource 08/26/23 05:25:38.352
    STEP: Verifying replicas where modified for replication controller "e2e-rc-sdlwl" 08/26/23 05:25:38.36
    Aug 26 05:25:38.361: INFO: Get Replication Controller "e2e-rc-sdlwl" to confirm replicas
    Aug 26 05:25:39.366: INFO: Get Replication Controller "e2e-rc-sdlwl" to confirm replicas
    Aug 26 05:25:39.370: INFO: Found 2 replicas for "e2e-rc-sdlwl" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:39.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9617" for this suite. 08/26/23 05:25:39.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:39.385
Aug 26 05:25:39.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubelet-test 08/26/23 05:25:39.387
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:39.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:39.414
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:43.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9064" for this suite. 08/26/23 05:25:43.454
------------------------------
• [4.075 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:39.385
    Aug 26 05:25:39.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubelet-test 08/26/23 05:25:39.387
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:39.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:39.414
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:43.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9064" for this suite. 08/26/23 05:25:43.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:43.463
Aug 26 05:25:43.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename endpointslice 08/26/23 05:25:43.466
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:43.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:43.489
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 26 05:25:45.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4489" for this suite. 08/26/23 05:25:45.601
------------------------------
• [2.150 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:43.463
    Aug 26 05:25:43.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename endpointslice 08/26/23 05:25:43.466
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:43.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:43.489
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:25:45.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4489" for this suite. 08/26/23 05:25:45.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:25:45.615
Aug 26 05:25:45.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-probe 08/26/23 05:25:45.617
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:45.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:45.672
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70 in namespace container-probe-4292 08/26/23 05:25:45.68
Aug 26 05:25:45.692: INFO: Waiting up to 5m0s for pod "test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70" in namespace "container-probe-4292" to be "not pending"
Aug 26 05:25:45.697: INFO: Pod "test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92905ms
Aug 26 05:25:47.703: INFO: Pod "test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70": Phase="Running", Reason="", readiness=true. Elapsed: 2.010895642s
Aug 26 05:25:47.703: INFO: Pod "test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70" satisfied condition "not pending"
Aug 26 05:25:47.703: INFO: Started pod test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70 in namespace container-probe-4292
STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 05:25:47.703
Aug 26 05:25:47.707: INFO: Initial restart count of pod test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70 is 0
STEP: deleting the pod 08/26/23 05:29:48.447
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 26 05:29:48.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4292" for this suite. 08/26/23 05:29:48.478
------------------------------
• [SLOW TEST] [242.870 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:25:45.615
    Aug 26 05:25:45.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-probe 08/26/23 05:25:45.617
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:25:45.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:25:45.672
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70 in namespace container-probe-4292 08/26/23 05:25:45.68
    Aug 26 05:25:45.692: INFO: Waiting up to 5m0s for pod "test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70" in namespace "container-probe-4292" to be "not pending"
    Aug 26 05:25:45.697: INFO: Pod "test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92905ms
    Aug 26 05:25:47.703: INFO: Pod "test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70": Phase="Running", Reason="", readiness=true. Elapsed: 2.010895642s
    Aug 26 05:25:47.703: INFO: Pod "test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70" satisfied condition "not pending"
    Aug 26 05:25:47.703: INFO: Started pod test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70 in namespace container-probe-4292
    STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 05:25:47.703
    Aug 26 05:25:47.707: INFO: Initial restart count of pod test-webserver-1b5b37f3-bbf8-4ecd-983a-0eecfb551f70 is 0
    STEP: deleting the pod 08/26/23 05:29:48.447
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:29:48.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4292" for this suite. 08/26/23 05:29:48.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:29:48.489
Aug 26 05:29:48.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename lease-test 08/26/23 05:29:48.49
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:48.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:48.518
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Aug 26 05:29:48.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-6818" for this suite. 08/26/23 05:29:48.595
------------------------------
• [0.112 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:29:48.489
    Aug 26 05:29:48.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename lease-test 08/26/23 05:29:48.49
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:48.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:48.518
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:29:48.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-6818" for this suite. 08/26/23 05:29:48.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:29:48.608
Aug 26 05:29:48.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-runtime 08/26/23 05:29:48.609
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:48.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:48.63
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 08/26/23 05:29:48.633
STEP: wait for the container to reach Failed 08/26/23 05:29:48.643
STEP: get the container status 08/26/23 05:29:52.667
STEP: the container should be terminated 08/26/23 05:29:52.671
STEP: the termination message should be set 08/26/23 05:29:52.671
Aug 26 05:29:52.671: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/26/23 05:29:52.671
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 26 05:29:52.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1963" for this suite. 08/26/23 05:29:52.695
------------------------------
• [4.102 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:29:48.608
    Aug 26 05:29:48.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-runtime 08/26/23 05:29:48.609
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:48.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:48.63
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 08/26/23 05:29:48.633
    STEP: wait for the container to reach Failed 08/26/23 05:29:48.643
    STEP: get the container status 08/26/23 05:29:52.667
    STEP: the container should be terminated 08/26/23 05:29:52.671
    STEP: the termination message should be set 08/26/23 05:29:52.671
    Aug 26 05:29:52.671: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/26/23 05:29:52.671
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:29:52.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1963" for this suite. 08/26/23 05:29:52.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:29:52.711
Aug 26 05:29:52.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 05:29:52.712
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:52.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:52.732
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-09fb0c1c-9d41-4d84-8123-f6fa89bcacb8 08/26/23 05:29:52.734
STEP: Creating a pod to test consume secrets 08/26/23 05:29:52.741
Aug 26 05:29:52.758: INFO: Waiting up to 5m0s for pod "pod-secrets-89135759-0de7-4f29-b107-6b655812850c" in namespace "secrets-217" to be "Succeeded or Failed"
Aug 26 05:29:52.762: INFO: Pod "pod-secrets-89135759-0de7-4f29-b107-6b655812850c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.349204ms
Aug 26 05:29:54.768: INFO: Pod "pod-secrets-89135759-0de7-4f29-b107-6b655812850c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01011828s
Aug 26 05:29:56.767: INFO: Pod "pod-secrets-89135759-0de7-4f29-b107-6b655812850c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009293859s
STEP: Saw pod success 08/26/23 05:29:56.767
Aug 26 05:29:56.768: INFO: Pod "pod-secrets-89135759-0de7-4f29-b107-6b655812850c" satisfied condition "Succeeded or Failed"
Aug 26 05:29:56.771: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-secrets-89135759-0de7-4f29-b107-6b655812850c container secret-volume-test: <nil>
STEP: delete the pod 08/26/23 05:29:56.79
Aug 26 05:29:56.811: INFO: Waiting for pod pod-secrets-89135759-0de7-4f29-b107-6b655812850c to disappear
Aug 26 05:29:56.816: INFO: Pod pod-secrets-89135759-0de7-4f29-b107-6b655812850c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 05:29:56.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-217" for this suite. 08/26/23 05:29:56.825
------------------------------
• [4.123 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:29:52.711
    Aug 26 05:29:52.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 05:29:52.712
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:52.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:52.732
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-09fb0c1c-9d41-4d84-8123-f6fa89bcacb8 08/26/23 05:29:52.734
    STEP: Creating a pod to test consume secrets 08/26/23 05:29:52.741
    Aug 26 05:29:52.758: INFO: Waiting up to 5m0s for pod "pod-secrets-89135759-0de7-4f29-b107-6b655812850c" in namespace "secrets-217" to be "Succeeded or Failed"
    Aug 26 05:29:52.762: INFO: Pod "pod-secrets-89135759-0de7-4f29-b107-6b655812850c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.349204ms
    Aug 26 05:29:54.768: INFO: Pod "pod-secrets-89135759-0de7-4f29-b107-6b655812850c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01011828s
    Aug 26 05:29:56.767: INFO: Pod "pod-secrets-89135759-0de7-4f29-b107-6b655812850c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009293859s
    STEP: Saw pod success 08/26/23 05:29:56.767
    Aug 26 05:29:56.768: INFO: Pod "pod-secrets-89135759-0de7-4f29-b107-6b655812850c" satisfied condition "Succeeded or Failed"
    Aug 26 05:29:56.771: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-secrets-89135759-0de7-4f29-b107-6b655812850c container secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 05:29:56.79
    Aug 26 05:29:56.811: INFO: Waiting for pod pod-secrets-89135759-0de7-4f29-b107-6b655812850c to disappear
    Aug 26 05:29:56.816: INFO: Pod pod-secrets-89135759-0de7-4f29-b107-6b655812850c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:29:56.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-217" for this suite. 08/26/23 05:29:56.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:29:56.835
Aug 26 05:29:56.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 05:29:56.836
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:56.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:56.857
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Aug 26 05:29:56.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: creating the pod 08/26/23 05:29:56.863
STEP: submitting the pod to kubernetes 08/26/23 05:29:56.863
Aug 26 05:29:56.873: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a" in namespace "pods-6062" to be "running and ready"
Aug 26 05:29:56.877: INFO: Pod "pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.840078ms
Aug 26 05:29:56.877: INFO: The phase of Pod pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:29:58.883: INFO: Pod "pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009839486s
Aug 26 05:29:58.883: INFO: The phase of Pod pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a is Running (Ready = true)
Aug 26 05:29:58.883: INFO: Pod "pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 26 05:29:58.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6062" for this suite. 08/26/23 05:29:59.003
------------------------------
• [2.178 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:29:56.835
    Aug 26 05:29:56.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 05:29:56.836
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:56.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:56.857
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Aug 26 05:29:56.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: creating the pod 08/26/23 05:29:56.863
    STEP: submitting the pod to kubernetes 08/26/23 05:29:56.863
    Aug 26 05:29:56.873: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a" in namespace "pods-6062" to be "running and ready"
    Aug 26 05:29:56.877: INFO: Pod "pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.840078ms
    Aug 26 05:29:56.877: INFO: The phase of Pod pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:29:58.883: INFO: Pod "pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009839486s
    Aug 26 05:29:58.883: INFO: The phase of Pod pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a is Running (Ready = true)
    Aug 26 05:29:58.883: INFO: Pod "pod-exec-websocket-e081c9ea-4bab-4f24-a6bf-4522bfca937a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:29:58.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6062" for this suite. 08/26/23 05:29:59.003
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:29:59.013
Aug 26 05:29:59.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename cronjob 08/26/23 05:29:59.014
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:59.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:59.044
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 08/26/23 05:29:59.047
STEP: creating 08/26/23 05:29:59.048
STEP: getting 08/26/23 05:29:59.054
STEP: listing 08/26/23 05:29:59.058
STEP: watching 08/26/23 05:29:59.062
Aug 26 05:29:59.062: INFO: starting watch
STEP: cluster-wide listing 08/26/23 05:29:59.063
STEP: cluster-wide watching 08/26/23 05:29:59.066
Aug 26 05:29:59.067: INFO: starting watch
STEP: patching 08/26/23 05:29:59.068
STEP: updating 08/26/23 05:29:59.076
Aug 26 05:29:59.085: INFO: waiting for watch events with expected annotations
Aug 26 05:29:59.085: INFO: saw patched and updated annotations
STEP: patching /status 08/26/23 05:29:59.086
STEP: updating /status 08/26/23 05:29:59.094
STEP: get /status 08/26/23 05:29:59.102
STEP: deleting 08/26/23 05:29:59.106
STEP: deleting a collection 08/26/23 05:29:59.126
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 26 05:29:59.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2354" for this suite. 08/26/23 05:29:59.146
------------------------------
• [0.146 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:29:59.013
    Aug 26 05:29:59.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename cronjob 08/26/23 05:29:59.014
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:59.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:59.044
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 08/26/23 05:29:59.047
    STEP: creating 08/26/23 05:29:59.048
    STEP: getting 08/26/23 05:29:59.054
    STEP: listing 08/26/23 05:29:59.058
    STEP: watching 08/26/23 05:29:59.062
    Aug 26 05:29:59.062: INFO: starting watch
    STEP: cluster-wide listing 08/26/23 05:29:59.063
    STEP: cluster-wide watching 08/26/23 05:29:59.066
    Aug 26 05:29:59.067: INFO: starting watch
    STEP: patching 08/26/23 05:29:59.068
    STEP: updating 08/26/23 05:29:59.076
    Aug 26 05:29:59.085: INFO: waiting for watch events with expected annotations
    Aug 26 05:29:59.085: INFO: saw patched and updated annotations
    STEP: patching /status 08/26/23 05:29:59.086
    STEP: updating /status 08/26/23 05:29:59.094
    STEP: get /status 08/26/23 05:29:59.102
    STEP: deleting 08/26/23 05:29:59.106
    STEP: deleting a collection 08/26/23 05:29:59.126
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:29:59.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2354" for this suite. 08/26/23 05:29:59.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:29:59.16
Aug 26 05:29:59.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename watch 08/26/23 05:29:59.162
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:59.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:59.19
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 08/26/23 05:29:59.194
STEP: starting a background goroutine to produce watch events 08/26/23 05:29:59.199
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/26/23 05:29:59.199
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 26 05:30:01.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3554" for this suite. 08/26/23 05:30:02.023
------------------------------
• [2.917 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:29:59.16
    Aug 26 05:29:59.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename watch 08/26/23 05:29:59.162
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:29:59.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:29:59.19
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 08/26/23 05:29:59.194
    STEP: starting a background goroutine to produce watch events 08/26/23 05:29:59.199
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/26/23 05:29:59.199
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:30:01.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3554" for this suite. 08/26/23 05:30:02.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:30:02.078
Aug 26 05:30:02.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:30:02.08
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:30:02.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:30:02.107
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:30:02.11
Aug 26 05:30:02.123: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38" in namespace "downward-api-591" to be "Succeeded or Failed"
Aug 26 05:30:02.127: INFO: Pod "downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.855535ms
Aug 26 05:30:04.132: INFO: Pod "downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008467063s
Aug 26 05:30:06.132: INFO: Pod "downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008847963s
STEP: Saw pod success 08/26/23 05:30:06.132
Aug 26 05:30:06.132: INFO: Pod "downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38" satisfied condition "Succeeded or Failed"
Aug 26 05:30:06.136: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38 container client-container: <nil>
STEP: delete the pod 08/26/23 05:30:06.153
Aug 26 05:30:06.168: INFO: Waiting for pod downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38 to disappear
Aug 26 05:30:06.172: INFO: Pod downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 05:30:06.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-591" for this suite. 08/26/23 05:30:06.183
------------------------------
• [4.114 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:30:02.078
    Aug 26 05:30:02.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:30:02.08
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:30:02.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:30:02.107
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:30:02.11
    Aug 26 05:30:02.123: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38" in namespace "downward-api-591" to be "Succeeded or Failed"
    Aug 26 05:30:02.127: INFO: Pod "downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.855535ms
    Aug 26 05:30:04.132: INFO: Pod "downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008467063s
    Aug 26 05:30:06.132: INFO: Pod "downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008847963s
    STEP: Saw pod success 08/26/23 05:30:06.132
    Aug 26 05:30:06.132: INFO: Pod "downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38" satisfied condition "Succeeded or Failed"
    Aug 26 05:30:06.136: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38 container client-container: <nil>
    STEP: delete the pod 08/26/23 05:30:06.153
    Aug 26 05:30:06.168: INFO: Waiting for pod downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38 to disappear
    Aug 26 05:30:06.172: INFO: Pod downwardapi-volume-b6e0437b-044c-4398-b9b8-d8814430bc38 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:30:06.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-591" for this suite. 08/26/23 05:30:06.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:30:06.193
Aug 26 05:30:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replication-controller 08/26/23 05:30:06.196
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:30:06.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:30:06.219
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a 08/26/23 05:30:06.225
Aug 26 05:30:06.237: INFO: Pod name my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a: Found 0 pods out of 1
Aug 26 05:30:11.244: INFO: Pod name my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a: Found 1 pods out of 1
Aug 26 05:30:11.244: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a" are running
Aug 26 05:30:11.244: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb" in namespace "replication-controller-6185" to be "running"
Aug 26 05:30:11.248: INFO: Pod "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb": Phase="Running", Reason="", readiness=true. Elapsed: 4.12094ms
Aug 26 05:30:11.248: INFO: Pod "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb" satisfied condition "running"
Aug 26 05:30:11.248: INFO: Pod "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-26 05:30:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-26 05:30:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-26 05:30:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-26 05:30:06 +0000 UTC Reason: Message:}])
Aug 26 05:30:11.248: INFO: Trying to dial the pod
Aug 26 05:30:16.262: INFO: Controller my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a: Got expected result from replica 1 [my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb]: "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 26 05:30:16.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6185" for this suite. 08/26/23 05:30:16.27
------------------------------
• [SLOW TEST] [10.086 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:30:06.193
    Aug 26 05:30:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replication-controller 08/26/23 05:30:06.196
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:30:06.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:30:06.219
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a 08/26/23 05:30:06.225
    Aug 26 05:30:06.237: INFO: Pod name my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a: Found 0 pods out of 1
    Aug 26 05:30:11.244: INFO: Pod name my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a: Found 1 pods out of 1
    Aug 26 05:30:11.244: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a" are running
    Aug 26 05:30:11.244: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb" in namespace "replication-controller-6185" to be "running"
    Aug 26 05:30:11.248: INFO: Pod "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb": Phase="Running", Reason="", readiness=true. Elapsed: 4.12094ms
    Aug 26 05:30:11.248: INFO: Pod "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb" satisfied condition "running"
    Aug 26 05:30:11.248: INFO: Pod "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-26 05:30:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-26 05:30:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-26 05:30:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-26 05:30:06 +0000 UTC Reason: Message:}])
    Aug 26 05:30:11.248: INFO: Trying to dial the pod
    Aug 26 05:30:16.262: INFO: Controller my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a: Got expected result from replica 1 [my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb]: "my-hostname-basic-1a8a0260-026b-48fe-8a39-b9cd4ca38a5a-wgnxb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:30:16.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6185" for this suite. 08/26/23 05:30:16.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:30:16.282
Aug 26 05:30:16.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-probe 08/26/23 05:30:16.283
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:30:16.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:30:16.304
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 in namespace container-probe-5624 08/26/23 05:30:16.306
Aug 26 05:30:16.317: INFO: Waiting up to 5m0s for pod "liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1" in namespace "container-probe-5624" to be "not pending"
Aug 26 05:30:16.323: INFO: Pod "liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.916104ms
Aug 26 05:30:18.330: INFO: Pod "liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012599284s
Aug 26 05:30:18.330: INFO: Pod "liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1" satisfied condition "not pending"
Aug 26 05:30:18.331: INFO: Started pod liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 in namespace container-probe-5624
STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 05:30:18.331
Aug 26 05:30:18.336: INFO: Initial restart count of pod liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is 0
Aug 26 05:30:38.402: INFO: Restart count of pod container-probe-5624/liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is now 1 (20.065348286s elapsed)
Aug 26 05:30:58.461: INFO: Restart count of pod container-probe-5624/liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is now 2 (40.124442259s elapsed)
Aug 26 05:31:18.517: INFO: Restart count of pod container-probe-5624/liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is now 3 (1m0.180478744s elapsed)
Aug 26 05:31:38.581: INFO: Restart count of pod container-probe-5624/liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is now 4 (1m20.2443259s elapsed)
Aug 26 05:32:40.764: INFO: Restart count of pod container-probe-5624/liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is now 5 (2m22.427998585s elapsed)
STEP: deleting the pod 08/26/23 05:32:40.764
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 26 05:32:40.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5624" for this suite. 08/26/23 05:32:40.787
------------------------------
• [SLOW TEST] [144.626 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:30:16.282
    Aug 26 05:30:16.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-probe 08/26/23 05:30:16.283
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:30:16.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:30:16.304
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 in namespace container-probe-5624 08/26/23 05:30:16.306
    Aug 26 05:30:16.317: INFO: Waiting up to 5m0s for pod "liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1" in namespace "container-probe-5624" to be "not pending"
    Aug 26 05:30:16.323: INFO: Pod "liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.916104ms
    Aug 26 05:30:18.330: INFO: Pod "liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012599284s
    Aug 26 05:30:18.330: INFO: Pod "liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1" satisfied condition "not pending"
    Aug 26 05:30:18.331: INFO: Started pod liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 in namespace container-probe-5624
    STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 05:30:18.331
    Aug 26 05:30:18.336: INFO: Initial restart count of pod liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is 0
    Aug 26 05:30:38.402: INFO: Restart count of pod container-probe-5624/liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is now 1 (20.065348286s elapsed)
    Aug 26 05:30:58.461: INFO: Restart count of pod container-probe-5624/liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is now 2 (40.124442259s elapsed)
    Aug 26 05:31:18.517: INFO: Restart count of pod container-probe-5624/liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is now 3 (1m0.180478744s elapsed)
    Aug 26 05:31:38.581: INFO: Restart count of pod container-probe-5624/liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is now 4 (1m20.2443259s elapsed)
    Aug 26 05:32:40.764: INFO: Restart count of pod container-probe-5624/liveness-54afa69b-c1e6-4076-bb40-f7d3b4eeefd1 is now 5 (2m22.427998585s elapsed)
    STEP: deleting the pod 08/26/23 05:32:40.764
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:32:40.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5624" for this suite. 08/26/23 05:32:40.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:32:40.913
Aug 26 05:32:40.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename dns 08/26/23 05:32:40.914
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:32:40.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:32:40.949
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 08/26/23 05:32:40.959
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
 08/26/23 05:32:40.969
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
 08/26/23 05:32:40.969
STEP: creating a pod to probe DNS 08/26/23 05:32:40.969
STEP: submitting the pod to kubernetes 08/26/23 05:32:40.969
Aug 26 05:32:41.091: INFO: Waiting up to 15m0s for pod "dns-test-56046d70-ab00-46fc-ada9-813dc7adcad7" in namespace "dns-872" to be "running"
Aug 26 05:32:41.099: INFO: Pod "dns-test-56046d70-ab00-46fc-ada9-813dc7adcad7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.848548ms
Aug 26 05:32:43.104: INFO: Pod "dns-test-56046d70-ab00-46fc-ada9-813dc7adcad7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01289163s
Aug 26 05:32:43.104: INFO: Pod "dns-test-56046d70-ab00-46fc-ada9-813dc7adcad7" satisfied condition "running"
STEP: retrieving the pod 08/26/23 05:32:43.104
STEP: looking for the results for each expected name from probers 08/26/23 05:32:43.108
Aug 26 05:32:43.117: INFO: DNS probes using dns-test-56046d70-ab00-46fc-ada9-813dc7adcad7 succeeded

STEP: deleting the pod 08/26/23 05:32:43.117
STEP: changing the externalName to bar.example.com 08/26/23 05:32:43.129
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
 08/26/23 05:32:43.14
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
 08/26/23 05:32:43.14
STEP: creating a second pod to probe DNS 08/26/23 05:32:43.14
STEP: submitting the pod to kubernetes 08/26/23 05:32:43.14
Aug 26 05:32:43.155: INFO: Waiting up to 15m0s for pod "dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc" in namespace "dns-872" to be "running"
Aug 26 05:32:43.161: INFO: Pod "dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.894204ms
Aug 26 05:32:45.167: INFO: Pod "dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01211728s
Aug 26 05:32:47.166: INFO: Pod "dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc": Phase="Running", Reason="", readiness=true. Elapsed: 4.011165327s
Aug 26 05:32:47.166: INFO: Pod "dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc" satisfied condition "running"
STEP: retrieving the pod 08/26/23 05:32:47.166
STEP: looking for the results for each expected name from probers 08/26/23 05:32:47.171
Aug 26 05:32:47.176: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:32:47.182: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:32:47.182: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

Aug 26 05:32:52.188: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:32:52.193: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:32:52.193: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

Aug 26 05:32:57.192: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:32:57.198: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:32:57.200: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

Aug 26 05:33:02.188: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:33:02.193: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:33:02.193: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

Aug 26 05:33:07.198: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:33:07.202: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:33:07.202: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

Aug 26 05:33:12.188: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:33:12.192: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 26 05:33:12.192: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

Aug 26 05:33:17.193: INFO: DNS probes using dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc succeeded

STEP: deleting the pod 08/26/23 05:33:17.193
STEP: changing the service to type=ClusterIP 08/26/23 05:33:17.21
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
 08/26/23 05:33:17.227
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
 08/26/23 05:33:17.228
STEP: creating a third pod to probe DNS 08/26/23 05:33:17.228
STEP: submitting the pod to kubernetes 08/26/23 05:33:17.232
Aug 26 05:33:17.244: INFO: Waiting up to 15m0s for pod "dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4" in namespace "dns-872" to be "running"
Aug 26 05:33:17.255: INFO: Pod "dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.025201ms
Aug 26 05:33:19.262: INFO: Pod "dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017326676s
Aug 26 05:33:21.260: INFO: Pod "dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4": Phase="Running", Reason="", readiness=true. Elapsed: 4.015766381s
Aug 26 05:33:21.260: INFO: Pod "dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4" satisfied condition "running"
STEP: retrieving the pod 08/26/23 05:33:21.26
STEP: looking for the results for each expected name from probers 08/26/23 05:33:21.264
Aug 26 05:33:21.274: INFO: DNS probes using dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4 succeeded

STEP: deleting the pod 08/26/23 05:33:21.274
STEP: deleting the test externalName service 08/26/23 05:33:21.292
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 26 05:33:21.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-872" for this suite. 08/26/23 05:33:21.353
------------------------------
• [SLOW TEST] [40.493 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:32:40.913
    Aug 26 05:32:40.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename dns 08/26/23 05:32:40.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:32:40.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:32:40.949
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 08/26/23 05:32:40.959
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
     08/26/23 05:32:40.969
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
     08/26/23 05:32:40.969
    STEP: creating a pod to probe DNS 08/26/23 05:32:40.969
    STEP: submitting the pod to kubernetes 08/26/23 05:32:40.969
    Aug 26 05:32:41.091: INFO: Waiting up to 15m0s for pod "dns-test-56046d70-ab00-46fc-ada9-813dc7adcad7" in namespace "dns-872" to be "running"
    Aug 26 05:32:41.099: INFO: Pod "dns-test-56046d70-ab00-46fc-ada9-813dc7adcad7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.848548ms
    Aug 26 05:32:43.104: INFO: Pod "dns-test-56046d70-ab00-46fc-ada9-813dc7adcad7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01289163s
    Aug 26 05:32:43.104: INFO: Pod "dns-test-56046d70-ab00-46fc-ada9-813dc7adcad7" satisfied condition "running"
    STEP: retrieving the pod 08/26/23 05:32:43.104
    STEP: looking for the results for each expected name from probers 08/26/23 05:32:43.108
    Aug 26 05:32:43.117: INFO: DNS probes using dns-test-56046d70-ab00-46fc-ada9-813dc7adcad7 succeeded

    STEP: deleting the pod 08/26/23 05:32:43.117
    STEP: changing the externalName to bar.example.com 08/26/23 05:32:43.129
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
     08/26/23 05:32:43.14
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
     08/26/23 05:32:43.14
    STEP: creating a second pod to probe DNS 08/26/23 05:32:43.14
    STEP: submitting the pod to kubernetes 08/26/23 05:32:43.14
    Aug 26 05:32:43.155: INFO: Waiting up to 15m0s for pod "dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc" in namespace "dns-872" to be "running"
    Aug 26 05:32:43.161: INFO: Pod "dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.894204ms
    Aug 26 05:32:45.167: INFO: Pod "dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01211728s
    Aug 26 05:32:47.166: INFO: Pod "dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc": Phase="Running", Reason="", readiness=true. Elapsed: 4.011165327s
    Aug 26 05:32:47.166: INFO: Pod "dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc" satisfied condition "running"
    STEP: retrieving the pod 08/26/23 05:32:47.166
    STEP: looking for the results for each expected name from probers 08/26/23 05:32:47.171
    Aug 26 05:32:47.176: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:32:47.182: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:32:47.182: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

    Aug 26 05:32:52.188: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:32:52.193: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:32:52.193: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

    Aug 26 05:32:57.192: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:32:57.198: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:32:57.200: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

    Aug 26 05:33:02.188: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:33:02.193: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:33:02.193: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

    Aug 26 05:33:07.198: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:33:07.202: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:33:07.202: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

    Aug 26 05:33:12.188: INFO: File wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:33:12.192: INFO: File jessie_udp@dns-test-service-3.dns-872.svc.cluster.local from pod  dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 26 05:33:12.192: INFO: Lookups using dns-872/dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc failed for: [wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local jessie_udp@dns-test-service-3.dns-872.svc.cluster.local]

    Aug 26 05:33:17.193: INFO: DNS probes using dns-test-c7ed2aeb-0170-4e61-a24b-91f04c5b66bc succeeded

    STEP: deleting the pod 08/26/23 05:33:17.193
    STEP: changing the service to type=ClusterIP 08/26/23 05:33:17.21
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
     08/26/23 05:33:17.227
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-872.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-872.svc.cluster.local; sleep 1; done
     08/26/23 05:33:17.228
    STEP: creating a third pod to probe DNS 08/26/23 05:33:17.228
    STEP: submitting the pod to kubernetes 08/26/23 05:33:17.232
    Aug 26 05:33:17.244: INFO: Waiting up to 15m0s for pod "dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4" in namespace "dns-872" to be "running"
    Aug 26 05:33:17.255: INFO: Pod "dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.025201ms
    Aug 26 05:33:19.262: INFO: Pod "dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017326676s
    Aug 26 05:33:21.260: INFO: Pod "dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4": Phase="Running", Reason="", readiness=true. Elapsed: 4.015766381s
    Aug 26 05:33:21.260: INFO: Pod "dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4" satisfied condition "running"
    STEP: retrieving the pod 08/26/23 05:33:21.26
    STEP: looking for the results for each expected name from probers 08/26/23 05:33:21.264
    Aug 26 05:33:21.274: INFO: DNS probes using dns-test-7eb977e7-8420-4ac1-aff4-3f8f7956acd4 succeeded

    STEP: deleting the pod 08/26/23 05:33:21.274
    STEP: deleting the test externalName service 08/26/23 05:33:21.292
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:33:21.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-872" for this suite. 08/26/23 05:33:21.353
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:33:21.407
Aug 26 05:33:21.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-lifecycle-hook 08/26/23 05:33:21.408
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:33:21.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:33:21.441
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/26/23 05:33:21.456
Aug 26 05:33:21.466: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5688" to be "running and ready"
Aug 26 05:33:21.475: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.96644ms
Aug 26 05:33:21.475: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:33:23.480: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014383878s
Aug 26 05:33:23.480: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 26 05:33:23.480: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 08/26/23 05:33:23.484
Aug 26 05:33:23.491: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5688" to be "running and ready"
Aug 26 05:33:23.497: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.432673ms
Aug 26 05:33:23.498: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:33:25.503: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011735221s
Aug 26 05:33:25.503: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Aug 26 05:33:25.503: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/26/23 05:33:25.507
STEP: delete the pod with lifecycle hook 08/26/23 05:33:25.526
Aug 26 05:33:25.648: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 26 05:33:25.653: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 26 05:33:27.653: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 26 05:33:27.665: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 26 05:33:27.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5688" for this suite. 08/26/23 05:33:27.674
------------------------------
• [SLOW TEST] [6.277 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:33:21.407
    Aug 26 05:33:21.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/26/23 05:33:21.408
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:33:21.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:33:21.441
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/26/23 05:33:21.456
    Aug 26 05:33:21.466: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5688" to be "running and ready"
    Aug 26 05:33:21.475: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.96644ms
    Aug 26 05:33:21.475: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:33:23.480: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014383878s
    Aug 26 05:33:23.480: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 26 05:33:23.480: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 08/26/23 05:33:23.484
    Aug 26 05:33:23.491: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5688" to be "running and ready"
    Aug 26 05:33:23.497: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.432673ms
    Aug 26 05:33:23.498: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:33:25.503: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011735221s
    Aug 26 05:33:25.503: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Aug 26 05:33:25.503: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/26/23 05:33:25.507
    STEP: delete the pod with lifecycle hook 08/26/23 05:33:25.526
    Aug 26 05:33:25.648: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 26 05:33:25.653: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 26 05:33:27.653: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 26 05:33:27.665: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:33:27.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5688" for this suite. 08/26/23 05:33:27.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:33:27.685
Aug 26 05:33:27.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 05:33:27.686
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:33:27.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:33:27.709
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-7339 08/26/23 05:33:27.712
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7339 to expose endpoints map[] 08/26/23 05:33:27.733
Aug 26 05:33:27.747: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Aug 26 05:33:28.756: INFO: successfully validated that service multi-endpoint-test in namespace services-7339 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7339 08/26/23 05:33:28.756
Aug 26 05:33:28.766: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7339" to be "running and ready"
Aug 26 05:33:28.770: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085119ms
Aug 26 05:33:28.770: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:33:30.778: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012874867s
Aug 26 05:33:30.778: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 26 05:33:30.778: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7339 to expose endpoints map[pod1:[100]] 08/26/23 05:33:30.785
Aug 26 05:33:30.805: INFO: successfully validated that service multi-endpoint-test in namespace services-7339 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7339 08/26/23 05:33:30.805
Aug 26 05:33:30.813: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7339" to be "running and ready"
Aug 26 05:33:30.817: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.552763ms
Aug 26 05:33:30.817: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:33:32.822: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008867206s
Aug 26 05:33:32.822: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 26 05:33:32.822: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7339 to expose endpoints map[pod1:[100] pod2:[101]] 08/26/23 05:33:32.832
Aug 26 05:33:32.853: INFO: successfully validated that service multi-endpoint-test in namespace services-7339 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 08/26/23 05:33:32.853
Aug 26 05:33:32.853: INFO: Creating new exec pod
Aug 26 05:33:32.860: INFO: Waiting up to 5m0s for pod "execpodwgz29" in namespace "services-7339" to be "running"
Aug 26 05:33:32.869: INFO: Pod "execpodwgz29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.72817ms
Aug 26 05:33:34.875: INFO: Pod "execpodwgz29": Phase="Running", Reason="", readiness=true. Elapsed: 2.015229163s
Aug 26 05:33:34.875: INFO: Pod "execpodwgz29" satisfied condition "running"
Aug 26 05:33:35.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7339 exec execpodwgz29 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Aug 26 05:33:38.571: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 26 05:33:38.571: INFO: stdout: ""
Aug 26 05:33:38.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7339 exec execpodwgz29 -- /bin/sh -x -c nc -v -z -w 2 10.21.225.120 80'
Aug 26 05:33:38.736: INFO: stderr: "+ nc -v -z -w 2 10.21.225.120 80\nConnection to 10.21.225.120 80 port [tcp/http] succeeded!\n"
Aug 26 05:33:38.736: INFO: stdout: ""
Aug 26 05:33:38.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7339 exec execpodwgz29 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Aug 26 05:33:38.943: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 26 05:33:38.943: INFO: stdout: ""
Aug 26 05:33:38.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7339 exec execpodwgz29 -- /bin/sh -x -c nc -v -z -w 2 10.21.225.120 81'
Aug 26 05:33:39.163: INFO: stderr: "+ nc -v -z -w 2 10.21.225.120 81\nConnection to 10.21.225.120 81 port [tcp/*] succeeded!\n"
Aug 26 05:33:39.163: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7339 08/26/23 05:33:39.163
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7339 to expose endpoints map[pod2:[101]] 08/26/23 05:33:39.194
Aug 26 05:33:39.213: INFO: successfully validated that service multi-endpoint-test in namespace services-7339 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7339 08/26/23 05:33:39.213
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7339 to expose endpoints map[] 08/26/23 05:33:39.243
Aug 26 05:33:39.261: INFO: successfully validated that service multi-endpoint-test in namespace services-7339 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 05:33:39.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7339" for this suite. 08/26/23 05:33:39.307
------------------------------
• [SLOW TEST] [11.645 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:33:27.685
    Aug 26 05:33:27.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 05:33:27.686
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:33:27.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:33:27.709
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-7339 08/26/23 05:33:27.712
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7339 to expose endpoints map[] 08/26/23 05:33:27.733
    Aug 26 05:33:27.747: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Aug 26 05:33:28.756: INFO: successfully validated that service multi-endpoint-test in namespace services-7339 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7339 08/26/23 05:33:28.756
    Aug 26 05:33:28.766: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7339" to be "running and ready"
    Aug 26 05:33:28.770: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085119ms
    Aug 26 05:33:28.770: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:33:30.778: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012874867s
    Aug 26 05:33:30.778: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 26 05:33:30.778: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7339 to expose endpoints map[pod1:[100]] 08/26/23 05:33:30.785
    Aug 26 05:33:30.805: INFO: successfully validated that service multi-endpoint-test in namespace services-7339 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7339 08/26/23 05:33:30.805
    Aug 26 05:33:30.813: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7339" to be "running and ready"
    Aug 26 05:33:30.817: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.552763ms
    Aug 26 05:33:30.817: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:33:32.822: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008867206s
    Aug 26 05:33:32.822: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 26 05:33:32.822: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7339 to expose endpoints map[pod1:[100] pod2:[101]] 08/26/23 05:33:32.832
    Aug 26 05:33:32.853: INFO: successfully validated that service multi-endpoint-test in namespace services-7339 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 08/26/23 05:33:32.853
    Aug 26 05:33:32.853: INFO: Creating new exec pod
    Aug 26 05:33:32.860: INFO: Waiting up to 5m0s for pod "execpodwgz29" in namespace "services-7339" to be "running"
    Aug 26 05:33:32.869: INFO: Pod "execpodwgz29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.72817ms
    Aug 26 05:33:34.875: INFO: Pod "execpodwgz29": Phase="Running", Reason="", readiness=true. Elapsed: 2.015229163s
    Aug 26 05:33:34.875: INFO: Pod "execpodwgz29" satisfied condition "running"
    Aug 26 05:33:35.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7339 exec execpodwgz29 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Aug 26 05:33:38.571: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Aug 26 05:33:38.571: INFO: stdout: ""
    Aug 26 05:33:38.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7339 exec execpodwgz29 -- /bin/sh -x -c nc -v -z -w 2 10.21.225.120 80'
    Aug 26 05:33:38.736: INFO: stderr: "+ nc -v -z -w 2 10.21.225.120 80\nConnection to 10.21.225.120 80 port [tcp/http] succeeded!\n"
    Aug 26 05:33:38.736: INFO: stdout: ""
    Aug 26 05:33:38.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7339 exec execpodwgz29 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Aug 26 05:33:38.943: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Aug 26 05:33:38.943: INFO: stdout: ""
    Aug 26 05:33:38.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7339 exec execpodwgz29 -- /bin/sh -x -c nc -v -z -w 2 10.21.225.120 81'
    Aug 26 05:33:39.163: INFO: stderr: "+ nc -v -z -w 2 10.21.225.120 81\nConnection to 10.21.225.120 81 port [tcp/*] succeeded!\n"
    Aug 26 05:33:39.163: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7339 08/26/23 05:33:39.163
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7339 to expose endpoints map[pod2:[101]] 08/26/23 05:33:39.194
    Aug 26 05:33:39.213: INFO: successfully validated that service multi-endpoint-test in namespace services-7339 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7339 08/26/23 05:33:39.213
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7339 to expose endpoints map[] 08/26/23 05:33:39.243
    Aug 26 05:33:39.261: INFO: successfully validated that service multi-endpoint-test in namespace services-7339 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:33:39.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7339" for this suite. 08/26/23 05:33:39.307
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:33:39.33
Aug 26 05:33:39.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replication-controller 08/26/23 05:33:39.331
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:33:39.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:33:39.356
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Aug 26 05:33:39.360: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/26/23 05:33:40.376
STEP: Checking rc "condition-test" has the desired failure condition set 08/26/23 05:33:40.384
STEP: Scaling down rc "condition-test" to satisfy pod quota 08/26/23 05:33:41.397
Aug 26 05:33:41.413: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 08/26/23 05:33:41.413
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 26 05:33:42.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1097" for this suite. 08/26/23 05:33:42.44
------------------------------
• [3.122 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:33:39.33
    Aug 26 05:33:39.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replication-controller 08/26/23 05:33:39.331
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:33:39.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:33:39.356
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Aug 26 05:33:39.360: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/26/23 05:33:40.376
    STEP: Checking rc "condition-test" has the desired failure condition set 08/26/23 05:33:40.384
    STEP: Scaling down rc "condition-test" to satisfy pod quota 08/26/23 05:33:41.397
    Aug 26 05:33:41.413: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 08/26/23 05:33:41.413
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:33:42.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1097" for this suite. 08/26/23 05:33:42.44
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:33:42.452
Aug 26 05:33:42.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 05:33:42.453
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:33:42.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:33:42.48
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-ctmsq" 08/26/23 05:33:42.487
Aug 26 05:33:42.502: INFO: Resource quota "e2e-rq-status-ctmsq" reports spec: hard cpu limit of 500m
Aug 26 05:33:42.502: INFO: Resource quota "e2e-rq-status-ctmsq" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-ctmsq" /status 08/26/23 05:33:42.502
STEP: Confirm /status for "e2e-rq-status-ctmsq" resourceQuota via watch 08/26/23 05:33:42.515
Aug 26 05:33:42.517: INFO: observed resourceQuota "e2e-rq-status-ctmsq" in namespace "resourcequota-6573" with hard status: v1.ResourceList(nil)
Aug 26 05:33:42.517: INFO: Found resourceQuota "e2e-rq-status-ctmsq" in namespace "resourcequota-6573" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 26 05:33:42.517: INFO: ResourceQuota "e2e-rq-status-ctmsq" /status was updated
STEP: Patching hard spec values for cpu & memory 08/26/23 05:33:42.521
Aug 26 05:33:42.532: INFO: Resource quota "e2e-rq-status-ctmsq" reports spec: hard cpu limit of 1
Aug 26 05:33:42.532: INFO: Resource quota "e2e-rq-status-ctmsq" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-ctmsq" /status 08/26/23 05:33:42.532
STEP: Confirm /status for "e2e-rq-status-ctmsq" resourceQuota via watch 08/26/23 05:33:42.54
Aug 26 05:33:42.547: INFO: observed resourceQuota "e2e-rq-status-ctmsq" in namespace "resourcequota-6573" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 26 05:33:42.547: INFO: Found resourceQuota "e2e-rq-status-ctmsq" in namespace "resourcequota-6573" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Aug 26 05:33:42.547: INFO: ResourceQuota "e2e-rq-status-ctmsq" /status was patched
STEP: Get "e2e-rq-status-ctmsq" /status 08/26/23 05:33:42.547
Aug 26 05:33:42.553: INFO: Resourcequota "e2e-rq-status-ctmsq" reports status: hard cpu of 1
Aug 26 05:33:42.553: INFO: Resourcequota "e2e-rq-status-ctmsq" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-ctmsq" /status before checking Spec is unchanged 08/26/23 05:33:42.559
Aug 26 05:33:42.566: INFO: Resourcequota "e2e-rq-status-ctmsq" reports status: hard cpu of 2
Aug 26 05:33:42.566: INFO: Resourcequota "e2e-rq-status-ctmsq" reports status: hard memory of 2Gi
Aug 26 05:33:42.567: INFO: Found resourceQuota "e2e-rq-status-ctmsq" in namespace "resourcequota-6573" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Aug 26 05:36:17.578: INFO: ResourceQuota "e2e-rq-status-ctmsq" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 05:36:17.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6573" for this suite. 08/26/23 05:36:17.598
------------------------------
• [SLOW TEST] [155.155 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:33:42.452
    Aug 26 05:33:42.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 05:33:42.453
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:33:42.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:33:42.48
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-ctmsq" 08/26/23 05:33:42.487
    Aug 26 05:33:42.502: INFO: Resource quota "e2e-rq-status-ctmsq" reports spec: hard cpu limit of 500m
    Aug 26 05:33:42.502: INFO: Resource quota "e2e-rq-status-ctmsq" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-ctmsq" /status 08/26/23 05:33:42.502
    STEP: Confirm /status for "e2e-rq-status-ctmsq" resourceQuota via watch 08/26/23 05:33:42.515
    Aug 26 05:33:42.517: INFO: observed resourceQuota "e2e-rq-status-ctmsq" in namespace "resourcequota-6573" with hard status: v1.ResourceList(nil)
    Aug 26 05:33:42.517: INFO: Found resourceQuota "e2e-rq-status-ctmsq" in namespace "resourcequota-6573" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 26 05:33:42.517: INFO: ResourceQuota "e2e-rq-status-ctmsq" /status was updated
    STEP: Patching hard spec values for cpu & memory 08/26/23 05:33:42.521
    Aug 26 05:33:42.532: INFO: Resource quota "e2e-rq-status-ctmsq" reports spec: hard cpu limit of 1
    Aug 26 05:33:42.532: INFO: Resource quota "e2e-rq-status-ctmsq" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-ctmsq" /status 08/26/23 05:33:42.532
    STEP: Confirm /status for "e2e-rq-status-ctmsq" resourceQuota via watch 08/26/23 05:33:42.54
    Aug 26 05:33:42.547: INFO: observed resourceQuota "e2e-rq-status-ctmsq" in namespace "resourcequota-6573" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 26 05:33:42.547: INFO: Found resourceQuota "e2e-rq-status-ctmsq" in namespace "resourcequota-6573" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Aug 26 05:33:42.547: INFO: ResourceQuota "e2e-rq-status-ctmsq" /status was patched
    STEP: Get "e2e-rq-status-ctmsq" /status 08/26/23 05:33:42.547
    Aug 26 05:33:42.553: INFO: Resourcequota "e2e-rq-status-ctmsq" reports status: hard cpu of 1
    Aug 26 05:33:42.553: INFO: Resourcequota "e2e-rq-status-ctmsq" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-ctmsq" /status before checking Spec is unchanged 08/26/23 05:33:42.559
    Aug 26 05:33:42.566: INFO: Resourcequota "e2e-rq-status-ctmsq" reports status: hard cpu of 2
    Aug 26 05:33:42.566: INFO: Resourcequota "e2e-rq-status-ctmsq" reports status: hard memory of 2Gi
    Aug 26 05:33:42.567: INFO: Found resourceQuota "e2e-rq-status-ctmsq" in namespace "resourcequota-6573" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Aug 26 05:36:17.578: INFO: ResourceQuota "e2e-rq-status-ctmsq" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:36:17.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6573" for this suite. 08/26/23 05:36:17.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:36:17.609
Aug 26 05:36:17.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename events 08/26/23 05:36:17.609
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:17.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:17.648
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 08/26/23 05:36:17.654
STEP: listing events in all namespaces 08/26/23 05:36:17.666
STEP: listing events in test namespace 08/26/23 05:36:17.762
STEP: listing events with field selection filtering on source 08/26/23 05:36:17.766
STEP: listing events with field selection filtering on reportingController 08/26/23 05:36:17.769
STEP: getting the test event 08/26/23 05:36:17.773
STEP: patching the test event 08/26/23 05:36:17.776
STEP: getting the test event 08/26/23 05:36:17.788
STEP: updating the test event 08/26/23 05:36:17.792
STEP: getting the test event 08/26/23 05:36:17.8
STEP: deleting the test event 08/26/23 05:36:17.805
STEP: listing events in all namespaces 08/26/23 05:36:17.813
STEP: listing events in test namespace 08/26/23 05:36:17.847
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 26 05:36:17.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5432" for this suite. 08/26/23 05:36:17.858
------------------------------
• [0.257 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:36:17.609
    Aug 26 05:36:17.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename events 08/26/23 05:36:17.609
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:17.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:17.648
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 08/26/23 05:36:17.654
    STEP: listing events in all namespaces 08/26/23 05:36:17.666
    STEP: listing events in test namespace 08/26/23 05:36:17.762
    STEP: listing events with field selection filtering on source 08/26/23 05:36:17.766
    STEP: listing events with field selection filtering on reportingController 08/26/23 05:36:17.769
    STEP: getting the test event 08/26/23 05:36:17.773
    STEP: patching the test event 08/26/23 05:36:17.776
    STEP: getting the test event 08/26/23 05:36:17.788
    STEP: updating the test event 08/26/23 05:36:17.792
    STEP: getting the test event 08/26/23 05:36:17.8
    STEP: deleting the test event 08/26/23 05:36:17.805
    STEP: listing events in all namespaces 08/26/23 05:36:17.813
    STEP: listing events in test namespace 08/26/23 05:36:17.847
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:36:17.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5432" for this suite. 08/26/23 05:36:17.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:36:17.869
Aug 26 05:36:17.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename runtimeclass 08/26/23 05:36:17.869
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:17.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:17.894
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Aug 26 05:36:17.915: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2953 to be scheduled
Aug 26 05:36:17.920: INFO: 1 pods are not scheduled: [runtimeclass-2953/test-runtimeclass-runtimeclass-2953-preconfigured-handler-47bg5(11fe17f6-4b55-4ea0-b01b-1c2b9fa630f8)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 26 05:36:19.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2953" for this suite. 08/26/23 05:36:19.941
------------------------------
• [2.080 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:36:17.869
    Aug 26 05:36:17.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename runtimeclass 08/26/23 05:36:17.869
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:17.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:17.894
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Aug 26 05:36:17.915: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2953 to be scheduled
    Aug 26 05:36:17.920: INFO: 1 pods are not scheduled: [runtimeclass-2953/test-runtimeclass-runtimeclass-2953-preconfigured-handler-47bg5(11fe17f6-4b55-4ea0-b01b-1c2b9fa630f8)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:36:19.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2953" for this suite. 08/26/23 05:36:19.941
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:36:19.949
Aug 26 05:36:19.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:36:19.95
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:19.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:19.972
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:36:20.014
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:36:20.798
STEP: Deploying the webhook pod 08/26/23 05:36:20.808
STEP: Wait for the deployment to be ready 08/26/23 05:36:20.824
Aug 26 05:36:20.837: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:36:22.851
STEP: Verifying the service has paired with the endpoint 08/26/23 05:36:22.865
Aug 26 05:36:23.865: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Aug 26 05:36:23.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3574-crds.webhook.example.com via the AdmissionRegistration API 08/26/23 05:36:24.381
STEP: Creating a custom resource that should be mutated by the webhook 08/26/23 05:36:24.397
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:36:26.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9814" for this suite. 08/26/23 05:36:27.063
STEP: Destroying namespace "webhook-9814-markers" for this suite. 08/26/23 05:36:27.077
------------------------------
• [SLOW TEST] [7.140 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:36:19.949
    Aug 26 05:36:19.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:36:19.95
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:19.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:19.972
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:36:20.014
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:36:20.798
    STEP: Deploying the webhook pod 08/26/23 05:36:20.808
    STEP: Wait for the deployment to be ready 08/26/23 05:36:20.824
    Aug 26 05:36:20.837: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:36:22.851
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:36:22.865
    Aug 26 05:36:23.865: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Aug 26 05:36:23.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3574-crds.webhook.example.com via the AdmissionRegistration API 08/26/23 05:36:24.381
    STEP: Creating a custom resource that should be mutated by the webhook 08/26/23 05:36:24.397
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:36:26.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9814" for this suite. 08/26/23 05:36:27.063
    STEP: Destroying namespace "webhook-9814-markers" for this suite. 08/26/23 05:36:27.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:36:27.104
Aug 26 05:36:27.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 05:36:27.108
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:27.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:27.137
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 08/26/23 05:36:27.141
Aug 26 05:36:27.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 create -f -'
Aug 26 05:36:28.215: INFO: stderr: ""
Aug 26 05:36:28.215: INFO: stdout: "pod/pause created\n"
Aug 26 05:36:28.215: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 26 05:36:28.215: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8314" to be "running and ready"
Aug 26 05:36:28.221: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.823657ms
Aug 26 05:36:28.221: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
Aug 26 05:36:30.226: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010754027s
Aug 26 05:36:30.226: INFO: Pod "pause" satisfied condition "running and ready"
Aug 26 05:36:30.226: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 08/26/23 05:36:30.226
Aug 26 05:36:30.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 label pods pause testing-label=testing-label-value'
Aug 26 05:36:30.355: INFO: stderr: ""
Aug 26 05:36:30.355: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 08/26/23 05:36:30.355
Aug 26 05:36:30.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 get pod pause -L testing-label'
Aug 26 05:36:30.437: INFO: stderr: ""
Aug 26 05:36:30.437: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 08/26/23 05:36:30.437
Aug 26 05:36:30.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 label pods pause testing-label-'
Aug 26 05:36:30.617: INFO: stderr: ""
Aug 26 05:36:30.617: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 08/26/23 05:36:30.617
Aug 26 05:36:30.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 get pod pause -L testing-label'
Aug 26 05:36:30.756: INFO: stderr: ""
Aug 26 05:36:30.756: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 08/26/23 05:36:30.756
Aug 26 05:36:30.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 delete --grace-period=0 --force -f -'
Aug 26 05:36:30.887: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 05:36:30.887: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 26 05:36:30.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 get rc,svc -l name=pause --no-headers'
Aug 26 05:36:30.998: INFO: stderr: "No resources found in kubectl-8314 namespace.\n"
Aug 26 05:36:30.998: INFO: stdout: ""
Aug 26 05:36:30.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 26 05:36:31.092: INFO: stderr: ""
Aug 26 05:36:31.092: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 05:36:31.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8314" for this suite. 08/26/23 05:36:31.106
------------------------------
• [4.011 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:36:27.104
    Aug 26 05:36:27.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 05:36:27.108
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:27.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:27.137
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 08/26/23 05:36:27.141
    Aug 26 05:36:27.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 create -f -'
    Aug 26 05:36:28.215: INFO: stderr: ""
    Aug 26 05:36:28.215: INFO: stdout: "pod/pause created\n"
    Aug 26 05:36:28.215: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Aug 26 05:36:28.215: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8314" to be "running and ready"
    Aug 26 05:36:28.221: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.823657ms
    Aug 26 05:36:28.221: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
    Aug 26 05:36:30.226: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010754027s
    Aug 26 05:36:30.226: INFO: Pod "pause" satisfied condition "running and ready"
    Aug 26 05:36:30.226: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 08/26/23 05:36:30.226
    Aug 26 05:36:30.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 label pods pause testing-label=testing-label-value'
    Aug 26 05:36:30.355: INFO: stderr: ""
    Aug 26 05:36:30.355: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 08/26/23 05:36:30.355
    Aug 26 05:36:30.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 get pod pause -L testing-label'
    Aug 26 05:36:30.437: INFO: stderr: ""
    Aug 26 05:36:30.437: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 08/26/23 05:36:30.437
    Aug 26 05:36:30.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 label pods pause testing-label-'
    Aug 26 05:36:30.617: INFO: stderr: ""
    Aug 26 05:36:30.617: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 08/26/23 05:36:30.617
    Aug 26 05:36:30.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 get pod pause -L testing-label'
    Aug 26 05:36:30.756: INFO: stderr: ""
    Aug 26 05:36:30.756: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 08/26/23 05:36:30.756
    Aug 26 05:36:30.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 delete --grace-period=0 --force -f -'
    Aug 26 05:36:30.887: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 26 05:36:30.887: INFO: stdout: "pod \"pause\" force deleted\n"
    Aug 26 05:36:30.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 get rc,svc -l name=pause --no-headers'
    Aug 26 05:36:30.998: INFO: stderr: "No resources found in kubectl-8314 namespace.\n"
    Aug 26 05:36:30.998: INFO: stdout: ""
    Aug 26 05:36:30.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-8314 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 26 05:36:31.092: INFO: stderr: ""
    Aug 26 05:36:31.092: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:36:31.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8314" for this suite. 08/26/23 05:36:31.106
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:36:31.115
Aug 26 05:36:31.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 05:36:31.116
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:31.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:31.144
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 08/26/23 05:36:31.148
Aug 26 05:36:31.161: INFO: Waiting up to 5m0s for pod "pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c" in namespace "emptydir-4628" to be "Succeeded or Failed"
Aug 26 05:36:31.165: INFO: Pod "pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979214ms
Aug 26 05:36:33.170: INFO: Pod "pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009337742s
Aug 26 05:36:35.170: INFO: Pod "pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009374279s
STEP: Saw pod success 08/26/23 05:36:35.17
Aug 26 05:36:35.170: INFO: Pod "pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c" satisfied condition "Succeeded or Failed"
Aug 26 05:36:35.174: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c container test-container: <nil>
STEP: delete the pod 08/26/23 05:36:35.192
Aug 26 05:36:35.210: INFO: Waiting for pod pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c to disappear
Aug 26 05:36:35.214: INFO: Pod pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 05:36:35.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4628" for this suite. 08/26/23 05:36:35.228
------------------------------
• [4.125 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:36:31.115
    Aug 26 05:36:31.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 05:36:31.116
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:31.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:31.144
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/26/23 05:36:31.148
    Aug 26 05:36:31.161: INFO: Waiting up to 5m0s for pod "pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c" in namespace "emptydir-4628" to be "Succeeded or Failed"
    Aug 26 05:36:31.165: INFO: Pod "pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979214ms
    Aug 26 05:36:33.170: INFO: Pod "pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009337742s
    Aug 26 05:36:35.170: INFO: Pod "pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009374279s
    STEP: Saw pod success 08/26/23 05:36:35.17
    Aug 26 05:36:35.170: INFO: Pod "pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c" satisfied condition "Succeeded or Failed"
    Aug 26 05:36:35.174: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c container test-container: <nil>
    STEP: delete the pod 08/26/23 05:36:35.192
    Aug 26 05:36:35.210: INFO: Waiting for pod pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c to disappear
    Aug 26 05:36:35.214: INFO: Pod pod-1ce5c2ab-5680-4036-9bfe-d5168b83f00c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:36:35.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4628" for this suite. 08/26/23 05:36:35.228
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:36:35.24
Aug 26 05:36:35.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 05:36:35.243
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:35.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:35.267
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 08/26/23 05:36:35.272
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 05:36:35.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9533" for this suite. 08/26/23 05:36:35.288
------------------------------
• [0.056 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:36:35.24
    Aug 26 05:36:35.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 05:36:35.243
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:35.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:35.267
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 08/26/23 05:36:35.272
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:36:35.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9533" for this suite. 08/26/23 05:36:35.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:36:35.303
Aug 26 05:36:35.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename containers 08/26/23 05:36:35.304
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:35.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:35.327
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 08/26/23 05:36:35.329
Aug 26 05:36:35.339: INFO: Waiting up to 5m0s for pod "client-containers-18389559-34d5-4c18-b64e-bce3ea435730" in namespace "containers-4247" to be "Succeeded or Failed"
Aug 26 05:36:35.343: INFO: Pod "client-containers-18389559-34d5-4c18-b64e-bce3ea435730": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512915ms
Aug 26 05:36:37.348: INFO: Pod "client-containers-18389559-34d5-4c18-b64e-bce3ea435730": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008987885s
Aug 26 05:36:39.348: INFO: Pod "client-containers-18389559-34d5-4c18-b64e-bce3ea435730": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00870726s
STEP: Saw pod success 08/26/23 05:36:39.348
Aug 26 05:36:39.348: INFO: Pod "client-containers-18389559-34d5-4c18-b64e-bce3ea435730" satisfied condition "Succeeded or Failed"
Aug 26 05:36:39.353: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod client-containers-18389559-34d5-4c18-b64e-bce3ea435730 container agnhost-container: <nil>
STEP: delete the pod 08/26/23 05:36:39.361
Aug 26 05:36:39.375: INFO: Waiting for pod client-containers-18389559-34d5-4c18-b64e-bce3ea435730 to disappear
Aug 26 05:36:39.379: INFO: Pod client-containers-18389559-34d5-4c18-b64e-bce3ea435730 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 26 05:36:39.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4247" for this suite. 08/26/23 05:36:39.387
------------------------------
• [4.101 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:36:35.303
    Aug 26 05:36:35.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename containers 08/26/23 05:36:35.304
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:35.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:35.327
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 08/26/23 05:36:35.329
    Aug 26 05:36:35.339: INFO: Waiting up to 5m0s for pod "client-containers-18389559-34d5-4c18-b64e-bce3ea435730" in namespace "containers-4247" to be "Succeeded or Failed"
    Aug 26 05:36:35.343: INFO: Pod "client-containers-18389559-34d5-4c18-b64e-bce3ea435730": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512915ms
    Aug 26 05:36:37.348: INFO: Pod "client-containers-18389559-34d5-4c18-b64e-bce3ea435730": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008987885s
    Aug 26 05:36:39.348: INFO: Pod "client-containers-18389559-34d5-4c18-b64e-bce3ea435730": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00870726s
    STEP: Saw pod success 08/26/23 05:36:39.348
    Aug 26 05:36:39.348: INFO: Pod "client-containers-18389559-34d5-4c18-b64e-bce3ea435730" satisfied condition "Succeeded or Failed"
    Aug 26 05:36:39.353: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod client-containers-18389559-34d5-4c18-b64e-bce3ea435730 container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 05:36:39.361
    Aug 26 05:36:39.375: INFO: Waiting for pod client-containers-18389559-34d5-4c18-b64e-bce3ea435730 to disappear
    Aug 26 05:36:39.379: INFO: Pod client-containers-18389559-34d5-4c18-b64e-bce3ea435730 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:36:39.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4247" for this suite. 08/26/23 05:36:39.387
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:36:39.402
Aug 26 05:36:39.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename svcaccounts 08/26/23 05:36:39.403
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:39.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:39.44
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Aug 26 05:36:39.462: INFO: created pod
Aug 26 05:36:39.462: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8537" to be "Succeeded or Failed"
Aug 26 05:36:39.473: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.097787ms
Aug 26 05:36:41.479: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016828288s
Aug 26 05:36:43.480: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017995427s
STEP: Saw pod success 08/26/23 05:36:43.48
Aug 26 05:36:43.480: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 26 05:37:13.481: INFO: polling logs
Aug 26 05:37:13.492: INFO: Pod logs: 
I0826 05:36:40.344424       1 log.go:198] OK: Got token
I0826 05:36:40.344507       1 log.go:198] validating with in-cluster discovery
I0826 05:36:40.345246       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I0826 05:36:40.345397       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8537:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693028799, NotBefore:1693028199, IssuedAt:1693028199, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8537", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"50a53a52-5add-43d9-80dd-5ba711192110"}}}
I0826 05:36:40.363727       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0826 05:36:40.368377       1 log.go:198] OK: Validated signature on JWT
I0826 05:36:40.368599       1 log.go:198] OK: Got valid claims from token!
I0826 05:36:40.368739       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8537:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693028799, NotBefore:1693028199, IssuedAt:1693028199, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8537", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"50a53a52-5add-43d9-80dd-5ba711192110"}}}

Aug 26 05:37:13.492: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 26 05:37:13.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8537" for this suite. 08/26/23 05:37:13.507
------------------------------
• [SLOW TEST] [34.112 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:36:39.402
    Aug 26 05:36:39.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename svcaccounts 08/26/23 05:36:39.403
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:36:39.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:36:39.44
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Aug 26 05:36:39.462: INFO: created pod
    Aug 26 05:36:39.462: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8537" to be "Succeeded or Failed"
    Aug 26 05:36:39.473: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.097787ms
    Aug 26 05:36:41.479: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016828288s
    Aug 26 05:36:43.480: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017995427s
    STEP: Saw pod success 08/26/23 05:36:43.48
    Aug 26 05:36:43.480: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Aug 26 05:37:13.481: INFO: polling logs
    Aug 26 05:37:13.492: INFO: Pod logs: 
    I0826 05:36:40.344424       1 log.go:198] OK: Got token
    I0826 05:36:40.344507       1 log.go:198] validating with in-cluster discovery
    I0826 05:36:40.345246       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I0826 05:36:40.345397       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8537:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693028799, NotBefore:1693028199, IssuedAt:1693028199, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8537", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"50a53a52-5add-43d9-80dd-5ba711192110"}}}
    I0826 05:36:40.363727       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0826 05:36:40.368377       1 log.go:198] OK: Validated signature on JWT
    I0826 05:36:40.368599       1 log.go:198] OK: Got valid claims from token!
    I0826 05:36:40.368739       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8537:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693028799, NotBefore:1693028199, IssuedAt:1693028199, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8537", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"50a53a52-5add-43d9-80dd-5ba711192110"}}}

    Aug 26 05:37:13.492: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:37:13.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8537" for this suite. 08/26/23 05:37:13.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:37:13.514
Aug 26 05:37:13.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename controllerrevisions 08/26/23 05:37:13.516
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:37:13.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:37:13.541
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-gwzz5-daemon-set" 08/26/23 05:37:13.574
STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 05:37:13.581
Aug 26 05:37:13.591: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:37:13.591: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:37:13.591: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:37:13.595: INFO: Number of nodes with available pods controlled by daemonset e2e-gwzz5-daemon-set: 0
Aug 26 05:37:13.595: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 05:37:14.603: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:37:14.603: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:37:14.603: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:37:14.609: INFO: Number of nodes with available pods controlled by daemonset e2e-gwzz5-daemon-set: 0
Aug 26 05:37:14.609: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 05:37:15.603: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:37:15.604: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:37:15.604: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:37:15.610: INFO: Number of nodes with available pods controlled by daemonset e2e-gwzz5-daemon-set: 5
Aug 26 05:37:15.610: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-gwzz5-daemon-set
STEP: Confirm DaemonSet "e2e-gwzz5-daemon-set" successfully created with "daemonset-name=e2e-gwzz5-daemon-set" label 08/26/23 05:37:15.613
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-gwzz5-daemon-set" 08/26/23 05:37:15.621
Aug 26 05:37:15.625: INFO: Located ControllerRevision: "e2e-gwzz5-daemon-set-5fdbc58778"
STEP: Patching ControllerRevision "e2e-gwzz5-daemon-set-5fdbc58778" 08/26/23 05:37:15.629
Aug 26 05:37:15.636: INFO: e2e-gwzz5-daemon-set-5fdbc58778 has been patched
STEP: Create a new ControllerRevision 08/26/23 05:37:15.636
Aug 26 05:37:15.643: INFO: Created ControllerRevision: e2e-gwzz5-daemon-set-744fd669d4
STEP: Confirm that there are two ControllerRevisions 08/26/23 05:37:15.643
Aug 26 05:37:15.643: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 26 05:37:15.647: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-gwzz5-daemon-set-5fdbc58778" 08/26/23 05:37:15.647
STEP: Confirm that there is only one ControllerRevision 08/26/23 05:37:15.653
Aug 26 05:37:15.653: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 26 05:37:15.656: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-gwzz5-daemon-set-744fd669d4" 08/26/23 05:37:15.66
Aug 26 05:37:15.808: INFO: e2e-gwzz5-daemon-set-744fd669d4 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 08/26/23 05:37:15.808
W0826 05:37:15.816255      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 08/26/23 05:37:15.816
Aug 26 05:37:15.816: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 26 05:37:16.822: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 26 05:37:16.829: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-gwzz5-daemon-set-744fd669d4=updated" 08/26/23 05:37:16.829
STEP: Confirm that there is only one ControllerRevision 08/26/23 05:37:16.839
Aug 26 05:37:16.840: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 26 05:37:16.846: INFO: Found 1 ControllerRevisions
Aug 26 05:37:16.855: INFO: ControllerRevision "e2e-gwzz5-daemon-set-6b95d6679c" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-gwzz5-daemon-set" 08/26/23 05:37:16.859
STEP: deleting DaemonSet.extensions e2e-gwzz5-daemon-set in namespace controllerrevisions-8378, will wait for the garbage collector to delete the pods 08/26/23 05:37:16.859
Aug 26 05:37:16.936: INFO: Deleting DaemonSet.extensions e2e-gwzz5-daemon-set took: 14.661231ms
Aug 26 05:37:17.037: INFO: Terminating DaemonSet.extensions e2e-gwzz5-daemon-set pods took: 100.709405ms
Aug 26 05:37:18.345: INFO: Number of nodes with available pods controlled by daemonset e2e-gwzz5-daemon-set: 0
Aug 26 05:37:18.345: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-gwzz5-daemon-set
Aug 26 05:37:18.349: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13281"},"items":null}

Aug 26 05:37:18.353: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13281"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:37:18.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-8378" for this suite. 08/26/23 05:37:18.391
------------------------------
• [4.890 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:37:13.514
    Aug 26 05:37:13.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename controllerrevisions 08/26/23 05:37:13.516
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:37:13.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:37:13.541
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-gwzz5-daemon-set" 08/26/23 05:37:13.574
    STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 05:37:13.581
    Aug 26 05:37:13.591: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:37:13.591: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:37:13.591: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:37:13.595: INFO: Number of nodes with available pods controlled by daemonset e2e-gwzz5-daemon-set: 0
    Aug 26 05:37:13.595: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 05:37:14.603: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:37:14.603: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:37:14.603: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:37:14.609: INFO: Number of nodes with available pods controlled by daemonset e2e-gwzz5-daemon-set: 0
    Aug 26 05:37:14.609: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 05:37:15.603: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:37:15.604: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:37:15.604: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:37:15.610: INFO: Number of nodes with available pods controlled by daemonset e2e-gwzz5-daemon-set: 5
    Aug 26 05:37:15.610: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-gwzz5-daemon-set
    STEP: Confirm DaemonSet "e2e-gwzz5-daemon-set" successfully created with "daemonset-name=e2e-gwzz5-daemon-set" label 08/26/23 05:37:15.613
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-gwzz5-daemon-set" 08/26/23 05:37:15.621
    Aug 26 05:37:15.625: INFO: Located ControllerRevision: "e2e-gwzz5-daemon-set-5fdbc58778"
    STEP: Patching ControllerRevision "e2e-gwzz5-daemon-set-5fdbc58778" 08/26/23 05:37:15.629
    Aug 26 05:37:15.636: INFO: e2e-gwzz5-daemon-set-5fdbc58778 has been patched
    STEP: Create a new ControllerRevision 08/26/23 05:37:15.636
    Aug 26 05:37:15.643: INFO: Created ControllerRevision: e2e-gwzz5-daemon-set-744fd669d4
    STEP: Confirm that there are two ControllerRevisions 08/26/23 05:37:15.643
    Aug 26 05:37:15.643: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 26 05:37:15.647: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-gwzz5-daemon-set-5fdbc58778" 08/26/23 05:37:15.647
    STEP: Confirm that there is only one ControllerRevision 08/26/23 05:37:15.653
    Aug 26 05:37:15.653: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 26 05:37:15.656: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-gwzz5-daemon-set-744fd669d4" 08/26/23 05:37:15.66
    Aug 26 05:37:15.808: INFO: e2e-gwzz5-daemon-set-744fd669d4 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 08/26/23 05:37:15.808
    W0826 05:37:15.816255      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 08/26/23 05:37:15.816
    Aug 26 05:37:15.816: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 26 05:37:16.822: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 26 05:37:16.829: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-gwzz5-daemon-set-744fd669d4=updated" 08/26/23 05:37:16.829
    STEP: Confirm that there is only one ControllerRevision 08/26/23 05:37:16.839
    Aug 26 05:37:16.840: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 26 05:37:16.846: INFO: Found 1 ControllerRevisions
    Aug 26 05:37:16.855: INFO: ControllerRevision "e2e-gwzz5-daemon-set-6b95d6679c" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-gwzz5-daemon-set" 08/26/23 05:37:16.859
    STEP: deleting DaemonSet.extensions e2e-gwzz5-daemon-set in namespace controllerrevisions-8378, will wait for the garbage collector to delete the pods 08/26/23 05:37:16.859
    Aug 26 05:37:16.936: INFO: Deleting DaemonSet.extensions e2e-gwzz5-daemon-set took: 14.661231ms
    Aug 26 05:37:17.037: INFO: Terminating DaemonSet.extensions e2e-gwzz5-daemon-set pods took: 100.709405ms
    Aug 26 05:37:18.345: INFO: Number of nodes with available pods controlled by daemonset e2e-gwzz5-daemon-set: 0
    Aug 26 05:37:18.345: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-gwzz5-daemon-set
    Aug 26 05:37:18.349: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13281"},"items":null}

    Aug 26 05:37:18.353: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13281"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:37:18.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-8378" for this suite. 08/26/23 05:37:18.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:37:18.405
Aug 26 05:37:18.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename job 08/26/23 05:37:18.406
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:37:18.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:37:18.432
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 08/26/23 05:37:18.434
STEP: Ensure pods equal to parallelism count is attached to the job 08/26/23 05:37:18.443
STEP: patching /status 08/26/23 05:37:20.449
STEP: updating /status 08/26/23 05:37:20.458
STEP: get /status 08/26/23 05:37:20.49
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 26 05:37:20.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-359" for this suite. 08/26/23 05:37:20.5
------------------------------
• [2.124 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:37:18.405
    Aug 26 05:37:18.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename job 08/26/23 05:37:18.406
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:37:18.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:37:18.432
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 08/26/23 05:37:18.434
    STEP: Ensure pods equal to parallelism count is attached to the job 08/26/23 05:37:18.443
    STEP: patching /status 08/26/23 05:37:20.449
    STEP: updating /status 08/26/23 05:37:20.458
    STEP: get /status 08/26/23 05:37:20.49
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:37:20.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-359" for this suite. 08/26/23 05:37:20.5
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:37:20.53
Aug 26 05:37:20.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename podtemplate 08/26/23 05:37:20.53
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:37:20.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:37:20.557
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 08/26/23 05:37:20.559
Aug 26 05:37:20.566: INFO: created test-podtemplate-1
Aug 26 05:37:20.574: INFO: created test-podtemplate-2
Aug 26 05:37:20.580: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 08/26/23 05:37:20.58
STEP: delete collection of pod templates 08/26/23 05:37:20.585
Aug 26 05:37:20.585: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 08/26/23 05:37:20.606
Aug 26 05:37:20.606: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 26 05:37:20.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9444" for this suite. 08/26/23 05:37:20.617
------------------------------
• [0.099 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:37:20.53
    Aug 26 05:37:20.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename podtemplate 08/26/23 05:37:20.53
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:37:20.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:37:20.557
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 08/26/23 05:37:20.559
    Aug 26 05:37:20.566: INFO: created test-podtemplate-1
    Aug 26 05:37:20.574: INFO: created test-podtemplate-2
    Aug 26 05:37:20.580: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 08/26/23 05:37:20.58
    STEP: delete collection of pod templates 08/26/23 05:37:20.585
    Aug 26 05:37:20.585: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 08/26/23 05:37:20.606
    Aug 26 05:37:20.606: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:37:20.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9444" for this suite. 08/26/23 05:37:20.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:37:20.629
Aug 26 05:37:20.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename init-container 08/26/23 05:37:20.63
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:37:20.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:37:20.651
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 08/26/23 05:37:20.653
Aug 26 05:37:20.653: INFO: PodSpec: initContainers in spec.initContainers
Aug 26 05:38:05.967: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-892c7c1c-a17f-45bc-b004-a00809ba1ea2", GenerateName:"", Namespace:"init-container-6910", SelfLink:"", UID:"6e5c8034-96e2-40c0-9008-985be89d5021", ResourceVersion:"13559", Generation:0, CreationTimestamp:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"653873339"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"0072586adc7997a008c9d195794412169bb44ce364a88f1ade17a8ee9af05b8c", "cni.projectcalico.org/podIP":"10.20.199.108/32", "cni.projectcalico.org/podIPs":"10.20.199.108/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00116dc20), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 26, 5, 37, 21, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00116dc50), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 26, 5, 38, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00116dc98), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-pfxsl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000e98ba0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pfxsl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pfxsl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pfxsl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005555560), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-1-5.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0004d1ea0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0055555e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005555600)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005555608), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00555560c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000504120), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.1.5", PodIP:"10.20.199.108", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.20.199.108"}}, StartTime:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005082a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000508310)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://e60d66e43f4d507b2e17b2e7726dade9c699f1ec0e66c99f3351e5fe6fcb13b0", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e98d00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e98cc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00555566c)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:38:05.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6910" for this suite. 08/26/23 05:38:05.98
------------------------------
• [SLOW TEST] [45.363 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:37:20.629
    Aug 26 05:37:20.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename init-container 08/26/23 05:37:20.63
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:37:20.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:37:20.651
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 08/26/23 05:37:20.653
    Aug 26 05:37:20.653: INFO: PodSpec: initContainers in spec.initContainers
    Aug 26 05:38:05.967: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-892c7c1c-a17f-45bc-b004-a00809ba1ea2", GenerateName:"", Namespace:"init-container-6910", SelfLink:"", UID:"6e5c8034-96e2-40c0-9008-985be89d5021", ResourceVersion:"13559", Generation:0, CreationTimestamp:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"653873339"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"0072586adc7997a008c9d195794412169bb44ce364a88f1ade17a8ee9af05b8c", "cni.projectcalico.org/podIP":"10.20.199.108/32", "cni.projectcalico.org/podIPs":"10.20.199.108/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00116dc20), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 26, 5, 37, 21, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00116dc50), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 26, 5, 38, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00116dc98), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-pfxsl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000e98ba0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pfxsl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pfxsl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pfxsl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005555560), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-1-5.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0004d1ea0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0055555e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005555600)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005555608), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00555560c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000504120), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.1.5", PodIP:"10.20.199.108", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.20.199.108"}}, StartTime:time.Date(2023, time.August, 26, 5, 37, 20, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005082a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000508310)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://e60d66e43f4d507b2e17b2e7726dade9c699f1ec0e66c99f3351e5fe6fcb13b0", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e98d00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e98cc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00555566c)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:38:05.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6910" for this suite. 08/26/23 05:38:05.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:38:05.992
Aug 26 05:38:05.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename server-version 08/26/23 05:38:05.993
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:06.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:06.026
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 08/26/23 05:38:06.029
STEP: Confirm major version 08/26/23 05:38:06.03
Aug 26 05:38:06.030: INFO: Major version: 1
STEP: Confirm minor version 08/26/23 05:38:06.03
Aug 26 05:38:06.030: INFO: cleanMinorVersion: 26
Aug 26 05:38:06.030: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Aug 26 05:38:06.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-3165" for this suite. 08/26/23 05:38:06.04
------------------------------
• [0.057 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:38:05.992
    Aug 26 05:38:05.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename server-version 08/26/23 05:38:05.993
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:06.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:06.026
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 08/26/23 05:38:06.029
    STEP: Confirm major version 08/26/23 05:38:06.03
    Aug 26 05:38:06.030: INFO: Major version: 1
    STEP: Confirm minor version 08/26/23 05:38:06.03
    Aug 26 05:38:06.030: INFO: cleanMinorVersion: 26
    Aug 26 05:38:06.030: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:38:06.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-3165" for this suite. 08/26/23 05:38:06.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:38:06.051
Aug 26 05:38:06.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 05:38:06.052
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:06.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:06.083
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-f8ac9fad-65ad-4103-be3d-68005f7c4343 08/26/23 05:38:06.086
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:38:06.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5159" for this suite. 08/26/23 05:38:06.095
------------------------------
• [0.053 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:38:06.051
    Aug 26 05:38:06.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 05:38:06.052
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:06.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:06.083
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-f8ac9fad-65ad-4103-be3d-68005f7c4343 08/26/23 05:38:06.086
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:38:06.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5159" for this suite. 08/26/23 05:38:06.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:38:06.105
Aug 26 05:38:06.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:38:06.107
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:06.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:06.134
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:38:06.156
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:38:06.553
STEP: Deploying the webhook pod 08/26/23 05:38:06.564
STEP: Wait for the deployment to be ready 08/26/23 05:38:06.586
Aug 26 05:38:06.595: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:38:08.612
STEP: Verifying the service has paired with the endpoint 08/26/23 05:38:08.626
Aug 26 05:38:09.627: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/26/23 05:38:09.631
STEP: create a configmap that should be updated by the webhook 08/26/23 05:38:09.652
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:38:09.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6011" for this suite. 08/26/23 05:38:09.795
STEP: Destroying namespace "webhook-6011-markers" for this suite. 08/26/23 05:38:09.807
------------------------------
• [3.710 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:38:06.105
    Aug 26 05:38:06.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:38:06.107
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:06.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:06.134
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:38:06.156
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:38:06.553
    STEP: Deploying the webhook pod 08/26/23 05:38:06.564
    STEP: Wait for the deployment to be ready 08/26/23 05:38:06.586
    Aug 26 05:38:06.595: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:38:08.612
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:38:08.626
    Aug 26 05:38:09.627: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/26/23 05:38:09.631
    STEP: create a configmap that should be updated by the webhook 08/26/23 05:38:09.652
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:38:09.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6011" for this suite. 08/26/23 05:38:09.795
    STEP: Destroying namespace "webhook-6011-markers" for this suite. 08/26/23 05:38:09.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:38:09.816
Aug 26 05:38:09.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 05:38:09.818
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:09.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:09.86
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 08/26/23 05:38:09.875
Aug 26 05:38:09.876: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 26 05:38:09.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
Aug 26 05:38:10.938: INFO: stderr: ""
Aug 26 05:38:10.938: INFO: stdout: "service/agnhost-replica created\n"
Aug 26 05:38:10.938: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 26 05:38:10.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
Aug 26 05:38:11.979: INFO: stderr: ""
Aug 26 05:38:11.979: INFO: stdout: "service/agnhost-primary created\n"
Aug 26 05:38:11.979: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 26 05:38:11.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
Aug 26 05:38:12.410: INFO: stderr: ""
Aug 26 05:38:12.410: INFO: stdout: "service/frontend created\n"
Aug 26 05:38:12.410: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 26 05:38:12.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
Aug 26 05:38:12.732: INFO: stderr: ""
Aug 26 05:38:12.732: INFO: stdout: "deployment.apps/frontend created\n"
Aug 26 05:38:12.732: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 26 05:38:12.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
Aug 26 05:38:13.013: INFO: stderr: ""
Aug 26 05:38:13.013: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 26 05:38:13.013: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 26 05:38:13.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
Aug 26 05:38:13.325: INFO: stderr: ""
Aug 26 05:38:13.325: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 08/26/23 05:38:13.325
Aug 26 05:38:13.325: INFO: Waiting for all frontend pods to be Running.
Aug 26 05:38:18.379: INFO: Waiting for frontend to serve content.
Aug 26 05:38:18.400: INFO: Trying to add a new entry to the guestbook.
Aug 26 05:38:18.415: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 08/26/23 05:38:18.424
Aug 26 05:38:18.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
Aug 26 05:38:18.553: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 05:38:18.553: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 08/26/23 05:38:18.553
Aug 26 05:38:18.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
Aug 26 05:38:18.676: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 05:38:18.676: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/26/23 05:38:18.676
Aug 26 05:38:18.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
Aug 26 05:38:18.795: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 05:38:18.795: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/26/23 05:38:18.795
Aug 26 05:38:18.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
Aug 26 05:38:18.874: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 05:38:18.874: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/26/23 05:38:18.874
Aug 26 05:38:18.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
Aug 26 05:38:18.958: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 05:38:18.958: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/26/23 05:38:18.958
Aug 26 05:38:18.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
Aug 26 05:38:19.056: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 05:38:19.056: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 05:38:19.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2848" for this suite. 08/26/23 05:38:19.065
------------------------------
• [SLOW TEST] [9.270 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:38:09.816
    Aug 26 05:38:09.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 05:38:09.818
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:09.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:09.86
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 08/26/23 05:38:09.875
    Aug 26 05:38:09.876: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Aug 26 05:38:09.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
    Aug 26 05:38:10.938: INFO: stderr: ""
    Aug 26 05:38:10.938: INFO: stdout: "service/agnhost-replica created\n"
    Aug 26 05:38:10.938: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Aug 26 05:38:10.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
    Aug 26 05:38:11.979: INFO: stderr: ""
    Aug 26 05:38:11.979: INFO: stdout: "service/agnhost-primary created\n"
    Aug 26 05:38:11.979: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Aug 26 05:38:11.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
    Aug 26 05:38:12.410: INFO: stderr: ""
    Aug 26 05:38:12.410: INFO: stdout: "service/frontend created\n"
    Aug 26 05:38:12.410: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Aug 26 05:38:12.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
    Aug 26 05:38:12.732: INFO: stderr: ""
    Aug 26 05:38:12.732: INFO: stdout: "deployment.apps/frontend created\n"
    Aug 26 05:38:12.732: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 26 05:38:12.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
    Aug 26 05:38:13.013: INFO: stderr: ""
    Aug 26 05:38:13.013: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Aug 26 05:38:13.013: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 26 05:38:13.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 create -f -'
    Aug 26 05:38:13.325: INFO: stderr: ""
    Aug 26 05:38:13.325: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 08/26/23 05:38:13.325
    Aug 26 05:38:13.325: INFO: Waiting for all frontend pods to be Running.
    Aug 26 05:38:18.379: INFO: Waiting for frontend to serve content.
    Aug 26 05:38:18.400: INFO: Trying to add a new entry to the guestbook.
    Aug 26 05:38:18.415: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 08/26/23 05:38:18.424
    Aug 26 05:38:18.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
    Aug 26 05:38:18.553: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 26 05:38:18.553: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 08/26/23 05:38:18.553
    Aug 26 05:38:18.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
    Aug 26 05:38:18.676: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 26 05:38:18.676: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/26/23 05:38:18.676
    Aug 26 05:38:18.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
    Aug 26 05:38:18.795: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 26 05:38:18.795: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/26/23 05:38:18.795
    Aug 26 05:38:18.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
    Aug 26 05:38:18.874: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 26 05:38:18.874: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/26/23 05:38:18.874
    Aug 26 05:38:18.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
    Aug 26 05:38:18.958: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 26 05:38:18.958: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/26/23 05:38:18.958
    Aug 26 05:38:18.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2848 delete --grace-period=0 --force -f -'
    Aug 26 05:38:19.056: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 26 05:38:19.056: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:38:19.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2848" for this suite. 08/26/23 05:38:19.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:38:19.087
Aug 26 05:38:19.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:38:19.088
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:19.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:19.12
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-f0430ea9-c846-434c-bd95-8643766eee79 08/26/23 05:38:19.125
STEP: Creating a pod to test consume secrets 08/26/23 05:38:19.134
Aug 26 05:38:19.149: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22" in namespace "projected-822" to be "Succeeded or Failed"
Aug 26 05:38:19.154: INFO: Pod "pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.910421ms
Aug 26 05:38:21.159: INFO: Pod "pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010130857s
Aug 26 05:38:23.161: INFO: Pod "pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012244774s
STEP: Saw pod success 08/26/23 05:38:23.161
Aug 26 05:38:23.161: INFO: Pod "pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22" satisfied condition "Succeeded or Failed"
Aug 26 05:38:23.165: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/26/23 05:38:23.183
Aug 26 05:38:23.202: INFO: Waiting for pod pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22 to disappear
Aug 26 05:38:23.206: INFO: Pod pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 26 05:38:23.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-822" for this suite. 08/26/23 05:38:23.217
------------------------------
• [4.139 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:38:19.087
    Aug 26 05:38:19.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:38:19.088
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:19.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:19.12
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-f0430ea9-c846-434c-bd95-8643766eee79 08/26/23 05:38:19.125
    STEP: Creating a pod to test consume secrets 08/26/23 05:38:19.134
    Aug 26 05:38:19.149: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22" in namespace "projected-822" to be "Succeeded or Failed"
    Aug 26 05:38:19.154: INFO: Pod "pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.910421ms
    Aug 26 05:38:21.159: INFO: Pod "pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010130857s
    Aug 26 05:38:23.161: INFO: Pod "pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012244774s
    STEP: Saw pod success 08/26/23 05:38:23.161
    Aug 26 05:38:23.161: INFO: Pod "pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22" satisfied condition "Succeeded or Failed"
    Aug 26 05:38:23.165: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 05:38:23.183
    Aug 26 05:38:23.202: INFO: Waiting for pod pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22 to disappear
    Aug 26 05:38:23.206: INFO: Pod pod-projected-secrets-41b4d508-2b15-4f84-9c91-cb4dfbe2ac22 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:38:23.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-822" for this suite. 08/26/23 05:38:23.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:38:23.228
Aug 26 05:38:23.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sched-preemption 08/26/23 05:38:23.229
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:23.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:23.259
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 26 05:38:23.282: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 26 05:39:23.359: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:39:23.364
Aug 26 05:39:23.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sched-preemption-path 08/26/23 05:39:23.364
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:23.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:23.396
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 08/26/23 05:39:23.401
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/26/23 05:39:23.401
Aug 26 05:39:23.415: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6305" to be "running"
Aug 26 05:39:23.425: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.346979ms
Aug 26 05:39:25.430: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014984842s
Aug 26 05:39:25.430: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/26/23 05:39:25.435
Aug 26 05:39:25.447: INFO: found a healthy node: ip-10-0-1-5.us-west-2.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Aug 26 05:39:31.574: INFO: pods created so far: [1 1 1]
Aug 26 05:39:31.574: INFO: length of pods created so far: 3
Aug 26 05:39:33.589: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Aug 26 05:39:40.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:39:40.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6305" for this suite. 08/26/23 05:39:40.732
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8032" for this suite. 08/26/23 05:39:40.739
------------------------------
• [SLOW TEST] [77.519 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:38:23.228
    Aug 26 05:38:23.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sched-preemption 08/26/23 05:38:23.229
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:38:23.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:38:23.259
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 26 05:38:23.282: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 26 05:39:23.359: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:39:23.364
    Aug 26 05:39:23.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sched-preemption-path 08/26/23 05:39:23.364
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:23.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:23.396
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 08/26/23 05:39:23.401
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/26/23 05:39:23.401
    Aug 26 05:39:23.415: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6305" to be "running"
    Aug 26 05:39:23.425: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.346979ms
    Aug 26 05:39:25.430: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014984842s
    Aug 26 05:39:25.430: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/26/23 05:39:25.435
    Aug 26 05:39:25.447: INFO: found a healthy node: ip-10-0-1-5.us-west-2.compute.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Aug 26 05:39:31.574: INFO: pods created so far: [1 1 1]
    Aug 26 05:39:31.574: INFO: length of pods created so far: 3
    Aug 26 05:39:33.589: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:39:40.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:39:40.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6305" for this suite. 08/26/23 05:39:40.732
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8032" for this suite. 08/26/23 05:39:40.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:39:40.747
Aug 26 05:39:40.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename custom-resource-definition 08/26/23 05:39:40.748
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:40.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:40.773
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Aug 26 05:39:40.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:39:47.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1904" for this suite. 08/26/23 05:39:47.051
------------------------------
• [SLOW TEST] [6.312 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:39:40.747
    Aug 26 05:39:40.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename custom-resource-definition 08/26/23 05:39:40.748
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:40.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:40.773
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Aug 26 05:39:40.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:39:47.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1904" for this suite. 08/26/23 05:39:47.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:39:47.06
Aug 26 05:39:47.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:39:47.061
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:47.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:47.103
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:39:47.11
Aug 26 05:39:47.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1" in namespace "downward-api-5638" to be "Succeeded or Failed"
Aug 26 05:39:47.131: INFO: Pod "downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.478869ms
Aug 26 05:39:49.135: INFO: Pod "downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009764816s
Aug 26 05:39:51.140: INFO: Pod "downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014779035s
STEP: Saw pod success 08/26/23 05:39:51.14
Aug 26 05:39:51.140: INFO: Pod "downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1" satisfied condition "Succeeded or Failed"
Aug 26 05:39:51.145: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1 container client-container: <nil>
STEP: delete the pod 08/26/23 05:39:51.16
Aug 26 05:39:51.180: INFO: Waiting for pod downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1 to disappear
Aug 26 05:39:51.184: INFO: Pod downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 05:39:51.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5638" for this suite. 08/26/23 05:39:51.195
------------------------------
• [4.143 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:39:47.06
    Aug 26 05:39:47.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:39:47.061
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:47.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:47.103
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:39:47.11
    Aug 26 05:39:47.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1" in namespace "downward-api-5638" to be "Succeeded or Failed"
    Aug 26 05:39:47.131: INFO: Pod "downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.478869ms
    Aug 26 05:39:49.135: INFO: Pod "downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009764816s
    Aug 26 05:39:51.140: INFO: Pod "downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014779035s
    STEP: Saw pod success 08/26/23 05:39:51.14
    Aug 26 05:39:51.140: INFO: Pod "downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1" satisfied condition "Succeeded or Failed"
    Aug 26 05:39:51.145: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1 container client-container: <nil>
    STEP: delete the pod 08/26/23 05:39:51.16
    Aug 26 05:39:51.180: INFO: Waiting for pod downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1 to disappear
    Aug 26 05:39:51.184: INFO: Pod downwardapi-volume-666ca807-5841-4e8f-9845-c4eea96fdad1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:39:51.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5638" for this suite. 08/26/23 05:39:51.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:39:51.203
Aug 26 05:39:51.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:39:51.204
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:51.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:51.226
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:39:51.231
Aug 26 05:39:51.245: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c" in namespace "projected-7874" to be "Succeeded or Failed"
Aug 26 05:39:51.252: INFO: Pod "downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.345082ms
Aug 26 05:39:53.257: INFO: Pod "downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011491988s
Aug 26 05:39:55.260: INFO: Pod "downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014491428s
STEP: Saw pod success 08/26/23 05:39:55.26
Aug 26 05:39:55.260: INFO: Pod "downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c" satisfied condition "Succeeded or Failed"
Aug 26 05:39:55.265: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c container client-container: <nil>
STEP: delete the pod 08/26/23 05:39:55.287
Aug 26 05:39:55.307: INFO: Waiting for pod downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c to disappear
Aug 26 05:39:55.310: INFO: Pod downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 05:39:55.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7874" for this suite. 08/26/23 05:39:55.316
------------------------------
• [4.122 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:39:51.203
    Aug 26 05:39:51.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:39:51.204
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:51.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:51.226
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:39:51.231
    Aug 26 05:39:51.245: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c" in namespace "projected-7874" to be "Succeeded or Failed"
    Aug 26 05:39:51.252: INFO: Pod "downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.345082ms
    Aug 26 05:39:53.257: INFO: Pod "downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011491988s
    Aug 26 05:39:55.260: INFO: Pod "downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014491428s
    STEP: Saw pod success 08/26/23 05:39:55.26
    Aug 26 05:39:55.260: INFO: Pod "downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c" satisfied condition "Succeeded or Failed"
    Aug 26 05:39:55.265: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c container client-container: <nil>
    STEP: delete the pod 08/26/23 05:39:55.287
    Aug 26 05:39:55.307: INFO: Waiting for pod downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c to disappear
    Aug 26 05:39:55.310: INFO: Pod downwardapi-volume-c52551d5-cbe7-44a7-a0f4-eb886d77355c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:39:55.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7874" for this suite. 08/26/23 05:39:55.316
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:39:55.326
Aug 26 05:39:55.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:39:55.328
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:55.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:55.353
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:39:55.357
Aug 26 05:39:55.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6" in namespace "projected-9910" to be "Succeeded or Failed"
Aug 26 05:39:55.376: INFO: Pod "downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.966684ms
Aug 26 05:39:57.382: INFO: Pod "downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012462529s
Aug 26 05:39:59.382: INFO: Pod "downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012355558s
STEP: Saw pod success 08/26/23 05:39:59.382
Aug 26 05:39:59.382: INFO: Pod "downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6" satisfied condition "Succeeded or Failed"
Aug 26 05:39:59.386: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6 container client-container: <nil>
STEP: delete the pod 08/26/23 05:39:59.394
Aug 26 05:39:59.405: INFO: Waiting for pod downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6 to disappear
Aug 26 05:39:59.411: INFO: Pod downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 05:39:59.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9910" for this suite. 08/26/23 05:39:59.423
------------------------------
• [4.107 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:39:55.326
    Aug 26 05:39:55.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:39:55.328
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:55.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:55.353
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:39:55.357
    Aug 26 05:39:55.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6" in namespace "projected-9910" to be "Succeeded or Failed"
    Aug 26 05:39:55.376: INFO: Pod "downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.966684ms
    Aug 26 05:39:57.382: INFO: Pod "downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012462529s
    Aug 26 05:39:59.382: INFO: Pod "downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012355558s
    STEP: Saw pod success 08/26/23 05:39:59.382
    Aug 26 05:39:59.382: INFO: Pod "downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6" satisfied condition "Succeeded or Failed"
    Aug 26 05:39:59.386: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6 container client-container: <nil>
    STEP: delete the pod 08/26/23 05:39:59.394
    Aug 26 05:39:59.405: INFO: Waiting for pod downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6 to disappear
    Aug 26 05:39:59.411: INFO: Pod downwardapi-volume-edd3b7c4-f49a-48d0-abfc-69b3e081e1c6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:39:59.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9910" for this suite. 08/26/23 05:39:59.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:39:59.434
Aug 26 05:39:59.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 05:39:59.435
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:59.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:59.456
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/26/23 05:39:59.458
Aug 26 05:39:59.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9777 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Aug 26 05:39:59.604: INFO: stderr: ""
Aug 26 05:39:59.604: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 08/26/23 05:39:59.604
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Aug 26 05:39:59.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9777 delete pods e2e-test-httpd-pod'
Aug 26 05:40:02.313: INFO: stderr: ""
Aug 26 05:40:02.313: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 05:40:02.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9777" for this suite. 08/26/23 05:40:02.328
------------------------------
• [2.901 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:39:59.434
    Aug 26 05:39:59.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 05:39:59.435
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:39:59.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:39:59.456
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/26/23 05:39:59.458
    Aug 26 05:39:59.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9777 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Aug 26 05:39:59.604: INFO: stderr: ""
    Aug 26 05:39:59.604: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 08/26/23 05:39:59.604
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Aug 26 05:39:59.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9777 delete pods e2e-test-httpd-pod'
    Aug 26 05:40:02.313: INFO: stderr: ""
    Aug 26 05:40:02.313: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:40:02.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9777" for this suite. 08/26/23 05:40:02.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:40:02.336
Aug 26 05:40:02.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replicaset 08/26/23 05:40:02.337
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:02.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:02.376
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Aug 26 05:40:02.385: INFO: Creating ReplicaSet my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab
Aug 26 05:40:02.408: INFO: Pod name my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab: Found 1 pods out of 1
Aug 26 05:40:02.408: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab" is running
Aug 26 05:40:02.408: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2" in namespace "replicaset-2697" to be "running"
Aug 26 05:40:02.415: INFO: Pod "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.825516ms
Aug 26 05:40:04.420: INFO: Pod "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012578876s
Aug 26 05:40:04.420: INFO: Pod "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2" satisfied condition "running"
Aug 26 05:40:04.420: INFO: Pod "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2" is running (conditions: [])
Aug 26 05:40:04.420: INFO: Trying to dial the pod
Aug 26 05:40:09.437: INFO: Controller my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab: Got expected result from replica 1 [my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2]: "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:40:09.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2697" for this suite. 08/26/23 05:40:09.445
------------------------------
• [SLOW TEST] [7.116 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:40:02.336
    Aug 26 05:40:02.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replicaset 08/26/23 05:40:02.337
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:02.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:02.376
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Aug 26 05:40:02.385: INFO: Creating ReplicaSet my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab
    Aug 26 05:40:02.408: INFO: Pod name my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab: Found 1 pods out of 1
    Aug 26 05:40:02.408: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab" is running
    Aug 26 05:40:02.408: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2" in namespace "replicaset-2697" to be "running"
    Aug 26 05:40:02.415: INFO: Pod "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.825516ms
    Aug 26 05:40:04.420: INFO: Pod "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012578876s
    Aug 26 05:40:04.420: INFO: Pod "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2" satisfied condition "running"
    Aug 26 05:40:04.420: INFO: Pod "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2" is running (conditions: [])
    Aug 26 05:40:04.420: INFO: Trying to dial the pod
    Aug 26 05:40:09.437: INFO: Controller my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab: Got expected result from replica 1 [my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2]: "my-hostname-basic-b6065e73-9ce0-42f0-98a0-554f5139a0ab-sczm2", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:40:09.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2697" for this suite. 08/26/23 05:40:09.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:40:09.453
Aug 26 05:40:09.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pod-network-test 08/26/23 05:40:09.454
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:09.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:09.477
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-7415 08/26/23 05:40:09.479
STEP: creating a selector 08/26/23 05:40:09.479
STEP: Creating the service pods in kubernetes 08/26/23 05:40:09.479
Aug 26 05:40:09.479: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 26 05:40:09.545: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7415" to be "running and ready"
Aug 26 05:40:09.555: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.193977ms
Aug 26 05:40:09.555: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:40:11.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015060883s
Aug 26 05:40:11.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:40:13.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01610075s
Aug 26 05:40:13.561: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:40:15.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015272694s
Aug 26 05:40:15.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:40:17.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01734737s
Aug 26 05:40:17.562: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:40:19.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016028483s
Aug 26 05:40:19.561: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:40:21.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015528345s
Aug 26 05:40:21.561: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:40:23.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016963736s
Aug 26 05:40:23.562: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:40:25.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019325835s
Aug 26 05:40:25.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:40:27.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.016457431s
Aug 26 05:40:27.562: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:40:29.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01467014s
Aug 26 05:40:29.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 05:40:31.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016507345s
Aug 26 05:40:31.562: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 26 05:40:31.562: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 26 05:40:31.566: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7415" to be "running and ready"
Aug 26 05:40:31.570: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.054733ms
Aug 26 05:40:31.570: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 26 05:40:31.570: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 26 05:40:31.574: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7415" to be "running and ready"
Aug 26 05:40:31.578: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.669843ms
Aug 26 05:40:31.578: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 26 05:40:31.578: INFO: Pod "netserver-2" satisfied condition "running and ready"
Aug 26 05:40:31.581: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7415" to be "running and ready"
Aug 26 05:40:31.586: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.142525ms
Aug 26 05:40:31.586: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Aug 26 05:40:31.586: INFO: Pod "netserver-3" satisfied condition "running and ready"
Aug 26 05:40:31.589: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-7415" to be "running and ready"
Aug 26 05:40:31.593: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.674617ms
Aug 26 05:40:31.593: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Aug 26 05:40:31.593: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 08/26/23 05:40:31.597
Aug 26 05:40:31.615: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7415" to be "running"
Aug 26 05:40:31.621: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.792699ms
Aug 26 05:40:33.627: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011562394s
Aug 26 05:40:33.627: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 26 05:40:33.631: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7415" to be "running"
Aug 26 05:40:33.635: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.549911ms
Aug 26 05:40:33.635: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 26 05:40:33.639: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Aug 26 05:40:33.639: INFO: Going to poll 10.20.50.238 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Aug 26 05:40:33.643: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.50.238 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:40:33.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:40:33.643: INFO: ExecWithOptions: Clientset creation
Aug 26 05:40:33.643: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-7415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.50.238+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 05:40:34.724: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 26 05:40:34.725: INFO: Going to poll 10.20.193.204 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Aug 26 05:40:34.731: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.193.204 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:40:34.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:40:34.732: INFO: ExecWithOptions: Clientset creation
Aug 26 05:40:34.732: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-7415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.193.204+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 05:40:35.809: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 26 05:40:35.809: INFO: Going to poll 10.20.62.142 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Aug 26 05:40:35.814: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.62.142 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:40:35.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:40:35.814: INFO: ExecWithOptions: Clientset creation
Aug 26 05:40:35.814: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-7415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.62.142+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 05:40:36.927: INFO: Found all 1 expected endpoints: [netserver-2]
Aug 26 05:40:36.927: INFO: Going to poll 10.20.8.232 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Aug 26 05:40:36.931: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.8.232 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:40:36.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:40:36.931: INFO: ExecWithOptions: Clientset creation
Aug 26 05:40:36.931: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-7415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.8.232+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 05:40:38.029: INFO: Found all 1 expected endpoints: [netserver-3]
Aug 26 05:40:38.029: INFO: Going to poll 10.20.199.120 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Aug 26 05:40:38.033: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.199.120 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:40:38.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:40:38.034: INFO: ExecWithOptions: Clientset creation
Aug 26 05:40:38.034: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-7415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.199.120+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 05:40:39.126: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 26 05:40:39.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7415" for this suite. 08/26/23 05:40:39.134
------------------------------
• [SLOW TEST] [29.688 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:40:09.453
    Aug 26 05:40:09.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pod-network-test 08/26/23 05:40:09.454
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:09.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:09.477
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-7415 08/26/23 05:40:09.479
    STEP: creating a selector 08/26/23 05:40:09.479
    STEP: Creating the service pods in kubernetes 08/26/23 05:40:09.479
    Aug 26 05:40:09.479: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 26 05:40:09.545: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7415" to be "running and ready"
    Aug 26 05:40:09.555: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.193977ms
    Aug 26 05:40:09.555: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:40:11.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015060883s
    Aug 26 05:40:11.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:40:13.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01610075s
    Aug 26 05:40:13.561: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:40:15.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015272694s
    Aug 26 05:40:15.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:40:17.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01734737s
    Aug 26 05:40:17.562: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:40:19.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016028483s
    Aug 26 05:40:19.561: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:40:21.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015528345s
    Aug 26 05:40:21.561: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:40:23.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016963736s
    Aug 26 05:40:23.562: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:40:25.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019325835s
    Aug 26 05:40:25.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:40:27.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.016457431s
    Aug 26 05:40:27.562: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:40:29.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01467014s
    Aug 26 05:40:29.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 05:40:31.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016507345s
    Aug 26 05:40:31.562: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 26 05:40:31.562: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 26 05:40:31.566: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7415" to be "running and ready"
    Aug 26 05:40:31.570: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.054733ms
    Aug 26 05:40:31.570: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 26 05:40:31.570: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 26 05:40:31.574: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7415" to be "running and ready"
    Aug 26 05:40:31.578: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.669843ms
    Aug 26 05:40:31.578: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 26 05:40:31.578: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Aug 26 05:40:31.581: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7415" to be "running and ready"
    Aug 26 05:40:31.586: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.142525ms
    Aug 26 05:40:31.586: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Aug 26 05:40:31.586: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Aug 26 05:40:31.589: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-7415" to be "running and ready"
    Aug 26 05:40:31.593: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.674617ms
    Aug 26 05:40:31.593: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Aug 26 05:40:31.593: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 08/26/23 05:40:31.597
    Aug 26 05:40:31.615: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7415" to be "running"
    Aug 26 05:40:31.621: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.792699ms
    Aug 26 05:40:33.627: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011562394s
    Aug 26 05:40:33.627: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 26 05:40:33.631: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7415" to be "running"
    Aug 26 05:40:33.635: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.549911ms
    Aug 26 05:40:33.635: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 26 05:40:33.639: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Aug 26 05:40:33.639: INFO: Going to poll 10.20.50.238 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Aug 26 05:40:33.643: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.50.238 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:40:33.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:40:33.643: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:40:33.643: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-7415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.50.238+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 05:40:34.724: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 26 05:40:34.725: INFO: Going to poll 10.20.193.204 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Aug 26 05:40:34.731: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.193.204 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:40:34.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:40:34.732: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:40:34.732: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-7415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.193.204+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 05:40:35.809: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 26 05:40:35.809: INFO: Going to poll 10.20.62.142 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Aug 26 05:40:35.814: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.62.142 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:40:35.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:40:35.814: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:40:35.814: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-7415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.62.142+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 05:40:36.927: INFO: Found all 1 expected endpoints: [netserver-2]
    Aug 26 05:40:36.927: INFO: Going to poll 10.20.8.232 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Aug 26 05:40:36.931: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.8.232 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:40:36.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:40:36.931: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:40:36.931: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-7415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.8.232+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 05:40:38.029: INFO: Found all 1 expected endpoints: [netserver-3]
    Aug 26 05:40:38.029: INFO: Going to poll 10.20.199.120 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Aug 26 05:40:38.033: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.199.120 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7415 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:40:38.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:40:38.034: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:40:38.034: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-7415/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.20.199.120+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 05:40:39.126: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:40:39.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7415" for this suite. 08/26/23 05:40:39.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:40:39.142
Aug 26 05:40:39.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:40:39.143
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:39.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:39.165
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:40:39.168
Aug 26 05:40:39.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170" in namespace "downward-api-9469" to be "Succeeded or Failed"
Aug 26 05:40:39.185: INFO: Pod "downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170": Phase="Pending", Reason="", readiness=false. Elapsed: 4.972418ms
Aug 26 05:40:41.191: INFO: Pod "downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010230358s
Aug 26 05:40:43.193: INFO: Pod "downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012746427s
STEP: Saw pod success 08/26/23 05:40:43.193
Aug 26 05:40:43.193: INFO: Pod "downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170" satisfied condition "Succeeded or Failed"
Aug 26 05:40:43.198: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170 container client-container: <nil>
STEP: delete the pod 08/26/23 05:40:43.207
Aug 26 05:40:43.225: INFO: Waiting for pod downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170 to disappear
Aug 26 05:40:43.228: INFO: Pod downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 05:40:43.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9469" for this suite. 08/26/23 05:40:43.235
------------------------------
• [4.101 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:40:39.142
    Aug 26 05:40:39.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:40:39.143
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:39.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:39.165
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:40:39.168
    Aug 26 05:40:39.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170" in namespace "downward-api-9469" to be "Succeeded or Failed"
    Aug 26 05:40:39.185: INFO: Pod "downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170": Phase="Pending", Reason="", readiness=false. Elapsed: 4.972418ms
    Aug 26 05:40:41.191: INFO: Pod "downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010230358s
    Aug 26 05:40:43.193: INFO: Pod "downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012746427s
    STEP: Saw pod success 08/26/23 05:40:43.193
    Aug 26 05:40:43.193: INFO: Pod "downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170" satisfied condition "Succeeded or Failed"
    Aug 26 05:40:43.198: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170 container client-container: <nil>
    STEP: delete the pod 08/26/23 05:40:43.207
    Aug 26 05:40:43.225: INFO: Waiting for pod downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170 to disappear
    Aug 26 05:40:43.228: INFO: Pod downwardapi-volume-38d3ef33-7e64-49a7-8959-ac9ed850b170 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:40:43.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9469" for this suite. 08/26/23 05:40:43.235
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:40:43.244
Aug 26 05:40:43.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 05:40:43.245
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:43.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:43.266
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-3840/secret-test-b1aa8c0d-7d4d-491f-90ea-00b1c97a52ba 08/26/23 05:40:43.268
STEP: Creating a pod to test consume secrets 08/26/23 05:40:43.275
Aug 26 05:40:43.284: INFO: Waiting up to 5m0s for pod "pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00" in namespace "secrets-3840" to be "Succeeded or Failed"
Aug 26 05:40:43.291: INFO: Pod "pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00": Phase="Pending", Reason="", readiness=false. Elapsed: 7.033958ms
Aug 26 05:40:45.297: INFO: Pod "pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012528011s
Aug 26 05:40:47.296: INFO: Pod "pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012098349s
STEP: Saw pod success 08/26/23 05:40:47.296
Aug 26 05:40:47.296: INFO: Pod "pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00" satisfied condition "Succeeded or Failed"
Aug 26 05:40:47.302: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00 container env-test: <nil>
STEP: delete the pod 08/26/23 05:40:47.311
Aug 26 05:40:47.327: INFO: Waiting for pod pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00 to disappear
Aug 26 05:40:47.339: INFO: Pod pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 05:40:47.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3840" for this suite. 08/26/23 05:40:47.347
------------------------------
• [4.111 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:40:43.244
    Aug 26 05:40:43.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 05:40:43.245
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:43.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:43.266
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-3840/secret-test-b1aa8c0d-7d4d-491f-90ea-00b1c97a52ba 08/26/23 05:40:43.268
    STEP: Creating a pod to test consume secrets 08/26/23 05:40:43.275
    Aug 26 05:40:43.284: INFO: Waiting up to 5m0s for pod "pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00" in namespace "secrets-3840" to be "Succeeded or Failed"
    Aug 26 05:40:43.291: INFO: Pod "pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00": Phase="Pending", Reason="", readiness=false. Elapsed: 7.033958ms
    Aug 26 05:40:45.297: INFO: Pod "pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012528011s
    Aug 26 05:40:47.296: INFO: Pod "pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012098349s
    STEP: Saw pod success 08/26/23 05:40:47.296
    Aug 26 05:40:47.296: INFO: Pod "pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00" satisfied condition "Succeeded or Failed"
    Aug 26 05:40:47.302: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00 container env-test: <nil>
    STEP: delete the pod 08/26/23 05:40:47.311
    Aug 26 05:40:47.327: INFO: Waiting for pod pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00 to disappear
    Aug 26 05:40:47.339: INFO: Pod pod-configmaps-8931ed87-61ff-4c2b-aade-f84c2aa8af00 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:40:47.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3840" for this suite. 08/26/23 05:40:47.347
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:40:47.355
Aug 26 05:40:47.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:40:47.356
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:47.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:47.386
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 08/26/23 05:40:47.388
Aug 26 05:40:47.401: INFO: Waiting up to 5m0s for pod "downward-api-31687f0d-e068-4ad9-9412-04df84902cd9" in namespace "downward-api-9190" to be "Succeeded or Failed"
Aug 26 05:40:47.407: INFO: Pod "downward-api-31687f0d-e068-4ad9-9412-04df84902cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.690236ms
Aug 26 05:40:49.412: INFO: Pod "downward-api-31687f0d-e068-4ad9-9412-04df84902cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011514319s
Aug 26 05:40:51.412: INFO: Pod "downward-api-31687f0d-e068-4ad9-9412-04df84902cd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011709497s
STEP: Saw pod success 08/26/23 05:40:51.412
Aug 26 05:40:51.412: INFO: Pod "downward-api-31687f0d-e068-4ad9-9412-04df84902cd9" satisfied condition "Succeeded or Failed"
Aug 26 05:40:51.416: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downward-api-31687f0d-e068-4ad9-9412-04df84902cd9 container dapi-container: <nil>
STEP: delete the pod 08/26/23 05:40:51.424
Aug 26 05:40:51.443: INFO: Waiting for pod downward-api-31687f0d-e068-4ad9-9412-04df84902cd9 to disappear
Aug 26 05:40:51.449: INFO: Pod downward-api-31687f0d-e068-4ad9-9412-04df84902cd9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 26 05:40:51.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9190" for this suite. 08/26/23 05:40:51.461
------------------------------
• [4.116 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:40:47.355
    Aug 26 05:40:47.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:40:47.356
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:47.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:47.386
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 08/26/23 05:40:47.388
    Aug 26 05:40:47.401: INFO: Waiting up to 5m0s for pod "downward-api-31687f0d-e068-4ad9-9412-04df84902cd9" in namespace "downward-api-9190" to be "Succeeded or Failed"
    Aug 26 05:40:47.407: INFO: Pod "downward-api-31687f0d-e068-4ad9-9412-04df84902cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.690236ms
    Aug 26 05:40:49.412: INFO: Pod "downward-api-31687f0d-e068-4ad9-9412-04df84902cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011514319s
    Aug 26 05:40:51.412: INFO: Pod "downward-api-31687f0d-e068-4ad9-9412-04df84902cd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011709497s
    STEP: Saw pod success 08/26/23 05:40:51.412
    Aug 26 05:40:51.412: INFO: Pod "downward-api-31687f0d-e068-4ad9-9412-04df84902cd9" satisfied condition "Succeeded or Failed"
    Aug 26 05:40:51.416: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downward-api-31687f0d-e068-4ad9-9412-04df84902cd9 container dapi-container: <nil>
    STEP: delete the pod 08/26/23 05:40:51.424
    Aug 26 05:40:51.443: INFO: Waiting for pod downward-api-31687f0d-e068-4ad9-9412-04df84902cd9 to disappear
    Aug 26 05:40:51.449: INFO: Pod downward-api-31687f0d-e068-4ad9-9412-04df84902cd9 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:40:51.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9190" for this suite. 08/26/23 05:40:51.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:40:51.473
Aug 26 05:40:51.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 05:40:51.479
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:51.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:51.52
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 08/26/23 05:40:51.535
STEP: watching for the Service to be added 08/26/23 05:40:51.654
Aug 26 05:40:51.657: INFO: Found Service test-service-tw5fw in namespace services-3982 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 26 05:40:51.657: INFO: Service test-service-tw5fw created
STEP: Getting /status 08/26/23 05:40:51.657
Aug 26 05:40:51.664: INFO: Service test-service-tw5fw has LoadBalancer: {[]}
STEP: patching the ServiceStatus 08/26/23 05:40:51.664
STEP: watching for the Service to be patched 08/26/23 05:40:51.672
Aug 26 05:40:51.677: INFO: observed Service test-service-tw5fw in namespace services-3982 with annotations: map[] & LoadBalancer: {[]}
Aug 26 05:40:51.677: INFO: Found Service test-service-tw5fw in namespace services-3982 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 26 05:40:51.677: INFO: Service test-service-tw5fw has service status patched
STEP: updating the ServiceStatus 08/26/23 05:40:51.677
Aug 26 05:40:51.690: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 08/26/23 05:40:51.69
Aug 26 05:40:51.693: INFO: Observed Service test-service-tw5fw in namespace services-3982 with annotations: map[] & Conditions: {[]}
Aug 26 05:40:51.693: INFO: Observed event: &Service{ObjectMeta:{test-service-tw5fw  services-3982  da5dfc29-7848-4c04-b5aa-c3d249d79f81 15147 0 2023-08-26 05:40:51 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-26 05:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-26 05:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.21.239.15,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.21.239.15],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 26 05:40:51.693: INFO: Found Service test-service-tw5fw in namespace services-3982 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 26 05:40:51.693: INFO: Service test-service-tw5fw has service status updated
STEP: patching the service 08/26/23 05:40:51.693
STEP: watching for the Service to be patched 08/26/23 05:40:51.711
Aug 26 05:40:51.718: INFO: observed Service test-service-tw5fw in namespace services-3982 with labels: map[test-service-static:true]
Aug 26 05:40:51.718: INFO: observed Service test-service-tw5fw in namespace services-3982 with labels: map[test-service-static:true]
Aug 26 05:40:51.718: INFO: observed Service test-service-tw5fw in namespace services-3982 with labels: map[test-service-static:true]
Aug 26 05:40:51.718: INFO: Found Service test-service-tw5fw in namespace services-3982 with labels: map[test-service:patched test-service-static:true]
Aug 26 05:40:51.718: INFO: Service test-service-tw5fw patched
STEP: deleting the service 08/26/23 05:40:51.718
STEP: watching for the Service to be deleted 08/26/23 05:40:51.859
Aug 26 05:40:51.862: INFO: Observed event: ADDED
Aug 26 05:40:51.862: INFO: Observed event: MODIFIED
Aug 26 05:40:51.862: INFO: Observed event: MODIFIED
Aug 26 05:40:51.862: INFO: Observed event: MODIFIED
Aug 26 05:40:51.862: INFO: Found Service test-service-tw5fw in namespace services-3982 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 26 05:40:51.862: INFO: Service test-service-tw5fw deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 05:40:51.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3982" for this suite. 08/26/23 05:40:51.872
------------------------------
• [0.410 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:40:51.473
    Aug 26 05:40:51.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 05:40:51.479
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:51.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:51.52
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 08/26/23 05:40:51.535
    STEP: watching for the Service to be added 08/26/23 05:40:51.654
    Aug 26 05:40:51.657: INFO: Found Service test-service-tw5fw in namespace services-3982 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Aug 26 05:40:51.657: INFO: Service test-service-tw5fw created
    STEP: Getting /status 08/26/23 05:40:51.657
    Aug 26 05:40:51.664: INFO: Service test-service-tw5fw has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 08/26/23 05:40:51.664
    STEP: watching for the Service to be patched 08/26/23 05:40:51.672
    Aug 26 05:40:51.677: INFO: observed Service test-service-tw5fw in namespace services-3982 with annotations: map[] & LoadBalancer: {[]}
    Aug 26 05:40:51.677: INFO: Found Service test-service-tw5fw in namespace services-3982 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Aug 26 05:40:51.677: INFO: Service test-service-tw5fw has service status patched
    STEP: updating the ServiceStatus 08/26/23 05:40:51.677
    Aug 26 05:40:51.690: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 08/26/23 05:40:51.69
    Aug 26 05:40:51.693: INFO: Observed Service test-service-tw5fw in namespace services-3982 with annotations: map[] & Conditions: {[]}
    Aug 26 05:40:51.693: INFO: Observed event: &Service{ObjectMeta:{test-service-tw5fw  services-3982  da5dfc29-7848-4c04-b5aa-c3d249d79f81 15147 0 2023-08-26 05:40:51 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-26 05:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-26 05:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.21.239.15,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.21.239.15],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Aug 26 05:40:51.693: INFO: Found Service test-service-tw5fw in namespace services-3982 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 26 05:40:51.693: INFO: Service test-service-tw5fw has service status updated
    STEP: patching the service 08/26/23 05:40:51.693
    STEP: watching for the Service to be patched 08/26/23 05:40:51.711
    Aug 26 05:40:51.718: INFO: observed Service test-service-tw5fw in namespace services-3982 with labels: map[test-service-static:true]
    Aug 26 05:40:51.718: INFO: observed Service test-service-tw5fw in namespace services-3982 with labels: map[test-service-static:true]
    Aug 26 05:40:51.718: INFO: observed Service test-service-tw5fw in namespace services-3982 with labels: map[test-service-static:true]
    Aug 26 05:40:51.718: INFO: Found Service test-service-tw5fw in namespace services-3982 with labels: map[test-service:patched test-service-static:true]
    Aug 26 05:40:51.718: INFO: Service test-service-tw5fw patched
    STEP: deleting the service 08/26/23 05:40:51.718
    STEP: watching for the Service to be deleted 08/26/23 05:40:51.859
    Aug 26 05:40:51.862: INFO: Observed event: ADDED
    Aug 26 05:40:51.862: INFO: Observed event: MODIFIED
    Aug 26 05:40:51.862: INFO: Observed event: MODIFIED
    Aug 26 05:40:51.862: INFO: Observed event: MODIFIED
    Aug 26 05:40:51.862: INFO: Found Service test-service-tw5fw in namespace services-3982 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Aug 26 05:40:51.862: INFO: Service test-service-tw5fw deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:40:51.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3982" for this suite. 08/26/23 05:40:51.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:40:51.883
Aug 26 05:40:51.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 05:40:51.884
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:51.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:51.921
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-5635a1f2-3243-4e35-8a89-947a6e2edf82 08/26/23 05:40:51.938
STEP: Creating the pod 08/26/23 05:40:51.944
Aug 26 05:40:51.958: INFO: Waiting up to 5m0s for pod "pod-configmaps-72815ece-7eb1-408d-9799-49cd431ae9dc" in namespace "configmap-8560" to be "running"
Aug 26 05:40:51.966: INFO: Pod "pod-configmaps-72815ece-7eb1-408d-9799-49cd431ae9dc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.427172ms
Aug 26 05:40:53.970: INFO: Pod "pod-configmaps-72815ece-7eb1-408d-9799-49cd431ae9dc": Phase="Running", Reason="", readiness=false. Elapsed: 2.012117504s
Aug 26 05:40:53.970: INFO: Pod "pod-configmaps-72815ece-7eb1-408d-9799-49cd431ae9dc" satisfied condition "running"
STEP: Waiting for pod with text data 08/26/23 05:40:53.97
STEP: Waiting for pod with binary data 08/26/23 05:40:53.978
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:40:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8560" for this suite. 08/26/23 05:40:53.998
------------------------------
• [2.127 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:40:51.883
    Aug 26 05:40:51.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 05:40:51.884
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:51.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:51.921
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-5635a1f2-3243-4e35-8a89-947a6e2edf82 08/26/23 05:40:51.938
    STEP: Creating the pod 08/26/23 05:40:51.944
    Aug 26 05:40:51.958: INFO: Waiting up to 5m0s for pod "pod-configmaps-72815ece-7eb1-408d-9799-49cd431ae9dc" in namespace "configmap-8560" to be "running"
    Aug 26 05:40:51.966: INFO: Pod "pod-configmaps-72815ece-7eb1-408d-9799-49cd431ae9dc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.427172ms
    Aug 26 05:40:53.970: INFO: Pod "pod-configmaps-72815ece-7eb1-408d-9799-49cd431ae9dc": Phase="Running", Reason="", readiness=false. Elapsed: 2.012117504s
    Aug 26 05:40:53.970: INFO: Pod "pod-configmaps-72815ece-7eb1-408d-9799-49cd431ae9dc" satisfied condition "running"
    STEP: Waiting for pod with text data 08/26/23 05:40:53.97
    STEP: Waiting for pod with binary data 08/26/23 05:40:53.978
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:40:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8560" for this suite. 08/26/23 05:40:53.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:40:54.01
Aug 26 05:40:54.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename daemonsets 08/26/23 05:40:54.012
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:54.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:54.037
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 08/26/23 05:40:54.089
STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 05:40:54.1
Aug 26 05:40:54.112: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:54.113: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:54.113: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:54.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 05:40:54.129: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 05:40:55.139: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:55.139: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:55.139: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:55.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 05:40:55.146: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 05:40:56.143: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:56.143: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:56.143: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:56.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 26 05:40:56.155: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/26/23 05:40:56.162
Aug 26 05:40:56.187: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:56.187: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:56.187: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 05:40:56.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 26 05:40:56.196: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 08/26/23 05:40:56.196
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/26/23 05:40:56.203
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6528, will wait for the garbage collector to delete the pods 08/26/23 05:40:56.203
Aug 26 05:40:56.267: INFO: Deleting DaemonSet.extensions daemon-set took: 7.295072ms
Aug 26 05:40:56.368: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.041631ms
Aug 26 05:40:58.972: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 05:40:58.972: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 26 05:40:58.976: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15355"},"items":null}

Aug 26 05:40:58.979: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15355"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:40:59.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6528" for this suite. 08/26/23 05:40:59.024
------------------------------
• [SLOW TEST] [5.028 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:40:54.01
    Aug 26 05:40:54.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename daemonsets 08/26/23 05:40:54.012
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:54.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:54.037
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 08/26/23 05:40:54.089
    STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 05:40:54.1
    Aug 26 05:40:54.112: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:54.113: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:54.113: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:54.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 05:40:54.129: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 05:40:55.139: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:55.139: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:55.139: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:55.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 05:40:55.146: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 05:40:56.143: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:56.143: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:56.143: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:56.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 26 05:40:56.155: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/26/23 05:40:56.162
    Aug 26 05:40:56.187: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:56.187: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:56.187: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 05:40:56.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 26 05:40:56.196: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 08/26/23 05:40:56.196
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/26/23 05:40:56.203
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6528, will wait for the garbage collector to delete the pods 08/26/23 05:40:56.203
    Aug 26 05:40:56.267: INFO: Deleting DaemonSet.extensions daemon-set took: 7.295072ms
    Aug 26 05:40:56.368: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.041631ms
    Aug 26 05:40:58.972: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 05:40:58.972: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 26 05:40:58.976: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15355"},"items":null}

    Aug 26 05:40:58.979: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15355"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:40:59.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6528" for this suite. 08/26/23 05:40:59.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:40:59.041
Aug 26 05:40:59.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 05:40:59.042
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:59.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:59.069
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-f4920e5c-56b6-4629-9a90-dcb74f55be38 08/26/23 05:40:59.075
STEP: Creating a pod to test consume configMaps 08/26/23 05:40:59.08
Aug 26 05:40:59.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e" in namespace "configmap-3957" to be "Succeeded or Failed"
Aug 26 05:40:59.096: INFO: Pod "pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.895559ms
Aug 26 05:41:01.102: INFO: Pod "pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009287812s
Aug 26 05:41:03.106: INFO: Pod "pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013228351s
STEP: Saw pod success 08/26/23 05:41:03.106
Aug 26 05:41:03.106: INFO: Pod "pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e" satisfied condition "Succeeded or Failed"
Aug 26 05:41:03.110: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e container agnhost-container: <nil>
STEP: delete the pod 08/26/23 05:41:03.118
Aug 26 05:41:03.134: INFO: Waiting for pod pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e to disappear
Aug 26 05:41:03.138: INFO: Pod pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:41:03.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3957" for this suite. 08/26/23 05:41:03.151
------------------------------
• [4.125 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:40:59.041
    Aug 26 05:40:59.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 05:40:59.042
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:40:59.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:40:59.069
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-f4920e5c-56b6-4629-9a90-dcb74f55be38 08/26/23 05:40:59.075
    STEP: Creating a pod to test consume configMaps 08/26/23 05:40:59.08
    Aug 26 05:40:59.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e" in namespace "configmap-3957" to be "Succeeded or Failed"
    Aug 26 05:40:59.096: INFO: Pod "pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.895559ms
    Aug 26 05:41:01.102: INFO: Pod "pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009287812s
    Aug 26 05:41:03.106: INFO: Pod "pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013228351s
    STEP: Saw pod success 08/26/23 05:41:03.106
    Aug 26 05:41:03.106: INFO: Pod "pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e" satisfied condition "Succeeded or Failed"
    Aug 26 05:41:03.110: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 05:41:03.118
    Aug 26 05:41:03.134: INFO: Waiting for pod pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e to disappear
    Aug 26 05:41:03.138: INFO: Pod pod-configmaps-825b483b-c676-46ba-afbc-33cbbcbed83e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:41:03.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3957" for this suite. 08/26/23 05:41:03.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:41:03.167
Aug 26 05:41:03.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename statefulset 08/26/23 05:41:03.168
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:41:03.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:41:03.199
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2982 08/26/23 05:41:03.207
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 08/26/23 05:41:03.217
STEP: Creating stateful set ss in namespace statefulset-2982 08/26/23 05:41:03.226
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2982 08/26/23 05:41:03.234
Aug 26 05:41:03.237: INFO: Found 0 stateful pods, waiting for 1
Aug 26 05:41:13.242: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/26/23 05:41:13.242
Aug 26 05:41:13.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 26 05:41:13.467: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 26 05:41:13.467: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 26 05:41:13.467: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 26 05:41:13.472: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 26 05:41:23.478: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 05:41:23.478: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 05:41:23.497: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999966s
Aug 26 05:41:24.503: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993038234s
Aug 26 05:41:25.524: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988376004s
Aug 26 05:41:26.530: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.966121691s
Aug 26 05:41:27.535: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.961057441s
Aug 26 05:41:28.540: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.956170298s
Aug 26 05:41:29.544: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.951616738s
Aug 26 05:41:30.550: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.946695869s
Aug 26 05:41:31.554: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.94175423s
Aug 26 05:41:32.558: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.228565ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2982 08/26/23 05:41:33.559
Aug 26 05:41:33.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 26 05:41:33.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 26 05:41:33.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 26 05:41:33.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 26 05:41:33.775: INFO: Found 1 stateful pods, waiting for 3
Aug 26 05:41:43.783: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 05:41:43.783: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 05:41:43.783: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 08/26/23 05:41:43.784
STEP: Scale down will halt with unhealthy stateful pod 08/26/23 05:41:43.784
Aug 26 05:41:43.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 26 05:41:44.009: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 26 05:41:44.009: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 26 05:41:44.009: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 26 05:41:44.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 26 05:41:44.171: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 26 05:41:44.171: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 26 05:41:44.171: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 26 05:41:44.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 26 05:41:44.356: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 26 05:41:44.356: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 26 05:41:44.356: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 26 05:41:44.356: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 05:41:44.362: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 26 05:41:54.375: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 05:41:54.375: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 05:41:54.375: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 05:41:54.400: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999666s
Aug 26 05:41:55.407: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991598244s
Aug 26 05:41:56.413: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983614796s
Aug 26 05:41:57.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978503616s
Aug 26 05:41:58.424: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.973150913s
Aug 26 05:41:59.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967424826s
Aug 26 05:42:00.436: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961190947s
Aug 26 05:42:01.441: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955566979s
Aug 26 05:42:02.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.94970981s
Aug 26 05:42:03.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.979188ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2982 08/26/23 05:42:04.454
Aug 26 05:42:04.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 26 05:42:04.631: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 26 05:42:04.631: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 26 05:42:04.631: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 26 05:42:04.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 26 05:42:04.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 26 05:42:04.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 26 05:42:04.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 26 05:42:04.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 26 05:42:04.991: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 26 05:42:04.991: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 26 05:42:04.991: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 26 05:42:04.991: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 08/26/23 05:42:15.027
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 26 05:42:15.027: INFO: Deleting all statefulset in ns statefulset-2982
Aug 26 05:42:15.037: INFO: Scaling statefulset ss to 0
Aug 26 05:42:15.055: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 05:42:15.059: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:15.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2982" for this suite. 08/26/23 05:42:15.102
------------------------------
• [SLOW TEST] [71.946 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:41:03.167
    Aug 26 05:41:03.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename statefulset 08/26/23 05:41:03.168
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:41:03.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:41:03.199
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2982 08/26/23 05:41:03.207
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 08/26/23 05:41:03.217
    STEP: Creating stateful set ss in namespace statefulset-2982 08/26/23 05:41:03.226
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2982 08/26/23 05:41:03.234
    Aug 26 05:41:03.237: INFO: Found 0 stateful pods, waiting for 1
    Aug 26 05:41:13.242: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/26/23 05:41:13.242
    Aug 26 05:41:13.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 26 05:41:13.467: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 26 05:41:13.467: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 26 05:41:13.467: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 26 05:41:13.472: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 26 05:41:23.478: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 26 05:41:23.478: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 05:41:23.497: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999966s
    Aug 26 05:41:24.503: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993038234s
    Aug 26 05:41:25.524: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988376004s
    Aug 26 05:41:26.530: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.966121691s
    Aug 26 05:41:27.535: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.961057441s
    Aug 26 05:41:28.540: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.956170298s
    Aug 26 05:41:29.544: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.951616738s
    Aug 26 05:41:30.550: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.946695869s
    Aug 26 05:41:31.554: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.94175423s
    Aug 26 05:41:32.558: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.228565ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2982 08/26/23 05:41:33.559
    Aug 26 05:41:33.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 26 05:41:33.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 26 05:41:33.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 26 05:41:33.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 26 05:41:33.775: INFO: Found 1 stateful pods, waiting for 3
    Aug 26 05:41:43.783: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 05:41:43.783: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 05:41:43.783: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 08/26/23 05:41:43.784
    STEP: Scale down will halt with unhealthy stateful pod 08/26/23 05:41:43.784
    Aug 26 05:41:43.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 26 05:41:44.009: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 26 05:41:44.009: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 26 05:41:44.009: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 26 05:41:44.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 26 05:41:44.171: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 26 05:41:44.171: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 26 05:41:44.171: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 26 05:41:44.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 26 05:41:44.356: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 26 05:41:44.356: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 26 05:41:44.356: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 26 05:41:44.356: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 05:41:44.362: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Aug 26 05:41:54.375: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 26 05:41:54.375: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 26 05:41:54.375: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 26 05:41:54.400: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999666s
    Aug 26 05:41:55.407: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991598244s
    Aug 26 05:41:56.413: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983614796s
    Aug 26 05:41:57.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978503616s
    Aug 26 05:41:58.424: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.973150913s
    Aug 26 05:41:59.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967424826s
    Aug 26 05:42:00.436: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961190947s
    Aug 26 05:42:01.441: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955566979s
    Aug 26 05:42:02.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.94970981s
    Aug 26 05:42:03.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.979188ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2982 08/26/23 05:42:04.454
    Aug 26 05:42:04.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 26 05:42:04.631: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 26 05:42:04.631: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 26 05:42:04.631: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 26 05:42:04.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 26 05:42:04.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 26 05:42:04.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 26 05:42:04.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 26 05:42:04.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=statefulset-2982 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 26 05:42:04.991: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 26 05:42:04.991: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 26 05:42:04.991: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 26 05:42:04.991: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 08/26/23 05:42:15.027
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 26 05:42:15.027: INFO: Deleting all statefulset in ns statefulset-2982
    Aug 26 05:42:15.037: INFO: Scaling statefulset ss to 0
    Aug 26 05:42:15.055: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 05:42:15.059: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:15.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2982" for this suite. 08/26/23 05:42:15.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:15.115
Aug 26 05:42:15.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 05:42:15.116
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:15.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:15.151
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Aug 26 05:42:15.167: INFO: Waiting up to 5m0s for pod "server-envvars-56f71056-0507-4889-947f-f4065d69ee33" in namespace "pods-6870" to be "running and ready"
Aug 26 05:42:15.174: INFO: Pod "server-envvars-56f71056-0507-4889-947f-f4065d69ee33": Phase="Pending", Reason="", readiness=false. Elapsed: 7.341307ms
Aug 26 05:42:15.174: INFO: The phase of Pod server-envvars-56f71056-0507-4889-947f-f4065d69ee33 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:42:17.179: INFO: Pod "server-envvars-56f71056-0507-4889-947f-f4065d69ee33": Phase="Running", Reason="", readiness=true. Elapsed: 2.012199567s
Aug 26 05:42:17.179: INFO: The phase of Pod server-envvars-56f71056-0507-4889-947f-f4065d69ee33 is Running (Ready = true)
Aug 26 05:42:17.179: INFO: Pod "server-envvars-56f71056-0507-4889-947f-f4065d69ee33" satisfied condition "running and ready"
Aug 26 05:42:17.214: INFO: Waiting up to 5m0s for pod "client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a" in namespace "pods-6870" to be "Succeeded or Failed"
Aug 26 05:42:17.226: INFO: Pod "client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019311ms
Aug 26 05:42:19.234: INFO: Pod "client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019818776s
Aug 26 05:42:21.231: INFO: Pod "client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016935403s
STEP: Saw pod success 08/26/23 05:42:21.231
Aug 26 05:42:21.231: INFO: Pod "client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a" satisfied condition "Succeeded or Failed"
Aug 26 05:42:21.239: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a container env3cont: <nil>
STEP: delete the pod 08/26/23 05:42:21.25
Aug 26 05:42:21.272: INFO: Waiting for pod client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a to disappear
Aug 26 05:42:21.281: INFO: Pod client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:21.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6870" for this suite. 08/26/23 05:42:21.299
------------------------------
• [SLOW TEST] [6.195 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:15.115
    Aug 26 05:42:15.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 05:42:15.116
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:15.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:15.151
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Aug 26 05:42:15.167: INFO: Waiting up to 5m0s for pod "server-envvars-56f71056-0507-4889-947f-f4065d69ee33" in namespace "pods-6870" to be "running and ready"
    Aug 26 05:42:15.174: INFO: Pod "server-envvars-56f71056-0507-4889-947f-f4065d69ee33": Phase="Pending", Reason="", readiness=false. Elapsed: 7.341307ms
    Aug 26 05:42:15.174: INFO: The phase of Pod server-envvars-56f71056-0507-4889-947f-f4065d69ee33 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:42:17.179: INFO: Pod "server-envvars-56f71056-0507-4889-947f-f4065d69ee33": Phase="Running", Reason="", readiness=true. Elapsed: 2.012199567s
    Aug 26 05:42:17.179: INFO: The phase of Pod server-envvars-56f71056-0507-4889-947f-f4065d69ee33 is Running (Ready = true)
    Aug 26 05:42:17.179: INFO: Pod "server-envvars-56f71056-0507-4889-947f-f4065d69ee33" satisfied condition "running and ready"
    Aug 26 05:42:17.214: INFO: Waiting up to 5m0s for pod "client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a" in namespace "pods-6870" to be "Succeeded or Failed"
    Aug 26 05:42:17.226: INFO: Pod "client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019311ms
    Aug 26 05:42:19.234: INFO: Pod "client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019818776s
    Aug 26 05:42:21.231: INFO: Pod "client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016935403s
    STEP: Saw pod success 08/26/23 05:42:21.231
    Aug 26 05:42:21.231: INFO: Pod "client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a" satisfied condition "Succeeded or Failed"
    Aug 26 05:42:21.239: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a container env3cont: <nil>
    STEP: delete the pod 08/26/23 05:42:21.25
    Aug 26 05:42:21.272: INFO: Waiting for pod client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a to disappear
    Aug 26 05:42:21.281: INFO: Pod client-envvars-cc100a0c-ae3d-48dc-ac97-9c0dd5638b9a no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:21.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6870" for this suite. 08/26/23 05:42:21.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:21.312
Aug 26 05:42:21.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename var-expansion 08/26/23 05:42:21.313
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:21.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:21.342
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Aug 26 05:42:21.361: INFO: Waiting up to 2m0s for pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2" in namespace "var-expansion-8079" to be "container 0 failed with reason CreateContainerConfigError"
Aug 26 05:42:21.365: INFO: Pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.557436ms
Aug 26 05:42:23.372: INFO: Pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010255336s
Aug 26 05:42:23.372: INFO: Pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 26 05:42:23.372: INFO: Deleting pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2" in namespace "var-expansion-8079"
Aug 26 05:42:23.381: INFO: Wait up to 5m0s for pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:27.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8079" for this suite. 08/26/23 05:42:27.397
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:21.312
    Aug 26 05:42:21.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename var-expansion 08/26/23 05:42:21.313
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:21.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:21.342
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Aug 26 05:42:21.361: INFO: Waiting up to 2m0s for pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2" in namespace "var-expansion-8079" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 26 05:42:21.365: INFO: Pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.557436ms
    Aug 26 05:42:23.372: INFO: Pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010255336s
    Aug 26 05:42:23.372: INFO: Pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 26 05:42:23.372: INFO: Deleting pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2" in namespace "var-expansion-8079"
    Aug 26 05:42:23.381: INFO: Wait up to 5m0s for pod "var-expansion-6898e0cf-1988-4ad7-803a-4756453df7f2" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:27.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8079" for this suite. 08/26/23 05:42:27.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:27.403
Aug 26 05:42:27.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename var-expansion 08/26/23 05:42:27.404
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:27.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:27.425
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 08/26/23 05:42:27.428
Aug 26 05:42:27.439: INFO: Waiting up to 5m0s for pod "var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2" in namespace "var-expansion-4817" to be "Succeeded or Failed"
Aug 26 05:42:27.442: INFO: Pod "var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.304426ms
Aug 26 05:42:29.448: INFO: Pod "var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009212202s
Aug 26 05:42:31.448: INFO: Pod "var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009070227s
STEP: Saw pod success 08/26/23 05:42:31.448
Aug 26 05:42:31.448: INFO: Pod "var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2" satisfied condition "Succeeded or Failed"
Aug 26 05:42:31.453: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2 container dapi-container: <nil>
STEP: delete the pod 08/26/23 05:42:31.463
Aug 26 05:42:31.478: INFO: Waiting for pod var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2 to disappear
Aug 26 05:42:31.481: INFO: Pod var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:31.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4817" for this suite. 08/26/23 05:42:31.489
------------------------------
• [4.094 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:27.403
    Aug 26 05:42:27.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename var-expansion 08/26/23 05:42:27.404
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:27.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:27.425
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 08/26/23 05:42:27.428
    Aug 26 05:42:27.439: INFO: Waiting up to 5m0s for pod "var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2" in namespace "var-expansion-4817" to be "Succeeded or Failed"
    Aug 26 05:42:27.442: INFO: Pod "var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.304426ms
    Aug 26 05:42:29.448: INFO: Pod "var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009212202s
    Aug 26 05:42:31.448: INFO: Pod "var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009070227s
    STEP: Saw pod success 08/26/23 05:42:31.448
    Aug 26 05:42:31.448: INFO: Pod "var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2" satisfied condition "Succeeded or Failed"
    Aug 26 05:42:31.453: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2 container dapi-container: <nil>
    STEP: delete the pod 08/26/23 05:42:31.463
    Aug 26 05:42:31.478: INFO: Waiting for pod var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2 to disappear
    Aug 26 05:42:31.481: INFO: Pod var-expansion-b73d2375-e607-4e43-b504-11241be2ccc2 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:31.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4817" for this suite. 08/26/23 05:42:31.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:31.499
Aug 26 05:42:31.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename proxy 08/26/23 05:42:31.5
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:31.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:31.539
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Aug 26 05:42:31.542: INFO: Creating pod...
Aug 26 05:42:31.555: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2269" to be "running"
Aug 26 05:42:31.573: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 18.218176ms
Aug 26 05:42:33.577: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.022602702s
Aug 26 05:42:33.577: INFO: Pod "agnhost" satisfied condition "running"
Aug 26 05:42:33.578: INFO: Creating service...
Aug 26 05:42:33.591: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=DELETE
Aug 26 05:42:33.598: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 26 05:42:33.599: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=OPTIONS
Aug 26 05:42:33.606: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 26 05:42:33.606: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=PATCH
Aug 26 05:42:33.610: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 26 05:42:33.610: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=POST
Aug 26 05:42:33.617: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 26 05:42:33.617: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=PUT
Aug 26 05:42:33.622: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 26 05:42:33.622: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 26 05:42:33.630: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 26 05:42:33.630: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 26 05:42:33.643: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 26 05:42:33.643: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 26 05:42:33.654: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 26 05:42:33.654: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=POST
Aug 26 05:42:33.662: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 26 05:42:33.663: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=PUT
Aug 26 05:42:33.670: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 26 05:42:33.670: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=GET
Aug 26 05:42:33.681: INFO: http.Client request:GET StatusCode:301
Aug 26 05:42:33.681: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=GET
Aug 26 05:42:33.688: INFO: http.Client request:GET StatusCode:301
Aug 26 05:42:33.688: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=HEAD
Aug 26 05:42:33.703: INFO: http.Client request:HEAD StatusCode:301
Aug 26 05:42:33.703: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 26 05:42:33.710: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:33.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2269" for this suite. 08/26/23 05:42:33.72
------------------------------
• [2.228 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:31.499
    Aug 26 05:42:31.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename proxy 08/26/23 05:42:31.5
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:31.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:31.539
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Aug 26 05:42:31.542: INFO: Creating pod...
    Aug 26 05:42:31.555: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2269" to be "running"
    Aug 26 05:42:31.573: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 18.218176ms
    Aug 26 05:42:33.577: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.022602702s
    Aug 26 05:42:33.577: INFO: Pod "agnhost" satisfied condition "running"
    Aug 26 05:42:33.578: INFO: Creating service...
    Aug 26 05:42:33.591: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=DELETE
    Aug 26 05:42:33.598: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 26 05:42:33.599: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=OPTIONS
    Aug 26 05:42:33.606: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 26 05:42:33.606: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=PATCH
    Aug 26 05:42:33.610: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 26 05:42:33.610: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=POST
    Aug 26 05:42:33.617: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 26 05:42:33.617: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=PUT
    Aug 26 05:42:33.622: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 26 05:42:33.622: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=DELETE
    Aug 26 05:42:33.630: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 26 05:42:33.630: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Aug 26 05:42:33.643: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 26 05:42:33.643: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=PATCH
    Aug 26 05:42:33.654: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 26 05:42:33.654: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=POST
    Aug 26 05:42:33.662: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 26 05:42:33.663: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=PUT
    Aug 26 05:42:33.670: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 26 05:42:33.670: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=GET
    Aug 26 05:42:33.681: INFO: http.Client request:GET StatusCode:301
    Aug 26 05:42:33.681: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=GET
    Aug 26 05:42:33.688: INFO: http.Client request:GET StatusCode:301
    Aug 26 05:42:33.688: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/pods/agnhost/proxy?method=HEAD
    Aug 26 05:42:33.703: INFO: http.Client request:HEAD StatusCode:301
    Aug 26 05:42:33.703: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-2269/services/e2e-proxy-test-service/proxy?method=HEAD
    Aug 26 05:42:33.710: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:33.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2269" for this suite. 08/26/23 05:42:33.72
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:33.728
Aug 26 05:42:33.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 05:42:33.73
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:33.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:33.755
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 08/26/23 05:42:33.759
STEP: submitting the pod to kubernetes 08/26/23 05:42:33.759
Aug 26 05:42:33.770: INFO: Waiting up to 5m0s for pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e" in namespace "pods-8014" to be "running and ready"
Aug 26 05:42:33.775: INFO: Pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.61336ms
Aug 26 05:42:33.775: INFO: The phase of Pod pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:42:35.780: INFO: Pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010722413s
Aug 26 05:42:35.780: INFO: The phase of Pod pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e is Running (Ready = true)
Aug 26 05:42:35.780: INFO: Pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/26/23 05:42:35.786
STEP: updating the pod 08/26/23 05:42:35.79
Aug 26 05:42:36.304: INFO: Successfully updated pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e"
Aug 26 05:42:36.304: INFO: Waiting up to 5m0s for pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e" in namespace "pods-8014" to be "running"
Aug 26 05:42:36.308: INFO: Pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e": Phase="Running", Reason="", readiness=true. Elapsed: 3.813783ms
Aug 26 05:42:36.308: INFO: Pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 08/26/23 05:42:36.308
Aug 26 05:42:36.312: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:36.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8014" for this suite. 08/26/23 05:42:36.318
------------------------------
• [2.597 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:33.728
    Aug 26 05:42:33.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 05:42:33.73
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:33.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:33.755
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 08/26/23 05:42:33.759
    STEP: submitting the pod to kubernetes 08/26/23 05:42:33.759
    Aug 26 05:42:33.770: INFO: Waiting up to 5m0s for pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e" in namespace "pods-8014" to be "running and ready"
    Aug 26 05:42:33.775: INFO: Pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.61336ms
    Aug 26 05:42:33.775: INFO: The phase of Pod pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:42:35.780: INFO: Pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010722413s
    Aug 26 05:42:35.780: INFO: The phase of Pod pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e is Running (Ready = true)
    Aug 26 05:42:35.780: INFO: Pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/26/23 05:42:35.786
    STEP: updating the pod 08/26/23 05:42:35.79
    Aug 26 05:42:36.304: INFO: Successfully updated pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e"
    Aug 26 05:42:36.304: INFO: Waiting up to 5m0s for pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e" in namespace "pods-8014" to be "running"
    Aug 26 05:42:36.308: INFO: Pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e": Phase="Running", Reason="", readiness=true. Elapsed: 3.813783ms
    Aug 26 05:42:36.308: INFO: Pod "pod-update-9557b33d-9b72-4b7a-81a0-8b920cae102e" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 08/26/23 05:42:36.308
    Aug 26 05:42:36.312: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:36.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8014" for this suite. 08/26/23 05:42:36.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:36.326
Aug 26 05:42:36.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename security-context-test 08/26/23 05:42:36.328
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:36.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:36.356
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Aug 26 05:42:36.370: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39" in namespace "security-context-test-1062" to be "Succeeded or Failed"
Aug 26 05:42:36.381: INFO: Pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.2951ms
Aug 26 05:42:38.386: INFO: Pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015453539s
Aug 26 05:42:40.386: INFO: Pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01594855s
Aug 26 05:42:40.386: INFO: Pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39" satisfied condition "Succeeded or Failed"
Aug 26 05:42:40.416: INFO: Got logs for pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:40.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1062" for this suite. 08/26/23 05:42:40.429
------------------------------
• [4.115 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:36.326
    Aug 26 05:42:36.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename security-context-test 08/26/23 05:42:36.328
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:36.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:36.356
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Aug 26 05:42:36.370: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39" in namespace "security-context-test-1062" to be "Succeeded or Failed"
    Aug 26 05:42:36.381: INFO: Pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.2951ms
    Aug 26 05:42:38.386: INFO: Pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015453539s
    Aug 26 05:42:40.386: INFO: Pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01594855s
    Aug 26 05:42:40.386: INFO: Pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39" satisfied condition "Succeeded or Failed"
    Aug 26 05:42:40.416: INFO: Got logs for pod "busybox-privileged-false-bb797e60-6470-4c0f-a2ac-5818f8b33a39": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:40.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1062" for this suite. 08/26/23 05:42:40.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:40.442
Aug 26 05:42:40.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir-wrapper 08/26/23 05:42:40.443
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:40.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:40.468
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Aug 26 05:42:40.495: INFO: Waiting up to 5m0s for pod "pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d" in namespace "emptydir-wrapper-1196" to be "running and ready"
Aug 26 05:42:40.503: INFO: Pod "pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.770343ms
Aug 26 05:42:40.503: INFO: The phase of Pod pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:42:42.508: INFO: Pod "pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.01308122s
Aug 26 05:42:42.508: INFO: The phase of Pod pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d is Running (Ready = true)
Aug 26 05:42:42.508: INFO: Pod "pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d" satisfied condition "running and ready"
STEP: Cleaning up the secret 08/26/23 05:42:42.512
STEP: Cleaning up the configmap 08/26/23 05:42:42.521
STEP: Cleaning up the pod 08/26/23 05:42:42.536
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:42.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1196" for this suite. 08/26/23 05:42:42.568
------------------------------
• [2.133 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:40.442
    Aug 26 05:42:40.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir-wrapper 08/26/23 05:42:40.443
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:40.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:40.468
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Aug 26 05:42:40.495: INFO: Waiting up to 5m0s for pod "pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d" in namespace "emptydir-wrapper-1196" to be "running and ready"
    Aug 26 05:42:40.503: INFO: Pod "pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.770343ms
    Aug 26 05:42:40.503: INFO: The phase of Pod pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:42:42.508: INFO: Pod "pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.01308122s
    Aug 26 05:42:42.508: INFO: The phase of Pod pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d is Running (Ready = true)
    Aug 26 05:42:42.508: INFO: Pod "pod-secrets-b0eb254b-1e9c-47c1-9c3b-2d0564b0fd0d" satisfied condition "running and ready"
    STEP: Cleaning up the secret 08/26/23 05:42:42.512
    STEP: Cleaning up the configmap 08/26/23 05:42:42.521
    STEP: Cleaning up the pod 08/26/23 05:42:42.536
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:42.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1196" for this suite. 08/26/23 05:42:42.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:42.576
Aug 26 05:42:42.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 05:42:42.577
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:42.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:42.61
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 08/26/23 05:42:42.614
Aug 26 05:42:42.626: INFO: Waiting up to 5m0s for pod "pod-630bcd82-6a8d-4b42-a339-485fac09ab68" in namespace "emptydir-5060" to be "Succeeded or Failed"
Aug 26 05:42:42.631: INFO: Pod "pod-630bcd82-6a8d-4b42-a339-485fac09ab68": Phase="Pending", Reason="", readiness=false. Elapsed: 5.457612ms
Aug 26 05:42:44.636: INFO: Pod "pod-630bcd82-6a8d-4b42-a339-485fac09ab68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009892514s
Aug 26 05:42:46.636: INFO: Pod "pod-630bcd82-6a8d-4b42-a339-485fac09ab68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01062129s
STEP: Saw pod success 08/26/23 05:42:46.636
Aug 26 05:42:46.637: INFO: Pod "pod-630bcd82-6a8d-4b42-a339-485fac09ab68" satisfied condition "Succeeded or Failed"
Aug 26 05:42:46.641: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-630bcd82-6a8d-4b42-a339-485fac09ab68 container test-container: <nil>
STEP: delete the pod 08/26/23 05:42:46.649
Aug 26 05:42:46.665: INFO: Waiting for pod pod-630bcd82-6a8d-4b42-a339-485fac09ab68 to disappear
Aug 26 05:42:46.668: INFO: Pod pod-630bcd82-6a8d-4b42-a339-485fac09ab68 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:46.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5060" for this suite. 08/26/23 05:42:46.678
------------------------------
• [4.111 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:42.576
    Aug 26 05:42:42.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 05:42:42.577
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:42.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:42.61
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 08/26/23 05:42:42.614
    Aug 26 05:42:42.626: INFO: Waiting up to 5m0s for pod "pod-630bcd82-6a8d-4b42-a339-485fac09ab68" in namespace "emptydir-5060" to be "Succeeded or Failed"
    Aug 26 05:42:42.631: INFO: Pod "pod-630bcd82-6a8d-4b42-a339-485fac09ab68": Phase="Pending", Reason="", readiness=false. Elapsed: 5.457612ms
    Aug 26 05:42:44.636: INFO: Pod "pod-630bcd82-6a8d-4b42-a339-485fac09ab68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009892514s
    Aug 26 05:42:46.636: INFO: Pod "pod-630bcd82-6a8d-4b42-a339-485fac09ab68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01062129s
    STEP: Saw pod success 08/26/23 05:42:46.636
    Aug 26 05:42:46.637: INFO: Pod "pod-630bcd82-6a8d-4b42-a339-485fac09ab68" satisfied condition "Succeeded or Failed"
    Aug 26 05:42:46.641: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-630bcd82-6a8d-4b42-a339-485fac09ab68 container test-container: <nil>
    STEP: delete the pod 08/26/23 05:42:46.649
    Aug 26 05:42:46.665: INFO: Waiting for pod pod-630bcd82-6a8d-4b42-a339-485fac09ab68 to disappear
    Aug 26 05:42:46.668: INFO: Pod pod-630bcd82-6a8d-4b42-a339-485fac09ab68 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:46.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5060" for this suite. 08/26/23 05:42:46.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:46.69
Aug 26 05:42:46.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 05:42:46.691
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:46.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:46.72
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Aug 26 05:42:46.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-645 version'
Aug 26 05:42:46.814: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 26 05:42:46.814: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:23:27Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:16:45Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:46.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-645" for this suite. 08/26/23 05:42:46.825
------------------------------
• [0.148 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:46.69
    Aug 26 05:42:46.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 05:42:46.691
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:46.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:46.72
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Aug 26 05:42:46.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-645 version'
    Aug 26 05:42:46.814: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Aug 26 05:42:46.814: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:23:27Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:16:45Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:46.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-645" for this suite. 08/26/23 05:42:46.825
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:46.841
Aug 26 05:42:46.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubelet-test 08/26/23 05:42:46.842
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:46.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:46.869
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Aug 26 05:42:46.884: INFO: Waiting up to 5m0s for pod "busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828" in namespace "kubelet-test-3504" to be "running and ready"
Aug 26 05:42:46.891: INFO: Pod "busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828": Phase="Pending", Reason="", readiness=false. Elapsed: 6.279518ms
Aug 26 05:42:46.891: INFO: The phase of Pod busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:42:48.896: INFO: Pod "busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828": Phase="Running", Reason="", readiness=true. Elapsed: 2.011869144s
Aug 26 05:42:48.896: INFO: The phase of Pod busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828 is Running (Ready = true)
Aug 26 05:42:48.896: INFO: Pod "busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:48.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3504" for this suite. 08/26/23 05:42:48.929
------------------------------
• [2.100 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:46.841
    Aug 26 05:42:46.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubelet-test 08/26/23 05:42:46.842
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:46.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:46.869
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Aug 26 05:42:46.884: INFO: Waiting up to 5m0s for pod "busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828" in namespace "kubelet-test-3504" to be "running and ready"
    Aug 26 05:42:46.891: INFO: Pod "busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828": Phase="Pending", Reason="", readiness=false. Elapsed: 6.279518ms
    Aug 26 05:42:46.891: INFO: The phase of Pod busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:42:48.896: INFO: Pod "busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828": Phase="Running", Reason="", readiness=true. Elapsed: 2.011869144s
    Aug 26 05:42:48.896: INFO: The phase of Pod busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828 is Running (Ready = true)
    Aug 26 05:42:48.896: INFO: Pod "busybox-scheduling-c518f296-db3f-441b-a18a-dc00f6558828" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:48.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3504" for this suite. 08/26/23 05:42:48.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:48.94
Aug 26 05:42:48.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename disruption 08/26/23 05:42:48.941
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:48.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:48.964
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 08/26/23 05:42:48.976
STEP: Waiting for all pods to be running 08/26/23 05:42:51.018
Aug 26 05:42:51.033: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:53.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5514" for this suite. 08/26/23 05:42:53.049
------------------------------
• [4.116 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:48.94
    Aug 26 05:42:48.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename disruption 08/26/23 05:42:48.941
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:48.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:48.964
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 08/26/23 05:42:48.976
    STEP: Waiting for all pods to be running 08/26/23 05:42:51.018
    Aug 26 05:42:51.033: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:53.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5514" for this suite. 08/26/23 05:42:53.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:53.057
Aug 26 05:42:53.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 05:42:53.058
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:53.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:53.077
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 08/26/23 05:42:53.08
Aug 26 05:42:53.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 create -f -'
Aug 26 05:42:53.945: INFO: stderr: ""
Aug 26 05:42:53.945: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/26/23 05:42:53.945
Aug 26 05:42:53.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 26 05:42:54.033: INFO: stderr: ""
Aug 26 05:42:54.033: INFO: stdout: "update-demo-nautilus-c298s update-demo-nautilus-p2hbz "
Aug 26 05:42:54.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods update-demo-nautilus-c298s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 26 05:42:54.116: INFO: stderr: ""
Aug 26 05:42:54.116: INFO: stdout: ""
Aug 26 05:42:54.116: INFO: update-demo-nautilus-c298s is created but not running
Aug 26 05:42:59.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 26 05:42:59.204: INFO: stderr: ""
Aug 26 05:42:59.204: INFO: stdout: "update-demo-nautilus-c298s update-demo-nautilus-p2hbz "
Aug 26 05:42:59.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods update-demo-nautilus-c298s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 26 05:42:59.300: INFO: stderr: ""
Aug 26 05:42:59.300: INFO: stdout: "true"
Aug 26 05:42:59.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods update-demo-nautilus-c298s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 26 05:42:59.396: INFO: stderr: ""
Aug 26 05:42:59.396: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 26 05:42:59.396: INFO: validating pod update-demo-nautilus-c298s
Aug 26 05:42:59.402: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 05:42:59.402: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 05:42:59.402: INFO: update-demo-nautilus-c298s is verified up and running
Aug 26 05:42:59.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods update-demo-nautilus-p2hbz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 26 05:42:59.538: INFO: stderr: ""
Aug 26 05:42:59.538: INFO: stdout: "true"
Aug 26 05:42:59.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods update-demo-nautilus-p2hbz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 26 05:42:59.615: INFO: stderr: ""
Aug 26 05:42:59.615: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 26 05:42:59.615: INFO: validating pod update-demo-nautilus-p2hbz
Aug 26 05:42:59.622: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 05:42:59.622: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 05:42:59.622: INFO: update-demo-nautilus-p2hbz is verified up and running
STEP: using delete to clean up resources 08/26/23 05:42:59.622
Aug 26 05:42:59.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 delete --grace-period=0 --force -f -'
Aug 26 05:42:59.710: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 05:42:59.710: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 26 05:42:59.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get rc,svc -l name=update-demo --no-headers'
Aug 26 05:42:59.802: INFO: stderr: "No resources found in kubectl-1794 namespace.\n"
Aug 26 05:42:59.802: INFO: stdout: ""
Aug 26 05:42:59.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 26 05:42:59.895: INFO: stderr: ""
Aug 26 05:42:59.895: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 05:42:59.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1794" for this suite. 08/26/23 05:42:59.902
------------------------------
• [SLOW TEST] [6.853 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:53.057
    Aug 26 05:42:53.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 05:42:53.058
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:53.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:53.077
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 08/26/23 05:42:53.08
    Aug 26 05:42:53.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 create -f -'
    Aug 26 05:42:53.945: INFO: stderr: ""
    Aug 26 05:42:53.945: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/26/23 05:42:53.945
    Aug 26 05:42:53.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 26 05:42:54.033: INFO: stderr: ""
    Aug 26 05:42:54.033: INFO: stdout: "update-demo-nautilus-c298s update-demo-nautilus-p2hbz "
    Aug 26 05:42:54.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods update-demo-nautilus-c298s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 26 05:42:54.116: INFO: stderr: ""
    Aug 26 05:42:54.116: INFO: stdout: ""
    Aug 26 05:42:54.116: INFO: update-demo-nautilus-c298s is created but not running
    Aug 26 05:42:59.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 26 05:42:59.204: INFO: stderr: ""
    Aug 26 05:42:59.204: INFO: stdout: "update-demo-nautilus-c298s update-demo-nautilus-p2hbz "
    Aug 26 05:42:59.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods update-demo-nautilus-c298s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 26 05:42:59.300: INFO: stderr: ""
    Aug 26 05:42:59.300: INFO: stdout: "true"
    Aug 26 05:42:59.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods update-demo-nautilus-c298s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 26 05:42:59.396: INFO: stderr: ""
    Aug 26 05:42:59.396: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 26 05:42:59.396: INFO: validating pod update-demo-nautilus-c298s
    Aug 26 05:42:59.402: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 26 05:42:59.402: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 26 05:42:59.402: INFO: update-demo-nautilus-c298s is verified up and running
    Aug 26 05:42:59.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods update-demo-nautilus-p2hbz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 26 05:42:59.538: INFO: stderr: ""
    Aug 26 05:42:59.538: INFO: stdout: "true"
    Aug 26 05:42:59.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods update-demo-nautilus-p2hbz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 26 05:42:59.615: INFO: stderr: ""
    Aug 26 05:42:59.615: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 26 05:42:59.615: INFO: validating pod update-demo-nautilus-p2hbz
    Aug 26 05:42:59.622: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 26 05:42:59.622: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 26 05:42:59.622: INFO: update-demo-nautilus-p2hbz is verified up and running
    STEP: using delete to clean up resources 08/26/23 05:42:59.622
    Aug 26 05:42:59.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 delete --grace-period=0 --force -f -'
    Aug 26 05:42:59.710: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 26 05:42:59.710: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 26 05:42:59.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get rc,svc -l name=update-demo --no-headers'
    Aug 26 05:42:59.802: INFO: stderr: "No resources found in kubectl-1794 namespace.\n"
    Aug 26 05:42:59.802: INFO: stdout: ""
    Aug 26 05:42:59.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1794 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 26 05:42:59.895: INFO: stderr: ""
    Aug 26 05:42:59.895: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:42:59.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1794" for this suite. 08/26/23 05:42:59.902
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:42:59.912
Aug 26 05:42:59.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename job 08/26/23 05:42:59.912
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:59.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:59.945
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 08/26/23 05:42:59.947
STEP: Ensuring active pods == parallelism 08/26/23 05:42:59.953
STEP: delete a job 08/26/23 05:43:03.958
STEP: deleting Job.batch foo in namespace job-1136, will wait for the garbage collector to delete the pods 08/26/23 05:43:03.959
Aug 26 05:43:04.026: INFO: Deleting Job.batch foo took: 11.028695ms
Aug 26 05:43:04.127: INFO: Terminating Job.batch foo pods took: 101.079587ms
STEP: Ensuring job was deleted 08/26/23 05:43:35.228
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 26 05:43:35.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1136" for this suite. 08/26/23 05:43:35.243
------------------------------
• [SLOW TEST] [35.341 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:42:59.912
    Aug 26 05:42:59.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename job 08/26/23 05:42:59.912
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:42:59.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:42:59.945
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 08/26/23 05:42:59.947
    STEP: Ensuring active pods == parallelism 08/26/23 05:42:59.953
    STEP: delete a job 08/26/23 05:43:03.958
    STEP: deleting Job.batch foo in namespace job-1136, will wait for the garbage collector to delete the pods 08/26/23 05:43:03.959
    Aug 26 05:43:04.026: INFO: Deleting Job.batch foo took: 11.028695ms
    Aug 26 05:43:04.127: INFO: Terminating Job.batch foo pods took: 101.079587ms
    STEP: Ensuring job was deleted 08/26/23 05:43:35.228
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:43:35.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1136" for this suite. 08/26/23 05:43:35.243
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:43:35.252
Aug 26 05:43:35.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:43:35.256
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:43:35.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:43:35.281
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 08/26/23 05:43:35.288
Aug 26 05:43:35.300: INFO: Waiting up to 5m0s for pod "downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89" in namespace "downward-api-6939" to be "Succeeded or Failed"
Aug 26 05:43:35.307: INFO: Pod "downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89": Phase="Pending", Reason="", readiness=false. Elapsed: 7.044804ms
Aug 26 05:43:37.315: INFO: Pod "downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014774741s
Aug 26 05:43:39.314: INFO: Pod "downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013824877s
STEP: Saw pod success 08/26/23 05:43:39.314
Aug 26 05:43:39.314: INFO: Pod "downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89" satisfied condition "Succeeded or Failed"
Aug 26 05:43:39.318: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89 container dapi-container: <nil>
STEP: delete the pod 08/26/23 05:43:39.326
Aug 26 05:43:39.342: INFO: Waiting for pod downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89 to disappear
Aug 26 05:43:39.345: INFO: Pod downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 26 05:43:39.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6939" for this suite. 08/26/23 05:43:39.352
------------------------------
• [4.109 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:43:35.252
    Aug 26 05:43:35.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:43:35.256
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:43:35.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:43:35.281
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 08/26/23 05:43:35.288
    Aug 26 05:43:35.300: INFO: Waiting up to 5m0s for pod "downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89" in namespace "downward-api-6939" to be "Succeeded or Failed"
    Aug 26 05:43:35.307: INFO: Pod "downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89": Phase="Pending", Reason="", readiness=false. Elapsed: 7.044804ms
    Aug 26 05:43:37.315: INFO: Pod "downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014774741s
    Aug 26 05:43:39.314: INFO: Pod "downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013824877s
    STEP: Saw pod success 08/26/23 05:43:39.314
    Aug 26 05:43:39.314: INFO: Pod "downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89" satisfied condition "Succeeded or Failed"
    Aug 26 05:43:39.318: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89 container dapi-container: <nil>
    STEP: delete the pod 08/26/23 05:43:39.326
    Aug 26 05:43:39.342: INFO: Waiting for pod downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89 to disappear
    Aug 26 05:43:39.345: INFO: Pod downward-api-bd1bce37-b4be-47bd-9878-e034f7195b89 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:43:39.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6939" for this suite. 08/26/23 05:43:39.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:43:39.362
Aug 26 05:43:39.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename security-context-test 08/26/23 05:43:39.363
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:43:39.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:43:39.392
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Aug 26 05:43:39.404: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe" in namespace "security-context-test-7875" to be "Succeeded or Failed"
Aug 26 05:43:39.407: INFO: Pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.529176ms
Aug 26 05:43:41.415: INFO: Pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011570351s
Aug 26 05:43:43.413: INFO: Pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009647491s
Aug 26 05:43:45.412: INFO: Pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008804195s
Aug 26 05:43:45.412: INFO: Pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 26 05:43:45.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7875" for this suite. 08/26/23 05:43:45.427
------------------------------
• [SLOW TEST] [6.073 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:43:39.362
    Aug 26 05:43:39.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename security-context-test 08/26/23 05:43:39.363
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:43:39.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:43:39.392
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Aug 26 05:43:39.404: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe" in namespace "security-context-test-7875" to be "Succeeded or Failed"
    Aug 26 05:43:39.407: INFO: Pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.529176ms
    Aug 26 05:43:41.415: INFO: Pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011570351s
    Aug 26 05:43:43.413: INFO: Pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009647491s
    Aug 26 05:43:45.412: INFO: Pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008804195s
    Aug 26 05:43:45.412: INFO: Pod "alpine-nnp-false-11bcd8e4-aa9d-4cad-9c28-63a7d47123fe" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:43:45.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7875" for this suite. 08/26/23 05:43:45.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:43:45.436
Aug 26 05:43:45.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename job 08/26/23 05:43:45.437
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:43:45.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:43:45.462
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 08/26/23 05:43:45.466
STEP: Ensuring job reaches completions 08/26/23 05:43:45.475
STEP: Ensuring pods with index for job exist 08/26/23 05:43:55.48
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 26 05:43:55.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2297" for this suite. 08/26/23 05:43:55.493
------------------------------
• [SLOW TEST] [10.064 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:43:45.436
    Aug 26 05:43:45.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename job 08/26/23 05:43:45.437
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:43:45.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:43:45.462
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 08/26/23 05:43:45.466
    STEP: Ensuring job reaches completions 08/26/23 05:43:45.475
    STEP: Ensuring pods with index for job exist 08/26/23 05:43:55.48
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:43:55.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2297" for this suite. 08/26/23 05:43:55.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:43:55.501
Aug 26 05:43:55.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename hostport 08/26/23 05:43:55.502
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:43:55.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:43:55.523
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/26/23 05:43:55.54
Aug 26 05:43:55.550: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9082" to be "running and ready"
Aug 26 05:43:55.555: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.511545ms
Aug 26 05:43:55.555: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:43:57.561: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010481558s
Aug 26 05:43:57.561: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 26 05:43:57.561: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.1.5 on the node which pod1 resides and expect scheduled 08/26/23 05:43:57.561
Aug 26 05:43:57.571: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9082" to be "running and ready"
Aug 26 05:43:57.577: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.996449ms
Aug 26 05:43:57.577: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:43:59.582: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.011083688s
Aug 26 05:43:59.582: INFO: The phase of Pod pod2 is Running (Ready = false)
Aug 26 05:44:01.584: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013542818s
Aug 26 05:44:01.584: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 26 05:44:01.584: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.1.5 but use UDP protocol on the node which pod2 resides 08/26/23 05:44:01.584
Aug 26 05:44:01.594: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9082" to be "running and ready"
Aug 26 05:44:01.598: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.888223ms
Aug 26 05:44:01.598: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:44:03.602: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008462518s
Aug 26 05:44:03.602: INFO: The phase of Pod pod3 is Running (Ready = true)
Aug 26 05:44:03.602: INFO: Pod "pod3" satisfied condition "running and ready"
Aug 26 05:44:03.610: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9082" to be "running and ready"
Aug 26 05:44:03.613: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.538033ms
Aug 26 05:44:03.613: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:44:05.619: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00881175s
Aug 26 05:44:05.619: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Aug 26 05:44:05.619: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/26/23 05:44:05.622
Aug 26 05:44:05.622: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.1.5 http://127.0.0.1:54323/hostname] Namespace:hostport-9082 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:05.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:05.623: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:05.623: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-9082/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.1.5+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.1.5, port: 54323 08/26/23 05:44:05.725
Aug 26 05:44:05.725: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.1.5:54323/hostname] Namespace:hostport-9082 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:05.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:05.725: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:05.725: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-9082/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.1.5%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.1.5, port: 54323 UDP 08/26/23 05:44:05.823
Aug 26 05:44:05.823: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.1.5 54323] Namespace:hostport-9082 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:05.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:05.823: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:05.824: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-9082/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.1.5+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Aug 26 05:44:10.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-9082" for this suite. 08/26/23 05:44:10.941
------------------------------
• [SLOW TEST] [15.449 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:43:55.501
    Aug 26 05:43:55.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename hostport 08/26/23 05:43:55.502
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:43:55.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:43:55.523
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/26/23 05:43:55.54
    Aug 26 05:43:55.550: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9082" to be "running and ready"
    Aug 26 05:43:55.555: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.511545ms
    Aug 26 05:43:55.555: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:43:57.561: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010481558s
    Aug 26 05:43:57.561: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 26 05:43:57.561: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.1.5 on the node which pod1 resides and expect scheduled 08/26/23 05:43:57.561
    Aug 26 05:43:57.571: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9082" to be "running and ready"
    Aug 26 05:43:57.577: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.996449ms
    Aug 26 05:43:57.577: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:43:59.582: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.011083688s
    Aug 26 05:43:59.582: INFO: The phase of Pod pod2 is Running (Ready = false)
    Aug 26 05:44:01.584: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013542818s
    Aug 26 05:44:01.584: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 26 05:44:01.584: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.1.5 but use UDP protocol on the node which pod2 resides 08/26/23 05:44:01.584
    Aug 26 05:44:01.594: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9082" to be "running and ready"
    Aug 26 05:44:01.598: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.888223ms
    Aug 26 05:44:01.598: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:44:03.602: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008462518s
    Aug 26 05:44:03.602: INFO: The phase of Pod pod3 is Running (Ready = true)
    Aug 26 05:44:03.602: INFO: Pod "pod3" satisfied condition "running and ready"
    Aug 26 05:44:03.610: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9082" to be "running and ready"
    Aug 26 05:44:03.613: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.538033ms
    Aug 26 05:44:03.613: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:44:05.619: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00881175s
    Aug 26 05:44:05.619: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Aug 26 05:44:05.619: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/26/23 05:44:05.622
    Aug 26 05:44:05.622: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.1.5 http://127.0.0.1:54323/hostname] Namespace:hostport-9082 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:05.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:05.623: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:05.623: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-9082/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.1.5+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.1.5, port: 54323 08/26/23 05:44:05.725
    Aug 26 05:44:05.725: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.1.5:54323/hostname] Namespace:hostport-9082 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:05.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:05.725: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:05.725: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-9082/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.1.5%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.1.5, port: 54323 UDP 08/26/23 05:44:05.823
    Aug 26 05:44:05.823: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.1.5 54323] Namespace:hostport-9082 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:05.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:05.823: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:05.824: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/hostport-9082/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.1.5+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:44:10.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-9082" for this suite. 08/26/23 05:44:10.941
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:44:10.95
Aug 26 05:44:10.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:44:10.951
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:10.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:10.978
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:44:10.998
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:44:11.385
STEP: Deploying the webhook pod 08/26/23 05:44:11.393
STEP: Wait for the deployment to be ready 08/26/23 05:44:11.413
Aug 26 05:44:11.429: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:44:13.445
STEP: Verifying the service has paired with the endpoint 08/26/23 05:44:13.458
Aug 26 05:44:14.459: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/26/23 05:44:14.463
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/26/23 05:44:14.485
STEP: Creating a dummy validating-webhook-configuration object 08/26/23 05:44:14.512
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/26/23 05:44:14.527
STEP: Creating a dummy mutating-webhook-configuration object 08/26/23 05:44:14.54
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/26/23 05:44:14.563
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:44:14.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-771" for this suite. 08/26/23 05:44:14.712
STEP: Destroying namespace "webhook-771-markers" for this suite. 08/26/23 05:44:14.727
------------------------------
• [3.788 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:44:10.95
    Aug 26 05:44:10.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:44:10.951
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:10.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:10.978
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:44:10.998
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:44:11.385
    STEP: Deploying the webhook pod 08/26/23 05:44:11.393
    STEP: Wait for the deployment to be ready 08/26/23 05:44:11.413
    Aug 26 05:44:11.429: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:44:13.445
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:44:13.458
    Aug 26 05:44:14.459: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/26/23 05:44:14.463
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/26/23 05:44:14.485
    STEP: Creating a dummy validating-webhook-configuration object 08/26/23 05:44:14.512
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/26/23 05:44:14.527
    STEP: Creating a dummy mutating-webhook-configuration object 08/26/23 05:44:14.54
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/26/23 05:44:14.563
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:44:14.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-771" for this suite. 08/26/23 05:44:14.712
    STEP: Destroying namespace "webhook-771-markers" for this suite. 08/26/23 05:44:14.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:44:14.74
Aug 26 05:44:14.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:44:14.741
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:14.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:14.778
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:44:14.797
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:44:15.223
STEP: Deploying the webhook pod 08/26/23 05:44:15.23
STEP: Wait for the deployment to be ready 08/26/23 05:44:15.247
Aug 26 05:44:15.262: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:44:17.334
STEP: Verifying the service has paired with the endpoint 08/26/23 05:44:17.346
Aug 26 05:44:18.347: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 08/26/23 05:44:18.351
STEP: create a pod 08/26/23 05:44:18.37
Aug 26 05:44:18.380: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1634" to be "running"
Aug 26 05:44:18.387: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.025151ms
Aug 26 05:44:20.395: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014584915s
Aug 26 05:44:20.395: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 08/26/23 05:44:20.395
Aug 26 05:44:20.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=webhook-1634 attach --namespace=webhook-1634 to-be-attached-pod -i -c=container1'
Aug 26 05:44:20.498: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:44:20.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1634" for this suite. 08/26/23 05:44:20.588
STEP: Destroying namespace "webhook-1634-markers" for this suite. 08/26/23 05:44:20.6
------------------------------
• [SLOW TEST] [5.872 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:44:14.74
    Aug 26 05:44:14.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:44:14.741
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:14.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:14.778
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:44:14.797
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:44:15.223
    STEP: Deploying the webhook pod 08/26/23 05:44:15.23
    STEP: Wait for the deployment to be ready 08/26/23 05:44:15.247
    Aug 26 05:44:15.262: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:44:17.334
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:44:17.346
    Aug 26 05:44:18.347: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 08/26/23 05:44:18.351
    STEP: create a pod 08/26/23 05:44:18.37
    Aug 26 05:44:18.380: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1634" to be "running"
    Aug 26 05:44:18.387: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.025151ms
    Aug 26 05:44:20.395: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014584915s
    Aug 26 05:44:20.395: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 08/26/23 05:44:20.395
    Aug 26 05:44:20.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=webhook-1634 attach --namespace=webhook-1634 to-be-attached-pod -i -c=container1'
    Aug 26 05:44:20.498: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:44:20.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1634" for this suite. 08/26/23 05:44:20.588
    STEP: Destroying namespace "webhook-1634-markers" for this suite. 08/26/23 05:44:20.6
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:44:20.612
Aug 26 05:44:20.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:44:20.613
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:20.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:20.665
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 08/26/23 05:44:20.669
Aug 26 05:44:20.682: INFO: Waiting up to 5m0s for pod "downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d" in namespace "downward-api-7964" to be "Succeeded or Failed"
Aug 26 05:44:20.686: INFO: Pod "downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.214136ms
Aug 26 05:44:22.692: INFO: Pod "downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010179611s
Aug 26 05:44:24.691: INFO: Pod "downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008930871s
STEP: Saw pod success 08/26/23 05:44:24.691
Aug 26 05:44:24.692: INFO: Pod "downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d" satisfied condition "Succeeded or Failed"
Aug 26 05:44:24.697: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d container dapi-container: <nil>
STEP: delete the pod 08/26/23 05:44:24.709
Aug 26 05:44:24.732: INFO: Waiting for pod downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d to disappear
Aug 26 05:44:24.737: INFO: Pod downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 26 05:44:24.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7964" for this suite. 08/26/23 05:44:24.748
------------------------------
• [4.149 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:44:20.612
    Aug 26 05:44:20.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:44:20.613
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:20.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:20.665
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 08/26/23 05:44:20.669
    Aug 26 05:44:20.682: INFO: Waiting up to 5m0s for pod "downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d" in namespace "downward-api-7964" to be "Succeeded or Failed"
    Aug 26 05:44:20.686: INFO: Pod "downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.214136ms
    Aug 26 05:44:22.692: INFO: Pod "downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010179611s
    Aug 26 05:44:24.691: INFO: Pod "downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008930871s
    STEP: Saw pod success 08/26/23 05:44:24.691
    Aug 26 05:44:24.692: INFO: Pod "downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d" satisfied condition "Succeeded or Failed"
    Aug 26 05:44:24.697: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d container dapi-container: <nil>
    STEP: delete the pod 08/26/23 05:44:24.709
    Aug 26 05:44:24.732: INFO: Waiting for pod downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d to disappear
    Aug 26 05:44:24.737: INFO: Pod downward-api-49ddee08-52ef-4114-a9c3-5ec0098a993d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:44:24.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7964" for this suite. 08/26/23 05:44:24.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:44:24.762
Aug 26 05:44:24.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 05:44:24.764
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:24.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:24.798
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Aug 26 05:44:24.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/26/23 05:44:26.986
Aug 26 05:44:26.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 create -f -'
Aug 26 05:44:27.811: INFO: stderr: ""
Aug 26 05:44:27.811: INFO: stdout: "e2e-test-crd-publish-openapi-1197-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 26 05:44:27.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 delete e2e-test-crd-publish-openapi-1197-crds test-foo'
Aug 26 05:44:27.972: INFO: stderr: ""
Aug 26 05:44:27.972: INFO: stdout: "e2e-test-crd-publish-openapi-1197-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 26 05:44:27.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 apply -f -'
Aug 26 05:44:28.749: INFO: stderr: ""
Aug 26 05:44:28.749: INFO: stdout: "e2e-test-crd-publish-openapi-1197-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 26 05:44:28.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 delete e2e-test-crd-publish-openapi-1197-crds test-foo'
Aug 26 05:44:28.857: INFO: stderr: ""
Aug 26 05:44:28.857: INFO: stdout: "e2e-test-crd-publish-openapi-1197-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/26/23 05:44:28.857
Aug 26 05:44:28.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 create -f -'
Aug 26 05:44:29.632: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/26/23 05:44:29.632
Aug 26 05:44:29.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 create -f -'
Aug 26 05:44:29.834: INFO: rc: 1
Aug 26 05:44:29.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 apply -f -'
Aug 26 05:44:30.034: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/26/23 05:44:30.034
Aug 26 05:44:30.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 create -f -'
Aug 26 05:44:30.320: INFO: rc: 1
Aug 26 05:44:30.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 apply -f -'
Aug 26 05:44:30.599: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 08/26/23 05:44:30.599
Aug 26 05:44:30.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 explain e2e-test-crd-publish-openapi-1197-crds'
Aug 26 05:44:30.859: INFO: stderr: ""
Aug 26 05:44:30.859: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1197-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 08/26/23 05:44:30.86
Aug 26 05:44:30.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 explain e2e-test-crd-publish-openapi-1197-crds.metadata'
Aug 26 05:44:31.187: INFO: stderr: ""
Aug 26 05:44:31.187: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1197-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 26 05:44:31.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 explain e2e-test-crd-publish-openapi-1197-crds.spec'
Aug 26 05:44:31.475: INFO: stderr: ""
Aug 26 05:44:31.475: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1197-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 26 05:44:31.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 explain e2e-test-crd-publish-openapi-1197-crds.spec.bars'
Aug 26 05:44:31.729: INFO: stderr: ""
Aug 26 05:44:31.729: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1197-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/26/23 05:44:31.73
Aug 26 05:44:31.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 explain e2e-test-crd-publish-openapi-1197-crds.spec.bars2'
Aug 26 05:44:31.997: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:44:34.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6004" for this suite. 08/26/23 05:44:34.158
------------------------------
• [SLOW TEST] [9.409 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:44:24.762
    Aug 26 05:44:24.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 05:44:24.764
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:24.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:24.798
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Aug 26 05:44:24.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/26/23 05:44:26.986
    Aug 26 05:44:26.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 create -f -'
    Aug 26 05:44:27.811: INFO: stderr: ""
    Aug 26 05:44:27.811: INFO: stdout: "e2e-test-crd-publish-openapi-1197-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 26 05:44:27.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 delete e2e-test-crd-publish-openapi-1197-crds test-foo'
    Aug 26 05:44:27.972: INFO: stderr: ""
    Aug 26 05:44:27.972: INFO: stdout: "e2e-test-crd-publish-openapi-1197-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Aug 26 05:44:27.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 apply -f -'
    Aug 26 05:44:28.749: INFO: stderr: ""
    Aug 26 05:44:28.749: INFO: stdout: "e2e-test-crd-publish-openapi-1197-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 26 05:44:28.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 delete e2e-test-crd-publish-openapi-1197-crds test-foo'
    Aug 26 05:44:28.857: INFO: stderr: ""
    Aug 26 05:44:28.857: INFO: stdout: "e2e-test-crd-publish-openapi-1197-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/26/23 05:44:28.857
    Aug 26 05:44:28.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 create -f -'
    Aug 26 05:44:29.632: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/26/23 05:44:29.632
    Aug 26 05:44:29.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 create -f -'
    Aug 26 05:44:29.834: INFO: rc: 1
    Aug 26 05:44:29.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 apply -f -'
    Aug 26 05:44:30.034: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/26/23 05:44:30.034
    Aug 26 05:44:30.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 create -f -'
    Aug 26 05:44:30.320: INFO: rc: 1
    Aug 26 05:44:30.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 --namespace=crd-publish-openapi-6004 apply -f -'
    Aug 26 05:44:30.599: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 08/26/23 05:44:30.599
    Aug 26 05:44:30.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 explain e2e-test-crd-publish-openapi-1197-crds'
    Aug 26 05:44:30.859: INFO: stderr: ""
    Aug 26 05:44:30.859: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1197-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 08/26/23 05:44:30.86
    Aug 26 05:44:30.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 explain e2e-test-crd-publish-openapi-1197-crds.metadata'
    Aug 26 05:44:31.187: INFO: stderr: ""
    Aug 26 05:44:31.187: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1197-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Aug 26 05:44:31.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 explain e2e-test-crd-publish-openapi-1197-crds.spec'
    Aug 26 05:44:31.475: INFO: stderr: ""
    Aug 26 05:44:31.475: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1197-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Aug 26 05:44:31.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 explain e2e-test-crd-publish-openapi-1197-crds.spec.bars'
    Aug 26 05:44:31.729: INFO: stderr: ""
    Aug 26 05:44:31.729: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1197-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/26/23 05:44:31.73
    Aug 26 05:44:31.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-6004 explain e2e-test-crd-publish-openapi-1197-crds.spec.bars2'
    Aug 26 05:44:31.997: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:44:34.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6004" for this suite. 08/26/23 05:44:34.158
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:44:34.171
Aug 26 05:44:34.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/26/23 05:44:34.173
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:34.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:34.192
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 08/26/23 05:44:34.195
STEP: Creating hostNetwork=false pod 08/26/23 05:44:34.195
Aug 26 05:44:34.213: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7872" to be "running and ready"
Aug 26 05:44:34.219: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.810471ms
Aug 26 05:44:34.219: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:44:36.225: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012584909s
Aug 26 05:44:36.225: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:44:38.227: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014037774s
Aug 26 05:44:38.227: INFO: The phase of Pod test-pod is Running (Ready = true)
Aug 26 05:44:38.227: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 08/26/23 05:44:38.231
Aug 26 05:44:38.245: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7872" to be "running and ready"
Aug 26 05:44:38.251: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.576017ms
Aug 26 05:44:38.251: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:44:40.257: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011562241s
Aug 26 05:44:40.257: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Aug 26 05:44:40.257: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 08/26/23 05:44:40.261
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/26/23 05:44:40.261
Aug 26 05:44:40.261: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:40.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:40.262: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:40.262: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 26 05:44:40.348: INFO: Exec stderr: ""
Aug 26 05:44:40.348: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:40.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:40.349: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:40.349: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 26 05:44:40.437: INFO: Exec stderr: ""
Aug 26 05:44:40.437: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:40.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:40.438: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:40.438: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 26 05:44:40.549: INFO: Exec stderr: ""
Aug 26 05:44:40.549: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:40.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:40.549: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:40.549: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 26 05:44:40.639: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/26/23 05:44:40.639
Aug 26 05:44:40.639: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:40.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:40.640: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:40.640: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 26 05:44:40.732: INFO: Exec stderr: ""
Aug 26 05:44:40.732: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:40.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:40.732: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:40.732: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 26 05:44:40.843: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/26/23 05:44:40.843
Aug 26 05:44:40.843: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:40.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:40.844: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:40.844: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 26 05:44:40.930: INFO: Exec stderr: ""
Aug 26 05:44:40.930: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:40.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:40.931: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:40.931: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 26 05:44:41.007: INFO: Exec stderr: ""
Aug 26 05:44:41.007: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:41.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:41.008: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:41.008: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 26 05:44:41.084: INFO: Exec stderr: ""
Aug 26 05:44:41.084: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 05:44:41.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 05:44:41.085: INFO: ExecWithOptions: Clientset creation
Aug 26 05:44:41.085: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 26 05:44:41.164: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Aug 26 05:44:41.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7872" for this suite. 08/26/23 05:44:41.174
------------------------------
• [SLOW TEST] [7.012 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:44:34.171
    Aug 26 05:44:34.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/26/23 05:44:34.173
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:34.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:34.192
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 08/26/23 05:44:34.195
    STEP: Creating hostNetwork=false pod 08/26/23 05:44:34.195
    Aug 26 05:44:34.213: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7872" to be "running and ready"
    Aug 26 05:44:34.219: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.810471ms
    Aug 26 05:44:34.219: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:44:36.225: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012584909s
    Aug 26 05:44:36.225: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:44:38.227: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014037774s
    Aug 26 05:44:38.227: INFO: The phase of Pod test-pod is Running (Ready = true)
    Aug 26 05:44:38.227: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 08/26/23 05:44:38.231
    Aug 26 05:44:38.245: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7872" to be "running and ready"
    Aug 26 05:44:38.251: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.576017ms
    Aug 26 05:44:38.251: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:44:40.257: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011562241s
    Aug 26 05:44:40.257: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Aug 26 05:44:40.257: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 08/26/23 05:44:40.261
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/26/23 05:44:40.261
    Aug 26 05:44:40.261: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:40.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:40.262: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:40.262: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 26 05:44:40.348: INFO: Exec stderr: ""
    Aug 26 05:44:40.348: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:40.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:40.349: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:40.349: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 26 05:44:40.437: INFO: Exec stderr: ""
    Aug 26 05:44:40.437: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:40.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:40.438: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:40.438: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 26 05:44:40.549: INFO: Exec stderr: ""
    Aug 26 05:44:40.549: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:40.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:40.549: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:40.549: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 26 05:44:40.639: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/26/23 05:44:40.639
    Aug 26 05:44:40.639: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:40.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:40.640: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:40.640: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 26 05:44:40.732: INFO: Exec stderr: ""
    Aug 26 05:44:40.732: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:40.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:40.732: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:40.732: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 26 05:44:40.843: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/26/23 05:44:40.843
    Aug 26 05:44:40.843: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:40.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:40.844: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:40.844: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 26 05:44:40.930: INFO: Exec stderr: ""
    Aug 26 05:44:40.930: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:40.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:40.931: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:40.931: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 26 05:44:41.007: INFO: Exec stderr: ""
    Aug 26 05:44:41.007: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:41.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:41.008: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:41.008: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 26 05:44:41.084: INFO: Exec stderr: ""
    Aug 26 05:44:41.084: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7872 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 05:44:41.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 05:44:41.085: INFO: ExecWithOptions: Clientset creation
    Aug 26 05:44:41.085: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7872/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 26 05:44:41.164: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:44:41.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-7872" for this suite. 08/26/23 05:44:41.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:44:41.185
Aug 26 05:44:41.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename svc-latency 08/26/23 05:44:41.186
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:41.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:41.221
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Aug 26 05:44:41.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6021 08/26/23 05:44:41.25
I0826 05:44:41.258112      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6021, replica count: 1
I0826 05:44:42.310105      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0826 05:44:43.310588      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 26 05:44:43.534: INFO: Created: latency-svc-dk5jp
Aug 26 05:44:43.544: INFO: Got endpoints: latency-svc-dk5jp [133.397076ms]
Aug 26 05:44:43.570: INFO: Created: latency-svc-9222h
Aug 26 05:44:43.576: INFO: Got endpoints: latency-svc-9222h [32.073242ms]
Aug 26 05:44:43.597: INFO: Created: latency-svc-c6k6n
Aug 26 05:44:43.602: INFO: Got endpoints: latency-svc-c6k6n [57.653771ms]
Aug 26 05:44:43.620: INFO: Created: latency-svc-6cqt6
Aug 26 05:44:43.629: INFO: Got endpoints: latency-svc-6cqt6 [84.167956ms]
Aug 26 05:44:43.637: INFO: Created: latency-svc-tlmr8
Aug 26 05:44:43.646: INFO: Got endpoints: latency-svc-tlmr8 [101.301117ms]
Aug 26 05:44:43.650: INFO: Created: latency-svc-j424z
Aug 26 05:44:43.653: INFO: Got endpoints: latency-svc-j424z [108.439667ms]
Aug 26 05:44:43.666: INFO: Created: latency-svc-6vtx5
Aug 26 05:44:43.674: INFO: Got endpoints: latency-svc-6vtx5 [129.702676ms]
Aug 26 05:44:43.678: INFO: Created: latency-svc-2j28b
Aug 26 05:44:43.686: INFO: Got endpoints: latency-svc-2j28b [140.845161ms]
Aug 26 05:44:43.694: INFO: Created: latency-svc-w7z8l
Aug 26 05:44:43.701: INFO: Got endpoints: latency-svc-w7z8l [156.191848ms]
Aug 26 05:44:43.706: INFO: Created: latency-svc-jfqzh
Aug 26 05:44:43.715: INFO: Got endpoints: latency-svc-jfqzh [169.870753ms]
Aug 26 05:44:43.721: INFO: Created: latency-svc-2sk5x
Aug 26 05:44:43.728: INFO: Got endpoints: latency-svc-2sk5x [183.677122ms]
Aug 26 05:44:43.733: INFO: Created: latency-svc-nsqs9
Aug 26 05:44:43.741: INFO: Got endpoints: latency-svc-nsqs9 [196.495791ms]
Aug 26 05:44:43.745: INFO: Created: latency-svc-wxkf8
Aug 26 05:44:43.755: INFO: Got endpoints: latency-svc-wxkf8 [209.770192ms]
Aug 26 05:44:43.758: INFO: Created: latency-svc-kqktb
Aug 26 05:44:43.764: INFO: Got endpoints: latency-svc-kqktb [219.48275ms]
Aug 26 05:44:43.769: INFO: Created: latency-svc-n66jw
Aug 26 05:44:43.776: INFO: Got endpoints: latency-svc-n66jw [230.979496ms]
Aug 26 05:44:43.781: INFO: Created: latency-svc-g9gf5
Aug 26 05:44:43.791: INFO: Got endpoints: latency-svc-g9gf5 [245.753772ms]
Aug 26 05:44:43.794: INFO: Created: latency-svc-vl28l
Aug 26 05:44:43.807: INFO: Created: latency-svc-ff5gq
Aug 26 05:44:43.809: INFO: Got endpoints: latency-svc-vl28l [232.725866ms]
Aug 26 05:44:43.820: INFO: Got endpoints: latency-svc-ff5gq [217.694062ms]
Aug 26 05:44:43.820: INFO: Created: latency-svc-zwwmz
Aug 26 05:44:43.831: INFO: Got endpoints: latency-svc-zwwmz [202.352728ms]
Aug 26 05:44:43.839: INFO: Created: latency-svc-449l6
Aug 26 05:44:43.842: INFO: Got endpoints: latency-svc-449l6 [196.016376ms]
Aug 26 05:44:43.852: INFO: Created: latency-svc-j8jcm
Aug 26 05:44:43.864: INFO: Got endpoints: latency-svc-j8jcm [210.445476ms]
Aug 26 05:44:43.868: INFO: Created: latency-svc-92d8x
Aug 26 05:44:43.883: INFO: Got endpoints: latency-svc-92d8x [208.833062ms]
Aug 26 05:44:43.888: INFO: Created: latency-svc-fkdvb
Aug 26 05:44:43.901: INFO: Got endpoints: latency-svc-fkdvb [215.383427ms]
Aug 26 05:44:43.906: INFO: Created: latency-svc-mb6pq
Aug 26 05:44:43.914: INFO: Got endpoints: latency-svc-mb6pq [213.041005ms]
Aug 26 05:44:43.918: INFO: Created: latency-svc-nz6lb
Aug 26 05:44:43.927: INFO: Got endpoints: latency-svc-nz6lb [212.589568ms]
Aug 26 05:44:43.935: INFO: Created: latency-svc-8zfwk
Aug 26 05:44:43.942: INFO: Got endpoints: latency-svc-8zfwk [213.551205ms]
Aug 26 05:44:43.947: INFO: Created: latency-svc-rqd67
Aug 26 05:44:43.963: INFO: Got endpoints: latency-svc-rqd67 [221.454072ms]
Aug 26 05:44:43.968: INFO: Created: latency-svc-sbxz5
Aug 26 05:44:43.974: INFO: Got endpoints: latency-svc-sbxz5 [219.054444ms]
Aug 26 05:44:43.991: INFO: Created: latency-svc-gdxrq
Aug 26 05:44:44.001: INFO: Created: latency-svc-s7hh5
Aug 26 05:44:44.006: INFO: Got endpoints: latency-svc-gdxrq [241.578668ms]
Aug 26 05:44:44.009: INFO: Got endpoints: latency-svc-s7hh5 [232.824857ms]
Aug 26 05:44:44.017: INFO: Created: latency-svc-v9lm2
Aug 26 05:44:44.029: INFO: Got endpoints: latency-svc-v9lm2 [238.689594ms]
Aug 26 05:44:44.037: INFO: Created: latency-svc-2xps8
Aug 26 05:44:44.038: INFO: Got endpoints: latency-svc-2xps8 [228.865672ms]
Aug 26 05:44:44.046: INFO: Created: latency-svc-9bcbm
Aug 26 05:44:44.054: INFO: Got endpoints: latency-svc-9bcbm [234.237971ms]
Aug 26 05:44:44.071: INFO: Created: latency-svc-2h6rs
Aug 26 05:44:44.071: INFO: Got endpoints: latency-svc-2h6rs [240.020462ms]
Aug 26 05:44:44.078: INFO: Created: latency-svc-x5jb7
Aug 26 05:44:44.151: INFO: Got endpoints: latency-svc-x5jb7 [308.727118ms]
Aug 26 05:44:44.164: INFO: Created: latency-svc-dbkgv
Aug 26 05:44:44.166: INFO: Got endpoints: latency-svc-dbkgv [302.176528ms]
Aug 26 05:44:44.251: INFO: Created: latency-svc-pbg5w
Aug 26 05:44:44.259: INFO: Got endpoints: latency-svc-pbg5w [375.612775ms]
Aug 26 05:44:44.268: INFO: Created: latency-svc-5zjc9
Aug 26 05:44:44.282: INFO: Got endpoints: latency-svc-5zjc9 [381.371157ms]
Aug 26 05:44:44.286: INFO: Created: latency-svc-v4g2q
Aug 26 05:44:44.296: INFO: Got endpoints: latency-svc-v4g2q [382.072063ms]
Aug 26 05:44:44.303: INFO: Created: latency-svc-vlppg
Aug 26 05:44:44.308: INFO: Got endpoints: latency-svc-vlppg [380.051785ms]
Aug 26 05:44:44.315: INFO: Created: latency-svc-lcdxs
Aug 26 05:44:44.321: INFO: Got endpoints: latency-svc-lcdxs [378.813845ms]
Aug 26 05:44:44.329: INFO: Created: latency-svc-4cflh
Aug 26 05:44:44.330: INFO: Got endpoints: latency-svc-4cflh [367.367404ms]
Aug 26 05:44:44.338: INFO: Created: latency-svc-mc4r7
Aug 26 05:44:44.350: INFO: Got endpoints: latency-svc-mc4r7 [375.868482ms]
Aug 26 05:44:44.354: INFO: Created: latency-svc-6n654
Aug 26 05:44:44.360: INFO: Got endpoints: latency-svc-6n654 [354.08397ms]
Aug 26 05:44:44.370: INFO: Created: latency-svc-9cv9j
Aug 26 05:44:44.373: INFO: Got endpoints: latency-svc-9cv9j [363.77832ms]
Aug 26 05:44:44.381: INFO: Created: latency-svc-hkp2t
Aug 26 05:44:44.388: INFO: Got endpoints: latency-svc-hkp2t [356.09213ms]
Aug 26 05:44:44.397: INFO: Created: latency-svc-7gcfh
Aug 26 05:44:44.405: INFO: Got endpoints: latency-svc-7gcfh [366.672955ms]
Aug 26 05:44:44.414: INFO: Created: latency-svc-8lkjz
Aug 26 05:44:44.448: INFO: Got endpoints: latency-svc-8lkjz [393.788699ms]
Aug 26 05:44:44.546: INFO: Created: latency-svc-5kkd2
Aug 26 05:44:44.553: INFO: Created: latency-svc-qstbt
Aug 26 05:44:44.553: INFO: Created: latency-svc-gzds8
Aug 26 05:44:44.554: INFO: Created: latency-svc-5mcb2
Aug 26 05:44:44.554: INFO: Created: latency-svc-dzt78
Aug 26 05:44:44.556: INFO: Created: latency-svc-5xdvz
Aug 26 05:44:44.556: INFO: Created: latency-svc-4j6fn
Aug 26 05:44:44.556: INFO: Created: latency-svc-x7k24
Aug 26 05:44:44.557: INFO: Created: latency-svc-szsd8
Aug 26 05:44:44.557: INFO: Created: latency-svc-79skx
Aug 26 05:44:44.557: INFO: Created: latency-svc-4r4vp
Aug 26 05:44:44.558: INFO: Created: latency-svc-8xq8z
Aug 26 05:44:44.558: INFO: Created: latency-svc-mvsxt
Aug 26 05:44:44.559: INFO: Created: latency-svc-dgjnc
Aug 26 05:44:44.560: INFO: Created: latency-svc-gvjjv
Aug 26 05:44:44.560: INFO: Got endpoints: latency-svc-x7k24 [112.107734ms]
Aug 26 05:44:44.560: INFO: Got endpoints: latency-svc-5kkd2 [277.931206ms]
Aug 26 05:44:44.582: INFO: Created: latency-svc-dtb46
Aug 26 05:44:44.591: INFO: Got endpoints: latency-svc-dzt78 [186.458112ms]
Aug 26 05:44:44.595: INFO: Created: latency-svc-8fh6k
Aug 26 05:44:44.607: INFO: Created: latency-svc-wztzn
Aug 26 05:44:44.641: INFO: Got endpoints: latency-svc-gzds8 [569.392966ms]
Aug 26 05:44:44.659: INFO: Created: latency-svc-4mmn2
Aug 26 05:44:44.692: INFO: Got endpoints: latency-svc-qstbt [319.202717ms]
Aug 26 05:44:44.707: INFO: Created: latency-svc-z8cfs
Aug 26 05:44:44.741: INFO: Got endpoints: latency-svc-szsd8 [444.816835ms]
Aug 26 05:44:44.759: INFO: Created: latency-svc-d5fj2
Aug 26 05:44:44.791: INFO: Got endpoints: latency-svc-4j6fn [483.649125ms]
Aug 26 05:44:44.807: INFO: Created: latency-svc-xwr47
Aug 26 05:44:44.841: INFO: Got endpoints: latency-svc-gvjjv [519.714107ms]
Aug 26 05:44:44.864: INFO: Created: latency-svc-s8v9k
Aug 26 05:44:44.894: INFO: Got endpoints: latency-svc-79skx [506.190035ms]
Aug 26 05:44:44.908: INFO: Created: latency-svc-rl9s9
Aug 26 05:44:44.941: INFO: Got endpoints: latency-svc-4r4vp [682.469263ms]
Aug 26 05:44:44.963: INFO: Created: latency-svc-pww9t
Aug 26 05:44:45.082: INFO: Got endpoints: latency-svc-5xdvz [731.711833ms]
Aug 26 05:44:45.085: INFO: Got endpoints: latency-svc-5mcb2 [724.848367ms]
Aug 26 05:44:45.094: INFO: Got endpoints: latency-svc-mvsxt [928.103148ms]
Aug 26 05:44:45.185: INFO: Got endpoints: latency-svc-8xq8z [1.034295501s]
Aug 26 05:44:45.193: INFO: Created: latency-svc-mf9x6
Aug 26 05:44:45.198: INFO: Got endpoints: latency-svc-dgjnc [867.571778ms]
Aug 26 05:44:45.210: INFO: Created: latency-svc-q642f
Aug 26 05:44:45.224: INFO: Created: latency-svc-j7jpk
Aug 26 05:44:45.244: INFO: Got endpoints: latency-svc-dtb46 [683.447499ms]
Aug 26 05:44:45.244: INFO: Created: latency-svc-6ndrt
Aug 26 05:44:45.254: INFO: Created: latency-svc-bw8mh
Aug 26 05:44:45.266: INFO: Created: latency-svc-v6rkk
Aug 26 05:44:45.290: INFO: Got endpoints: latency-svc-8fh6k [729.561516ms]
Aug 26 05:44:45.306: INFO: Created: latency-svc-xwppm
Aug 26 05:44:45.343: INFO: Got endpoints: latency-svc-wztzn [751.98223ms]
Aug 26 05:44:45.360: INFO: Created: latency-svc-qcvg5
Aug 26 05:44:45.391: INFO: Got endpoints: latency-svc-4mmn2 [750.532425ms]
Aug 26 05:44:45.409: INFO: Created: latency-svc-fth8v
Aug 26 05:44:45.452: INFO: Got endpoints: latency-svc-z8cfs [759.797435ms]
Aug 26 05:44:45.466: INFO: Created: latency-svc-r8hjh
Aug 26 05:44:45.496: INFO: Got endpoints: latency-svc-d5fj2 [754.306555ms]
Aug 26 05:44:45.514: INFO: Created: latency-svc-4b5cw
Aug 26 05:44:45.548: INFO: Got endpoints: latency-svc-xwr47 [756.24189ms]
Aug 26 05:44:45.568: INFO: Created: latency-svc-vg6xr
Aug 26 05:44:45.594: INFO: Got endpoints: latency-svc-s8v9k [752.778564ms]
Aug 26 05:44:45.612: INFO: Created: latency-svc-bw4jq
Aug 26 05:44:45.643: INFO: Got endpoints: latency-svc-rl9s9 [748.282531ms]
Aug 26 05:44:45.666: INFO: Created: latency-svc-kvsfb
Aug 26 05:44:45.696: INFO: Got endpoints: latency-svc-pww9t [753.23132ms]
Aug 26 05:44:45.715: INFO: Created: latency-svc-rkkzf
Aug 26 05:44:45.741: INFO: Got endpoints: latency-svc-mf9x6 [659.385579ms]
Aug 26 05:44:45.758: INFO: Created: latency-svc-x5mpj
Aug 26 05:44:45.791: INFO: Got endpoints: latency-svc-q642f [705.730574ms]
Aug 26 05:44:45.809: INFO: Created: latency-svc-86wh4
Aug 26 05:44:45.846: INFO: Got endpoints: latency-svc-j7jpk [751.937827ms]
Aug 26 05:44:45.866: INFO: Created: latency-svc-fcghc
Aug 26 05:44:45.892: INFO: Got endpoints: latency-svc-6ndrt [706.925405ms]
Aug 26 05:44:45.912: INFO: Created: latency-svc-c542w
Aug 26 05:44:45.941: INFO: Got endpoints: latency-svc-bw8mh [743.369326ms]
Aug 26 05:44:45.958: INFO: Created: latency-svc-p22c5
Aug 26 05:44:45.992: INFO: Got endpoints: latency-svc-v6rkk [748.477887ms]
Aug 26 05:44:46.011: INFO: Created: latency-svc-5kl4j
Aug 26 05:44:46.040: INFO: Got endpoints: latency-svc-xwppm [750.130342ms]
Aug 26 05:44:46.056: INFO: Created: latency-svc-mbs4d
Aug 26 05:44:46.094: INFO: Got endpoints: latency-svc-qcvg5 [750.812618ms]
Aug 26 05:44:46.108: INFO: Created: latency-svc-k9wbj
Aug 26 05:44:46.142: INFO: Got endpoints: latency-svc-fth8v [750.682854ms]
Aug 26 05:44:46.160: INFO: Created: latency-svc-wmtfd
Aug 26 05:44:46.191: INFO: Got endpoints: latency-svc-r8hjh [739.248153ms]
Aug 26 05:44:46.208: INFO: Created: latency-svc-bd6dz
Aug 26 05:44:46.243: INFO: Got endpoints: latency-svc-4b5cw [747.044063ms]
Aug 26 05:44:46.273: INFO: Created: latency-svc-gbqlm
Aug 26 05:44:46.292: INFO: Got endpoints: latency-svc-vg6xr [743.809578ms]
Aug 26 05:44:46.391: INFO: Got endpoints: latency-svc-bw4jq [796.676791ms]
Aug 26 05:44:46.396: INFO: Got endpoints: latency-svc-kvsfb [753.35886ms]
Aug 26 05:44:46.403: INFO: Created: latency-svc-96pbn
Aug 26 05:44:46.412: INFO: Created: latency-svc-wljgz
Aug 26 05:44:46.421: INFO: Created: latency-svc-psc9w
Aug 26 05:44:46.444: INFO: Got endpoints: latency-svc-rkkzf [748.09854ms]
Aug 26 05:44:46.464: INFO: Created: latency-svc-5zb95
Aug 26 05:44:46.610: INFO: Got endpoints: latency-svc-x5mpj [869.007129ms]
Aug 26 05:44:46.613: INFO: Got endpoints: latency-svc-86wh4 [822.395788ms]
Aug 26 05:44:46.614: INFO: Got endpoints: latency-svc-fcghc [768.045873ms]
Aug 26 05:44:46.715: INFO: Got endpoints: latency-svc-c542w [822.389355ms]
Aug 26 05:44:46.716: INFO: Got endpoints: latency-svc-p22c5 [774.257362ms]
Aug 26 05:44:46.722: INFO: Created: latency-svc-t4mqp
Aug 26 05:44:46.730: INFO: Created: latency-svc-p8m2k
Aug 26 05:44:46.742: INFO: Got endpoints: latency-svc-5kl4j [749.250715ms]
Aug 26 05:44:46.749: INFO: Created: latency-svc-l6gmz
Aug 26 05:44:46.756: INFO: Created: latency-svc-82d9c
Aug 26 05:44:46.769: INFO: Created: latency-svc-5d9k6
Aug 26 05:44:46.782: INFO: Created: latency-svc-ljh69
Aug 26 05:44:46.801: INFO: Got endpoints: latency-svc-mbs4d [760.740197ms]
Aug 26 05:44:46.816: INFO: Created: latency-svc-475bx
Aug 26 05:44:46.842: INFO: Got endpoints: latency-svc-k9wbj [747.972591ms]
Aug 26 05:44:46.858: INFO: Created: latency-svc-kcctk
Aug 26 05:44:46.892: INFO: Got endpoints: latency-svc-wmtfd [749.845183ms]
Aug 26 05:44:46.917: INFO: Created: latency-svc-rtjxq
Aug 26 05:44:46.948: INFO: Got endpoints: latency-svc-bd6dz [756.849307ms]
Aug 26 05:44:46.964: INFO: Created: latency-svc-d76gj
Aug 26 05:44:46.993: INFO: Got endpoints: latency-svc-gbqlm [749.802381ms]
Aug 26 05:44:47.014: INFO: Created: latency-svc-9pwck
Aug 26 05:44:47.046: INFO: Got endpoints: latency-svc-96pbn [754.03993ms]
Aug 26 05:44:47.068: INFO: Created: latency-svc-4qg7g
Aug 26 05:44:47.102: INFO: Got endpoints: latency-svc-wljgz [711.48988ms]
Aug 26 05:44:47.124: INFO: Created: latency-svc-s9zqf
Aug 26 05:44:47.144: INFO: Got endpoints: latency-svc-psc9w [748.199437ms]
Aug 26 05:44:47.166: INFO: Created: latency-svc-blmt8
Aug 26 05:44:47.194: INFO: Got endpoints: latency-svc-5zb95 [749.408219ms]
Aug 26 05:44:47.214: INFO: Created: latency-svc-mw8f9
Aug 26 05:44:47.242: INFO: Got endpoints: latency-svc-t4mqp [631.371286ms]
Aug 26 05:44:47.258: INFO: Created: latency-svc-6khhn
Aug 26 05:44:47.292: INFO: Got endpoints: latency-svc-p8m2k [678.354529ms]
Aug 26 05:44:47.306: INFO: Created: latency-svc-lhhhs
Aug 26 05:44:47.345: INFO: Got endpoints: latency-svc-l6gmz [730.601506ms]
Aug 26 05:44:47.372: INFO: Created: latency-svc-xdccj
Aug 26 05:44:47.392: INFO: Got endpoints: latency-svc-82d9c [676.508464ms]
Aug 26 05:44:47.413: INFO: Created: latency-svc-pw4kr
Aug 26 05:44:47.493: INFO: Got endpoints: latency-svc-5d9k6 [776.783302ms]
Aug 26 05:44:47.501: INFO: Got endpoints: latency-svc-ljh69 [758.936129ms]
Aug 26 05:44:47.512: INFO: Created: latency-svc-kljcn
Aug 26 05:44:47.522: INFO: Created: latency-svc-b6jtn
Aug 26 05:44:47.541: INFO: Got endpoints: latency-svc-475bx [740.525539ms]
Aug 26 05:44:47.572: INFO: Created: latency-svc-nwjhm
Aug 26 05:44:47.596: INFO: Got endpoints: latency-svc-kcctk [753.931254ms]
Aug 26 05:44:47.611: INFO: Created: latency-svc-vsm42
Aug 26 05:44:47.641: INFO: Got endpoints: latency-svc-rtjxq [749.074765ms]
Aug 26 05:44:47.663: INFO: Created: latency-svc-tlnwf
Aug 26 05:44:47.694: INFO: Got endpoints: latency-svc-d76gj [746.106514ms]
Aug 26 05:44:47.717: INFO: Created: latency-svc-frjfc
Aug 26 05:44:47.744: INFO: Got endpoints: latency-svc-9pwck [750.914661ms]
Aug 26 05:44:47.759: INFO: Created: latency-svc-trqsq
Aug 26 05:44:47.792: INFO: Got endpoints: latency-svc-4qg7g [745.846598ms]
Aug 26 05:44:47.810: INFO: Created: latency-svc-nqvh8
Aug 26 05:44:47.842: INFO: Got endpoints: latency-svc-s9zqf [739.746081ms]
Aug 26 05:44:47.855: INFO: Created: latency-svc-hmp7n
Aug 26 05:44:47.891: INFO: Got endpoints: latency-svc-blmt8 [746.891178ms]
Aug 26 05:44:47.907: INFO: Created: latency-svc-7cs46
Aug 26 05:44:48.031: INFO: Got endpoints: latency-svc-mw8f9 [837.670403ms]
Aug 26 05:44:48.036: INFO: Got endpoints: latency-svc-6khhn [793.632414ms]
Aug 26 05:44:48.118: INFO: Got endpoints: latency-svc-lhhhs [825.960981ms]
Aug 26 05:44:48.120: INFO: Got endpoints: latency-svc-xdccj [775.105859ms]
Aug 26 05:44:48.129: INFO: Created: latency-svc-96gr2
Aug 26 05:44:48.140: INFO: Created: latency-svc-h482j
Aug 26 05:44:48.143: INFO: Got endpoints: latency-svc-pw4kr [751.207268ms]
Aug 26 05:44:48.153: INFO: Created: latency-svc-wst48
Aug 26 05:44:48.162: INFO: Created: latency-svc-qrl7v
Aug 26 05:44:48.186: INFO: Created: latency-svc-zsqrr
Aug 26 05:44:48.197: INFO: Got endpoints: latency-svc-kljcn [703.680794ms]
Aug 26 05:44:48.222: INFO: Created: latency-svc-2zngl
Aug 26 05:44:48.246: INFO: Got endpoints: latency-svc-b6jtn [745.264229ms]
Aug 26 05:44:48.270: INFO: Created: latency-svc-trn6j
Aug 26 05:44:48.293: INFO: Got endpoints: latency-svc-nwjhm [751.859948ms]
Aug 26 05:44:48.328: INFO: Created: latency-svc-8s4mq
Aug 26 05:44:48.345: INFO: Got endpoints: latency-svc-vsm42 [749.291899ms]
Aug 26 05:44:48.370: INFO: Created: latency-svc-hzjrp
Aug 26 05:44:48.392: INFO: Got endpoints: latency-svc-tlnwf [750.889639ms]
Aug 26 05:44:48.408: INFO: Created: latency-svc-kwbvm
Aug 26 05:44:48.443: INFO: Got endpoints: latency-svc-frjfc [748.083402ms]
Aug 26 05:44:48.458: INFO: Created: latency-svc-bmw2n
Aug 26 05:44:48.491: INFO: Got endpoints: latency-svc-trqsq [746.865765ms]
Aug 26 05:44:48.506: INFO: Created: latency-svc-pvhbj
Aug 26 05:44:48.541: INFO: Got endpoints: latency-svc-nqvh8 [749.034084ms]
Aug 26 05:44:48.554: INFO: Created: latency-svc-x84ss
Aug 26 05:44:48.591: INFO: Got endpoints: latency-svc-hmp7n [749.016307ms]
Aug 26 05:44:48.606: INFO: Created: latency-svc-lbdn2
Aug 26 05:44:48.769: INFO: Got endpoints: latency-svc-7cs46 [877.873137ms]
Aug 26 05:44:48.769: INFO: Got endpoints: latency-svc-h482j [733.90163ms]
Aug 26 05:44:48.770: INFO: Got endpoints: latency-svc-96gr2 [737.919802ms]
Aug 26 05:44:48.785: INFO: Created: latency-svc-6hgjk
Aug 26 05:44:48.799: INFO: Got endpoints: latency-svc-wst48 [680.851807ms]
Aug 26 05:44:48.811: INFO: Created: latency-svc-bsg77
Aug 26 05:44:48.827: INFO: Created: latency-svc-866nb
Aug 26 05:44:48.839: INFO: Created: latency-svc-ldvh6
Aug 26 05:44:48.841: INFO: Got endpoints: latency-svc-qrl7v [721.057175ms]
Aug 26 05:44:48.855: INFO: Created: latency-svc-qwn92
Aug 26 05:44:48.891: INFO: Got endpoints: latency-svc-zsqrr [748.029457ms]
Aug 26 05:44:48.910: INFO: Created: latency-svc-pgglf
Aug 26 05:44:48.943: INFO: Got endpoints: latency-svc-2zngl [745.910202ms]
Aug 26 05:44:48.959: INFO: Created: latency-svc-g2hzh
Aug 26 05:44:48.992: INFO: Got endpoints: latency-svc-trn6j [745.167065ms]
Aug 26 05:44:49.011: INFO: Created: latency-svc-fzpg8
Aug 26 05:44:49.092: INFO: Got endpoints: latency-svc-8s4mq [798.673179ms]
Aug 26 05:44:49.111: INFO: Created: latency-svc-qtlgk
Aug 26 05:44:49.141: INFO: Got endpoints: latency-svc-hzjrp [795.283022ms]
Aug 26 05:44:49.157: INFO: Created: latency-svc-fqwx6
Aug 26 05:44:49.190: INFO: Got endpoints: latency-svc-kwbvm [798.059147ms]
Aug 26 05:44:49.208: INFO: Created: latency-svc-4rnb6
Aug 26 05:44:49.244: INFO: Got endpoints: latency-svc-bmw2n [801.162045ms]
Aug 26 05:44:49.268: INFO: Created: latency-svc-mvzw8
Aug 26 05:44:49.291: INFO: Got endpoints: latency-svc-pvhbj [800.219648ms]
Aug 26 05:44:49.308: INFO: Created: latency-svc-mvp9v
Aug 26 05:44:49.348: INFO: Got endpoints: latency-svc-x84ss [806.700423ms]
Aug 26 05:44:49.365: INFO: Created: latency-svc-q7f6m
Aug 26 05:44:49.395: INFO: Got endpoints: latency-svc-lbdn2 [803.700926ms]
Aug 26 05:44:49.415: INFO: Created: latency-svc-hfwb2
Aug 26 05:44:49.446: INFO: Got endpoints: latency-svc-6hgjk [676.379835ms]
Aug 26 05:44:49.467: INFO: Created: latency-svc-vc99r
Aug 26 05:44:49.495: INFO: Got endpoints: latency-svc-bsg77 [725.836071ms]
Aug 26 05:44:49.515: INFO: Created: latency-svc-sx78s
Aug 26 05:44:49.545: INFO: Got endpoints: latency-svc-866nb [774.968416ms]
Aug 26 05:44:49.627: INFO: Got endpoints: latency-svc-ldvh6 [828.37006ms]
Aug 26 05:44:49.630: INFO: Created: latency-svc-b8p7s
Aug 26 05:44:49.735: INFO: Got endpoints: latency-svc-qwn92 [893.705671ms]
Aug 26 05:44:49.735: INFO: Got endpoints: latency-svc-pgglf [844.028142ms]
Aug 26 05:44:49.745: INFO: Created: latency-svc-8wt48
Aug 26 05:44:49.746: INFO: Got endpoints: latency-svc-g2hzh [803.127128ms]
Aug 26 05:44:49.757: INFO: Created: latency-svc-xr528
Aug 26 05:44:49.765: INFO: Created: latency-svc-zdd7z
Aug 26 05:44:49.776: INFO: Created: latency-svc-tbp56
Aug 26 05:44:49.795: INFO: Got endpoints: latency-svc-fzpg8 [803.650334ms]
Aug 26 05:44:49.814: INFO: Created: latency-svc-549b6
Aug 26 05:44:49.841: INFO: Got endpoints: latency-svc-qtlgk [748.625868ms]
Aug 26 05:44:49.856: INFO: Created: latency-svc-tllnw
Aug 26 05:44:49.893: INFO: Got endpoints: latency-svc-fqwx6 [752.229528ms]
Aug 26 05:44:49.908: INFO: Created: latency-svc-hzp9j
Aug 26 05:44:49.944: INFO: Got endpoints: latency-svc-4rnb6 [753.150801ms]
Aug 26 05:44:49.962: INFO: Created: latency-svc-7lnhl
Aug 26 05:44:49.998: INFO: Got endpoints: latency-svc-mvzw8 [754.623825ms]
Aug 26 05:44:50.043: INFO: Got endpoints: latency-svc-mvp9v [751.754074ms]
Aug 26 05:44:50.044: INFO: Created: latency-svc-tbj2g
Aug 26 05:44:50.061: INFO: Created: latency-svc-zdn9n
Aug 26 05:44:50.091: INFO: Got endpoints: latency-svc-q7f6m [743.191972ms]
Aug 26 05:44:50.110: INFO: Created: latency-svc-9fgjl
Aug 26 05:44:50.197: INFO: Got endpoints: latency-svc-hfwb2 [801.778511ms]
Aug 26 05:44:50.198: INFO: Got endpoints: latency-svc-vc99r [751.940074ms]
Aug 26 05:44:50.216: INFO: Created: latency-svc-m8d7x
Aug 26 05:44:50.229: INFO: Created: latency-svc-nd2xb
Aug 26 05:44:50.242: INFO: Got endpoints: latency-svc-sx78s [746.836844ms]
Aug 26 05:44:50.260: INFO: Created: latency-svc-j2m82
Aug 26 05:44:50.300: INFO: Got endpoints: latency-svc-b8p7s [755.666731ms]
Aug 26 05:44:50.317: INFO: Created: latency-svc-jkwqd
Aug 26 05:44:50.343: INFO: Got endpoints: latency-svc-8wt48 [715.458861ms]
Aug 26 05:44:50.360: INFO: Created: latency-svc-qshmf
Aug 26 05:44:50.392: INFO: Got endpoints: latency-svc-xr528 [656.56839ms]
Aug 26 05:44:50.409: INFO: Created: latency-svc-nqpwj
Aug 26 05:44:50.443: INFO: Got endpoints: latency-svc-zdd7z [707.825571ms]
Aug 26 05:44:50.467: INFO: Created: latency-svc-rxpjs
Aug 26 05:44:50.493: INFO: Got endpoints: latency-svc-tbp56 [747.072767ms]
Aug 26 05:44:50.510: INFO: Created: latency-svc-vtjg5
Aug 26 05:44:50.541: INFO: Got endpoints: latency-svc-549b6 [745.656002ms]
Aug 26 05:44:50.558: INFO: Created: latency-svc-c7d7z
Aug 26 05:44:50.595: INFO: Got endpoints: latency-svc-tllnw [753.993382ms]
Aug 26 05:44:50.613: INFO: Created: latency-svc-z2d5c
Aug 26 05:44:50.642: INFO: Got endpoints: latency-svc-hzp9j [748.592303ms]
Aug 26 05:44:50.657: INFO: Created: latency-svc-67qgh
Aug 26 05:44:50.700: INFO: Got endpoints: latency-svc-7lnhl [756.558001ms]
Aug 26 05:44:50.722: INFO: Created: latency-svc-xkqhn
Aug 26 05:44:50.743: INFO: Got endpoints: latency-svc-tbj2g [744.222249ms]
Aug 26 05:44:50.758: INFO: Created: latency-svc-4qq8h
Aug 26 05:44:50.791: INFO: Got endpoints: latency-svc-zdn9n [747.983173ms]
Aug 26 05:44:50.813: INFO: Created: latency-svc-jlgst
Aug 26 05:44:50.843: INFO: Got endpoints: latency-svc-9fgjl [751.606327ms]
Aug 26 05:44:50.859: INFO: Created: latency-svc-495q2
Aug 26 05:44:50.893: INFO: Got endpoints: latency-svc-m8d7x [696.196815ms]
Aug 26 05:44:50.912: INFO: Created: latency-svc-x7hkd
Aug 26 05:44:50.941: INFO: Got endpoints: latency-svc-nd2xb [743.580753ms]
Aug 26 05:44:50.956: INFO: Created: latency-svc-qlh5k
Aug 26 05:44:50.991: INFO: Got endpoints: latency-svc-j2m82 [749.114446ms]
Aug 26 05:44:51.006: INFO: Created: latency-svc-z6mcs
Aug 26 05:44:51.057: INFO: Got endpoints: latency-svc-jkwqd [756.308376ms]
Aug 26 05:44:51.073: INFO: Created: latency-svc-crr65
Aug 26 05:44:51.093: INFO: Got endpoints: latency-svc-qshmf [749.475139ms]
Aug 26 05:44:51.111: INFO: Created: latency-svc-hb65t
Aug 26 05:44:51.144: INFO: Got endpoints: latency-svc-nqpwj [752.018163ms]
Aug 26 05:44:51.163: INFO: Created: latency-svc-9wxzc
Aug 26 05:44:51.192: INFO: Got endpoints: latency-svc-rxpjs [748.596446ms]
Aug 26 05:44:51.215: INFO: Created: latency-svc-6tq6x
Aug 26 05:44:51.242: INFO: Got endpoints: latency-svc-vtjg5 [748.46588ms]
Aug 26 05:44:51.359: INFO: Got endpoints: latency-svc-c7d7z [818.055755ms]
Aug 26 05:44:51.360: INFO: Got endpoints: latency-svc-z2d5c [765.295817ms]
Aug 26 05:44:51.363: INFO: Created: latency-svc-hpbls
Aug 26 05:44:51.376: INFO: Created: latency-svc-pbr7p
Aug 26 05:44:51.383: INFO: Created: latency-svc-t8fbx
Aug 26 05:44:51.396: INFO: Got endpoints: latency-svc-67qgh [754.493894ms]
Aug 26 05:44:51.412: INFO: Created: latency-svc-q2mrh
Aug 26 05:44:51.443: INFO: Got endpoints: latency-svc-xkqhn [742.919619ms]
Aug 26 05:44:51.491: INFO: Got endpoints: latency-svc-4qq8h [748.437069ms]
Aug 26 05:44:51.545: INFO: Got endpoints: latency-svc-jlgst [753.976035ms]
Aug 26 05:44:51.594: INFO: Got endpoints: latency-svc-495q2 [751.714469ms]
Aug 26 05:44:51.641: INFO: Got endpoints: latency-svc-x7hkd [747.947616ms]
Aug 26 05:44:51.691: INFO: Got endpoints: latency-svc-qlh5k [750.108723ms]
Aug 26 05:44:51.743: INFO: Got endpoints: latency-svc-z6mcs [751.450368ms]
Aug 26 05:44:51.791: INFO: Got endpoints: latency-svc-crr65 [733.75902ms]
Aug 26 05:44:51.845: INFO: Got endpoints: latency-svc-hb65t [752.097938ms]
Aug 26 05:44:51.893: INFO: Got endpoints: latency-svc-9wxzc [748.858838ms]
Aug 26 05:44:51.945: INFO: Got endpoints: latency-svc-6tq6x [753.228722ms]
Aug 26 05:44:52.005: INFO: Got endpoints: latency-svc-hpbls [763.524116ms]
Aug 26 05:44:52.047: INFO: Got endpoints: latency-svc-pbr7p [687.835826ms]
Aug 26 05:44:52.092: INFO: Got endpoints: latency-svc-t8fbx [731.490286ms]
Aug 26 05:44:52.142: INFO: Got endpoints: latency-svc-q2mrh [746.075273ms]
Aug 26 05:44:52.142: INFO: Latencies: [32.073242ms 57.653771ms 84.167956ms 101.301117ms 108.439667ms 112.107734ms 129.702676ms 140.845161ms 156.191848ms 169.870753ms 183.677122ms 186.458112ms 196.016376ms 196.495791ms 202.352728ms 208.833062ms 209.770192ms 210.445476ms 212.589568ms 213.041005ms 213.551205ms 215.383427ms 217.694062ms 219.054444ms 219.48275ms 221.454072ms 228.865672ms 230.979496ms 232.725866ms 232.824857ms 234.237971ms 238.689594ms 240.020462ms 241.578668ms 245.753772ms 277.931206ms 302.176528ms 308.727118ms 319.202717ms 354.08397ms 356.09213ms 363.77832ms 366.672955ms 367.367404ms 375.612775ms 375.868482ms 378.813845ms 380.051785ms 381.371157ms 382.072063ms 393.788699ms 444.816835ms 483.649125ms 506.190035ms 519.714107ms 569.392966ms 631.371286ms 656.56839ms 659.385579ms 676.379835ms 676.508464ms 678.354529ms 680.851807ms 682.469263ms 683.447499ms 687.835826ms 696.196815ms 703.680794ms 705.730574ms 706.925405ms 707.825571ms 711.48988ms 715.458861ms 721.057175ms 724.848367ms 725.836071ms 729.561516ms 730.601506ms 731.490286ms 731.711833ms 733.75902ms 733.90163ms 737.919802ms 739.248153ms 739.746081ms 740.525539ms 742.919619ms 743.191972ms 743.369326ms 743.580753ms 743.809578ms 744.222249ms 745.167065ms 745.264229ms 745.656002ms 745.846598ms 745.910202ms 746.075273ms 746.106514ms 746.836844ms 746.865765ms 746.891178ms 747.044063ms 747.072767ms 747.947616ms 747.972591ms 747.983173ms 748.029457ms 748.083402ms 748.09854ms 748.199437ms 748.282531ms 748.437069ms 748.46588ms 748.477887ms 748.592303ms 748.596446ms 748.625868ms 748.858838ms 749.016307ms 749.034084ms 749.074765ms 749.114446ms 749.250715ms 749.291899ms 749.408219ms 749.475139ms 749.802381ms 749.845183ms 750.108723ms 750.130342ms 750.532425ms 750.682854ms 750.812618ms 750.889639ms 750.914661ms 751.207268ms 751.450368ms 751.606327ms 751.714469ms 751.754074ms 751.859948ms 751.937827ms 751.940074ms 751.98223ms 752.018163ms 752.097938ms 752.229528ms 752.778564ms 753.150801ms 753.228722ms 753.23132ms 753.35886ms 753.931254ms 753.976035ms 753.993382ms 754.03993ms 754.306555ms 754.493894ms 754.623825ms 755.666731ms 756.24189ms 756.308376ms 756.558001ms 756.849307ms 758.936129ms 759.797435ms 760.740197ms 763.524116ms 765.295817ms 768.045873ms 774.257362ms 774.968416ms 775.105859ms 776.783302ms 793.632414ms 795.283022ms 796.676791ms 798.059147ms 798.673179ms 800.219648ms 801.162045ms 801.778511ms 803.127128ms 803.650334ms 803.700926ms 806.700423ms 818.055755ms 822.389355ms 822.395788ms 825.960981ms 828.37006ms 837.670403ms 844.028142ms 867.571778ms 869.007129ms 877.873137ms 893.705671ms 928.103148ms 1.034295501s]
Aug 26 05:44:52.143: INFO: 50 %ile: 746.865765ms
Aug 26 05:44:52.143: INFO: 90 %ile: 800.219648ms
Aug 26 05:44:52.143: INFO: 99 %ile: 928.103148ms
Aug 26 05:44:52.143: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Aug 26 05:44:52.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-6021" for this suite. 08/26/23 05:44:52.166
------------------------------
• [SLOW TEST] [11.078 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:44:41.185
    Aug 26 05:44:41.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename svc-latency 08/26/23 05:44:41.186
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:41.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:41.221
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Aug 26 05:44:41.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-6021 08/26/23 05:44:41.25
    I0826 05:44:41.258112      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6021, replica count: 1
    I0826 05:44:42.310105      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0826 05:44:43.310588      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 26 05:44:43.534: INFO: Created: latency-svc-dk5jp
    Aug 26 05:44:43.544: INFO: Got endpoints: latency-svc-dk5jp [133.397076ms]
    Aug 26 05:44:43.570: INFO: Created: latency-svc-9222h
    Aug 26 05:44:43.576: INFO: Got endpoints: latency-svc-9222h [32.073242ms]
    Aug 26 05:44:43.597: INFO: Created: latency-svc-c6k6n
    Aug 26 05:44:43.602: INFO: Got endpoints: latency-svc-c6k6n [57.653771ms]
    Aug 26 05:44:43.620: INFO: Created: latency-svc-6cqt6
    Aug 26 05:44:43.629: INFO: Got endpoints: latency-svc-6cqt6 [84.167956ms]
    Aug 26 05:44:43.637: INFO: Created: latency-svc-tlmr8
    Aug 26 05:44:43.646: INFO: Got endpoints: latency-svc-tlmr8 [101.301117ms]
    Aug 26 05:44:43.650: INFO: Created: latency-svc-j424z
    Aug 26 05:44:43.653: INFO: Got endpoints: latency-svc-j424z [108.439667ms]
    Aug 26 05:44:43.666: INFO: Created: latency-svc-6vtx5
    Aug 26 05:44:43.674: INFO: Got endpoints: latency-svc-6vtx5 [129.702676ms]
    Aug 26 05:44:43.678: INFO: Created: latency-svc-2j28b
    Aug 26 05:44:43.686: INFO: Got endpoints: latency-svc-2j28b [140.845161ms]
    Aug 26 05:44:43.694: INFO: Created: latency-svc-w7z8l
    Aug 26 05:44:43.701: INFO: Got endpoints: latency-svc-w7z8l [156.191848ms]
    Aug 26 05:44:43.706: INFO: Created: latency-svc-jfqzh
    Aug 26 05:44:43.715: INFO: Got endpoints: latency-svc-jfqzh [169.870753ms]
    Aug 26 05:44:43.721: INFO: Created: latency-svc-2sk5x
    Aug 26 05:44:43.728: INFO: Got endpoints: latency-svc-2sk5x [183.677122ms]
    Aug 26 05:44:43.733: INFO: Created: latency-svc-nsqs9
    Aug 26 05:44:43.741: INFO: Got endpoints: latency-svc-nsqs9 [196.495791ms]
    Aug 26 05:44:43.745: INFO: Created: latency-svc-wxkf8
    Aug 26 05:44:43.755: INFO: Got endpoints: latency-svc-wxkf8 [209.770192ms]
    Aug 26 05:44:43.758: INFO: Created: latency-svc-kqktb
    Aug 26 05:44:43.764: INFO: Got endpoints: latency-svc-kqktb [219.48275ms]
    Aug 26 05:44:43.769: INFO: Created: latency-svc-n66jw
    Aug 26 05:44:43.776: INFO: Got endpoints: latency-svc-n66jw [230.979496ms]
    Aug 26 05:44:43.781: INFO: Created: latency-svc-g9gf5
    Aug 26 05:44:43.791: INFO: Got endpoints: latency-svc-g9gf5 [245.753772ms]
    Aug 26 05:44:43.794: INFO: Created: latency-svc-vl28l
    Aug 26 05:44:43.807: INFO: Created: latency-svc-ff5gq
    Aug 26 05:44:43.809: INFO: Got endpoints: latency-svc-vl28l [232.725866ms]
    Aug 26 05:44:43.820: INFO: Got endpoints: latency-svc-ff5gq [217.694062ms]
    Aug 26 05:44:43.820: INFO: Created: latency-svc-zwwmz
    Aug 26 05:44:43.831: INFO: Got endpoints: latency-svc-zwwmz [202.352728ms]
    Aug 26 05:44:43.839: INFO: Created: latency-svc-449l6
    Aug 26 05:44:43.842: INFO: Got endpoints: latency-svc-449l6 [196.016376ms]
    Aug 26 05:44:43.852: INFO: Created: latency-svc-j8jcm
    Aug 26 05:44:43.864: INFO: Got endpoints: latency-svc-j8jcm [210.445476ms]
    Aug 26 05:44:43.868: INFO: Created: latency-svc-92d8x
    Aug 26 05:44:43.883: INFO: Got endpoints: latency-svc-92d8x [208.833062ms]
    Aug 26 05:44:43.888: INFO: Created: latency-svc-fkdvb
    Aug 26 05:44:43.901: INFO: Got endpoints: latency-svc-fkdvb [215.383427ms]
    Aug 26 05:44:43.906: INFO: Created: latency-svc-mb6pq
    Aug 26 05:44:43.914: INFO: Got endpoints: latency-svc-mb6pq [213.041005ms]
    Aug 26 05:44:43.918: INFO: Created: latency-svc-nz6lb
    Aug 26 05:44:43.927: INFO: Got endpoints: latency-svc-nz6lb [212.589568ms]
    Aug 26 05:44:43.935: INFO: Created: latency-svc-8zfwk
    Aug 26 05:44:43.942: INFO: Got endpoints: latency-svc-8zfwk [213.551205ms]
    Aug 26 05:44:43.947: INFO: Created: latency-svc-rqd67
    Aug 26 05:44:43.963: INFO: Got endpoints: latency-svc-rqd67 [221.454072ms]
    Aug 26 05:44:43.968: INFO: Created: latency-svc-sbxz5
    Aug 26 05:44:43.974: INFO: Got endpoints: latency-svc-sbxz5 [219.054444ms]
    Aug 26 05:44:43.991: INFO: Created: latency-svc-gdxrq
    Aug 26 05:44:44.001: INFO: Created: latency-svc-s7hh5
    Aug 26 05:44:44.006: INFO: Got endpoints: latency-svc-gdxrq [241.578668ms]
    Aug 26 05:44:44.009: INFO: Got endpoints: latency-svc-s7hh5 [232.824857ms]
    Aug 26 05:44:44.017: INFO: Created: latency-svc-v9lm2
    Aug 26 05:44:44.029: INFO: Got endpoints: latency-svc-v9lm2 [238.689594ms]
    Aug 26 05:44:44.037: INFO: Created: latency-svc-2xps8
    Aug 26 05:44:44.038: INFO: Got endpoints: latency-svc-2xps8 [228.865672ms]
    Aug 26 05:44:44.046: INFO: Created: latency-svc-9bcbm
    Aug 26 05:44:44.054: INFO: Got endpoints: latency-svc-9bcbm [234.237971ms]
    Aug 26 05:44:44.071: INFO: Created: latency-svc-2h6rs
    Aug 26 05:44:44.071: INFO: Got endpoints: latency-svc-2h6rs [240.020462ms]
    Aug 26 05:44:44.078: INFO: Created: latency-svc-x5jb7
    Aug 26 05:44:44.151: INFO: Got endpoints: latency-svc-x5jb7 [308.727118ms]
    Aug 26 05:44:44.164: INFO: Created: latency-svc-dbkgv
    Aug 26 05:44:44.166: INFO: Got endpoints: latency-svc-dbkgv [302.176528ms]
    Aug 26 05:44:44.251: INFO: Created: latency-svc-pbg5w
    Aug 26 05:44:44.259: INFO: Got endpoints: latency-svc-pbg5w [375.612775ms]
    Aug 26 05:44:44.268: INFO: Created: latency-svc-5zjc9
    Aug 26 05:44:44.282: INFO: Got endpoints: latency-svc-5zjc9 [381.371157ms]
    Aug 26 05:44:44.286: INFO: Created: latency-svc-v4g2q
    Aug 26 05:44:44.296: INFO: Got endpoints: latency-svc-v4g2q [382.072063ms]
    Aug 26 05:44:44.303: INFO: Created: latency-svc-vlppg
    Aug 26 05:44:44.308: INFO: Got endpoints: latency-svc-vlppg [380.051785ms]
    Aug 26 05:44:44.315: INFO: Created: latency-svc-lcdxs
    Aug 26 05:44:44.321: INFO: Got endpoints: latency-svc-lcdxs [378.813845ms]
    Aug 26 05:44:44.329: INFO: Created: latency-svc-4cflh
    Aug 26 05:44:44.330: INFO: Got endpoints: latency-svc-4cflh [367.367404ms]
    Aug 26 05:44:44.338: INFO: Created: latency-svc-mc4r7
    Aug 26 05:44:44.350: INFO: Got endpoints: latency-svc-mc4r7 [375.868482ms]
    Aug 26 05:44:44.354: INFO: Created: latency-svc-6n654
    Aug 26 05:44:44.360: INFO: Got endpoints: latency-svc-6n654 [354.08397ms]
    Aug 26 05:44:44.370: INFO: Created: latency-svc-9cv9j
    Aug 26 05:44:44.373: INFO: Got endpoints: latency-svc-9cv9j [363.77832ms]
    Aug 26 05:44:44.381: INFO: Created: latency-svc-hkp2t
    Aug 26 05:44:44.388: INFO: Got endpoints: latency-svc-hkp2t [356.09213ms]
    Aug 26 05:44:44.397: INFO: Created: latency-svc-7gcfh
    Aug 26 05:44:44.405: INFO: Got endpoints: latency-svc-7gcfh [366.672955ms]
    Aug 26 05:44:44.414: INFO: Created: latency-svc-8lkjz
    Aug 26 05:44:44.448: INFO: Got endpoints: latency-svc-8lkjz [393.788699ms]
    Aug 26 05:44:44.546: INFO: Created: latency-svc-5kkd2
    Aug 26 05:44:44.553: INFO: Created: latency-svc-qstbt
    Aug 26 05:44:44.553: INFO: Created: latency-svc-gzds8
    Aug 26 05:44:44.554: INFO: Created: latency-svc-5mcb2
    Aug 26 05:44:44.554: INFO: Created: latency-svc-dzt78
    Aug 26 05:44:44.556: INFO: Created: latency-svc-5xdvz
    Aug 26 05:44:44.556: INFO: Created: latency-svc-4j6fn
    Aug 26 05:44:44.556: INFO: Created: latency-svc-x7k24
    Aug 26 05:44:44.557: INFO: Created: latency-svc-szsd8
    Aug 26 05:44:44.557: INFO: Created: latency-svc-79skx
    Aug 26 05:44:44.557: INFO: Created: latency-svc-4r4vp
    Aug 26 05:44:44.558: INFO: Created: latency-svc-8xq8z
    Aug 26 05:44:44.558: INFO: Created: latency-svc-mvsxt
    Aug 26 05:44:44.559: INFO: Created: latency-svc-dgjnc
    Aug 26 05:44:44.560: INFO: Created: latency-svc-gvjjv
    Aug 26 05:44:44.560: INFO: Got endpoints: latency-svc-x7k24 [112.107734ms]
    Aug 26 05:44:44.560: INFO: Got endpoints: latency-svc-5kkd2 [277.931206ms]
    Aug 26 05:44:44.582: INFO: Created: latency-svc-dtb46
    Aug 26 05:44:44.591: INFO: Got endpoints: latency-svc-dzt78 [186.458112ms]
    Aug 26 05:44:44.595: INFO: Created: latency-svc-8fh6k
    Aug 26 05:44:44.607: INFO: Created: latency-svc-wztzn
    Aug 26 05:44:44.641: INFO: Got endpoints: latency-svc-gzds8 [569.392966ms]
    Aug 26 05:44:44.659: INFO: Created: latency-svc-4mmn2
    Aug 26 05:44:44.692: INFO: Got endpoints: latency-svc-qstbt [319.202717ms]
    Aug 26 05:44:44.707: INFO: Created: latency-svc-z8cfs
    Aug 26 05:44:44.741: INFO: Got endpoints: latency-svc-szsd8 [444.816835ms]
    Aug 26 05:44:44.759: INFO: Created: latency-svc-d5fj2
    Aug 26 05:44:44.791: INFO: Got endpoints: latency-svc-4j6fn [483.649125ms]
    Aug 26 05:44:44.807: INFO: Created: latency-svc-xwr47
    Aug 26 05:44:44.841: INFO: Got endpoints: latency-svc-gvjjv [519.714107ms]
    Aug 26 05:44:44.864: INFO: Created: latency-svc-s8v9k
    Aug 26 05:44:44.894: INFO: Got endpoints: latency-svc-79skx [506.190035ms]
    Aug 26 05:44:44.908: INFO: Created: latency-svc-rl9s9
    Aug 26 05:44:44.941: INFO: Got endpoints: latency-svc-4r4vp [682.469263ms]
    Aug 26 05:44:44.963: INFO: Created: latency-svc-pww9t
    Aug 26 05:44:45.082: INFO: Got endpoints: latency-svc-5xdvz [731.711833ms]
    Aug 26 05:44:45.085: INFO: Got endpoints: latency-svc-5mcb2 [724.848367ms]
    Aug 26 05:44:45.094: INFO: Got endpoints: latency-svc-mvsxt [928.103148ms]
    Aug 26 05:44:45.185: INFO: Got endpoints: latency-svc-8xq8z [1.034295501s]
    Aug 26 05:44:45.193: INFO: Created: latency-svc-mf9x6
    Aug 26 05:44:45.198: INFO: Got endpoints: latency-svc-dgjnc [867.571778ms]
    Aug 26 05:44:45.210: INFO: Created: latency-svc-q642f
    Aug 26 05:44:45.224: INFO: Created: latency-svc-j7jpk
    Aug 26 05:44:45.244: INFO: Got endpoints: latency-svc-dtb46 [683.447499ms]
    Aug 26 05:44:45.244: INFO: Created: latency-svc-6ndrt
    Aug 26 05:44:45.254: INFO: Created: latency-svc-bw8mh
    Aug 26 05:44:45.266: INFO: Created: latency-svc-v6rkk
    Aug 26 05:44:45.290: INFO: Got endpoints: latency-svc-8fh6k [729.561516ms]
    Aug 26 05:44:45.306: INFO: Created: latency-svc-xwppm
    Aug 26 05:44:45.343: INFO: Got endpoints: latency-svc-wztzn [751.98223ms]
    Aug 26 05:44:45.360: INFO: Created: latency-svc-qcvg5
    Aug 26 05:44:45.391: INFO: Got endpoints: latency-svc-4mmn2 [750.532425ms]
    Aug 26 05:44:45.409: INFO: Created: latency-svc-fth8v
    Aug 26 05:44:45.452: INFO: Got endpoints: latency-svc-z8cfs [759.797435ms]
    Aug 26 05:44:45.466: INFO: Created: latency-svc-r8hjh
    Aug 26 05:44:45.496: INFO: Got endpoints: latency-svc-d5fj2 [754.306555ms]
    Aug 26 05:44:45.514: INFO: Created: latency-svc-4b5cw
    Aug 26 05:44:45.548: INFO: Got endpoints: latency-svc-xwr47 [756.24189ms]
    Aug 26 05:44:45.568: INFO: Created: latency-svc-vg6xr
    Aug 26 05:44:45.594: INFO: Got endpoints: latency-svc-s8v9k [752.778564ms]
    Aug 26 05:44:45.612: INFO: Created: latency-svc-bw4jq
    Aug 26 05:44:45.643: INFO: Got endpoints: latency-svc-rl9s9 [748.282531ms]
    Aug 26 05:44:45.666: INFO: Created: latency-svc-kvsfb
    Aug 26 05:44:45.696: INFO: Got endpoints: latency-svc-pww9t [753.23132ms]
    Aug 26 05:44:45.715: INFO: Created: latency-svc-rkkzf
    Aug 26 05:44:45.741: INFO: Got endpoints: latency-svc-mf9x6 [659.385579ms]
    Aug 26 05:44:45.758: INFO: Created: latency-svc-x5mpj
    Aug 26 05:44:45.791: INFO: Got endpoints: latency-svc-q642f [705.730574ms]
    Aug 26 05:44:45.809: INFO: Created: latency-svc-86wh4
    Aug 26 05:44:45.846: INFO: Got endpoints: latency-svc-j7jpk [751.937827ms]
    Aug 26 05:44:45.866: INFO: Created: latency-svc-fcghc
    Aug 26 05:44:45.892: INFO: Got endpoints: latency-svc-6ndrt [706.925405ms]
    Aug 26 05:44:45.912: INFO: Created: latency-svc-c542w
    Aug 26 05:44:45.941: INFO: Got endpoints: latency-svc-bw8mh [743.369326ms]
    Aug 26 05:44:45.958: INFO: Created: latency-svc-p22c5
    Aug 26 05:44:45.992: INFO: Got endpoints: latency-svc-v6rkk [748.477887ms]
    Aug 26 05:44:46.011: INFO: Created: latency-svc-5kl4j
    Aug 26 05:44:46.040: INFO: Got endpoints: latency-svc-xwppm [750.130342ms]
    Aug 26 05:44:46.056: INFO: Created: latency-svc-mbs4d
    Aug 26 05:44:46.094: INFO: Got endpoints: latency-svc-qcvg5 [750.812618ms]
    Aug 26 05:44:46.108: INFO: Created: latency-svc-k9wbj
    Aug 26 05:44:46.142: INFO: Got endpoints: latency-svc-fth8v [750.682854ms]
    Aug 26 05:44:46.160: INFO: Created: latency-svc-wmtfd
    Aug 26 05:44:46.191: INFO: Got endpoints: latency-svc-r8hjh [739.248153ms]
    Aug 26 05:44:46.208: INFO: Created: latency-svc-bd6dz
    Aug 26 05:44:46.243: INFO: Got endpoints: latency-svc-4b5cw [747.044063ms]
    Aug 26 05:44:46.273: INFO: Created: latency-svc-gbqlm
    Aug 26 05:44:46.292: INFO: Got endpoints: latency-svc-vg6xr [743.809578ms]
    Aug 26 05:44:46.391: INFO: Got endpoints: latency-svc-bw4jq [796.676791ms]
    Aug 26 05:44:46.396: INFO: Got endpoints: latency-svc-kvsfb [753.35886ms]
    Aug 26 05:44:46.403: INFO: Created: latency-svc-96pbn
    Aug 26 05:44:46.412: INFO: Created: latency-svc-wljgz
    Aug 26 05:44:46.421: INFO: Created: latency-svc-psc9w
    Aug 26 05:44:46.444: INFO: Got endpoints: latency-svc-rkkzf [748.09854ms]
    Aug 26 05:44:46.464: INFO: Created: latency-svc-5zb95
    Aug 26 05:44:46.610: INFO: Got endpoints: latency-svc-x5mpj [869.007129ms]
    Aug 26 05:44:46.613: INFO: Got endpoints: latency-svc-86wh4 [822.395788ms]
    Aug 26 05:44:46.614: INFO: Got endpoints: latency-svc-fcghc [768.045873ms]
    Aug 26 05:44:46.715: INFO: Got endpoints: latency-svc-c542w [822.389355ms]
    Aug 26 05:44:46.716: INFO: Got endpoints: latency-svc-p22c5 [774.257362ms]
    Aug 26 05:44:46.722: INFO: Created: latency-svc-t4mqp
    Aug 26 05:44:46.730: INFO: Created: latency-svc-p8m2k
    Aug 26 05:44:46.742: INFO: Got endpoints: latency-svc-5kl4j [749.250715ms]
    Aug 26 05:44:46.749: INFO: Created: latency-svc-l6gmz
    Aug 26 05:44:46.756: INFO: Created: latency-svc-82d9c
    Aug 26 05:44:46.769: INFO: Created: latency-svc-5d9k6
    Aug 26 05:44:46.782: INFO: Created: latency-svc-ljh69
    Aug 26 05:44:46.801: INFO: Got endpoints: latency-svc-mbs4d [760.740197ms]
    Aug 26 05:44:46.816: INFO: Created: latency-svc-475bx
    Aug 26 05:44:46.842: INFO: Got endpoints: latency-svc-k9wbj [747.972591ms]
    Aug 26 05:44:46.858: INFO: Created: latency-svc-kcctk
    Aug 26 05:44:46.892: INFO: Got endpoints: latency-svc-wmtfd [749.845183ms]
    Aug 26 05:44:46.917: INFO: Created: latency-svc-rtjxq
    Aug 26 05:44:46.948: INFO: Got endpoints: latency-svc-bd6dz [756.849307ms]
    Aug 26 05:44:46.964: INFO: Created: latency-svc-d76gj
    Aug 26 05:44:46.993: INFO: Got endpoints: latency-svc-gbqlm [749.802381ms]
    Aug 26 05:44:47.014: INFO: Created: latency-svc-9pwck
    Aug 26 05:44:47.046: INFO: Got endpoints: latency-svc-96pbn [754.03993ms]
    Aug 26 05:44:47.068: INFO: Created: latency-svc-4qg7g
    Aug 26 05:44:47.102: INFO: Got endpoints: latency-svc-wljgz [711.48988ms]
    Aug 26 05:44:47.124: INFO: Created: latency-svc-s9zqf
    Aug 26 05:44:47.144: INFO: Got endpoints: latency-svc-psc9w [748.199437ms]
    Aug 26 05:44:47.166: INFO: Created: latency-svc-blmt8
    Aug 26 05:44:47.194: INFO: Got endpoints: latency-svc-5zb95 [749.408219ms]
    Aug 26 05:44:47.214: INFO: Created: latency-svc-mw8f9
    Aug 26 05:44:47.242: INFO: Got endpoints: latency-svc-t4mqp [631.371286ms]
    Aug 26 05:44:47.258: INFO: Created: latency-svc-6khhn
    Aug 26 05:44:47.292: INFO: Got endpoints: latency-svc-p8m2k [678.354529ms]
    Aug 26 05:44:47.306: INFO: Created: latency-svc-lhhhs
    Aug 26 05:44:47.345: INFO: Got endpoints: latency-svc-l6gmz [730.601506ms]
    Aug 26 05:44:47.372: INFO: Created: latency-svc-xdccj
    Aug 26 05:44:47.392: INFO: Got endpoints: latency-svc-82d9c [676.508464ms]
    Aug 26 05:44:47.413: INFO: Created: latency-svc-pw4kr
    Aug 26 05:44:47.493: INFO: Got endpoints: latency-svc-5d9k6 [776.783302ms]
    Aug 26 05:44:47.501: INFO: Got endpoints: latency-svc-ljh69 [758.936129ms]
    Aug 26 05:44:47.512: INFO: Created: latency-svc-kljcn
    Aug 26 05:44:47.522: INFO: Created: latency-svc-b6jtn
    Aug 26 05:44:47.541: INFO: Got endpoints: latency-svc-475bx [740.525539ms]
    Aug 26 05:44:47.572: INFO: Created: latency-svc-nwjhm
    Aug 26 05:44:47.596: INFO: Got endpoints: latency-svc-kcctk [753.931254ms]
    Aug 26 05:44:47.611: INFO: Created: latency-svc-vsm42
    Aug 26 05:44:47.641: INFO: Got endpoints: latency-svc-rtjxq [749.074765ms]
    Aug 26 05:44:47.663: INFO: Created: latency-svc-tlnwf
    Aug 26 05:44:47.694: INFO: Got endpoints: latency-svc-d76gj [746.106514ms]
    Aug 26 05:44:47.717: INFO: Created: latency-svc-frjfc
    Aug 26 05:44:47.744: INFO: Got endpoints: latency-svc-9pwck [750.914661ms]
    Aug 26 05:44:47.759: INFO: Created: latency-svc-trqsq
    Aug 26 05:44:47.792: INFO: Got endpoints: latency-svc-4qg7g [745.846598ms]
    Aug 26 05:44:47.810: INFO: Created: latency-svc-nqvh8
    Aug 26 05:44:47.842: INFO: Got endpoints: latency-svc-s9zqf [739.746081ms]
    Aug 26 05:44:47.855: INFO: Created: latency-svc-hmp7n
    Aug 26 05:44:47.891: INFO: Got endpoints: latency-svc-blmt8 [746.891178ms]
    Aug 26 05:44:47.907: INFO: Created: latency-svc-7cs46
    Aug 26 05:44:48.031: INFO: Got endpoints: latency-svc-mw8f9 [837.670403ms]
    Aug 26 05:44:48.036: INFO: Got endpoints: latency-svc-6khhn [793.632414ms]
    Aug 26 05:44:48.118: INFO: Got endpoints: latency-svc-lhhhs [825.960981ms]
    Aug 26 05:44:48.120: INFO: Got endpoints: latency-svc-xdccj [775.105859ms]
    Aug 26 05:44:48.129: INFO: Created: latency-svc-96gr2
    Aug 26 05:44:48.140: INFO: Created: latency-svc-h482j
    Aug 26 05:44:48.143: INFO: Got endpoints: latency-svc-pw4kr [751.207268ms]
    Aug 26 05:44:48.153: INFO: Created: latency-svc-wst48
    Aug 26 05:44:48.162: INFO: Created: latency-svc-qrl7v
    Aug 26 05:44:48.186: INFO: Created: latency-svc-zsqrr
    Aug 26 05:44:48.197: INFO: Got endpoints: latency-svc-kljcn [703.680794ms]
    Aug 26 05:44:48.222: INFO: Created: latency-svc-2zngl
    Aug 26 05:44:48.246: INFO: Got endpoints: latency-svc-b6jtn [745.264229ms]
    Aug 26 05:44:48.270: INFO: Created: latency-svc-trn6j
    Aug 26 05:44:48.293: INFO: Got endpoints: latency-svc-nwjhm [751.859948ms]
    Aug 26 05:44:48.328: INFO: Created: latency-svc-8s4mq
    Aug 26 05:44:48.345: INFO: Got endpoints: latency-svc-vsm42 [749.291899ms]
    Aug 26 05:44:48.370: INFO: Created: latency-svc-hzjrp
    Aug 26 05:44:48.392: INFO: Got endpoints: latency-svc-tlnwf [750.889639ms]
    Aug 26 05:44:48.408: INFO: Created: latency-svc-kwbvm
    Aug 26 05:44:48.443: INFO: Got endpoints: latency-svc-frjfc [748.083402ms]
    Aug 26 05:44:48.458: INFO: Created: latency-svc-bmw2n
    Aug 26 05:44:48.491: INFO: Got endpoints: latency-svc-trqsq [746.865765ms]
    Aug 26 05:44:48.506: INFO: Created: latency-svc-pvhbj
    Aug 26 05:44:48.541: INFO: Got endpoints: latency-svc-nqvh8 [749.034084ms]
    Aug 26 05:44:48.554: INFO: Created: latency-svc-x84ss
    Aug 26 05:44:48.591: INFO: Got endpoints: latency-svc-hmp7n [749.016307ms]
    Aug 26 05:44:48.606: INFO: Created: latency-svc-lbdn2
    Aug 26 05:44:48.769: INFO: Got endpoints: latency-svc-7cs46 [877.873137ms]
    Aug 26 05:44:48.769: INFO: Got endpoints: latency-svc-h482j [733.90163ms]
    Aug 26 05:44:48.770: INFO: Got endpoints: latency-svc-96gr2 [737.919802ms]
    Aug 26 05:44:48.785: INFO: Created: latency-svc-6hgjk
    Aug 26 05:44:48.799: INFO: Got endpoints: latency-svc-wst48 [680.851807ms]
    Aug 26 05:44:48.811: INFO: Created: latency-svc-bsg77
    Aug 26 05:44:48.827: INFO: Created: latency-svc-866nb
    Aug 26 05:44:48.839: INFO: Created: latency-svc-ldvh6
    Aug 26 05:44:48.841: INFO: Got endpoints: latency-svc-qrl7v [721.057175ms]
    Aug 26 05:44:48.855: INFO: Created: latency-svc-qwn92
    Aug 26 05:44:48.891: INFO: Got endpoints: latency-svc-zsqrr [748.029457ms]
    Aug 26 05:44:48.910: INFO: Created: latency-svc-pgglf
    Aug 26 05:44:48.943: INFO: Got endpoints: latency-svc-2zngl [745.910202ms]
    Aug 26 05:44:48.959: INFO: Created: latency-svc-g2hzh
    Aug 26 05:44:48.992: INFO: Got endpoints: latency-svc-trn6j [745.167065ms]
    Aug 26 05:44:49.011: INFO: Created: latency-svc-fzpg8
    Aug 26 05:44:49.092: INFO: Got endpoints: latency-svc-8s4mq [798.673179ms]
    Aug 26 05:44:49.111: INFO: Created: latency-svc-qtlgk
    Aug 26 05:44:49.141: INFO: Got endpoints: latency-svc-hzjrp [795.283022ms]
    Aug 26 05:44:49.157: INFO: Created: latency-svc-fqwx6
    Aug 26 05:44:49.190: INFO: Got endpoints: latency-svc-kwbvm [798.059147ms]
    Aug 26 05:44:49.208: INFO: Created: latency-svc-4rnb6
    Aug 26 05:44:49.244: INFO: Got endpoints: latency-svc-bmw2n [801.162045ms]
    Aug 26 05:44:49.268: INFO: Created: latency-svc-mvzw8
    Aug 26 05:44:49.291: INFO: Got endpoints: latency-svc-pvhbj [800.219648ms]
    Aug 26 05:44:49.308: INFO: Created: latency-svc-mvp9v
    Aug 26 05:44:49.348: INFO: Got endpoints: latency-svc-x84ss [806.700423ms]
    Aug 26 05:44:49.365: INFO: Created: latency-svc-q7f6m
    Aug 26 05:44:49.395: INFO: Got endpoints: latency-svc-lbdn2 [803.700926ms]
    Aug 26 05:44:49.415: INFO: Created: latency-svc-hfwb2
    Aug 26 05:44:49.446: INFO: Got endpoints: latency-svc-6hgjk [676.379835ms]
    Aug 26 05:44:49.467: INFO: Created: latency-svc-vc99r
    Aug 26 05:44:49.495: INFO: Got endpoints: latency-svc-bsg77 [725.836071ms]
    Aug 26 05:44:49.515: INFO: Created: latency-svc-sx78s
    Aug 26 05:44:49.545: INFO: Got endpoints: latency-svc-866nb [774.968416ms]
    Aug 26 05:44:49.627: INFO: Got endpoints: latency-svc-ldvh6 [828.37006ms]
    Aug 26 05:44:49.630: INFO: Created: latency-svc-b8p7s
    Aug 26 05:44:49.735: INFO: Got endpoints: latency-svc-qwn92 [893.705671ms]
    Aug 26 05:44:49.735: INFO: Got endpoints: latency-svc-pgglf [844.028142ms]
    Aug 26 05:44:49.745: INFO: Created: latency-svc-8wt48
    Aug 26 05:44:49.746: INFO: Got endpoints: latency-svc-g2hzh [803.127128ms]
    Aug 26 05:44:49.757: INFO: Created: latency-svc-xr528
    Aug 26 05:44:49.765: INFO: Created: latency-svc-zdd7z
    Aug 26 05:44:49.776: INFO: Created: latency-svc-tbp56
    Aug 26 05:44:49.795: INFO: Got endpoints: latency-svc-fzpg8 [803.650334ms]
    Aug 26 05:44:49.814: INFO: Created: latency-svc-549b6
    Aug 26 05:44:49.841: INFO: Got endpoints: latency-svc-qtlgk [748.625868ms]
    Aug 26 05:44:49.856: INFO: Created: latency-svc-tllnw
    Aug 26 05:44:49.893: INFO: Got endpoints: latency-svc-fqwx6 [752.229528ms]
    Aug 26 05:44:49.908: INFO: Created: latency-svc-hzp9j
    Aug 26 05:44:49.944: INFO: Got endpoints: latency-svc-4rnb6 [753.150801ms]
    Aug 26 05:44:49.962: INFO: Created: latency-svc-7lnhl
    Aug 26 05:44:49.998: INFO: Got endpoints: latency-svc-mvzw8 [754.623825ms]
    Aug 26 05:44:50.043: INFO: Got endpoints: latency-svc-mvp9v [751.754074ms]
    Aug 26 05:44:50.044: INFO: Created: latency-svc-tbj2g
    Aug 26 05:44:50.061: INFO: Created: latency-svc-zdn9n
    Aug 26 05:44:50.091: INFO: Got endpoints: latency-svc-q7f6m [743.191972ms]
    Aug 26 05:44:50.110: INFO: Created: latency-svc-9fgjl
    Aug 26 05:44:50.197: INFO: Got endpoints: latency-svc-hfwb2 [801.778511ms]
    Aug 26 05:44:50.198: INFO: Got endpoints: latency-svc-vc99r [751.940074ms]
    Aug 26 05:44:50.216: INFO: Created: latency-svc-m8d7x
    Aug 26 05:44:50.229: INFO: Created: latency-svc-nd2xb
    Aug 26 05:44:50.242: INFO: Got endpoints: latency-svc-sx78s [746.836844ms]
    Aug 26 05:44:50.260: INFO: Created: latency-svc-j2m82
    Aug 26 05:44:50.300: INFO: Got endpoints: latency-svc-b8p7s [755.666731ms]
    Aug 26 05:44:50.317: INFO: Created: latency-svc-jkwqd
    Aug 26 05:44:50.343: INFO: Got endpoints: latency-svc-8wt48 [715.458861ms]
    Aug 26 05:44:50.360: INFO: Created: latency-svc-qshmf
    Aug 26 05:44:50.392: INFO: Got endpoints: latency-svc-xr528 [656.56839ms]
    Aug 26 05:44:50.409: INFO: Created: latency-svc-nqpwj
    Aug 26 05:44:50.443: INFO: Got endpoints: latency-svc-zdd7z [707.825571ms]
    Aug 26 05:44:50.467: INFO: Created: latency-svc-rxpjs
    Aug 26 05:44:50.493: INFO: Got endpoints: latency-svc-tbp56 [747.072767ms]
    Aug 26 05:44:50.510: INFO: Created: latency-svc-vtjg5
    Aug 26 05:44:50.541: INFO: Got endpoints: latency-svc-549b6 [745.656002ms]
    Aug 26 05:44:50.558: INFO: Created: latency-svc-c7d7z
    Aug 26 05:44:50.595: INFO: Got endpoints: latency-svc-tllnw [753.993382ms]
    Aug 26 05:44:50.613: INFO: Created: latency-svc-z2d5c
    Aug 26 05:44:50.642: INFO: Got endpoints: latency-svc-hzp9j [748.592303ms]
    Aug 26 05:44:50.657: INFO: Created: latency-svc-67qgh
    Aug 26 05:44:50.700: INFO: Got endpoints: latency-svc-7lnhl [756.558001ms]
    Aug 26 05:44:50.722: INFO: Created: latency-svc-xkqhn
    Aug 26 05:44:50.743: INFO: Got endpoints: latency-svc-tbj2g [744.222249ms]
    Aug 26 05:44:50.758: INFO: Created: latency-svc-4qq8h
    Aug 26 05:44:50.791: INFO: Got endpoints: latency-svc-zdn9n [747.983173ms]
    Aug 26 05:44:50.813: INFO: Created: latency-svc-jlgst
    Aug 26 05:44:50.843: INFO: Got endpoints: latency-svc-9fgjl [751.606327ms]
    Aug 26 05:44:50.859: INFO: Created: latency-svc-495q2
    Aug 26 05:44:50.893: INFO: Got endpoints: latency-svc-m8d7x [696.196815ms]
    Aug 26 05:44:50.912: INFO: Created: latency-svc-x7hkd
    Aug 26 05:44:50.941: INFO: Got endpoints: latency-svc-nd2xb [743.580753ms]
    Aug 26 05:44:50.956: INFO: Created: latency-svc-qlh5k
    Aug 26 05:44:50.991: INFO: Got endpoints: latency-svc-j2m82 [749.114446ms]
    Aug 26 05:44:51.006: INFO: Created: latency-svc-z6mcs
    Aug 26 05:44:51.057: INFO: Got endpoints: latency-svc-jkwqd [756.308376ms]
    Aug 26 05:44:51.073: INFO: Created: latency-svc-crr65
    Aug 26 05:44:51.093: INFO: Got endpoints: latency-svc-qshmf [749.475139ms]
    Aug 26 05:44:51.111: INFO: Created: latency-svc-hb65t
    Aug 26 05:44:51.144: INFO: Got endpoints: latency-svc-nqpwj [752.018163ms]
    Aug 26 05:44:51.163: INFO: Created: latency-svc-9wxzc
    Aug 26 05:44:51.192: INFO: Got endpoints: latency-svc-rxpjs [748.596446ms]
    Aug 26 05:44:51.215: INFO: Created: latency-svc-6tq6x
    Aug 26 05:44:51.242: INFO: Got endpoints: latency-svc-vtjg5 [748.46588ms]
    Aug 26 05:44:51.359: INFO: Got endpoints: latency-svc-c7d7z [818.055755ms]
    Aug 26 05:44:51.360: INFO: Got endpoints: latency-svc-z2d5c [765.295817ms]
    Aug 26 05:44:51.363: INFO: Created: latency-svc-hpbls
    Aug 26 05:44:51.376: INFO: Created: latency-svc-pbr7p
    Aug 26 05:44:51.383: INFO: Created: latency-svc-t8fbx
    Aug 26 05:44:51.396: INFO: Got endpoints: latency-svc-67qgh [754.493894ms]
    Aug 26 05:44:51.412: INFO: Created: latency-svc-q2mrh
    Aug 26 05:44:51.443: INFO: Got endpoints: latency-svc-xkqhn [742.919619ms]
    Aug 26 05:44:51.491: INFO: Got endpoints: latency-svc-4qq8h [748.437069ms]
    Aug 26 05:44:51.545: INFO: Got endpoints: latency-svc-jlgst [753.976035ms]
    Aug 26 05:44:51.594: INFO: Got endpoints: latency-svc-495q2 [751.714469ms]
    Aug 26 05:44:51.641: INFO: Got endpoints: latency-svc-x7hkd [747.947616ms]
    Aug 26 05:44:51.691: INFO: Got endpoints: latency-svc-qlh5k [750.108723ms]
    Aug 26 05:44:51.743: INFO: Got endpoints: latency-svc-z6mcs [751.450368ms]
    Aug 26 05:44:51.791: INFO: Got endpoints: latency-svc-crr65 [733.75902ms]
    Aug 26 05:44:51.845: INFO: Got endpoints: latency-svc-hb65t [752.097938ms]
    Aug 26 05:44:51.893: INFO: Got endpoints: latency-svc-9wxzc [748.858838ms]
    Aug 26 05:44:51.945: INFO: Got endpoints: latency-svc-6tq6x [753.228722ms]
    Aug 26 05:44:52.005: INFO: Got endpoints: latency-svc-hpbls [763.524116ms]
    Aug 26 05:44:52.047: INFO: Got endpoints: latency-svc-pbr7p [687.835826ms]
    Aug 26 05:44:52.092: INFO: Got endpoints: latency-svc-t8fbx [731.490286ms]
    Aug 26 05:44:52.142: INFO: Got endpoints: latency-svc-q2mrh [746.075273ms]
    Aug 26 05:44:52.142: INFO: Latencies: [32.073242ms 57.653771ms 84.167956ms 101.301117ms 108.439667ms 112.107734ms 129.702676ms 140.845161ms 156.191848ms 169.870753ms 183.677122ms 186.458112ms 196.016376ms 196.495791ms 202.352728ms 208.833062ms 209.770192ms 210.445476ms 212.589568ms 213.041005ms 213.551205ms 215.383427ms 217.694062ms 219.054444ms 219.48275ms 221.454072ms 228.865672ms 230.979496ms 232.725866ms 232.824857ms 234.237971ms 238.689594ms 240.020462ms 241.578668ms 245.753772ms 277.931206ms 302.176528ms 308.727118ms 319.202717ms 354.08397ms 356.09213ms 363.77832ms 366.672955ms 367.367404ms 375.612775ms 375.868482ms 378.813845ms 380.051785ms 381.371157ms 382.072063ms 393.788699ms 444.816835ms 483.649125ms 506.190035ms 519.714107ms 569.392966ms 631.371286ms 656.56839ms 659.385579ms 676.379835ms 676.508464ms 678.354529ms 680.851807ms 682.469263ms 683.447499ms 687.835826ms 696.196815ms 703.680794ms 705.730574ms 706.925405ms 707.825571ms 711.48988ms 715.458861ms 721.057175ms 724.848367ms 725.836071ms 729.561516ms 730.601506ms 731.490286ms 731.711833ms 733.75902ms 733.90163ms 737.919802ms 739.248153ms 739.746081ms 740.525539ms 742.919619ms 743.191972ms 743.369326ms 743.580753ms 743.809578ms 744.222249ms 745.167065ms 745.264229ms 745.656002ms 745.846598ms 745.910202ms 746.075273ms 746.106514ms 746.836844ms 746.865765ms 746.891178ms 747.044063ms 747.072767ms 747.947616ms 747.972591ms 747.983173ms 748.029457ms 748.083402ms 748.09854ms 748.199437ms 748.282531ms 748.437069ms 748.46588ms 748.477887ms 748.592303ms 748.596446ms 748.625868ms 748.858838ms 749.016307ms 749.034084ms 749.074765ms 749.114446ms 749.250715ms 749.291899ms 749.408219ms 749.475139ms 749.802381ms 749.845183ms 750.108723ms 750.130342ms 750.532425ms 750.682854ms 750.812618ms 750.889639ms 750.914661ms 751.207268ms 751.450368ms 751.606327ms 751.714469ms 751.754074ms 751.859948ms 751.937827ms 751.940074ms 751.98223ms 752.018163ms 752.097938ms 752.229528ms 752.778564ms 753.150801ms 753.228722ms 753.23132ms 753.35886ms 753.931254ms 753.976035ms 753.993382ms 754.03993ms 754.306555ms 754.493894ms 754.623825ms 755.666731ms 756.24189ms 756.308376ms 756.558001ms 756.849307ms 758.936129ms 759.797435ms 760.740197ms 763.524116ms 765.295817ms 768.045873ms 774.257362ms 774.968416ms 775.105859ms 776.783302ms 793.632414ms 795.283022ms 796.676791ms 798.059147ms 798.673179ms 800.219648ms 801.162045ms 801.778511ms 803.127128ms 803.650334ms 803.700926ms 806.700423ms 818.055755ms 822.389355ms 822.395788ms 825.960981ms 828.37006ms 837.670403ms 844.028142ms 867.571778ms 869.007129ms 877.873137ms 893.705671ms 928.103148ms 1.034295501s]
    Aug 26 05:44:52.143: INFO: 50 %ile: 746.865765ms
    Aug 26 05:44:52.143: INFO: 90 %ile: 800.219648ms
    Aug 26 05:44:52.143: INFO: 99 %ile: 928.103148ms
    Aug 26 05:44:52.143: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:44:52.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-6021" for this suite. 08/26/23 05:44:52.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:44:52.264
Aug 26 05:44:52.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename tables 08/26/23 05:44:52.265
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:52.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:52.317
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Aug 26 05:44:52.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-238" for this suite. 08/26/23 05:44:52.332
------------------------------
• [0.076 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:44:52.264
    Aug 26 05:44:52.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename tables 08/26/23 05:44:52.265
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:52.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:52.317
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:44:52.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-238" for this suite. 08/26/23 05:44:52.332
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:44:52.341
Aug 26 05:44:52.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename deployment 08/26/23 05:44:52.342
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:52.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:52.373
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Aug 26 05:44:52.376: INFO: Creating deployment "test-recreate-deployment"
Aug 26 05:44:52.383: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 26 05:44:52.390: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 26 05:44:54.399: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 26 05:44:54.403: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 26 05:44:54.418: INFO: Updating deployment test-recreate-deployment
Aug 26 05:44:54.418: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 26 05:44:54.539: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-416  74d0b8ae-2409-4adb-8f99-f97380df98e3 18370 2 2023-08-26 05:44:52 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00455a998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-26 05:44:54 +0000 UTC,LastTransitionTime:2023-08-26 05:44:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-26 05:44:54 +0000 UTC,LastTransitionTime:2023-08-26 05:44:52 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 26 05:44:54.543: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-416  613e7cee-e853-42ce-b651-5155cf81cc85 18366 1 2023-08-26 05:44:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 74d0b8ae-2409-4adb-8f99-f97380df98e3 0xc0051a7430 0xc0051a7431}] [] [{kube-controller-manager Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74d0b8ae-2409-4adb-8f99-f97380df98e3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a74d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 26 05:44:54.543: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 26 05:44:54.543: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-416  ff43be5c-2ae3-426a-bb77-62ec44860f80 18358 2 2023-08-26 05:44:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 74d0b8ae-2409-4adb-8f99-f97380df98e3 0xc0051a7317 0xc0051a7318}] [] [{kube-controller-manager Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74d0b8ae-2409-4adb-8f99-f97380df98e3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a73c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 26 05:44:54.547: INFO: Pod "test-recreate-deployment-cff6dc657-4drn6" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-4drn6 test-recreate-deployment-cff6dc657- deployment-416  1347317d-b51a-4c39-ba07-3144b1850c6a 18369 0 2023-08-26 05:44:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 613e7cee-e853-42ce-b651-5155cf81cc85 0xc0051a7a30 0xc0051a7a31}] [] [{kube-controller-manager Update v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"613e7cee-e853-42ce-b651-5155cf81cc85\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g5pzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g5pzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:44:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:44:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:44:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:44:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:,StartTime:2023-08-26 05:44:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 26 05:44:54.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-416" for this suite. 08/26/23 05:44:54.553
------------------------------
• [2.221 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:44:52.341
    Aug 26 05:44:52.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename deployment 08/26/23 05:44:52.342
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:52.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:52.373
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Aug 26 05:44:52.376: INFO: Creating deployment "test-recreate-deployment"
    Aug 26 05:44:52.383: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Aug 26 05:44:52.390: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Aug 26 05:44:54.399: INFO: Waiting deployment "test-recreate-deployment" to complete
    Aug 26 05:44:54.403: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Aug 26 05:44:54.418: INFO: Updating deployment test-recreate-deployment
    Aug 26 05:44:54.418: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 26 05:44:54.539: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-416  74d0b8ae-2409-4adb-8f99-f97380df98e3 18370 2 2023-08-26 05:44:52 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00455a998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-26 05:44:54 +0000 UTC,LastTransitionTime:2023-08-26 05:44:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-26 05:44:54 +0000 UTC,LastTransitionTime:2023-08-26 05:44:52 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 26 05:44:54.543: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-416  613e7cee-e853-42ce-b651-5155cf81cc85 18366 1 2023-08-26 05:44:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 74d0b8ae-2409-4adb-8f99-f97380df98e3 0xc0051a7430 0xc0051a7431}] [] [{kube-controller-manager Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74d0b8ae-2409-4adb-8f99-f97380df98e3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a74d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 05:44:54.543: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Aug 26 05:44:54.543: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-416  ff43be5c-2ae3-426a-bb77-62ec44860f80 18358 2 2023-08-26 05:44:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 74d0b8ae-2409-4adb-8f99-f97380df98e3 0xc0051a7317 0xc0051a7318}] [] [{kube-controller-manager Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74d0b8ae-2409-4adb-8f99-f97380df98e3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a73c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 05:44:54.547: INFO: Pod "test-recreate-deployment-cff6dc657-4drn6" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-4drn6 test-recreate-deployment-cff6dc657- deployment-416  1347317d-b51a-4c39-ba07-3144b1850c6a 18369 0 2023-08-26 05:44:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 613e7cee-e853-42ce-b651-5155cf81cc85 0xc0051a7a30 0xc0051a7a31}] [] [{kube-controller-manager Update v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"613e7cee-e853-42ce-b651-5155cf81cc85\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 05:44:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g5pzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g5pzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:44:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:44:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:44:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:44:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:,StartTime:2023-08-26 05:44:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:44:54.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-416" for this suite. 08/26/23 05:44:54.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:44:54.563
Aug 26 05:44:54.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:44:54.567
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:54.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:54.587
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-c6cde5df-e8cd-4f57-b2d5-220eb68c8b7f 08/26/23 05:44:54.591
STEP: Creating a pod to test consume secrets 08/26/23 05:44:54.596
Aug 26 05:44:54.605: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e" in namespace "projected-6091" to be "Succeeded or Failed"
Aug 26 05:44:54.609: INFO: Pod "pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815662ms
Aug 26 05:44:56.614: INFO: Pod "pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008975026s
Aug 26 05:44:58.616: INFO: Pod "pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011147427s
STEP: Saw pod success 08/26/23 05:44:58.616
Aug 26 05:44:58.616: INFO: Pod "pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e" satisfied condition "Succeeded or Failed"
Aug 26 05:44:58.622: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e container projected-secret-volume-test: <nil>
STEP: delete the pod 08/26/23 05:44:58.647
Aug 26 05:44:58.688: INFO: Waiting for pod pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e to disappear
Aug 26 05:44:58.695: INFO: Pod pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 26 05:44:58.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6091" for this suite. 08/26/23 05:44:58.712
------------------------------
• [4.158 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:44:54.563
    Aug 26 05:44:54.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:44:54.567
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:54.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:54.587
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-c6cde5df-e8cd-4f57-b2d5-220eb68c8b7f 08/26/23 05:44:54.591
    STEP: Creating a pod to test consume secrets 08/26/23 05:44:54.596
    Aug 26 05:44:54.605: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e" in namespace "projected-6091" to be "Succeeded or Failed"
    Aug 26 05:44:54.609: INFO: Pod "pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815662ms
    Aug 26 05:44:56.614: INFO: Pod "pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008975026s
    Aug 26 05:44:58.616: INFO: Pod "pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011147427s
    STEP: Saw pod success 08/26/23 05:44:58.616
    Aug 26 05:44:58.616: INFO: Pod "pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e" satisfied condition "Succeeded or Failed"
    Aug 26 05:44:58.622: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 05:44:58.647
    Aug 26 05:44:58.688: INFO: Waiting for pod pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e to disappear
    Aug 26 05:44:58.695: INFO: Pod pod-projected-secrets-3ff00cd8-3040-48b6-88db-d32864324a9e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:44:58.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6091" for this suite. 08/26/23 05:44:58.712
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:44:58.722
Aug 26 05:44:58.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename custom-resource-definition 08/26/23 05:44:58.723
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:58.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:58.75
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Aug 26 05:44:58.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:45:02.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5689" for this suite. 08/26/23 05:45:02.186
------------------------------
• [3.502 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:44:58.722
    Aug 26 05:44:58.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename custom-resource-definition 08/26/23 05:44:58.723
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:44:58.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:44:58.75
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Aug 26 05:44:58.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:45:02.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5689" for this suite. 08/26/23 05:45:02.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:45:02.224
Aug 26 05:45:02.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename statefulset 08/26/23 05:45:02.232
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:02.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:02.284
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2836 08/26/23 05:45:02.287
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Aug 26 05:45:02.330: INFO: Found 0 stateful pods, waiting for 1
Aug 26 05:45:12.335: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 08/26/23 05:45:12.343
W0826 05:45:12.355369      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 26 05:45:12.368: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 05:45:12.368: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Aug 26 05:45:22.375: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 05:45:22.375: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 08/26/23 05:45:22.385
STEP: Delete all of the StatefulSets 08/26/23 05:45:22.389
STEP: Verify that StatefulSets have been deleted 08/26/23 05:45:22.401
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 26 05:45:22.405: INFO: Deleting all statefulset in ns statefulset-2836
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:45:22.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2836" for this suite. 08/26/23 05:45:22.427
------------------------------
• [SLOW TEST] [20.211 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:45:02.224
    Aug 26 05:45:02.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename statefulset 08/26/23 05:45:02.232
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:02.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:02.284
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2836 08/26/23 05:45:02.287
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Aug 26 05:45:02.330: INFO: Found 0 stateful pods, waiting for 1
    Aug 26 05:45:12.335: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 08/26/23 05:45:12.343
    W0826 05:45:12.355369      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 26 05:45:12.368: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 05:45:12.368: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Aug 26 05:45:22.375: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 05:45:22.375: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 08/26/23 05:45:22.385
    STEP: Delete all of the StatefulSets 08/26/23 05:45:22.389
    STEP: Verify that StatefulSets have been deleted 08/26/23 05:45:22.401
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 26 05:45:22.405: INFO: Deleting all statefulset in ns statefulset-2836
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:45:22.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2836" for this suite. 08/26/23 05:45:22.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:45:22.439
Aug 26 05:45:22.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename disruption 08/26/23 05:45:22.44
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:22.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:22.462
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 08/26/23 05:45:22.471
STEP: Updating PodDisruptionBudget status 08/26/23 05:45:24.48
STEP: Waiting for all pods to be running 08/26/23 05:45:24.49
Aug 26 05:45:24.498: INFO: running pods: 0 < 1
STEP: locating a running pod 08/26/23 05:45:26.504
STEP: Waiting for the pdb to be processed 08/26/23 05:45:26.516
STEP: Patching PodDisruptionBudget status 08/26/23 05:45:26.524
STEP: Waiting for the pdb to be processed 08/26/23 05:45:26.54
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 26 05:45:26.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9105" for this suite. 08/26/23 05:45:26.56
------------------------------
• [4.131 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:45:22.439
    Aug 26 05:45:22.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename disruption 08/26/23 05:45:22.44
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:22.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:22.462
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 08/26/23 05:45:22.471
    STEP: Updating PodDisruptionBudget status 08/26/23 05:45:24.48
    STEP: Waiting for all pods to be running 08/26/23 05:45:24.49
    Aug 26 05:45:24.498: INFO: running pods: 0 < 1
    STEP: locating a running pod 08/26/23 05:45:26.504
    STEP: Waiting for the pdb to be processed 08/26/23 05:45:26.516
    STEP: Patching PodDisruptionBudget status 08/26/23 05:45:26.524
    STEP: Waiting for the pdb to be processed 08/26/23 05:45:26.54
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:45:26.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9105" for this suite. 08/26/23 05:45:26.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:45:26.571
Aug 26 05:45:26.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:45:26.572
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:26.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:26.6
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:45:26.602
Aug 26 05:45:26.611: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6" in namespace "downward-api-4193" to be "Succeeded or Failed"
Aug 26 05:45:26.619: INFO: Pod "downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.425156ms
Aug 26 05:45:28.627: INFO: Pod "downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6": Phase="Running", Reason="", readiness=false. Elapsed: 2.016188697s
Aug 26 05:45:30.629: INFO: Pod "downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017895063s
STEP: Saw pod success 08/26/23 05:45:30.629
Aug 26 05:45:30.629: INFO: Pod "downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6" satisfied condition "Succeeded or Failed"
Aug 26 05:45:30.634: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6 container client-container: <nil>
STEP: delete the pod 08/26/23 05:45:30.646
Aug 26 05:45:30.806: INFO: Waiting for pod downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6 to disappear
Aug 26 05:45:30.810: INFO: Pod downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 05:45:30.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4193" for this suite. 08/26/23 05:45:30.82
------------------------------
• [4.265 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:45:26.571
    Aug 26 05:45:26.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:45:26.572
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:26.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:26.6
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:45:26.602
    Aug 26 05:45:26.611: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6" in namespace "downward-api-4193" to be "Succeeded or Failed"
    Aug 26 05:45:26.619: INFO: Pod "downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.425156ms
    Aug 26 05:45:28.627: INFO: Pod "downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6": Phase="Running", Reason="", readiness=false. Elapsed: 2.016188697s
    Aug 26 05:45:30.629: INFO: Pod "downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017895063s
    STEP: Saw pod success 08/26/23 05:45:30.629
    Aug 26 05:45:30.629: INFO: Pod "downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6" satisfied condition "Succeeded or Failed"
    Aug 26 05:45:30.634: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6 container client-container: <nil>
    STEP: delete the pod 08/26/23 05:45:30.646
    Aug 26 05:45:30.806: INFO: Waiting for pod downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6 to disappear
    Aug 26 05:45:30.810: INFO: Pod downwardapi-volume-b19d73e0-cb3f-4fc1-adfc-753acc9495c6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:45:30.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4193" for this suite. 08/26/23 05:45:30.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:45:30.837
Aug 26 05:45:30.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename proxy 08/26/23 05:45:30.839
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:30.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:30.864
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 08/26/23 05:45:30.894
STEP: creating replication controller proxy-service-5vjxk in namespace proxy-7655 08/26/23 05:45:30.895
I0826 05:45:30.908706      20 runners.go:193] Created replication controller with name: proxy-service-5vjxk, namespace: proxy-7655, replica count: 1
I0826 05:45:31.959945      20 runners.go:193] proxy-service-5vjxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0826 05:45:32.960101      20 runners.go:193] proxy-service-5vjxk Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 26 05:45:32.965: INFO: setup took 2.088960531s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/26/23 05:45:32.965
Aug 26 05:45:32.978: INFO: (0) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 12.585802ms)
Aug 26 05:45:32.978: INFO: (0) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 12.548734ms)
Aug 26 05:45:32.978: INFO: (0) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 12.777448ms)
Aug 26 05:45:32.979: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 13.847735ms)
Aug 26 05:45:32.980: INFO: (0) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 14.584011ms)
Aug 26 05:45:32.980: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 14.429593ms)
Aug 26 05:45:32.980: INFO: (0) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.424324ms)
Aug 26 05:45:32.980: INFO: (0) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 15.308707ms)
Aug 26 05:45:32.980: INFO: (0) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 15.221992ms)
Aug 26 05:45:32.981: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 15.622021ms)
Aug 26 05:45:32.981: INFO: (0) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 15.860112ms)
Aug 26 05:45:32.981: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 15.948102ms)
Aug 26 05:45:32.981: INFO: (0) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 15.894771ms)
Aug 26 05:45:32.992: INFO: (0) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 27.022394ms)
Aug 26 05:45:32.995: INFO: (0) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 30.071528ms)
Aug 26 05:45:32.999: INFO: (0) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 33.616187ms)
Aug 26 05:45:33.010: INFO: (1) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 10.870953ms)
Aug 26 05:45:33.010: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.158598ms)
Aug 26 05:45:33.011: INFO: (1) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 11.926875ms)
Aug 26 05:45:33.011: INFO: (1) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 12.299087ms)
Aug 26 05:45:33.011: INFO: (1) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 12.445779ms)
Aug 26 05:45:33.011: INFO: (1) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.147511ms)
Aug 26 05:45:33.012: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.689035ms)
Aug 26 05:45:33.013: INFO: (1) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 13.937182ms)
Aug 26 05:45:33.013: INFO: (1) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 13.777739ms)
Aug 26 05:45:33.014: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 14.521516ms)
Aug 26 05:45:33.014: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 14.621283ms)
Aug 26 05:45:33.015: INFO: (1) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 15.705761ms)
Aug 26 05:45:33.017: INFO: (1) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 17.62122ms)
Aug 26 05:45:33.017: INFO: (1) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 17.686468ms)
Aug 26 05:45:33.017: INFO: (1) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 17.644425ms)
Aug 26 05:45:33.018: INFO: (1) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 19.035212ms)
Aug 26 05:45:33.023: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 5.257903ms)
Aug 26 05:45:33.029: INFO: (2) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 10.814821ms)
Aug 26 05:45:33.029: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.706443ms)
Aug 26 05:45:33.029: INFO: (2) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.93049ms)
Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.399317ms)
Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 11.468542ms)
Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.497208ms)
Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 11.757804ms)
Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 11.669138ms)
Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.60259ms)
Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 11.597044ms)
Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 11.676612ms)
Aug 26 05:45:33.031: INFO: (2) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 12.870946ms)
Aug 26 05:45:33.031: INFO: (2) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 13.056229ms)
Aug 26 05:45:33.031: INFO: (2) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 13.052089ms)
Aug 26 05:45:33.031: INFO: (2) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 12.938389ms)
Aug 26 05:45:33.041: INFO: (3) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 9.368782ms)
Aug 26 05:45:33.041: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.088586ms)
Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.928194ms)
Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 10.923258ms)
Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 11.195086ms)
Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 11.032478ms)
Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 11.159866ms)
Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 11.353828ms)
Aug 26 05:45:33.044: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 11.925061ms)
Aug 26 05:45:33.044: INFO: (3) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 12.04357ms)
Aug 26 05:45:33.044: INFO: (3) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 13.180462ms)
Aug 26 05:45:33.045: INFO: (3) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 13.148361ms)
Aug 26 05:45:33.046: INFO: (3) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 14.360223ms)
Aug 26 05:45:33.046: INFO: (3) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 14.27736ms)
Aug 26 05:45:33.046: INFO: (3) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 14.127858ms)
Aug 26 05:45:33.046: INFO: (3) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 14.072451ms)
Aug 26 05:45:33.054: INFO: (4) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 8.092691ms)
Aug 26 05:45:33.054: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 8.032185ms)
Aug 26 05:45:33.054: INFO: (4) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 8.16874ms)
Aug 26 05:45:33.054: INFO: (4) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 8.225492ms)
Aug 26 05:45:33.054: INFO: (4) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 8.175226ms)
Aug 26 05:45:33.055: INFO: (4) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 8.9731ms)
Aug 26 05:45:33.055: INFO: (4) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 9.24867ms)
Aug 26 05:45:33.055: INFO: (4) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 9.524158ms)
Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.006221ms)
Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.791314ms)
Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 9.973627ms)
Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.059143ms)
Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 10.205516ms)
Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 10.167825ms)
Aug 26 05:45:33.057: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.035393ms)
Aug 26 05:45:33.057: INFO: (4) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 11.441256ms)
Aug 26 05:45:33.067: INFO: (5) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 9.317164ms)
Aug 26 05:45:33.070: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 12.16259ms)
Aug 26 05:45:33.070: INFO: (5) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 12.195287ms)
Aug 26 05:45:33.070: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.177081ms)
Aug 26 05:45:33.071: INFO: (5) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 13.643674ms)
Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 18.225687ms)
Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 18.370531ms)
Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 18.437377ms)
Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 18.622695ms)
Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 18.620475ms)
Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 18.512214ms)
Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 18.922908ms)
Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 18.877498ms)
Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 18.726289ms)
Aug 26 05:45:33.077: INFO: (5) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 19.046549ms)
Aug 26 05:45:33.077: INFO: (5) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 19.063182ms)
Aug 26 05:45:33.092: INFO: (6) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.832294ms)
Aug 26 05:45:33.092: INFO: (6) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 15.081894ms)
Aug 26 05:45:33.092: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 15.210364ms)
Aug 26 05:45:33.092: INFO: (6) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 15.049634ms)
Aug 26 05:45:33.092: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 15.262413ms)
Aug 26 05:45:33.093: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 16.142732ms)
Aug 26 05:45:33.093: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 16.38741ms)
Aug 26 05:45:33.093: INFO: (6) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 16.416995ms)
Aug 26 05:45:33.093: INFO: (6) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 16.309169ms)
Aug 26 05:45:33.095: INFO: (6) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 17.43081ms)
Aug 26 05:45:33.100: INFO: (6) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 22.585396ms)
Aug 26 05:45:33.101: INFO: (6) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 23.561404ms)
Aug 26 05:45:33.101: INFO: (6) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 23.975781ms)
Aug 26 05:45:33.101: INFO: (6) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 23.82851ms)
Aug 26 05:45:33.101: INFO: (6) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 23.742429ms)
Aug 26 05:45:33.101: INFO: (6) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 24.244917ms)
Aug 26 05:45:33.115: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 13.087659ms)
Aug 26 05:45:33.116: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 14.233017ms)
Aug 26 05:45:33.116: INFO: (7) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 14.156051ms)
Aug 26 05:45:33.116: INFO: (7) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.291096ms)
Aug 26 05:45:33.116: INFO: (7) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 13.88917ms)
Aug 26 05:45:33.116: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 14.646659ms)
Aug 26 05:45:33.118: INFO: (7) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 16.073131ms)
Aug 26 05:45:33.119: INFO: (7) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 17.127759ms)
Aug 26 05:45:33.119: INFO: (7) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 17.906516ms)
Aug 26 05:45:33.119: INFO: (7) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 17.678656ms)
Aug 26 05:45:33.120: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 17.796188ms)
Aug 26 05:45:33.121: INFO: (7) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 19.051202ms)
Aug 26 05:45:33.121: INFO: (7) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 19.337704ms)
Aug 26 05:45:33.121: INFO: (7) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 19.262406ms)
Aug 26 05:45:33.122: INFO: (7) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 19.888502ms)
Aug 26 05:45:33.122: INFO: (7) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 19.933868ms)
Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 12.152549ms)
Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.059392ms)
Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.593113ms)
Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 11.804933ms)
Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 11.896416ms)
Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 12.085282ms)
Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 12.588224ms)
Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 12.082126ms)
Aug 26 05:45:33.135: INFO: (8) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 12.11258ms)
Aug 26 05:45:33.135: INFO: (8) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 11.795759ms)
Aug 26 05:45:33.135: INFO: (8) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 11.841173ms)
Aug 26 05:45:33.137: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.213518ms)
Aug 26 05:45:33.139: INFO: (8) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 16.335529ms)
Aug 26 05:45:33.139: INFO: (8) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 16.426138ms)
Aug 26 05:45:33.139: INFO: (8) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 16.588523ms)
Aug 26 05:45:33.140: INFO: (8) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 17.506794ms)
Aug 26 05:45:33.150: INFO: (9) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 9.547793ms)
Aug 26 05:45:33.153: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 13.082404ms)
Aug 26 05:45:33.153: INFO: (9) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 13.228895ms)
Aug 26 05:45:33.153: INFO: (9) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 13.051902ms)
Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 15.079202ms)
Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 14.957538ms)
Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 15.010465ms)
Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 14.96331ms)
Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 14.963616ms)
Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 15.234748ms)
Aug 26 05:45:33.156: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 15.157118ms)
Aug 26 05:45:33.156: INFO: (9) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 16.059525ms)
Aug 26 05:45:33.156: INFO: (9) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 16.161222ms)
Aug 26 05:45:33.160: INFO: (9) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 19.849901ms)
Aug 26 05:45:33.160: INFO: (9) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 19.761874ms)
Aug 26 05:45:33.160: INFO: (9) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 19.859031ms)
Aug 26 05:45:33.168: INFO: (10) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 7.720262ms)
Aug 26 05:45:33.170: INFO: (10) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.915111ms)
Aug 26 05:45:33.170: INFO: (10) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 9.983627ms)
Aug 26 05:45:33.170: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.939364ms)
Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 10.412168ms)
Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.134452ms)
Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 10.322797ms)
Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 10.350681ms)
Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 10.260553ms)
Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.51785ms)
Aug 26 05:45:33.172: INFO: (10) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 12.213715ms)
Aug 26 05:45:33.175: INFO: (10) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 14.53786ms)
Aug 26 05:45:33.175: INFO: (10) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 14.625502ms)
Aug 26 05:45:33.176: INFO: (10) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 15.018312ms)
Aug 26 05:45:33.176: INFO: (10) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 15.335708ms)
Aug 26 05:45:33.176: INFO: (10) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 15.197237ms)
Aug 26 05:45:33.189: INFO: (11) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 13.57053ms)
Aug 26 05:45:33.194: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 18.26934ms)
Aug 26 05:45:33.194: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 18.081194ms)
Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 18.958761ms)
Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 18.993419ms)
Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 18.990511ms)
Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 18.93394ms)
Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 19.012208ms)
Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 19.040562ms)
Aug 26 05:45:33.196: INFO: (11) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 20.425ms)
Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 21.01764ms)
Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 21.073956ms)
Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 20.995203ms)
Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 21.011707ms)
Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 21.349155ms)
Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 21.715314ms)
Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 11.199682ms)
Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.285665ms)
Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.729342ms)
Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 11.344691ms)
Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 11.464429ms)
Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 11.303336ms)
Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 11.517024ms)
Aug 26 05:45:33.210: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.663092ms)
Aug 26 05:45:33.211: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 12.660148ms)
Aug 26 05:45:33.211: INFO: (12) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 13.238169ms)
Aug 26 05:45:33.211: INFO: (12) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 13.081044ms)
Aug 26 05:45:33.211: INFO: (12) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 13.708267ms)
Aug 26 05:45:33.211: INFO: (12) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 13.137452ms)
Aug 26 05:45:33.212: INFO: (12) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 13.971411ms)
Aug 26 05:45:33.212: INFO: (12) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.071443ms)
Aug 26 05:45:33.212: INFO: (12) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 14.018271ms)
Aug 26 05:45:33.222: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 10.221195ms)
Aug 26 05:45:33.223: INFO: (13) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.70667ms)
Aug 26 05:45:33.223: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.016507ms)
Aug 26 05:45:33.223: INFO: (13) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 11.088968ms)
Aug 26 05:45:33.224: INFO: (13) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 11.514659ms)
Aug 26 05:45:33.224: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 11.674007ms)
Aug 26 05:45:33.224: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.899883ms)
Aug 26 05:45:33.228: INFO: (13) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 16.094598ms)
Aug 26 05:45:33.228: INFO: (13) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 16.267693ms)
Aug 26 05:45:33.228: INFO: (13) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 16.419583ms)
Aug 26 05:45:33.228: INFO: (13) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 16.52064ms)
Aug 26 05:45:33.234: INFO: (13) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 21.585668ms)
Aug 26 05:45:33.234: INFO: (13) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 22.212225ms)
Aug 26 05:45:33.235: INFO: (13) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 23.010843ms)
Aug 26 05:45:33.238: INFO: (13) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 26.140167ms)
Aug 26 05:45:33.238: INFO: (13) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 25.947602ms)
Aug 26 05:45:33.250: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 10.846862ms)
Aug 26 05:45:33.250: INFO: (14) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 11.370199ms)
Aug 26 05:45:33.250: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 10.990715ms)
Aug 26 05:45:33.251: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 12.185455ms)
Aug 26 05:45:33.251: INFO: (14) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 12.621601ms)
Aug 26 05:45:33.252: INFO: (14) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 12.651154ms)
Aug 26 05:45:33.253: INFO: (14) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 13.702901ms)
Aug 26 05:45:33.253: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 13.95561ms)
Aug 26 05:45:33.254: INFO: (14) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 15.254671ms)
Aug 26 05:45:33.254: INFO: (14) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.951151ms)
Aug 26 05:45:33.254: INFO: (14) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 15.163693ms)
Aug 26 05:45:33.255: INFO: (14) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 16.136412ms)
Aug 26 05:45:33.256: INFO: (14) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 17.723024ms)
Aug 26 05:45:33.256: INFO: (14) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 17.481125ms)
Aug 26 05:45:33.257: INFO: (14) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 17.643326ms)
Aug 26 05:45:33.257: INFO: (14) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 18.517411ms)
Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 9.571763ms)
Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 9.76674ms)
Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.845809ms)
Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.873883ms)
Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 9.967694ms)
Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 9.70962ms)
Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 10.221735ms)
Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 9.808552ms)
Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 9.66769ms)
Aug 26 05:45:33.275: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 17.213701ms)
Aug 26 05:45:33.276: INFO: (15) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 18.10189ms)
Aug 26 05:45:33.276: INFO: (15) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 18.511582ms)
Aug 26 05:45:33.276: INFO: (15) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 18.692093ms)
Aug 26 05:45:33.277: INFO: (15) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 18.756895ms)
Aug 26 05:45:33.277: INFO: (15) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 18.667092ms)
Aug 26 05:45:33.277: INFO: (15) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 19.1118ms)
Aug 26 05:45:33.286: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 8.483952ms)
Aug 26 05:45:33.289: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.673838ms)
Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 11.980264ms)
Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 12.058035ms)
Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 12.162577ms)
Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 12.01362ms)
Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 12.621225ms)
Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 13.00053ms)
Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.649999ms)
Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 12.734869ms)
Aug 26 05:45:33.291: INFO: (16) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 13.689094ms)
Aug 26 05:45:33.291: INFO: (16) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 13.503247ms)
Aug 26 05:45:33.292: INFO: (16) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 14.651628ms)
Aug 26 05:45:33.294: INFO: (16) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 16.21632ms)
Aug 26 05:45:33.294: INFO: (16) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 16.009716ms)
Aug 26 05:45:33.294: INFO: (16) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 16.443309ms)
Aug 26 05:45:33.301: INFO: (17) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 6.363564ms)
Aug 26 05:45:33.305: INFO: (17) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 10.582085ms)
Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 11.290755ms)
Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 11.250194ms)
Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.477746ms)
Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.423597ms)
Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.479042ms)
Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.150045ms)
Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.644722ms)
Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 11.990955ms)
Aug 26 05:45:33.307: INFO: (17) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 12.808758ms)
Aug 26 05:45:33.310: INFO: (17) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 15.418451ms)
Aug 26 05:45:33.310: INFO: (17) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 15.585347ms)
Aug 26 05:45:33.316: INFO: (17) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 21.147303ms)
Aug 26 05:45:33.317: INFO: (17) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 22.557828ms)
Aug 26 05:45:33.317: INFO: (17) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 22.694326ms)
Aug 26 05:45:33.330: INFO: (18) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 12.059419ms)
Aug 26 05:45:33.330: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 12.478233ms)
Aug 26 05:45:33.330: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 12.46002ms)
Aug 26 05:45:33.331: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 13.483135ms)
Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 14.206874ms)
Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 14.096283ms)
Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 14.095705ms)
Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.130754ms)
Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 14.369612ms)
Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 14.159596ms)
Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 14.277012ms)
Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 14.264323ms)
Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 14.927248ms)
Aug 26 05:45:33.337: INFO: (18) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 19.011034ms)
Aug 26 05:45:33.341: INFO: (18) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 23.179666ms)
Aug 26 05:45:33.341: INFO: (18) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 23.242334ms)
Aug 26 05:45:33.347: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 6.165076ms)
Aug 26 05:45:33.354: INFO: (19) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 13.126677ms)
Aug 26 05:45:33.354: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 13.15901ms)
Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 13.534665ms)
Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 13.565143ms)
Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 13.606499ms)
Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 13.853816ms)
Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 13.9563ms)
Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 14.031661ms)
Aug 26 05:45:33.356: INFO: (19) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 14.811425ms)
Aug 26 05:45:33.357: INFO: (19) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 15.622223ms)
Aug 26 05:45:33.359: INFO: (19) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 18.032089ms)
Aug 26 05:45:33.359: INFO: (19) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 18.213071ms)
Aug 26 05:45:33.360: INFO: (19) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 18.592409ms)
Aug 26 05:45:33.360: INFO: (19) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 19.10374ms)
Aug 26 05:45:33.361: INFO: (19) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 20.233613ms)
STEP: deleting ReplicationController proxy-service-5vjxk in namespace proxy-7655, will wait for the garbage collector to delete the pods 08/26/23 05:45:33.361
Aug 26 05:45:33.430: INFO: Deleting ReplicationController proxy-service-5vjxk took: 13.411126ms
Aug 26 05:45:33.531: INFO: Terminating ReplicationController proxy-service-5vjxk pods took: 101.103762ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 26 05:45:35.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7655" for this suite. 08/26/23 05:45:35.648
------------------------------
• [4.824 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:45:30.837
    Aug 26 05:45:30.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename proxy 08/26/23 05:45:30.839
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:30.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:30.864
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 08/26/23 05:45:30.894
    STEP: creating replication controller proxy-service-5vjxk in namespace proxy-7655 08/26/23 05:45:30.895
    I0826 05:45:30.908706      20 runners.go:193] Created replication controller with name: proxy-service-5vjxk, namespace: proxy-7655, replica count: 1
    I0826 05:45:31.959945      20 runners.go:193] proxy-service-5vjxk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0826 05:45:32.960101      20 runners.go:193] proxy-service-5vjxk Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 26 05:45:32.965: INFO: setup took 2.088960531s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/26/23 05:45:32.965
    Aug 26 05:45:32.978: INFO: (0) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 12.585802ms)
    Aug 26 05:45:32.978: INFO: (0) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 12.548734ms)
    Aug 26 05:45:32.978: INFO: (0) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 12.777448ms)
    Aug 26 05:45:32.979: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 13.847735ms)
    Aug 26 05:45:32.980: INFO: (0) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 14.584011ms)
    Aug 26 05:45:32.980: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 14.429593ms)
    Aug 26 05:45:32.980: INFO: (0) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.424324ms)
    Aug 26 05:45:32.980: INFO: (0) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 15.308707ms)
    Aug 26 05:45:32.980: INFO: (0) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 15.221992ms)
    Aug 26 05:45:32.981: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 15.622021ms)
    Aug 26 05:45:32.981: INFO: (0) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 15.860112ms)
    Aug 26 05:45:32.981: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 15.948102ms)
    Aug 26 05:45:32.981: INFO: (0) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 15.894771ms)
    Aug 26 05:45:32.992: INFO: (0) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 27.022394ms)
    Aug 26 05:45:32.995: INFO: (0) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 30.071528ms)
    Aug 26 05:45:32.999: INFO: (0) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 33.616187ms)
    Aug 26 05:45:33.010: INFO: (1) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 10.870953ms)
    Aug 26 05:45:33.010: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.158598ms)
    Aug 26 05:45:33.011: INFO: (1) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 11.926875ms)
    Aug 26 05:45:33.011: INFO: (1) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 12.299087ms)
    Aug 26 05:45:33.011: INFO: (1) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 12.445779ms)
    Aug 26 05:45:33.011: INFO: (1) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.147511ms)
    Aug 26 05:45:33.012: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.689035ms)
    Aug 26 05:45:33.013: INFO: (1) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 13.937182ms)
    Aug 26 05:45:33.013: INFO: (1) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 13.777739ms)
    Aug 26 05:45:33.014: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 14.521516ms)
    Aug 26 05:45:33.014: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 14.621283ms)
    Aug 26 05:45:33.015: INFO: (1) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 15.705761ms)
    Aug 26 05:45:33.017: INFO: (1) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 17.62122ms)
    Aug 26 05:45:33.017: INFO: (1) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 17.686468ms)
    Aug 26 05:45:33.017: INFO: (1) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 17.644425ms)
    Aug 26 05:45:33.018: INFO: (1) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 19.035212ms)
    Aug 26 05:45:33.023: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 5.257903ms)
    Aug 26 05:45:33.029: INFO: (2) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 10.814821ms)
    Aug 26 05:45:33.029: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.706443ms)
    Aug 26 05:45:33.029: INFO: (2) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.93049ms)
    Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.399317ms)
    Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 11.468542ms)
    Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.497208ms)
    Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 11.757804ms)
    Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 11.669138ms)
    Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.60259ms)
    Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 11.597044ms)
    Aug 26 05:45:33.030: INFO: (2) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 11.676612ms)
    Aug 26 05:45:33.031: INFO: (2) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 12.870946ms)
    Aug 26 05:45:33.031: INFO: (2) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 13.056229ms)
    Aug 26 05:45:33.031: INFO: (2) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 13.052089ms)
    Aug 26 05:45:33.031: INFO: (2) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 12.938389ms)
    Aug 26 05:45:33.041: INFO: (3) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 9.368782ms)
    Aug 26 05:45:33.041: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.088586ms)
    Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.928194ms)
    Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 10.923258ms)
    Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 11.195086ms)
    Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 11.032478ms)
    Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 11.159866ms)
    Aug 26 05:45:33.043: INFO: (3) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 11.353828ms)
    Aug 26 05:45:33.044: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 11.925061ms)
    Aug 26 05:45:33.044: INFO: (3) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 12.04357ms)
    Aug 26 05:45:33.044: INFO: (3) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 13.180462ms)
    Aug 26 05:45:33.045: INFO: (3) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 13.148361ms)
    Aug 26 05:45:33.046: INFO: (3) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 14.360223ms)
    Aug 26 05:45:33.046: INFO: (3) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 14.27736ms)
    Aug 26 05:45:33.046: INFO: (3) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 14.127858ms)
    Aug 26 05:45:33.046: INFO: (3) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 14.072451ms)
    Aug 26 05:45:33.054: INFO: (4) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 8.092691ms)
    Aug 26 05:45:33.054: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 8.032185ms)
    Aug 26 05:45:33.054: INFO: (4) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 8.16874ms)
    Aug 26 05:45:33.054: INFO: (4) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 8.225492ms)
    Aug 26 05:45:33.054: INFO: (4) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 8.175226ms)
    Aug 26 05:45:33.055: INFO: (4) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 8.9731ms)
    Aug 26 05:45:33.055: INFO: (4) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 9.24867ms)
    Aug 26 05:45:33.055: INFO: (4) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 9.524158ms)
    Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.006221ms)
    Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.791314ms)
    Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 9.973627ms)
    Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.059143ms)
    Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 10.205516ms)
    Aug 26 05:45:33.056: INFO: (4) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 10.167825ms)
    Aug 26 05:45:33.057: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.035393ms)
    Aug 26 05:45:33.057: INFO: (4) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 11.441256ms)
    Aug 26 05:45:33.067: INFO: (5) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 9.317164ms)
    Aug 26 05:45:33.070: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 12.16259ms)
    Aug 26 05:45:33.070: INFO: (5) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 12.195287ms)
    Aug 26 05:45:33.070: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.177081ms)
    Aug 26 05:45:33.071: INFO: (5) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 13.643674ms)
    Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 18.225687ms)
    Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 18.370531ms)
    Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 18.437377ms)
    Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 18.622695ms)
    Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 18.620475ms)
    Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 18.512214ms)
    Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 18.922908ms)
    Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 18.877498ms)
    Aug 26 05:45:33.076: INFO: (5) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 18.726289ms)
    Aug 26 05:45:33.077: INFO: (5) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 19.046549ms)
    Aug 26 05:45:33.077: INFO: (5) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 19.063182ms)
    Aug 26 05:45:33.092: INFO: (6) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.832294ms)
    Aug 26 05:45:33.092: INFO: (6) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 15.081894ms)
    Aug 26 05:45:33.092: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 15.210364ms)
    Aug 26 05:45:33.092: INFO: (6) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 15.049634ms)
    Aug 26 05:45:33.092: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 15.262413ms)
    Aug 26 05:45:33.093: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 16.142732ms)
    Aug 26 05:45:33.093: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 16.38741ms)
    Aug 26 05:45:33.093: INFO: (6) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 16.416995ms)
    Aug 26 05:45:33.093: INFO: (6) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 16.309169ms)
    Aug 26 05:45:33.095: INFO: (6) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 17.43081ms)
    Aug 26 05:45:33.100: INFO: (6) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 22.585396ms)
    Aug 26 05:45:33.101: INFO: (6) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 23.561404ms)
    Aug 26 05:45:33.101: INFO: (6) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 23.975781ms)
    Aug 26 05:45:33.101: INFO: (6) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 23.82851ms)
    Aug 26 05:45:33.101: INFO: (6) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 23.742429ms)
    Aug 26 05:45:33.101: INFO: (6) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 24.244917ms)
    Aug 26 05:45:33.115: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 13.087659ms)
    Aug 26 05:45:33.116: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 14.233017ms)
    Aug 26 05:45:33.116: INFO: (7) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 14.156051ms)
    Aug 26 05:45:33.116: INFO: (7) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.291096ms)
    Aug 26 05:45:33.116: INFO: (7) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 13.88917ms)
    Aug 26 05:45:33.116: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 14.646659ms)
    Aug 26 05:45:33.118: INFO: (7) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 16.073131ms)
    Aug 26 05:45:33.119: INFO: (7) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 17.127759ms)
    Aug 26 05:45:33.119: INFO: (7) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 17.906516ms)
    Aug 26 05:45:33.119: INFO: (7) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 17.678656ms)
    Aug 26 05:45:33.120: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 17.796188ms)
    Aug 26 05:45:33.121: INFO: (7) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 19.051202ms)
    Aug 26 05:45:33.121: INFO: (7) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 19.337704ms)
    Aug 26 05:45:33.121: INFO: (7) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 19.262406ms)
    Aug 26 05:45:33.122: INFO: (7) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 19.888502ms)
    Aug 26 05:45:33.122: INFO: (7) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 19.933868ms)
    Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 12.152549ms)
    Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.059392ms)
    Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.593113ms)
    Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 11.804933ms)
    Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 11.896416ms)
    Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 12.085282ms)
    Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 12.588224ms)
    Aug 26 05:45:33.134: INFO: (8) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 12.082126ms)
    Aug 26 05:45:33.135: INFO: (8) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 12.11258ms)
    Aug 26 05:45:33.135: INFO: (8) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 11.795759ms)
    Aug 26 05:45:33.135: INFO: (8) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 11.841173ms)
    Aug 26 05:45:33.137: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.213518ms)
    Aug 26 05:45:33.139: INFO: (8) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 16.335529ms)
    Aug 26 05:45:33.139: INFO: (8) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 16.426138ms)
    Aug 26 05:45:33.139: INFO: (8) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 16.588523ms)
    Aug 26 05:45:33.140: INFO: (8) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 17.506794ms)
    Aug 26 05:45:33.150: INFO: (9) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 9.547793ms)
    Aug 26 05:45:33.153: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 13.082404ms)
    Aug 26 05:45:33.153: INFO: (9) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 13.228895ms)
    Aug 26 05:45:33.153: INFO: (9) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 13.051902ms)
    Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 15.079202ms)
    Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 14.957538ms)
    Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 15.010465ms)
    Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 14.96331ms)
    Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 14.963616ms)
    Aug 26 05:45:33.155: INFO: (9) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 15.234748ms)
    Aug 26 05:45:33.156: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 15.157118ms)
    Aug 26 05:45:33.156: INFO: (9) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 16.059525ms)
    Aug 26 05:45:33.156: INFO: (9) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 16.161222ms)
    Aug 26 05:45:33.160: INFO: (9) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 19.849901ms)
    Aug 26 05:45:33.160: INFO: (9) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 19.761874ms)
    Aug 26 05:45:33.160: INFO: (9) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 19.859031ms)
    Aug 26 05:45:33.168: INFO: (10) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 7.720262ms)
    Aug 26 05:45:33.170: INFO: (10) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.915111ms)
    Aug 26 05:45:33.170: INFO: (10) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 9.983627ms)
    Aug 26 05:45:33.170: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.939364ms)
    Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 10.412168ms)
    Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.134452ms)
    Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 10.322797ms)
    Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 10.350681ms)
    Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 10.260553ms)
    Aug 26 05:45:33.171: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.51785ms)
    Aug 26 05:45:33.172: INFO: (10) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 12.213715ms)
    Aug 26 05:45:33.175: INFO: (10) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 14.53786ms)
    Aug 26 05:45:33.175: INFO: (10) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 14.625502ms)
    Aug 26 05:45:33.176: INFO: (10) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 15.018312ms)
    Aug 26 05:45:33.176: INFO: (10) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 15.335708ms)
    Aug 26 05:45:33.176: INFO: (10) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 15.197237ms)
    Aug 26 05:45:33.189: INFO: (11) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 13.57053ms)
    Aug 26 05:45:33.194: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 18.26934ms)
    Aug 26 05:45:33.194: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 18.081194ms)
    Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 18.958761ms)
    Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 18.993419ms)
    Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 18.990511ms)
    Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 18.93394ms)
    Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 19.012208ms)
    Aug 26 05:45:33.195: INFO: (11) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 19.040562ms)
    Aug 26 05:45:33.196: INFO: (11) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 20.425ms)
    Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 21.01764ms)
    Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 21.073956ms)
    Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 20.995203ms)
    Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 21.011707ms)
    Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 21.349155ms)
    Aug 26 05:45:33.197: INFO: (11) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 21.715314ms)
    Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 11.199682ms)
    Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.285665ms)
    Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.729342ms)
    Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 11.344691ms)
    Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 11.464429ms)
    Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 11.303336ms)
    Aug 26 05:45:33.209: INFO: (12) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 11.517024ms)
    Aug 26 05:45:33.210: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.663092ms)
    Aug 26 05:45:33.211: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 12.660148ms)
    Aug 26 05:45:33.211: INFO: (12) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 13.238169ms)
    Aug 26 05:45:33.211: INFO: (12) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 13.081044ms)
    Aug 26 05:45:33.211: INFO: (12) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 13.708267ms)
    Aug 26 05:45:33.211: INFO: (12) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 13.137452ms)
    Aug 26 05:45:33.212: INFO: (12) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 13.971411ms)
    Aug 26 05:45:33.212: INFO: (12) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.071443ms)
    Aug 26 05:45:33.212: INFO: (12) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 14.018271ms)
    Aug 26 05:45:33.222: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 10.221195ms)
    Aug 26 05:45:33.223: INFO: (13) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 10.70667ms)
    Aug 26 05:45:33.223: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.016507ms)
    Aug 26 05:45:33.223: INFO: (13) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 11.088968ms)
    Aug 26 05:45:33.224: INFO: (13) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 11.514659ms)
    Aug 26 05:45:33.224: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 11.674007ms)
    Aug 26 05:45:33.224: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.899883ms)
    Aug 26 05:45:33.228: INFO: (13) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 16.094598ms)
    Aug 26 05:45:33.228: INFO: (13) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 16.267693ms)
    Aug 26 05:45:33.228: INFO: (13) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 16.419583ms)
    Aug 26 05:45:33.228: INFO: (13) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 16.52064ms)
    Aug 26 05:45:33.234: INFO: (13) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 21.585668ms)
    Aug 26 05:45:33.234: INFO: (13) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 22.212225ms)
    Aug 26 05:45:33.235: INFO: (13) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 23.010843ms)
    Aug 26 05:45:33.238: INFO: (13) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 26.140167ms)
    Aug 26 05:45:33.238: INFO: (13) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 25.947602ms)
    Aug 26 05:45:33.250: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 10.846862ms)
    Aug 26 05:45:33.250: INFO: (14) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 11.370199ms)
    Aug 26 05:45:33.250: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 10.990715ms)
    Aug 26 05:45:33.251: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 12.185455ms)
    Aug 26 05:45:33.251: INFO: (14) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 12.621601ms)
    Aug 26 05:45:33.252: INFO: (14) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 12.651154ms)
    Aug 26 05:45:33.253: INFO: (14) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 13.702901ms)
    Aug 26 05:45:33.253: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 13.95561ms)
    Aug 26 05:45:33.254: INFO: (14) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 15.254671ms)
    Aug 26 05:45:33.254: INFO: (14) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.951151ms)
    Aug 26 05:45:33.254: INFO: (14) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 15.163693ms)
    Aug 26 05:45:33.255: INFO: (14) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 16.136412ms)
    Aug 26 05:45:33.256: INFO: (14) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 17.723024ms)
    Aug 26 05:45:33.256: INFO: (14) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 17.481125ms)
    Aug 26 05:45:33.257: INFO: (14) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 17.643326ms)
    Aug 26 05:45:33.257: INFO: (14) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 18.517411ms)
    Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 9.571763ms)
    Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 9.76674ms)
    Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.845809ms)
    Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 9.873883ms)
    Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 9.967694ms)
    Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 9.70962ms)
    Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 10.221735ms)
    Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 9.808552ms)
    Aug 26 05:45:33.268: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 9.66769ms)
    Aug 26 05:45:33.275: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 17.213701ms)
    Aug 26 05:45:33.276: INFO: (15) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 18.10189ms)
    Aug 26 05:45:33.276: INFO: (15) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 18.511582ms)
    Aug 26 05:45:33.276: INFO: (15) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 18.692093ms)
    Aug 26 05:45:33.277: INFO: (15) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 18.756895ms)
    Aug 26 05:45:33.277: INFO: (15) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 18.667092ms)
    Aug 26 05:45:33.277: INFO: (15) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 19.1118ms)
    Aug 26 05:45:33.286: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 8.483952ms)
    Aug 26 05:45:33.289: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.673838ms)
    Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 11.980264ms)
    Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 12.058035ms)
    Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 12.162577ms)
    Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 12.01362ms)
    Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 12.621225ms)
    Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 13.00053ms)
    Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.649999ms)
    Aug 26 05:45:33.290: INFO: (16) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 12.734869ms)
    Aug 26 05:45:33.291: INFO: (16) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 13.689094ms)
    Aug 26 05:45:33.291: INFO: (16) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 13.503247ms)
    Aug 26 05:45:33.292: INFO: (16) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 14.651628ms)
    Aug 26 05:45:33.294: INFO: (16) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 16.21632ms)
    Aug 26 05:45:33.294: INFO: (16) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 16.009716ms)
    Aug 26 05:45:33.294: INFO: (16) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 16.443309ms)
    Aug 26 05:45:33.301: INFO: (17) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 6.363564ms)
    Aug 26 05:45:33.305: INFO: (17) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 10.582085ms)
    Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 11.290755ms)
    Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 11.250194ms)
    Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 11.477746ms)
    Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.423597ms)
    Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 11.479042ms)
    Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.150045ms)
    Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 12.644722ms)
    Aug 26 05:45:33.306: INFO: (17) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 11.990955ms)
    Aug 26 05:45:33.307: INFO: (17) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 12.808758ms)
    Aug 26 05:45:33.310: INFO: (17) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 15.418451ms)
    Aug 26 05:45:33.310: INFO: (17) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 15.585347ms)
    Aug 26 05:45:33.316: INFO: (17) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 21.147303ms)
    Aug 26 05:45:33.317: INFO: (17) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 22.557828ms)
    Aug 26 05:45:33.317: INFO: (17) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 22.694326ms)
    Aug 26 05:45:33.330: INFO: (18) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 12.059419ms)
    Aug 26 05:45:33.330: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 12.478233ms)
    Aug 26 05:45:33.330: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 12.46002ms)
    Aug 26 05:45:33.331: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 13.483135ms)
    Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 14.206874ms)
    Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 14.096283ms)
    Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 14.095705ms)
    Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 14.130754ms)
    Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 14.369612ms)
    Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 14.159596ms)
    Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 14.277012ms)
    Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 14.264323ms)
    Aug 26 05:45:33.332: INFO: (18) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 14.927248ms)
    Aug 26 05:45:33.337: INFO: (18) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 19.011034ms)
    Aug 26 05:45:33.341: INFO: (18) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 23.179666ms)
    Aug 26 05:45:33.341: INFO: (18) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 23.242334ms)
    Aug 26 05:45:33.347: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 6.165076ms)
    Aug 26 05:45:33.354: INFO: (19) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:443/proxy/tlsrewritem... (200; 13.126677ms)
    Aug 26 05:45:33.354: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6/proxy/rewriteme">test</a> (200; 13.15901ms)
    Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">... (200; 13.534665ms)
    Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 13.565143ms)
    Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:160/proxy/: foo (200; 13.606499ms)
    Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-5vjxk-tnqp6:162/proxy/: bar (200; 13.853816ms)
    Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-5vjxk-tnqp6:1080/proxy/rewriteme">test<... (200; 13.9563ms)
    Aug 26 05:45:33.355: INFO: (19) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:462/proxy/: tls qux (200; 14.031661ms)
    Aug 26 05:45:33.356: INFO: (19) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-5vjxk-tnqp6:460/proxy/: tls baz (200; 14.811425ms)
    Aug 26 05:45:33.357: INFO: (19) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname2/proxy/: tls qux (200; 15.622223ms)
    Aug 26 05:45:33.359: INFO: (19) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname1/proxy/: foo (200; 18.032089ms)
    Aug 26 05:45:33.359: INFO: (19) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname2/proxy/: bar (200; 18.213071ms)
    Aug 26 05:45:33.360: INFO: (19) /api/v1/namespaces/proxy-7655/services/http:proxy-service-5vjxk:portname1/proxy/: foo (200; 18.592409ms)
    Aug 26 05:45:33.360: INFO: (19) /api/v1/namespaces/proxy-7655/services/https:proxy-service-5vjxk:tlsportname1/proxy/: tls baz (200; 19.10374ms)
    Aug 26 05:45:33.361: INFO: (19) /api/v1/namespaces/proxy-7655/services/proxy-service-5vjxk:portname2/proxy/: bar (200; 20.233613ms)
    STEP: deleting ReplicationController proxy-service-5vjxk in namespace proxy-7655, will wait for the garbage collector to delete the pods 08/26/23 05:45:33.361
    Aug 26 05:45:33.430: INFO: Deleting ReplicationController proxy-service-5vjxk took: 13.411126ms
    Aug 26 05:45:33.531: INFO: Terminating ReplicationController proxy-service-5vjxk pods took: 101.103762ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:45:35.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7655" for this suite. 08/26/23 05:45:35.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:45:35.662
Aug 26 05:45:35.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename events 08/26/23 05:45:35.663
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:35.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:35.688
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 08/26/23 05:45:35.692
STEP: get a list of Events with a label in the current namespace 08/26/23 05:45:35.717
STEP: delete a list of events 08/26/23 05:45:35.722
Aug 26 05:45:35.722: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/26/23 05:45:35.755
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 26 05:45:35.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1187" for this suite. 08/26/23 05:45:35.777
------------------------------
• [0.123 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:45:35.662
    Aug 26 05:45:35.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename events 08/26/23 05:45:35.663
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:35.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:35.688
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 08/26/23 05:45:35.692
    STEP: get a list of Events with a label in the current namespace 08/26/23 05:45:35.717
    STEP: delete a list of events 08/26/23 05:45:35.722
    Aug 26 05:45:35.722: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/26/23 05:45:35.755
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:45:35.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1187" for this suite. 08/26/23 05:45:35.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:45:35.786
Aug 26 05:45:35.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename gc 08/26/23 05:45:35.786
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:35.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:35.815
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 08/26/23 05:45:35.828
STEP: delete the rc 08/26/23 05:45:40.85
STEP: wait for the rc to be deleted 08/26/23 05:45:40.875
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/26/23 05:45:45.88
STEP: Gathering metrics 08/26/23 05:46:15.928
W0826 05:46:15.952450      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 26 05:46:15.952: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 26 05:46:15.952: INFO: Deleting pod "simpletest.rc-24rhx" in namespace "gc-6156"
Aug 26 05:46:15.970: INFO: Deleting pod "simpletest.rc-2b2vq" in namespace "gc-6156"
Aug 26 05:46:15.997: INFO: Deleting pod "simpletest.rc-2h558" in namespace "gc-6156"
Aug 26 05:46:16.038: INFO: Deleting pod "simpletest.rc-2v7fn" in namespace "gc-6156"
Aug 26 05:46:16.057: INFO: Deleting pod "simpletest.rc-2xxg8" in namespace "gc-6156"
Aug 26 05:46:16.075: INFO: Deleting pod "simpletest.rc-45frm" in namespace "gc-6156"
Aug 26 05:46:16.108: INFO: Deleting pod "simpletest.rc-4jn4k" in namespace "gc-6156"
Aug 26 05:46:16.129: INFO: Deleting pod "simpletest.rc-4q8l4" in namespace "gc-6156"
Aug 26 05:46:16.152: INFO: Deleting pod "simpletest.rc-4zgtv" in namespace "gc-6156"
Aug 26 05:46:16.177: INFO: Deleting pod "simpletest.rc-5lvbl" in namespace "gc-6156"
Aug 26 05:46:16.203: INFO: Deleting pod "simpletest.rc-5nbs6" in namespace "gc-6156"
Aug 26 05:46:16.226: INFO: Deleting pod "simpletest.rc-5xf6k" in namespace "gc-6156"
Aug 26 05:46:16.259: INFO: Deleting pod "simpletest.rc-5zfnc" in namespace "gc-6156"
Aug 26 05:46:16.290: INFO: Deleting pod "simpletest.rc-6dmvl" in namespace "gc-6156"
Aug 26 05:46:16.320: INFO: Deleting pod "simpletest.rc-6f7hp" in namespace "gc-6156"
Aug 26 05:46:16.346: INFO: Deleting pod "simpletest.rc-72tdq" in namespace "gc-6156"
Aug 26 05:46:16.366: INFO: Deleting pod "simpletest.rc-85lf6" in namespace "gc-6156"
Aug 26 05:46:16.392: INFO: Deleting pod "simpletest.rc-89rw7" in namespace "gc-6156"
Aug 26 05:46:16.407: INFO: Deleting pod "simpletest.rc-8v6n4" in namespace "gc-6156"
Aug 26 05:46:16.440: INFO: Deleting pod "simpletest.rc-9hpcp" in namespace "gc-6156"
Aug 26 05:46:16.576: INFO: Deleting pod "simpletest.rc-9hpl2" in namespace "gc-6156"
Aug 26 05:46:16.696: INFO: Deleting pod "simpletest.rc-9whpn" in namespace "gc-6156"
Aug 26 05:46:16.734: INFO: Deleting pod "simpletest.rc-9zzlv" in namespace "gc-6156"
Aug 26 05:46:16.765: INFO: Deleting pod "simpletest.rc-b4fnw" in namespace "gc-6156"
Aug 26 05:46:16.804: INFO: Deleting pod "simpletest.rc-b7b8f" in namespace "gc-6156"
Aug 26 05:46:16.828: INFO: Deleting pod "simpletest.rc-bbvg8" in namespace "gc-6156"
Aug 26 05:46:16.848: INFO: Deleting pod "simpletest.rc-bgkq2" in namespace "gc-6156"
Aug 26 05:46:16.869: INFO: Deleting pod "simpletest.rc-bmj7s" in namespace "gc-6156"
Aug 26 05:46:16.882: INFO: Deleting pod "simpletest.rc-bzlrn" in namespace "gc-6156"
Aug 26 05:46:16.899: INFO: Deleting pod "simpletest.rc-c2c9s" in namespace "gc-6156"
Aug 26 05:46:16.923: INFO: Deleting pod "simpletest.rc-c659x" in namespace "gc-6156"
Aug 26 05:46:16.946: INFO: Deleting pod "simpletest.rc-c9skd" in namespace "gc-6156"
Aug 26 05:46:16.969: INFO: Deleting pod "simpletest.rc-cbjbr" in namespace "gc-6156"
Aug 26 05:46:16.994: INFO: Deleting pod "simpletest.rc-cdkrl" in namespace "gc-6156"
Aug 26 05:46:17.022: INFO: Deleting pod "simpletest.rc-cgxlc" in namespace "gc-6156"
Aug 26 05:46:17.040: INFO: Deleting pod "simpletest.rc-cm8bb" in namespace "gc-6156"
Aug 26 05:46:17.061: INFO: Deleting pod "simpletest.rc-cpl64" in namespace "gc-6156"
Aug 26 05:46:17.082: INFO: Deleting pod "simpletest.rc-cs7vz" in namespace "gc-6156"
Aug 26 05:46:17.098: INFO: Deleting pod "simpletest.rc-cxw29" in namespace "gc-6156"
Aug 26 05:46:17.124: INFO: Deleting pod "simpletest.rc-dgxc9" in namespace "gc-6156"
Aug 26 05:46:17.141: INFO: Deleting pod "simpletest.rc-dlxzp" in namespace "gc-6156"
Aug 26 05:46:17.160: INFO: Deleting pod "simpletest.rc-f2rkp" in namespace "gc-6156"
Aug 26 05:46:17.187: INFO: Deleting pod "simpletest.rc-fzhhl" in namespace "gc-6156"
Aug 26 05:46:17.201: INFO: Deleting pod "simpletest.rc-g4z6j" in namespace "gc-6156"
Aug 26 05:46:17.231: INFO: Deleting pod "simpletest.rc-g8v26" in namespace "gc-6156"
Aug 26 05:46:17.256: INFO: Deleting pod "simpletest.rc-gckm5" in namespace "gc-6156"
Aug 26 05:46:17.278: INFO: Deleting pod "simpletest.rc-h4w9s" in namespace "gc-6156"
Aug 26 05:46:17.298: INFO: Deleting pod "simpletest.rc-h55gf" in namespace "gc-6156"
Aug 26 05:46:17.318: INFO: Deleting pod "simpletest.rc-hd4vn" in namespace "gc-6156"
Aug 26 05:46:17.342: INFO: Deleting pod "simpletest.rc-hq9f4" in namespace "gc-6156"
Aug 26 05:46:17.379: INFO: Deleting pod "simpletest.rc-jb7vk" in namespace "gc-6156"
Aug 26 05:46:17.393: INFO: Deleting pod "simpletest.rc-jbkp9" in namespace "gc-6156"
Aug 26 05:46:17.415: INFO: Deleting pod "simpletest.rc-jq4vs" in namespace "gc-6156"
Aug 26 05:46:17.447: INFO: Deleting pod "simpletest.rc-jx8qp" in namespace "gc-6156"
Aug 26 05:46:17.475: INFO: Deleting pod "simpletest.rc-k2cwp" in namespace "gc-6156"
Aug 26 05:46:17.497: INFO: Deleting pod "simpletest.rc-klxmd" in namespace "gc-6156"
Aug 26 05:46:17.516: INFO: Deleting pod "simpletest.rc-l6b4h" in namespace "gc-6156"
Aug 26 05:46:17.544: INFO: Deleting pod "simpletest.rc-l6fmt" in namespace "gc-6156"
Aug 26 05:46:17.569: INFO: Deleting pod "simpletest.rc-l9kfx" in namespace "gc-6156"
Aug 26 05:46:17.590: INFO: Deleting pod "simpletest.rc-ldbp9" in namespace "gc-6156"
Aug 26 05:46:17.617: INFO: Deleting pod "simpletest.rc-lfmwz" in namespace "gc-6156"
Aug 26 05:46:17.632: INFO: Deleting pod "simpletest.rc-lh78m" in namespace "gc-6156"
Aug 26 05:46:17.657: INFO: Deleting pod "simpletest.rc-lzxfh" in namespace "gc-6156"
Aug 26 05:46:17.680: INFO: Deleting pod "simpletest.rc-m8z9v" in namespace "gc-6156"
Aug 26 05:46:17.700: INFO: Deleting pod "simpletest.rc-mfgnm" in namespace "gc-6156"
Aug 26 05:46:17.726: INFO: Deleting pod "simpletest.rc-mjskr" in namespace "gc-6156"
Aug 26 05:46:17.856: INFO: Deleting pod "simpletest.rc-mqsnm" in namespace "gc-6156"
Aug 26 05:46:17.904: INFO: Deleting pod "simpletest.rc-mqvcl" in namespace "gc-6156"
Aug 26 05:46:17.919: INFO: Deleting pod "simpletest.rc-msckx" in namespace "gc-6156"
Aug 26 05:46:17.937: INFO: Deleting pod "simpletest.rc-nmmnw" in namespace "gc-6156"
Aug 26 05:46:17.951: INFO: Deleting pod "simpletest.rc-nprch" in namespace "gc-6156"
Aug 26 05:46:17.978: INFO: Deleting pod "simpletest.rc-p42xh" in namespace "gc-6156"
Aug 26 05:46:18.011: INFO: Deleting pod "simpletest.rc-p6hfn" in namespace "gc-6156"
Aug 26 05:46:18.035: INFO: Deleting pod "simpletest.rc-p79xh" in namespace "gc-6156"
Aug 26 05:46:18.050: INFO: Deleting pod "simpletest.rc-p8288" in namespace "gc-6156"
Aug 26 05:46:18.064: INFO: Deleting pod "simpletest.rc-pcvlt" in namespace "gc-6156"
Aug 26 05:46:18.086: INFO: Deleting pod "simpletest.rc-pqqpw" in namespace "gc-6156"
Aug 26 05:46:18.109: INFO: Deleting pod "simpletest.rc-pvpnt" in namespace "gc-6156"
Aug 26 05:46:18.139: INFO: Deleting pod "simpletest.rc-pxtf5" in namespace "gc-6156"
Aug 26 05:46:18.171: INFO: Deleting pod "simpletest.rc-r64rh" in namespace "gc-6156"
Aug 26 05:46:18.199: INFO: Deleting pod "simpletest.rc-r9dmz" in namespace "gc-6156"
Aug 26 05:46:18.221: INFO: Deleting pod "simpletest.rc-rltv5" in namespace "gc-6156"
Aug 26 05:46:18.253: INFO: Deleting pod "simpletest.rc-rmvjt" in namespace "gc-6156"
Aug 26 05:46:18.274: INFO: Deleting pod "simpletest.rc-s65hk" in namespace "gc-6156"
Aug 26 05:46:18.302: INFO: Deleting pod "simpletest.rc-sbt29" in namespace "gc-6156"
Aug 26 05:46:18.325: INFO: Deleting pod "simpletest.rc-sh8k7" in namespace "gc-6156"
Aug 26 05:46:18.346: INFO: Deleting pod "simpletest.rc-t5fll" in namespace "gc-6156"
Aug 26 05:46:18.391: INFO: Deleting pod "simpletest.rc-vjknd" in namespace "gc-6156"
Aug 26 05:46:18.408: INFO: Deleting pod "simpletest.rc-w8zzl" in namespace "gc-6156"
Aug 26 05:46:18.429: INFO: Deleting pod "simpletest.rc-w97dg" in namespace "gc-6156"
Aug 26 05:46:18.458: INFO: Deleting pod "simpletest.rc-wgc58" in namespace "gc-6156"
Aug 26 05:46:18.482: INFO: Deleting pod "simpletest.rc-wthdq" in namespace "gc-6156"
Aug 26 05:46:18.526: INFO: Deleting pod "simpletest.rc-x2xf6" in namespace "gc-6156"
Aug 26 05:46:18.552: INFO: Deleting pod "simpletest.rc-xnf8s" in namespace "gc-6156"
Aug 26 05:46:18.572: INFO: Deleting pod "simpletest.rc-xrjrd" in namespace "gc-6156"
Aug 26 05:46:18.592: INFO: Deleting pod "simpletest.rc-xvtpj" in namespace "gc-6156"
Aug 26 05:46:18.616: INFO: Deleting pod "simpletest.rc-z2jw9" in namespace "gc-6156"
Aug 26 05:46:18.638: INFO: Deleting pod "simpletest.rc-z65td" in namespace "gc-6156"
Aug 26 05:46:18.672: INFO: Deleting pod "simpletest.rc-zf9pq" in namespace "gc-6156"
Aug 26 05:46:18.699: INFO: Deleting pod "simpletest.rc-zmw2h" in namespace "gc-6156"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 26 05:46:18.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6156" for this suite. 08/26/23 05:46:18.734
------------------------------
• [SLOW TEST] [42.956 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:45:35.786
    Aug 26 05:45:35.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename gc 08/26/23 05:45:35.786
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:45:35.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:45:35.815
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 08/26/23 05:45:35.828
    STEP: delete the rc 08/26/23 05:45:40.85
    STEP: wait for the rc to be deleted 08/26/23 05:45:40.875
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/26/23 05:45:45.88
    STEP: Gathering metrics 08/26/23 05:46:15.928
    W0826 05:46:15.952450      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 26 05:46:15.952: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 26 05:46:15.952: INFO: Deleting pod "simpletest.rc-24rhx" in namespace "gc-6156"
    Aug 26 05:46:15.970: INFO: Deleting pod "simpletest.rc-2b2vq" in namespace "gc-6156"
    Aug 26 05:46:15.997: INFO: Deleting pod "simpletest.rc-2h558" in namespace "gc-6156"
    Aug 26 05:46:16.038: INFO: Deleting pod "simpletest.rc-2v7fn" in namespace "gc-6156"
    Aug 26 05:46:16.057: INFO: Deleting pod "simpletest.rc-2xxg8" in namespace "gc-6156"
    Aug 26 05:46:16.075: INFO: Deleting pod "simpletest.rc-45frm" in namespace "gc-6156"
    Aug 26 05:46:16.108: INFO: Deleting pod "simpletest.rc-4jn4k" in namespace "gc-6156"
    Aug 26 05:46:16.129: INFO: Deleting pod "simpletest.rc-4q8l4" in namespace "gc-6156"
    Aug 26 05:46:16.152: INFO: Deleting pod "simpletest.rc-4zgtv" in namespace "gc-6156"
    Aug 26 05:46:16.177: INFO: Deleting pod "simpletest.rc-5lvbl" in namespace "gc-6156"
    Aug 26 05:46:16.203: INFO: Deleting pod "simpletest.rc-5nbs6" in namespace "gc-6156"
    Aug 26 05:46:16.226: INFO: Deleting pod "simpletest.rc-5xf6k" in namespace "gc-6156"
    Aug 26 05:46:16.259: INFO: Deleting pod "simpletest.rc-5zfnc" in namespace "gc-6156"
    Aug 26 05:46:16.290: INFO: Deleting pod "simpletest.rc-6dmvl" in namespace "gc-6156"
    Aug 26 05:46:16.320: INFO: Deleting pod "simpletest.rc-6f7hp" in namespace "gc-6156"
    Aug 26 05:46:16.346: INFO: Deleting pod "simpletest.rc-72tdq" in namespace "gc-6156"
    Aug 26 05:46:16.366: INFO: Deleting pod "simpletest.rc-85lf6" in namespace "gc-6156"
    Aug 26 05:46:16.392: INFO: Deleting pod "simpletest.rc-89rw7" in namespace "gc-6156"
    Aug 26 05:46:16.407: INFO: Deleting pod "simpletest.rc-8v6n4" in namespace "gc-6156"
    Aug 26 05:46:16.440: INFO: Deleting pod "simpletest.rc-9hpcp" in namespace "gc-6156"
    Aug 26 05:46:16.576: INFO: Deleting pod "simpletest.rc-9hpl2" in namespace "gc-6156"
    Aug 26 05:46:16.696: INFO: Deleting pod "simpletest.rc-9whpn" in namespace "gc-6156"
    Aug 26 05:46:16.734: INFO: Deleting pod "simpletest.rc-9zzlv" in namespace "gc-6156"
    Aug 26 05:46:16.765: INFO: Deleting pod "simpletest.rc-b4fnw" in namespace "gc-6156"
    Aug 26 05:46:16.804: INFO: Deleting pod "simpletest.rc-b7b8f" in namespace "gc-6156"
    Aug 26 05:46:16.828: INFO: Deleting pod "simpletest.rc-bbvg8" in namespace "gc-6156"
    Aug 26 05:46:16.848: INFO: Deleting pod "simpletest.rc-bgkq2" in namespace "gc-6156"
    Aug 26 05:46:16.869: INFO: Deleting pod "simpletest.rc-bmj7s" in namespace "gc-6156"
    Aug 26 05:46:16.882: INFO: Deleting pod "simpletest.rc-bzlrn" in namespace "gc-6156"
    Aug 26 05:46:16.899: INFO: Deleting pod "simpletest.rc-c2c9s" in namespace "gc-6156"
    Aug 26 05:46:16.923: INFO: Deleting pod "simpletest.rc-c659x" in namespace "gc-6156"
    Aug 26 05:46:16.946: INFO: Deleting pod "simpletest.rc-c9skd" in namespace "gc-6156"
    Aug 26 05:46:16.969: INFO: Deleting pod "simpletest.rc-cbjbr" in namespace "gc-6156"
    Aug 26 05:46:16.994: INFO: Deleting pod "simpletest.rc-cdkrl" in namespace "gc-6156"
    Aug 26 05:46:17.022: INFO: Deleting pod "simpletest.rc-cgxlc" in namespace "gc-6156"
    Aug 26 05:46:17.040: INFO: Deleting pod "simpletest.rc-cm8bb" in namespace "gc-6156"
    Aug 26 05:46:17.061: INFO: Deleting pod "simpletest.rc-cpl64" in namespace "gc-6156"
    Aug 26 05:46:17.082: INFO: Deleting pod "simpletest.rc-cs7vz" in namespace "gc-6156"
    Aug 26 05:46:17.098: INFO: Deleting pod "simpletest.rc-cxw29" in namespace "gc-6156"
    Aug 26 05:46:17.124: INFO: Deleting pod "simpletest.rc-dgxc9" in namespace "gc-6156"
    Aug 26 05:46:17.141: INFO: Deleting pod "simpletest.rc-dlxzp" in namespace "gc-6156"
    Aug 26 05:46:17.160: INFO: Deleting pod "simpletest.rc-f2rkp" in namespace "gc-6156"
    Aug 26 05:46:17.187: INFO: Deleting pod "simpletest.rc-fzhhl" in namespace "gc-6156"
    Aug 26 05:46:17.201: INFO: Deleting pod "simpletest.rc-g4z6j" in namespace "gc-6156"
    Aug 26 05:46:17.231: INFO: Deleting pod "simpletest.rc-g8v26" in namespace "gc-6156"
    Aug 26 05:46:17.256: INFO: Deleting pod "simpletest.rc-gckm5" in namespace "gc-6156"
    Aug 26 05:46:17.278: INFO: Deleting pod "simpletest.rc-h4w9s" in namespace "gc-6156"
    Aug 26 05:46:17.298: INFO: Deleting pod "simpletest.rc-h55gf" in namespace "gc-6156"
    Aug 26 05:46:17.318: INFO: Deleting pod "simpletest.rc-hd4vn" in namespace "gc-6156"
    Aug 26 05:46:17.342: INFO: Deleting pod "simpletest.rc-hq9f4" in namespace "gc-6156"
    Aug 26 05:46:17.379: INFO: Deleting pod "simpletest.rc-jb7vk" in namespace "gc-6156"
    Aug 26 05:46:17.393: INFO: Deleting pod "simpletest.rc-jbkp9" in namespace "gc-6156"
    Aug 26 05:46:17.415: INFO: Deleting pod "simpletest.rc-jq4vs" in namespace "gc-6156"
    Aug 26 05:46:17.447: INFO: Deleting pod "simpletest.rc-jx8qp" in namespace "gc-6156"
    Aug 26 05:46:17.475: INFO: Deleting pod "simpletest.rc-k2cwp" in namespace "gc-6156"
    Aug 26 05:46:17.497: INFO: Deleting pod "simpletest.rc-klxmd" in namespace "gc-6156"
    Aug 26 05:46:17.516: INFO: Deleting pod "simpletest.rc-l6b4h" in namespace "gc-6156"
    Aug 26 05:46:17.544: INFO: Deleting pod "simpletest.rc-l6fmt" in namespace "gc-6156"
    Aug 26 05:46:17.569: INFO: Deleting pod "simpletest.rc-l9kfx" in namespace "gc-6156"
    Aug 26 05:46:17.590: INFO: Deleting pod "simpletest.rc-ldbp9" in namespace "gc-6156"
    Aug 26 05:46:17.617: INFO: Deleting pod "simpletest.rc-lfmwz" in namespace "gc-6156"
    Aug 26 05:46:17.632: INFO: Deleting pod "simpletest.rc-lh78m" in namespace "gc-6156"
    Aug 26 05:46:17.657: INFO: Deleting pod "simpletest.rc-lzxfh" in namespace "gc-6156"
    Aug 26 05:46:17.680: INFO: Deleting pod "simpletest.rc-m8z9v" in namespace "gc-6156"
    Aug 26 05:46:17.700: INFO: Deleting pod "simpletest.rc-mfgnm" in namespace "gc-6156"
    Aug 26 05:46:17.726: INFO: Deleting pod "simpletest.rc-mjskr" in namespace "gc-6156"
    Aug 26 05:46:17.856: INFO: Deleting pod "simpletest.rc-mqsnm" in namespace "gc-6156"
    Aug 26 05:46:17.904: INFO: Deleting pod "simpletest.rc-mqvcl" in namespace "gc-6156"
    Aug 26 05:46:17.919: INFO: Deleting pod "simpletest.rc-msckx" in namespace "gc-6156"
    Aug 26 05:46:17.937: INFO: Deleting pod "simpletest.rc-nmmnw" in namespace "gc-6156"
    Aug 26 05:46:17.951: INFO: Deleting pod "simpletest.rc-nprch" in namespace "gc-6156"
    Aug 26 05:46:17.978: INFO: Deleting pod "simpletest.rc-p42xh" in namespace "gc-6156"
    Aug 26 05:46:18.011: INFO: Deleting pod "simpletest.rc-p6hfn" in namespace "gc-6156"
    Aug 26 05:46:18.035: INFO: Deleting pod "simpletest.rc-p79xh" in namespace "gc-6156"
    Aug 26 05:46:18.050: INFO: Deleting pod "simpletest.rc-p8288" in namespace "gc-6156"
    Aug 26 05:46:18.064: INFO: Deleting pod "simpletest.rc-pcvlt" in namespace "gc-6156"
    Aug 26 05:46:18.086: INFO: Deleting pod "simpletest.rc-pqqpw" in namespace "gc-6156"
    Aug 26 05:46:18.109: INFO: Deleting pod "simpletest.rc-pvpnt" in namespace "gc-6156"
    Aug 26 05:46:18.139: INFO: Deleting pod "simpletest.rc-pxtf5" in namespace "gc-6156"
    Aug 26 05:46:18.171: INFO: Deleting pod "simpletest.rc-r64rh" in namespace "gc-6156"
    Aug 26 05:46:18.199: INFO: Deleting pod "simpletest.rc-r9dmz" in namespace "gc-6156"
    Aug 26 05:46:18.221: INFO: Deleting pod "simpletest.rc-rltv5" in namespace "gc-6156"
    Aug 26 05:46:18.253: INFO: Deleting pod "simpletest.rc-rmvjt" in namespace "gc-6156"
    Aug 26 05:46:18.274: INFO: Deleting pod "simpletest.rc-s65hk" in namespace "gc-6156"
    Aug 26 05:46:18.302: INFO: Deleting pod "simpletest.rc-sbt29" in namespace "gc-6156"
    Aug 26 05:46:18.325: INFO: Deleting pod "simpletest.rc-sh8k7" in namespace "gc-6156"
    Aug 26 05:46:18.346: INFO: Deleting pod "simpletest.rc-t5fll" in namespace "gc-6156"
    Aug 26 05:46:18.391: INFO: Deleting pod "simpletest.rc-vjknd" in namespace "gc-6156"
    Aug 26 05:46:18.408: INFO: Deleting pod "simpletest.rc-w8zzl" in namespace "gc-6156"
    Aug 26 05:46:18.429: INFO: Deleting pod "simpletest.rc-w97dg" in namespace "gc-6156"
    Aug 26 05:46:18.458: INFO: Deleting pod "simpletest.rc-wgc58" in namespace "gc-6156"
    Aug 26 05:46:18.482: INFO: Deleting pod "simpletest.rc-wthdq" in namespace "gc-6156"
    Aug 26 05:46:18.526: INFO: Deleting pod "simpletest.rc-x2xf6" in namespace "gc-6156"
    Aug 26 05:46:18.552: INFO: Deleting pod "simpletest.rc-xnf8s" in namespace "gc-6156"
    Aug 26 05:46:18.572: INFO: Deleting pod "simpletest.rc-xrjrd" in namespace "gc-6156"
    Aug 26 05:46:18.592: INFO: Deleting pod "simpletest.rc-xvtpj" in namespace "gc-6156"
    Aug 26 05:46:18.616: INFO: Deleting pod "simpletest.rc-z2jw9" in namespace "gc-6156"
    Aug 26 05:46:18.638: INFO: Deleting pod "simpletest.rc-z65td" in namespace "gc-6156"
    Aug 26 05:46:18.672: INFO: Deleting pod "simpletest.rc-zf9pq" in namespace "gc-6156"
    Aug 26 05:46:18.699: INFO: Deleting pod "simpletest.rc-zmw2h" in namespace "gc-6156"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:46:18.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6156" for this suite. 08/26/23 05:46:18.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:46:18.743
Aug 26 05:46:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename runtimeclass 08/26/23 05:46:18.745
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:18.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:18.891
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 26 05:46:18.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2816" for this suite. 08/26/23 05:46:19.032
------------------------------
• [0.303 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:46:18.743
    Aug 26 05:46:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename runtimeclass 08/26/23 05:46:18.745
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:18.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:18.891
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:46:18.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2816" for this suite. 08/26/23 05:46:19.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:46:19.052
Aug 26 05:46:19.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename gc 08/26/23 05:46:19.054
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:19.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:19.106
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 08/26/23 05:46:19.132
STEP: delete the rc 08/26/23 05:46:24.156
STEP: wait for the rc to be deleted 08/26/23 05:46:24.168
Aug 26 05:46:25.220: INFO: 80 pods remaining
Aug 26 05:46:25.220: INFO: 80 pods has nil DeletionTimestamp
Aug 26 05:46:25.220: INFO: 
Aug 26 05:46:26.201: INFO: 72 pods remaining
Aug 26 05:46:26.201: INFO: 71 pods has nil DeletionTimestamp
Aug 26 05:46:26.201: INFO: 
Aug 26 05:46:27.212: INFO: 59 pods remaining
Aug 26 05:46:27.212: INFO: 59 pods has nil DeletionTimestamp
Aug 26 05:46:27.212: INFO: 
Aug 26 05:46:28.223: INFO: 41 pods remaining
Aug 26 05:46:28.223: INFO: 41 pods has nil DeletionTimestamp
Aug 26 05:46:28.223: INFO: 
Aug 26 05:46:29.211: INFO: 32 pods remaining
Aug 26 05:46:29.211: INFO: 32 pods has nil DeletionTimestamp
Aug 26 05:46:29.211: INFO: 
Aug 26 05:46:30.180: INFO: 20 pods remaining
Aug 26 05:46:30.180: INFO: 20 pods has nil DeletionTimestamp
Aug 26 05:46:30.180: INFO: 
STEP: Gathering metrics 08/26/23 05:46:31.178
W0826 05:46:31.202032      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 26 05:46:31.202: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 26 05:46:31.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3243" for this suite. 08/26/23 05:46:31.215
------------------------------
• [SLOW TEST] [12.175 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:46:19.052
    Aug 26 05:46:19.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename gc 08/26/23 05:46:19.054
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:19.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:19.106
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 08/26/23 05:46:19.132
    STEP: delete the rc 08/26/23 05:46:24.156
    STEP: wait for the rc to be deleted 08/26/23 05:46:24.168
    Aug 26 05:46:25.220: INFO: 80 pods remaining
    Aug 26 05:46:25.220: INFO: 80 pods has nil DeletionTimestamp
    Aug 26 05:46:25.220: INFO: 
    Aug 26 05:46:26.201: INFO: 72 pods remaining
    Aug 26 05:46:26.201: INFO: 71 pods has nil DeletionTimestamp
    Aug 26 05:46:26.201: INFO: 
    Aug 26 05:46:27.212: INFO: 59 pods remaining
    Aug 26 05:46:27.212: INFO: 59 pods has nil DeletionTimestamp
    Aug 26 05:46:27.212: INFO: 
    Aug 26 05:46:28.223: INFO: 41 pods remaining
    Aug 26 05:46:28.223: INFO: 41 pods has nil DeletionTimestamp
    Aug 26 05:46:28.223: INFO: 
    Aug 26 05:46:29.211: INFO: 32 pods remaining
    Aug 26 05:46:29.211: INFO: 32 pods has nil DeletionTimestamp
    Aug 26 05:46:29.211: INFO: 
    Aug 26 05:46:30.180: INFO: 20 pods remaining
    Aug 26 05:46:30.180: INFO: 20 pods has nil DeletionTimestamp
    Aug 26 05:46:30.180: INFO: 
    STEP: Gathering metrics 08/26/23 05:46:31.178
    W0826 05:46:31.202032      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 26 05:46:31.202: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:46:31.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3243" for this suite. 08/26/23 05:46:31.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:46:31.228
Aug 26 05:46:31.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename disruption 08/26/23 05:46:31.235
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:31.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:31.262
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:46:31.268
Aug 26 05:46:31.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename disruption-2 08/26/23 05:46:31.27
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:31.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:31.288
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 08/26/23 05:46:31.302
STEP: Waiting for the pdb to be processed 08/26/23 05:46:31.323
STEP: Waiting for the pdb to be processed 08/26/23 05:46:33.339
STEP: listing a collection of PDBs across all namespaces 08/26/23 05:46:33.351
STEP: listing a collection of PDBs in namespace disruption-8009 08/26/23 05:46:33.36
STEP: deleting a collection of PDBs 08/26/23 05:46:33.368
STEP: Waiting for the PDB collection to be deleted 08/26/23 05:46:33.385
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Aug 26 05:46:33.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 26 05:46:33.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-3158" for this suite. 08/26/23 05:46:33.402
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8009" for this suite. 08/26/23 05:46:33.413
------------------------------
• [2.195 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:46:31.228
    Aug 26 05:46:31.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename disruption 08/26/23 05:46:31.235
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:31.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:31.262
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:46:31.268
    Aug 26 05:46:31.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename disruption-2 08/26/23 05:46:31.27
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:31.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:31.288
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 08/26/23 05:46:31.302
    STEP: Waiting for the pdb to be processed 08/26/23 05:46:31.323
    STEP: Waiting for the pdb to be processed 08/26/23 05:46:33.339
    STEP: listing a collection of PDBs across all namespaces 08/26/23 05:46:33.351
    STEP: listing a collection of PDBs in namespace disruption-8009 08/26/23 05:46:33.36
    STEP: deleting a collection of PDBs 08/26/23 05:46:33.368
    STEP: Waiting for the PDB collection to be deleted 08/26/23 05:46:33.385
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:46:33.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:46:33.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-3158" for this suite. 08/26/23 05:46:33.402
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8009" for this suite. 08/26/23 05:46:33.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:46:33.424
Aug 26 05:46:33.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 05:46:33.426
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:33.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:33.451
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 08/26/23 05:46:33.457
Aug 26 05:46:33.457: INFO: Creating e2e-svc-a-2p44l
Aug 26 05:46:33.472: INFO: Creating e2e-svc-b-wz47l
Aug 26 05:46:33.497: INFO: Creating e2e-svc-c-f4qkv
STEP: deleting service collection 08/26/23 05:46:33.528
Aug 26 05:46:33.701: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 05:46:33.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2576" for this suite. 08/26/23 05:46:33.71
------------------------------
• [0.295 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:46:33.424
    Aug 26 05:46:33.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 05:46:33.426
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:33.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:33.451
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 08/26/23 05:46:33.457
    Aug 26 05:46:33.457: INFO: Creating e2e-svc-a-2p44l
    Aug 26 05:46:33.472: INFO: Creating e2e-svc-b-wz47l
    Aug 26 05:46:33.497: INFO: Creating e2e-svc-c-f4qkv
    STEP: deleting service collection 08/26/23 05:46:33.528
    Aug 26 05:46:33.701: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:46:33.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2576" for this suite. 08/26/23 05:46:33.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:46:33.722
Aug 26 05:46:33.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:46:33.724
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:33.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:33.808
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:46:33.825
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:46:34.19
STEP: Deploying the webhook pod 08/26/23 05:46:34.204
STEP: Wait for the deployment to be ready 08/26/23 05:46:34.225
Aug 26 05:46:34.241: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:46:36.258
STEP: Verifying the service has paired with the endpoint 08/26/23 05:46:36.291
Aug 26 05:46:37.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/26/23 05:46:37.295
STEP: create a namespace for the webhook 08/26/23 05:46:37.31
STEP: create a configmap should be unconditionally rejected by the webhook 08/26/23 05:46:37.327
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:46:37.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1520" for this suite. 08/26/23 05:46:37.44
STEP: Destroying namespace "webhook-1520-markers" for this suite. 08/26/23 05:46:37.452
------------------------------
• [3.739 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:46:33.722
    Aug 26 05:46:33.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:46:33.724
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:33.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:33.808
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:46:33.825
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:46:34.19
    STEP: Deploying the webhook pod 08/26/23 05:46:34.204
    STEP: Wait for the deployment to be ready 08/26/23 05:46:34.225
    Aug 26 05:46:34.241: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:46:36.258
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:46:36.291
    Aug 26 05:46:37.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/26/23 05:46:37.295
    STEP: create a namespace for the webhook 08/26/23 05:46:37.31
    STEP: create a configmap should be unconditionally rejected by the webhook 08/26/23 05:46:37.327
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:46:37.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1520" for this suite. 08/26/23 05:46:37.44
    STEP: Destroying namespace "webhook-1520-markers" for this suite. 08/26/23 05:46:37.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:46:37.462
Aug 26 05:46:37.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 05:46:37.463
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:37.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:37.488
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 08/26/23 05:46:54.497
STEP: Creating a ResourceQuota 08/26/23 05:46:59.504
STEP: Ensuring resource quota status is calculated 08/26/23 05:46:59.511
STEP: Creating a ConfigMap 08/26/23 05:47:01.517
STEP: Ensuring resource quota status captures configMap creation 08/26/23 05:47:01.532
STEP: Deleting a ConfigMap 08/26/23 05:47:03.537
STEP: Ensuring resource quota status released usage 08/26/23 05:47:03.544
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 05:47:05.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6303" for this suite. 08/26/23 05:47:05.558
------------------------------
• [SLOW TEST] [28.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:46:37.462
    Aug 26 05:46:37.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 05:46:37.463
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:46:37.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:46:37.488
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 08/26/23 05:46:54.497
    STEP: Creating a ResourceQuota 08/26/23 05:46:59.504
    STEP: Ensuring resource quota status is calculated 08/26/23 05:46:59.511
    STEP: Creating a ConfigMap 08/26/23 05:47:01.517
    STEP: Ensuring resource quota status captures configMap creation 08/26/23 05:47:01.532
    STEP: Deleting a ConfigMap 08/26/23 05:47:03.537
    STEP: Ensuring resource quota status released usage 08/26/23 05:47:03.544
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:47:05.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6303" for this suite. 08/26/23 05:47:05.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:47:05.567
Aug 26 05:47:05.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename security-context 08/26/23 05:47:05.568
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:05.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:05.594
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/26/23 05:47:05.599
Aug 26 05:47:05.614: INFO: Waiting up to 5m0s for pod "security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca" in namespace "security-context-6093" to be "Succeeded or Failed"
Aug 26 05:47:05.623: INFO: Pod "security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.214845ms
Aug 26 05:47:07.628: INFO: Pod "security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013515921s
Aug 26 05:47:09.628: INFO: Pod "security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013850753s
STEP: Saw pod success 08/26/23 05:47:09.628
Aug 26 05:47:09.628: INFO: Pod "security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca" satisfied condition "Succeeded or Failed"
Aug 26 05:47:09.632: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca container test-container: <nil>
STEP: delete the pod 08/26/23 05:47:09.652
Aug 26 05:47:09.668: INFO: Waiting for pod security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca to disappear
Aug 26 05:47:09.672: INFO: Pod security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 26 05:47:09.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-6093" for this suite. 08/26/23 05:47:09.68
------------------------------
• [4.127 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:47:05.567
    Aug 26 05:47:05.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename security-context 08/26/23 05:47:05.568
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:05.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:05.594
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/26/23 05:47:05.599
    Aug 26 05:47:05.614: INFO: Waiting up to 5m0s for pod "security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca" in namespace "security-context-6093" to be "Succeeded or Failed"
    Aug 26 05:47:05.623: INFO: Pod "security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.214845ms
    Aug 26 05:47:07.628: INFO: Pod "security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013515921s
    Aug 26 05:47:09.628: INFO: Pod "security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013850753s
    STEP: Saw pod success 08/26/23 05:47:09.628
    Aug 26 05:47:09.628: INFO: Pod "security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca" satisfied condition "Succeeded or Failed"
    Aug 26 05:47:09.632: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca container test-container: <nil>
    STEP: delete the pod 08/26/23 05:47:09.652
    Aug 26 05:47:09.668: INFO: Waiting for pod security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca to disappear
    Aug 26 05:47:09.672: INFO: Pod security-context-4d9e2e72-69ac-43ec-a03e-60f41ff65dca no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:47:09.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-6093" for this suite. 08/26/23 05:47:09.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:47:09.697
Aug 26 05:47:09.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-probe 08/26/23 05:47:09.699
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:09.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:09.721
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08 in namespace container-probe-1643 08/26/23 05:47:09.724
Aug 26 05:47:09.736: INFO: Waiting up to 5m0s for pod "liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08" in namespace "container-probe-1643" to be "not pending"
Aug 26 05:47:09.740: INFO: Pod "liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08": Phase="Pending", Reason="", readiness=false. Elapsed: 3.887585ms
Aug 26 05:47:11.745: INFO: Pod "liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08": Phase="Running", Reason="", readiness=true. Elapsed: 2.009445082s
Aug 26 05:47:11.745: INFO: Pod "liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08" satisfied condition "not pending"
Aug 26 05:47:11.746: INFO: Started pod liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08 in namespace container-probe-1643
STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 05:47:11.746
Aug 26 05:47:11.750: INFO: Initial restart count of pod liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08 is 0
Aug 26 05:47:31.809: INFO: Restart count of pod container-probe-1643/liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08 is now 1 (20.058408461s elapsed)
STEP: deleting the pod 08/26/23 05:47:31.809
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 26 05:47:31.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1643" for this suite. 08/26/23 05:47:31.83
------------------------------
• [SLOW TEST] [22.145 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:47:09.697
    Aug 26 05:47:09.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-probe 08/26/23 05:47:09.699
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:09.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:09.721
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08 in namespace container-probe-1643 08/26/23 05:47:09.724
    Aug 26 05:47:09.736: INFO: Waiting up to 5m0s for pod "liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08" in namespace "container-probe-1643" to be "not pending"
    Aug 26 05:47:09.740: INFO: Pod "liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08": Phase="Pending", Reason="", readiness=false. Elapsed: 3.887585ms
    Aug 26 05:47:11.745: INFO: Pod "liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08": Phase="Running", Reason="", readiness=true. Elapsed: 2.009445082s
    Aug 26 05:47:11.745: INFO: Pod "liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08" satisfied condition "not pending"
    Aug 26 05:47:11.746: INFO: Started pod liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08 in namespace container-probe-1643
    STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 05:47:11.746
    Aug 26 05:47:11.750: INFO: Initial restart count of pod liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08 is 0
    Aug 26 05:47:31.809: INFO: Restart count of pod container-probe-1643/liveness-ee8a69b2-5ee6-49bf-984e-ac45171bdb08 is now 1 (20.058408461s elapsed)
    STEP: deleting the pod 08/26/23 05:47:31.809
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:47:31.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1643" for this suite. 08/26/23 05:47:31.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:47:31.842
Aug 26 05:47:31.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename init-container 08/26/23 05:47:31.843
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:31.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:31.87
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 08/26/23 05:47:31.875
Aug 26 05:47:31.875: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:47:36.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1677" for this suite. 08/26/23 05:47:36.81
------------------------------
• [4.975 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:47:31.842
    Aug 26 05:47:31.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename init-container 08/26/23 05:47:31.843
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:31.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:31.87
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 08/26/23 05:47:31.875
    Aug 26 05:47:31.875: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:47:36.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1677" for this suite. 08/26/23 05:47:36.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:47:36.818
Aug 26 05:47:36.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 05:47:36.819
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:36.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:36.838
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8395 08/26/23 05:47:36.841
STEP: changing the ExternalName service to type=NodePort 08/26/23 05:47:36.849
STEP: creating replication controller externalname-service in namespace services-8395 08/26/23 05:47:36.903
I0826 05:47:36.914255      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8395, replica count: 2
I0826 05:47:39.967238      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 26 05:47:39.967: INFO: Creating new exec pod
Aug 26 05:47:39.993: INFO: Waiting up to 5m0s for pod "execpodr9mgb" in namespace "services-8395" to be "running"
Aug 26 05:47:40.008: INFO: Pod "execpodr9mgb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.999941ms
Aug 26 05:47:42.016: INFO: Pod "execpodr9mgb": Phase="Running", Reason="", readiness=true. Elapsed: 2.022625418s
Aug 26 05:47:42.016: INFO: Pod "execpodr9mgb" satisfied condition "running"
Aug 26 05:47:43.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8395 exec execpodr9mgb -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 26 05:47:43.265: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 26 05:47:43.265: INFO: stdout: ""
Aug 26 05:47:43.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8395 exec execpodr9mgb -- /bin/sh -x -c nc -v -z -w 2 10.21.203.177 80'
Aug 26 05:47:43.448: INFO: stderr: "+ nc -v -z -w 2 10.21.203.177 80\nConnection to 10.21.203.177 80 port [tcp/http] succeeded!\n"
Aug 26 05:47:43.448: INFO: stdout: ""
Aug 26 05:47:43.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8395 exec execpodr9mgb -- /bin/sh -x -c nc -v -z -w 2 10.0.1.31 31801'
Aug 26 05:47:43.672: INFO: stderr: "+ nc -v -z -w 2 10.0.1.31 31801\nConnection to 10.0.1.31 31801 port [tcp/*] succeeded!\n"
Aug 26 05:47:43.673: INFO: stdout: ""
Aug 26 05:47:43.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8395 exec execpodr9mgb -- /bin/sh -x -c nc -v -z -w 2 10.0.1.5 31801'
Aug 26 05:47:43.856: INFO: stderr: "+ nc -v -z -w 2 10.0.1.5 31801\nConnection to 10.0.1.5 31801 port [tcp/*] succeeded!\n"
Aug 26 05:47:43.856: INFO: stdout: ""
Aug 26 05:47:43.856: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 05:47:43.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8395" for this suite. 08/26/23 05:47:43.931
------------------------------
• [SLOW TEST] [7.125 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:47:36.818
    Aug 26 05:47:36.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 05:47:36.819
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:36.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:36.838
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8395 08/26/23 05:47:36.841
    STEP: changing the ExternalName service to type=NodePort 08/26/23 05:47:36.849
    STEP: creating replication controller externalname-service in namespace services-8395 08/26/23 05:47:36.903
    I0826 05:47:36.914255      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8395, replica count: 2
    I0826 05:47:39.967238      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 26 05:47:39.967: INFO: Creating new exec pod
    Aug 26 05:47:39.993: INFO: Waiting up to 5m0s for pod "execpodr9mgb" in namespace "services-8395" to be "running"
    Aug 26 05:47:40.008: INFO: Pod "execpodr9mgb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.999941ms
    Aug 26 05:47:42.016: INFO: Pod "execpodr9mgb": Phase="Running", Reason="", readiness=true. Elapsed: 2.022625418s
    Aug 26 05:47:42.016: INFO: Pod "execpodr9mgb" satisfied condition "running"
    Aug 26 05:47:43.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8395 exec execpodr9mgb -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 26 05:47:43.265: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 26 05:47:43.265: INFO: stdout: ""
    Aug 26 05:47:43.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8395 exec execpodr9mgb -- /bin/sh -x -c nc -v -z -w 2 10.21.203.177 80'
    Aug 26 05:47:43.448: INFO: stderr: "+ nc -v -z -w 2 10.21.203.177 80\nConnection to 10.21.203.177 80 port [tcp/http] succeeded!\n"
    Aug 26 05:47:43.448: INFO: stdout: ""
    Aug 26 05:47:43.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8395 exec execpodr9mgb -- /bin/sh -x -c nc -v -z -w 2 10.0.1.31 31801'
    Aug 26 05:47:43.672: INFO: stderr: "+ nc -v -z -w 2 10.0.1.31 31801\nConnection to 10.0.1.31 31801 port [tcp/*] succeeded!\n"
    Aug 26 05:47:43.673: INFO: stdout: ""
    Aug 26 05:47:43.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8395 exec execpodr9mgb -- /bin/sh -x -c nc -v -z -w 2 10.0.1.5 31801'
    Aug 26 05:47:43.856: INFO: stderr: "+ nc -v -z -w 2 10.0.1.5 31801\nConnection to 10.0.1.5 31801 port [tcp/*] succeeded!\n"
    Aug 26 05:47:43.856: INFO: stdout: ""
    Aug 26 05:47:43.856: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:47:43.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8395" for this suite. 08/26/23 05:47:43.931
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:47:43.943
Aug 26 05:47:43.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-runtime 08/26/23 05:47:43.944
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:43.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:43.964
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 08/26/23 05:47:43.968
STEP: wait for the container to reach Succeeded 08/26/23 05:47:43.979
STEP: get the container status 08/26/23 05:47:48.011
STEP: the container should be terminated 08/26/23 05:47:48.015
STEP: the termination message should be set 08/26/23 05:47:48.015
Aug 26 05:47:48.015: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/26/23 05:47:48.015
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 26 05:47:48.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5055" for this suite. 08/26/23 05:47:48.051
------------------------------
• [4.124 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:47:43.943
    Aug 26 05:47:43.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-runtime 08/26/23 05:47:43.944
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:43.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:43.964
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 08/26/23 05:47:43.968
    STEP: wait for the container to reach Succeeded 08/26/23 05:47:43.979
    STEP: get the container status 08/26/23 05:47:48.011
    STEP: the container should be terminated 08/26/23 05:47:48.015
    STEP: the termination message should be set 08/26/23 05:47:48.015
    Aug 26 05:47:48.015: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/26/23 05:47:48.015
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:47:48.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5055" for this suite. 08/26/23 05:47:48.051
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:47:48.067
Aug 26 05:47:48.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replication-controller 08/26/23 05:47:48.069
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:48.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:48.105
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 08/26/23 05:47:48.114
STEP: waiting for RC to be added 08/26/23 05:47:48.122
STEP: waiting for available Replicas 08/26/23 05:47:48.122
STEP: patching ReplicationController 08/26/23 05:47:49.39
STEP: waiting for RC to be modified 08/26/23 05:47:49.4
STEP: patching ReplicationController status 08/26/23 05:47:49.401
STEP: waiting for RC to be modified 08/26/23 05:47:49.414
STEP: waiting for available Replicas 08/26/23 05:47:49.415
STEP: fetching ReplicationController status 08/26/23 05:47:49.427
STEP: patching ReplicationController scale 08/26/23 05:47:49.434
STEP: waiting for RC to be modified 08/26/23 05:47:49.442
STEP: waiting for ReplicationController's scale to be the max amount 08/26/23 05:47:49.442
STEP: fetching ReplicationController; ensuring that it's patched 08/26/23 05:47:50.89
STEP: updating ReplicationController status 08/26/23 05:47:50.895
STEP: waiting for RC to be modified 08/26/23 05:47:50.905
STEP: listing all ReplicationControllers 08/26/23 05:47:50.905
STEP: checking that ReplicationController has expected values 08/26/23 05:47:50.913
STEP: deleting ReplicationControllers by collection 08/26/23 05:47:50.913
STEP: waiting for ReplicationController to have a DELETED watchEvent 08/26/23 05:47:50.923
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 26 05:47:50.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5296" for this suite. 08/26/23 05:47:50.983
------------------------------
• [2.926 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:47:48.067
    Aug 26 05:47:48.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replication-controller 08/26/23 05:47:48.069
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:48.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:48.105
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 08/26/23 05:47:48.114
    STEP: waiting for RC to be added 08/26/23 05:47:48.122
    STEP: waiting for available Replicas 08/26/23 05:47:48.122
    STEP: patching ReplicationController 08/26/23 05:47:49.39
    STEP: waiting for RC to be modified 08/26/23 05:47:49.4
    STEP: patching ReplicationController status 08/26/23 05:47:49.401
    STEP: waiting for RC to be modified 08/26/23 05:47:49.414
    STEP: waiting for available Replicas 08/26/23 05:47:49.415
    STEP: fetching ReplicationController status 08/26/23 05:47:49.427
    STEP: patching ReplicationController scale 08/26/23 05:47:49.434
    STEP: waiting for RC to be modified 08/26/23 05:47:49.442
    STEP: waiting for ReplicationController's scale to be the max amount 08/26/23 05:47:49.442
    STEP: fetching ReplicationController; ensuring that it's patched 08/26/23 05:47:50.89
    STEP: updating ReplicationController status 08/26/23 05:47:50.895
    STEP: waiting for RC to be modified 08/26/23 05:47:50.905
    STEP: listing all ReplicationControllers 08/26/23 05:47:50.905
    STEP: checking that ReplicationController has expected values 08/26/23 05:47:50.913
    STEP: deleting ReplicationControllers by collection 08/26/23 05:47:50.913
    STEP: waiting for ReplicationController to have a DELETED watchEvent 08/26/23 05:47:50.923
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:47:50.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5296" for this suite. 08/26/23 05:47:50.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:47:50.995
Aug 26 05:47:50.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename endpointslice 08/26/23 05:47:50.996
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:51.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:51.018
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 08/26/23 05:47:51.021
STEP: getting /apis/discovery.k8s.io 08/26/23 05:47:51.023
STEP: getting /apis/discovery.k8s.iov1 08/26/23 05:47:51.023
STEP: creating 08/26/23 05:47:51.025
STEP: getting 08/26/23 05:47:51.046
STEP: listing 08/26/23 05:47:51.049
STEP: watching 08/26/23 05:47:51.052
Aug 26 05:47:51.053: INFO: starting watch
STEP: cluster-wide listing 08/26/23 05:47:51.054
STEP: cluster-wide watching 08/26/23 05:47:51.059
Aug 26 05:47:51.059: INFO: starting watch
STEP: patching 08/26/23 05:47:51.06
STEP: updating 08/26/23 05:47:51.067
Aug 26 05:47:51.077: INFO: waiting for watch events with expected annotations
Aug 26 05:47:51.077: INFO: saw patched and updated annotations
STEP: deleting 08/26/23 05:47:51.077
STEP: deleting a collection 08/26/23 05:47:51.091
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 26 05:47:51.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9438" for this suite. 08/26/23 05:47:51.118
------------------------------
• [0.131 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:47:50.995
    Aug 26 05:47:50.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename endpointslice 08/26/23 05:47:50.996
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:51.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:51.018
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 08/26/23 05:47:51.021
    STEP: getting /apis/discovery.k8s.io 08/26/23 05:47:51.023
    STEP: getting /apis/discovery.k8s.iov1 08/26/23 05:47:51.023
    STEP: creating 08/26/23 05:47:51.025
    STEP: getting 08/26/23 05:47:51.046
    STEP: listing 08/26/23 05:47:51.049
    STEP: watching 08/26/23 05:47:51.052
    Aug 26 05:47:51.053: INFO: starting watch
    STEP: cluster-wide listing 08/26/23 05:47:51.054
    STEP: cluster-wide watching 08/26/23 05:47:51.059
    Aug 26 05:47:51.059: INFO: starting watch
    STEP: patching 08/26/23 05:47:51.06
    STEP: updating 08/26/23 05:47:51.067
    Aug 26 05:47:51.077: INFO: waiting for watch events with expected annotations
    Aug 26 05:47:51.077: INFO: saw patched and updated annotations
    STEP: deleting 08/26/23 05:47:51.077
    STEP: deleting a collection 08/26/23 05:47:51.091
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:47:51.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9438" for this suite. 08/26/23 05:47:51.118
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:47:51.126
Aug 26 05:47:51.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename var-expansion 08/26/23 05:47:51.127
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:51.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:51.145
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Aug 26 05:47:51.156: INFO: Waiting up to 2m0s for pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de" in namespace "var-expansion-6910" to be "container 0 failed with reason CreateContainerConfigError"
Aug 26 05:47:51.160: INFO: Pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.440002ms
Aug 26 05:47:53.164: INFO: Pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00800602s
Aug 26 05:47:53.164: INFO: Pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 26 05:47:53.164: INFO: Deleting pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de" in namespace "var-expansion-6910"
Aug 26 05:47:53.177: INFO: Wait up to 5m0s for pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 26 05:47:57.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6910" for this suite. 08/26/23 05:47:57.195
------------------------------
• [SLOW TEST] [6.077 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:47:51.126
    Aug 26 05:47:51.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename var-expansion 08/26/23 05:47:51.127
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:51.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:51.145
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Aug 26 05:47:51.156: INFO: Waiting up to 2m0s for pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de" in namespace "var-expansion-6910" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 26 05:47:51.160: INFO: Pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.440002ms
    Aug 26 05:47:53.164: INFO: Pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00800602s
    Aug 26 05:47:53.164: INFO: Pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 26 05:47:53.164: INFO: Deleting pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de" in namespace "var-expansion-6910"
    Aug 26 05:47:53.177: INFO: Wait up to 5m0s for pod "var-expansion-95dd052c-2d95-4cdf-8d90-82ea1f3ab5de" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:47:57.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6910" for this suite. 08/26/23 05:47:57.195
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:47:57.204
Aug 26 05:47:57.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 05:47:57.205
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:57.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:57.228
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 08/26/23 05:47:57.23
Aug 26 05:47:57.241: INFO: Waiting up to 5m0s for pod "pod-3e552a34-149a-4e11-9729-cdb03d46ba32" in namespace "emptydir-1615" to be "Succeeded or Failed"
Aug 26 05:47:57.245: INFO: Pod "pod-3e552a34-149a-4e11-9729-cdb03d46ba32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.732273ms
Aug 26 05:47:59.250: INFO: Pod "pod-3e552a34-149a-4e11-9729-cdb03d46ba32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008875792s
Aug 26 05:48:01.255: INFO: Pod "pod-3e552a34-149a-4e11-9729-cdb03d46ba32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014181596s
STEP: Saw pod success 08/26/23 05:48:01.255
Aug 26 05:48:01.255: INFO: Pod "pod-3e552a34-149a-4e11-9729-cdb03d46ba32" satisfied condition "Succeeded or Failed"
Aug 26 05:48:01.260: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-3e552a34-149a-4e11-9729-cdb03d46ba32 container test-container: <nil>
STEP: delete the pod 08/26/23 05:48:01.269
Aug 26 05:48:01.290: INFO: Waiting for pod pod-3e552a34-149a-4e11-9729-cdb03d46ba32 to disappear
Aug 26 05:48:01.295: INFO: Pod pod-3e552a34-149a-4e11-9729-cdb03d46ba32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 05:48:01.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1615" for this suite. 08/26/23 05:48:01.302
------------------------------
• [4.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:47:57.204
    Aug 26 05:47:57.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 05:47:57.205
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:47:57.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:47:57.228
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/26/23 05:47:57.23
    Aug 26 05:47:57.241: INFO: Waiting up to 5m0s for pod "pod-3e552a34-149a-4e11-9729-cdb03d46ba32" in namespace "emptydir-1615" to be "Succeeded or Failed"
    Aug 26 05:47:57.245: INFO: Pod "pod-3e552a34-149a-4e11-9729-cdb03d46ba32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.732273ms
    Aug 26 05:47:59.250: INFO: Pod "pod-3e552a34-149a-4e11-9729-cdb03d46ba32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008875792s
    Aug 26 05:48:01.255: INFO: Pod "pod-3e552a34-149a-4e11-9729-cdb03d46ba32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014181596s
    STEP: Saw pod success 08/26/23 05:48:01.255
    Aug 26 05:48:01.255: INFO: Pod "pod-3e552a34-149a-4e11-9729-cdb03d46ba32" satisfied condition "Succeeded or Failed"
    Aug 26 05:48:01.260: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-3e552a34-149a-4e11-9729-cdb03d46ba32 container test-container: <nil>
    STEP: delete the pod 08/26/23 05:48:01.269
    Aug 26 05:48:01.290: INFO: Waiting for pod pod-3e552a34-149a-4e11-9729-cdb03d46ba32 to disappear
    Aug 26 05:48:01.295: INFO: Pod pod-3e552a34-149a-4e11-9729-cdb03d46ba32 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:48:01.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1615" for this suite. 08/26/23 05:48:01.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:48:01.313
Aug 26 05:48:01.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 05:48:01.314
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:01.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:01.334
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-f4072a95-767c-4358-a422-39373b9d7e13 08/26/23 05:48:01.336
STEP: Creating a pod to test consume configMaps 08/26/23 05:48:01.345
Aug 26 05:48:01.358: INFO: Waiting up to 5m0s for pod "pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900" in namespace "configmap-5510" to be "Succeeded or Failed"
Aug 26 05:48:01.369: INFO: Pod "pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900": Phase="Pending", Reason="", readiness=false. Elapsed: 11.763832ms
Aug 26 05:48:03.376: INFO: Pod "pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017926202s
Aug 26 05:48:05.376: INFO: Pod "pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018038469s
STEP: Saw pod success 08/26/23 05:48:05.376
Aug 26 05:48:05.376: INFO: Pod "pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900" satisfied condition "Succeeded or Failed"
Aug 26 05:48:05.380: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900 container configmap-volume-test: <nil>
STEP: delete the pod 08/26/23 05:48:05.388
Aug 26 05:48:05.401: INFO: Waiting for pod pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900 to disappear
Aug 26 05:48:05.405: INFO: Pod pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:48:05.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5510" for this suite. 08/26/23 05:48:05.414
------------------------------
• [4.109 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:48:01.313
    Aug 26 05:48:01.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 05:48:01.314
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:01.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:01.334
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-f4072a95-767c-4358-a422-39373b9d7e13 08/26/23 05:48:01.336
    STEP: Creating a pod to test consume configMaps 08/26/23 05:48:01.345
    Aug 26 05:48:01.358: INFO: Waiting up to 5m0s for pod "pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900" in namespace "configmap-5510" to be "Succeeded or Failed"
    Aug 26 05:48:01.369: INFO: Pod "pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900": Phase="Pending", Reason="", readiness=false. Elapsed: 11.763832ms
    Aug 26 05:48:03.376: INFO: Pod "pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017926202s
    Aug 26 05:48:05.376: INFO: Pod "pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018038469s
    STEP: Saw pod success 08/26/23 05:48:05.376
    Aug 26 05:48:05.376: INFO: Pod "pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900" satisfied condition "Succeeded or Failed"
    Aug 26 05:48:05.380: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900 container configmap-volume-test: <nil>
    STEP: delete the pod 08/26/23 05:48:05.388
    Aug 26 05:48:05.401: INFO: Waiting for pod pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900 to disappear
    Aug 26 05:48:05.405: INFO: Pod pod-configmaps-c020ef4a-c9c1-4609-8432-1854182c9900 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:48:05.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5510" for this suite. 08/26/23 05:48:05.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:48:05.423
Aug 26 05:48:05.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replication-controller 08/26/23 05:48:05.424
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:05.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:05.45
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 08/26/23 05:48:05.453
Aug 26 05:48:05.466: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7476" to be "running and ready"
Aug 26 05:48:05.475: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.617932ms
Aug 26 05:48:05.475: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:48:07.480: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.013809265s
Aug 26 05:48:07.480: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Aug 26 05:48:07.480: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 08/26/23 05:48:07.486
STEP: Then the orphan pod is adopted 08/26/23 05:48:07.498
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 26 05:48:08.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7476" for this suite. 08/26/23 05:48:08.528
------------------------------
• [3.114 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:48:05.423
    Aug 26 05:48:05.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replication-controller 08/26/23 05:48:05.424
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:05.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:05.45
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 08/26/23 05:48:05.453
    Aug 26 05:48:05.466: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7476" to be "running and ready"
    Aug 26 05:48:05.475: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.617932ms
    Aug 26 05:48:05.475: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:48:07.480: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.013809265s
    Aug 26 05:48:07.480: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Aug 26 05:48:07.480: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 08/26/23 05:48:07.486
    STEP: Then the orphan pod is adopted 08/26/23 05:48:07.498
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:48:08.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7476" for this suite. 08/26/23 05:48:08.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:48:08.538
Aug 26 05:48:08.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:48:08.539
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:08.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:08.56
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:48:08.564
Aug 26 05:48:08.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f" in namespace "downward-api-6645" to be "Succeeded or Failed"
Aug 26 05:48:08.580: INFO: Pod "downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.794885ms
Aug 26 05:48:10.585: INFO: Pod "downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009818125s
Aug 26 05:48:12.587: INFO: Pod "downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011204509s
STEP: Saw pod success 08/26/23 05:48:12.587
Aug 26 05:48:12.587: INFO: Pod "downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f" satisfied condition "Succeeded or Failed"
Aug 26 05:48:12.596: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f container client-container: <nil>
STEP: delete the pod 08/26/23 05:48:12.604
Aug 26 05:48:12.624: INFO: Waiting for pod downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f to disappear
Aug 26 05:48:12.629: INFO: Pod downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 05:48:12.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6645" for this suite. 08/26/23 05:48:12.639
------------------------------
• [4.111 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:48:08.538
    Aug 26 05:48:08.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:48:08.539
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:08.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:08.56
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:48:08.564
    Aug 26 05:48:08.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f" in namespace "downward-api-6645" to be "Succeeded or Failed"
    Aug 26 05:48:08.580: INFO: Pod "downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.794885ms
    Aug 26 05:48:10.585: INFO: Pod "downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009818125s
    Aug 26 05:48:12.587: INFO: Pod "downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011204509s
    STEP: Saw pod success 08/26/23 05:48:12.587
    Aug 26 05:48:12.587: INFO: Pod "downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f" satisfied condition "Succeeded or Failed"
    Aug 26 05:48:12.596: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f container client-container: <nil>
    STEP: delete the pod 08/26/23 05:48:12.604
    Aug 26 05:48:12.624: INFO: Waiting for pod downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f to disappear
    Aug 26 05:48:12.629: INFO: Pod downwardapi-volume-61cf510e-3c5f-46cf-9833-33d570fb3b2f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:48:12.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6645" for this suite. 08/26/23 05:48:12.639
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:48:12.649
Aug 26 05:48:12.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 05:48:12.653
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:12.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:12.68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 08/26/23 05:48:12.683
Aug 26 05:48:12.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: mark a version not serverd 08/26/23 05:48:17.304
STEP: check the unserved version gets removed 08/26/23 05:48:17.33
STEP: check the other version is not changed 08/26/23 05:48:19.946
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:48:23.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6825" for this suite. 08/26/23 05:48:23.611
------------------------------
• [SLOW TEST] [10.969 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:48:12.649
    Aug 26 05:48:12.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 05:48:12.653
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:12.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:12.68
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 08/26/23 05:48:12.683
    Aug 26 05:48:12.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: mark a version not serverd 08/26/23 05:48:17.304
    STEP: check the unserved version gets removed 08/26/23 05:48:17.33
    STEP: check the other version is not changed 08/26/23 05:48:19.946
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:48:23.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6825" for this suite. 08/26/23 05:48:23.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:48:23.619
Aug 26 05:48:23.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename dns 08/26/23 05:48:23.62
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:23.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:23.635
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 08/26/23 05:48:23.637
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 40.254.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.254.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.254.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.254.40_tcp@PTR;sleep 1; done
 08/26/23 05:48:23.662
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 40.254.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.254.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.254.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.254.40_tcp@PTR;sleep 1; done
 08/26/23 05:48:23.662
STEP: creating a pod to probe DNS 08/26/23 05:48:23.662
STEP: submitting the pod to kubernetes 08/26/23 05:48:23.662
Aug 26 05:48:23.680: INFO: Waiting up to 15m0s for pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4" in namespace "dns-9324" to be "running"
Aug 26 05:48:23.688: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.711202ms
Aug 26 05:48:25.694: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013963748s
Aug 26 05:48:27.693: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012974558s
Aug 26 05:48:29.693: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012942403s
Aug 26 05:48:31.698: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01758904s
Aug 26 05:48:33.694: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Running", Reason="", readiness=true. Elapsed: 10.013680518s
Aug 26 05:48:33.694: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4" satisfied condition "running"
STEP: retrieving the pod 08/26/23 05:48:33.694
STEP: looking for the results for each expected name from probers 08/26/23 05:48:33.699
Aug 26 05:48:33.705: INFO: Unable to read wheezy_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:33.710: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:33.716: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:33.722: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:33.745: INFO: Unable to read jessie_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:33.750: INFO: Unable to read jessie_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:33.756: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:33.761: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:33.780: INFO: Lookups using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 failed for: [wheezy_udp@dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_udp@dns-test-service.dns-9324.svc.cluster.local jessie_tcp@dns-test-service.dns-9324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local]

Aug 26 05:48:38.787: INFO: Unable to read wheezy_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:38.793: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:38.797: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:38.803: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:38.824: INFO: Unable to read jessie_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:38.829: INFO: Unable to read jessie_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:38.833: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:38.837: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:38.855: INFO: Lookups using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 failed for: [wheezy_udp@dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_udp@dns-test-service.dns-9324.svc.cluster.local jessie_tcp@dns-test-service.dns-9324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local]

Aug 26 05:48:43.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:43.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:43.795: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:43.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:43.824: INFO: Unable to read jessie_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:43.828: INFO: Unable to read jessie_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:43.834: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:43.838: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:43.871: INFO: Lookups using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 failed for: [wheezy_udp@dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_udp@dns-test-service.dns-9324.svc.cluster.local jessie_tcp@dns-test-service.dns-9324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local]

Aug 26 05:48:48.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:48.790: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:48.797: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:48.802: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:48.829: INFO: Unable to read jessie_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:48.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:48.837: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:48.840: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:48.863: INFO: Lookups using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 failed for: [wheezy_udp@dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_udp@dns-test-service.dns-9324.svc.cluster.local jessie_tcp@dns-test-service.dns-9324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local]

Aug 26 05:48:53.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:53.798: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:53.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:53.806: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:53.830: INFO: Unable to read jessie_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:53.834: INFO: Unable to read jessie_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:53.838: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:53.842: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
Aug 26 05:48:53.856: INFO: Lookups using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 failed for: [wheezy_udp@dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_udp@dns-test-service.dns-9324.svc.cluster.local jessie_tcp@dns-test-service.dns-9324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local]

Aug 26 05:48:58.922: INFO: DNS probes using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 succeeded

STEP: deleting the pod 08/26/23 05:48:58.922
STEP: deleting the test service 08/26/23 05:48:58.94
STEP: deleting the test headless service 08/26/23 05:48:58.975
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 26 05:48:59.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9324" for this suite. 08/26/23 05:48:59.026
------------------------------
• [SLOW TEST] [35.414 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:48:23.619
    Aug 26 05:48:23.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename dns 08/26/23 05:48:23.62
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:23.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:23.635
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 08/26/23 05:48:23.637
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 40.254.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.254.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.254.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.254.40_tcp@PTR;sleep 1; done
     08/26/23 05:48:23.662
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 40.254.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.254.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.254.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.254.40_tcp@PTR;sleep 1; done
     08/26/23 05:48:23.662
    STEP: creating a pod to probe DNS 08/26/23 05:48:23.662
    STEP: submitting the pod to kubernetes 08/26/23 05:48:23.662
    Aug 26 05:48:23.680: INFO: Waiting up to 15m0s for pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4" in namespace "dns-9324" to be "running"
    Aug 26 05:48:23.688: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.711202ms
    Aug 26 05:48:25.694: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013963748s
    Aug 26 05:48:27.693: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012974558s
    Aug 26 05:48:29.693: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012942403s
    Aug 26 05:48:31.698: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01758904s
    Aug 26 05:48:33.694: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4": Phase="Running", Reason="", readiness=true. Elapsed: 10.013680518s
    Aug 26 05:48:33.694: INFO: Pod "dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4" satisfied condition "running"
    STEP: retrieving the pod 08/26/23 05:48:33.694
    STEP: looking for the results for each expected name from probers 08/26/23 05:48:33.699
    Aug 26 05:48:33.705: INFO: Unable to read wheezy_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:33.710: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:33.716: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:33.722: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:33.745: INFO: Unable to read jessie_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:33.750: INFO: Unable to read jessie_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:33.756: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:33.761: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:33.780: INFO: Lookups using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 failed for: [wheezy_udp@dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_udp@dns-test-service.dns-9324.svc.cluster.local jessie_tcp@dns-test-service.dns-9324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local]

    Aug 26 05:48:38.787: INFO: Unable to read wheezy_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:38.793: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:38.797: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:38.803: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:38.824: INFO: Unable to read jessie_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:38.829: INFO: Unable to read jessie_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:38.833: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:38.837: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:38.855: INFO: Lookups using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 failed for: [wheezy_udp@dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_udp@dns-test-service.dns-9324.svc.cluster.local jessie_tcp@dns-test-service.dns-9324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local]

    Aug 26 05:48:43.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:43.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:43.795: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:43.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:43.824: INFO: Unable to read jessie_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:43.828: INFO: Unable to read jessie_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:43.834: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:43.838: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:43.871: INFO: Lookups using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 failed for: [wheezy_udp@dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_udp@dns-test-service.dns-9324.svc.cluster.local jessie_tcp@dns-test-service.dns-9324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local]

    Aug 26 05:48:48.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:48.790: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:48.797: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:48.802: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:48.829: INFO: Unable to read jessie_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:48.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:48.837: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:48.840: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:48.863: INFO: Lookups using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 failed for: [wheezy_udp@dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_udp@dns-test-service.dns-9324.svc.cluster.local jessie_tcp@dns-test-service.dns-9324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local]

    Aug 26 05:48:53.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:53.798: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:53.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:53.806: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:53.830: INFO: Unable to read jessie_udp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:53.834: INFO: Unable to read jessie_tcp@dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:53.838: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:53.842: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local from pod dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4: the server could not find the requested resource (get pods dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4)
    Aug 26 05:48:53.856: INFO: Lookups using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 failed for: [wheezy_udp@dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@dns-test-service.dns-9324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_udp@dns-test-service.dns-9324.svc.cluster.local jessie_tcp@dns-test-service.dns-9324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9324.svc.cluster.local]

    Aug 26 05:48:58.922: INFO: DNS probes using dns-9324/dns-test-3866d80a-8871-4c2f-b2de-3a4448218fd4 succeeded

    STEP: deleting the pod 08/26/23 05:48:58.922
    STEP: deleting the test service 08/26/23 05:48:58.94
    STEP: deleting the test headless service 08/26/23 05:48:58.975
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:48:59.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9324" for this suite. 08/26/23 05:48:59.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:48:59.034
Aug 26 05:48:59.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sched-pred 08/26/23 05:48:59.036
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:59.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:59.064
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 26 05:48:59.067: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 26 05:48:59.085: INFO: Waiting for terminating namespaces to be deleted...
Aug 26 05:48:59.089: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-101.us-west-2.compute.internal before test
Aug 26 05:48:59.118: INFO: calico-node-hgm7c from kube-system started at 2023-08-26 04:58:59 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.118: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 05:48:59.119: INFO: calico-typha-6dd9648c8f-85hjp from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.119: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 05:48:59.119: INFO: metrics-server-b7db9955-nwll9 from kube-system started at 2023-08-26 05:48:25 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.119: INFO: 	Container metrics-server ready: true, restart count 0
Aug 26 05:48:59.119: INFO: sonobuoy from sonobuoy started at 2023-08-26 05:16:19 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.119: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 26 05:48:59.119: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:48:59.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:48:59.119: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 05:48:59.119: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-126.us-west-2.compute.internal before test
Aug 26 05:48:59.148: INFO: calico-node-8272m from kube-system started at 2023-08-26 04:58:47 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.148: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 05:48:59.148: INFO: calico-typha-6dd9648c8f-th65n from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.148: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 05:48:59.148: INFO: sonobuoy-e2e-job-c6e00385bb4e4fb8 from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:48:59.148: INFO: 	Container e2e ready: true, restart count 0
Aug 26 05:48:59.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:48:59.148: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-ckdgn from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:48:59.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:48:59.148: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 05:48:59.148: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-23.us-west-2.compute.internal before test
Aug 26 05:48:59.183: INFO: calico-kube-controllers-76798f54cb-cdxwm from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.183: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 26 05:48:59.183: INFO: calico-node-trdnz from kube-system started at 2023-08-26 04:59:15 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.183: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 05:48:59.183: INFO: calico-typha-6dd9648c8f-x4b7r from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.183: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 05:48:59.183: INFO: calico-typha-autoscaler-54c8866496-tmj59 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.183: INFO: 	Container autoscaler ready: true, restart count 0
Aug 26 05:48:59.183: INFO: coredns-58ffcc48df-2q4rv from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.183: INFO: 	Container coredns ready: true, restart count 0
Aug 26 05:48:59.183: INFO: kube-dns-autoscaler-f68f756b6-78kdn from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.183: INFO: 	Container autoscaler ready: true, restart count 0
Aug 26 05:48:59.183: INFO: kube-state-metrics-99bbfb4cd-g7vn6 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.183: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 26 05:48:59.183: INFO: dashboard-metrics-scraper-6c57f89c7c-2vtv5 from kubernetes-dashboard started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.183: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 26 05:48:59.183: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-g65bg from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:48:59.183: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:48:59.183: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 05:48:59.183: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-31.us-west-2.compute.internal before test
Aug 26 05:48:59.218: INFO: calico-node-mxmcm from kube-system started at 2023-08-26 04:59:09 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.218: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 05:48:59.218: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-6gxwx from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:48:59.218: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:48:59.218: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 05:48:59.218: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-5.us-west-2.compute.internal before test
Aug 26 05:48:59.244: INFO: calico-node-jftpx from kube-system started at 2023-08-26 04:58:49 +0000 UTC (1 container statuses recorded)
Aug 26 05:48:59.244: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 05:48:59.244: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-tpq9m from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:48:59.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:48:59.244: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node ip-10-0-1-101.us-west-2.compute.internal 08/26/23 05:48:59.292
STEP: verifying the node has the label node ip-10-0-1-126.us-west-2.compute.internal 08/26/23 05:48:59.308
STEP: verifying the node has the label node ip-10-0-1-23.us-west-2.compute.internal 08/26/23 05:48:59.327
STEP: verifying the node has the label node ip-10-0-1-31.us-west-2.compute.internal 08/26/23 05:48:59.345
STEP: verifying the node has the label node ip-10-0-1-5.us-west-2.compute.internal 08/26/23 05:48:59.369
Aug 26 05:48:59.409: INFO: Pod calico-kube-controllers-76798f54cb-cdxwm requesting resource cpu=1m on Node ip-10-0-1-23.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod calico-node-8272m requesting resource cpu=250m on Node ip-10-0-1-126.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod calico-node-hgm7c requesting resource cpu=250m on Node ip-10-0-1-101.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod calico-node-jftpx requesting resource cpu=250m on Node ip-10-0-1-5.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod calico-node-mxmcm requesting resource cpu=250m on Node ip-10-0-1-31.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod calico-node-trdnz requesting resource cpu=250m on Node ip-10-0-1-23.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod calico-typha-6dd9648c8f-85hjp requesting resource cpu=10m on Node ip-10-0-1-101.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod calico-typha-6dd9648c8f-th65n requesting resource cpu=10m on Node ip-10-0-1-126.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod calico-typha-6dd9648c8f-x4b7r requesting resource cpu=10m on Node ip-10-0-1-23.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod calico-typha-autoscaler-54c8866496-tmj59 requesting resource cpu=20m on Node ip-10-0-1-23.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod coredns-58ffcc48df-2q4rv requesting resource cpu=100m on Node ip-10-0-1-23.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod kube-dns-autoscaler-f68f756b6-78kdn requesting resource cpu=20m on Node ip-10-0-1-23.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod kube-state-metrics-99bbfb4cd-g7vn6 requesting resource cpu=0m on Node ip-10-0-1-23.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod metrics-server-b7db9955-nwll9 requesting resource cpu=100m on Node ip-10-0-1-101.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod dashboard-metrics-scraper-6c57f89c7c-2vtv5 requesting resource cpu=0m on Node ip-10-0-1-23.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-1-101.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod sonobuoy-e2e-job-c6e00385bb4e4fb8 requesting resource cpu=0m on Node ip-10-0-1-126.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-6gxwx requesting resource cpu=0m on Node ip-10-0-1-31.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-ckdgn requesting resource cpu=0m on Node ip-10-0-1-126.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-g65bg requesting resource cpu=0m on Node ip-10-0-1-23.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c requesting resource cpu=0m on Node ip-10-0-1-101.us-west-2.compute.internal
Aug 26 05:48:59.409: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-tpq9m requesting resource cpu=0m on Node ip-10-0-1-5.us-west-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU. 08/26/23 05:48:59.409
Aug 26 05:48:59.409: INFO: Creating a pod which consumes cpu=2548m on Node ip-10-0-1-101.us-west-2.compute.internal
Aug 26 05:48:59.421: INFO: Creating a pod which consumes cpu=2618m on Node ip-10-0-1-126.us-west-2.compute.internal
Aug 26 05:48:59.432: INFO: Creating a pod which consumes cpu=2519m on Node ip-10-0-1-23.us-west-2.compute.internal
Aug 26 05:48:59.446: INFO: Creating a pod which consumes cpu=2625m on Node ip-10-0-1-31.us-west-2.compute.internal
Aug 26 05:48:59.462: INFO: Creating a pod which consumes cpu=2625m on Node ip-10-0-1-5.us-west-2.compute.internal
Aug 26 05:48:59.474: INFO: Waiting up to 5m0s for pod "filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db" in namespace "sched-pred-6517" to be "running"
Aug 26 05:48:59.483: INFO: Pod "filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.727405ms
Aug 26 05:49:01.488: INFO: Pod "filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db": Phase="Running", Reason="", readiness=true. Elapsed: 2.013309003s
Aug 26 05:49:01.488: INFO: Pod "filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db" satisfied condition "running"
Aug 26 05:49:01.488: INFO: Waiting up to 5m0s for pod "filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90" in namespace "sched-pred-6517" to be "running"
Aug 26 05:49:01.491: INFO: Pod "filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90": Phase="Running", Reason="", readiness=true. Elapsed: 3.430165ms
Aug 26 05:49:01.491: INFO: Pod "filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90" satisfied condition "running"
Aug 26 05:49:01.491: INFO: Waiting up to 5m0s for pod "filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b" in namespace "sched-pred-6517" to be "running"
Aug 26 05:49:01.494: INFO: Pod "filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b": Phase="Running", Reason="", readiness=true. Elapsed: 3.25647ms
Aug 26 05:49:01.494: INFO: Pod "filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b" satisfied condition "running"
Aug 26 05:49:01.494: INFO: Waiting up to 5m0s for pod "filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745" in namespace "sched-pred-6517" to be "running"
Aug 26 05:49:01.503: INFO: Pod "filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745": Phase="Running", Reason="", readiness=true. Elapsed: 8.337987ms
Aug 26 05:49:01.503: INFO: Pod "filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745" satisfied condition "running"
Aug 26 05:49:01.503: INFO: Waiting up to 5m0s for pod "filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306" in namespace "sched-pred-6517" to be "running"
Aug 26 05:49:01.508: INFO: Pod "filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306": Phase="Running", Reason="", readiness=true. Elapsed: 5.086098ms
Aug 26 05:49:01.508: INFO: Pod "filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 08/26/23 05:49:01.508
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b.177ed8d9b32c694d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6517/filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b to ip-10-0-1-23.us-west-2.compute.internal] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b.177ed8d9dcd83100], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b.177ed8d9fa5f5876], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 495.350396ms (495.366855ms including waiting)] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b.177ed8d9fb7e796c], Reason = [Created], Message = [Created container filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b.177ed8da01e62dad], Reason = [Started], Message = [Started container filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90.177ed8d9b27e186d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6517/filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90 to ip-10-0-1-126.us-west-2.compute.internal] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90.177ed8d9e18f1702], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90.177ed8d9e2eb0e57], Reason = [Created], Message = [Created container filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90.177ed8d9ea88fd57], Reason = [Started], Message = [Started container filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306.177ed8d9b4757371], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6517/filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306 to ip-10-0-1-5.us-west-2.compute.internal] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306.177ed8d9e76b82a7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306.177ed8d9e8d5123b], Reason = [Created], Message = [Created container filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306.177ed8d9f23ae91b], Reason = [Started], Message = [Started container filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db.177ed8d9b151eb08], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6517/filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db to ip-10-0-1-101.us-west-2.compute.internal] 08/26/23 05:49:01.514
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db.177ed8d9dc612ba0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/26/23 05:49:01.515
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db.177ed8d9de1cf95a], Reason = [Created], Message = [Created container filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db] 08/26/23 05:49:01.515
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db.177ed8d9e7c60a9b], Reason = [Started], Message = [Started container filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db] 08/26/23 05:49:01.515
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745.177ed8d9b422e1b1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6517/filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745 to ip-10-0-1-31.us-west-2.compute.internal] 08/26/23 05:49:01.515
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745.177ed8d9e5e3eb05], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/26/23 05:49:01.515
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745.177ed8d9e727007b], Reason = [Created], Message = [Created container filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745] 08/26/23 05:49:01.515
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745.177ed8d9ed23d843], Reason = [Started], Message = [Started container filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745] 08/26/23 05:49:01.515
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.177ed8da2ed34ba5], Reason = [FailedScheduling], Message = [0/8 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}, 5 Insufficient cpu. preemption: 0/8 nodes are available: 3 Preemption is not helpful for scheduling, 5 No preemption victims found for incoming pod..] 08/26/23 05:49:01.545
STEP: removing the label node off the node ip-10-0-1-5.us-west-2.compute.internal 08/26/23 05:49:02.544
STEP: verifying the node doesn't have the label node 08/26/23 05:49:02.564
STEP: removing the label node off the node ip-10-0-1-101.us-west-2.compute.internal 08/26/23 05:49:02.569
STEP: verifying the node doesn't have the label node 08/26/23 05:49:02.583
STEP: removing the label node off the node ip-10-0-1-126.us-west-2.compute.internal 08/26/23 05:49:02.587
STEP: verifying the node doesn't have the label node 08/26/23 05:49:02.602
STEP: removing the label node off the node ip-10-0-1-23.us-west-2.compute.internal 08/26/23 05:49:02.606
STEP: verifying the node doesn't have the label node 08/26/23 05:49:02.626
STEP: removing the label node off the node ip-10-0-1-31.us-west-2.compute.internal 08/26/23 05:49:02.63
STEP: verifying the node doesn't have the label node 08/26/23 05:49:02.644
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:02.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6517" for this suite. 08/26/23 05:49:02.659
------------------------------
• [3.631 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:48:59.034
    Aug 26 05:48:59.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sched-pred 08/26/23 05:48:59.036
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:48:59.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:48:59.064
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 26 05:48:59.067: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 26 05:48:59.085: INFO: Waiting for terminating namespaces to be deleted...
    Aug 26 05:48:59.089: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-101.us-west-2.compute.internal before test
    Aug 26 05:48:59.118: INFO: calico-node-hgm7c from kube-system started at 2023-08-26 04:58:59 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.118: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 05:48:59.119: INFO: calico-typha-6dd9648c8f-85hjp from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.119: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 05:48:59.119: INFO: metrics-server-b7db9955-nwll9 from kube-system started at 2023-08-26 05:48:25 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.119: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 26 05:48:59.119: INFO: sonobuoy from sonobuoy started at 2023-08-26 05:16:19 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.119: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 26 05:48:59.119: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:48:59.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:48:59.119: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 05:48:59.119: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-126.us-west-2.compute.internal before test
    Aug 26 05:48:59.148: INFO: calico-node-8272m from kube-system started at 2023-08-26 04:58:47 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.148: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 05:48:59.148: INFO: calico-typha-6dd9648c8f-th65n from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.148: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 05:48:59.148: INFO: sonobuoy-e2e-job-c6e00385bb4e4fb8 from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:48:59.148: INFO: 	Container e2e ready: true, restart count 0
    Aug 26 05:48:59.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:48:59.148: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-ckdgn from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:48:59.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:48:59.148: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 05:48:59.148: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-23.us-west-2.compute.internal before test
    Aug 26 05:48:59.183: INFO: calico-kube-controllers-76798f54cb-cdxwm from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.183: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 26 05:48:59.183: INFO: calico-node-trdnz from kube-system started at 2023-08-26 04:59:15 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.183: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 05:48:59.183: INFO: calico-typha-6dd9648c8f-x4b7r from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.183: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 05:48:59.183: INFO: calico-typha-autoscaler-54c8866496-tmj59 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.183: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 26 05:48:59.183: INFO: coredns-58ffcc48df-2q4rv from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.183: INFO: 	Container coredns ready: true, restart count 0
    Aug 26 05:48:59.183: INFO: kube-dns-autoscaler-f68f756b6-78kdn from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.183: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 26 05:48:59.183: INFO: kube-state-metrics-99bbfb4cd-g7vn6 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.183: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 26 05:48:59.183: INFO: dashboard-metrics-scraper-6c57f89c7c-2vtv5 from kubernetes-dashboard started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.183: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Aug 26 05:48:59.183: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-g65bg from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:48:59.183: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:48:59.183: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 05:48:59.183: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-31.us-west-2.compute.internal before test
    Aug 26 05:48:59.218: INFO: calico-node-mxmcm from kube-system started at 2023-08-26 04:59:09 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.218: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 05:48:59.218: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-6gxwx from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:48:59.218: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:48:59.218: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 05:48:59.218: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-5.us-west-2.compute.internal before test
    Aug 26 05:48:59.244: INFO: calico-node-jftpx from kube-system started at 2023-08-26 04:58:49 +0000 UTC (1 container statuses recorded)
    Aug 26 05:48:59.244: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 05:48:59.244: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-tpq9m from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:48:59.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:48:59.244: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node ip-10-0-1-101.us-west-2.compute.internal 08/26/23 05:48:59.292
    STEP: verifying the node has the label node ip-10-0-1-126.us-west-2.compute.internal 08/26/23 05:48:59.308
    STEP: verifying the node has the label node ip-10-0-1-23.us-west-2.compute.internal 08/26/23 05:48:59.327
    STEP: verifying the node has the label node ip-10-0-1-31.us-west-2.compute.internal 08/26/23 05:48:59.345
    STEP: verifying the node has the label node ip-10-0-1-5.us-west-2.compute.internal 08/26/23 05:48:59.369
    Aug 26 05:48:59.409: INFO: Pod calico-kube-controllers-76798f54cb-cdxwm requesting resource cpu=1m on Node ip-10-0-1-23.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod calico-node-8272m requesting resource cpu=250m on Node ip-10-0-1-126.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod calico-node-hgm7c requesting resource cpu=250m on Node ip-10-0-1-101.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod calico-node-jftpx requesting resource cpu=250m on Node ip-10-0-1-5.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod calico-node-mxmcm requesting resource cpu=250m on Node ip-10-0-1-31.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod calico-node-trdnz requesting resource cpu=250m on Node ip-10-0-1-23.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod calico-typha-6dd9648c8f-85hjp requesting resource cpu=10m on Node ip-10-0-1-101.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod calico-typha-6dd9648c8f-th65n requesting resource cpu=10m on Node ip-10-0-1-126.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod calico-typha-6dd9648c8f-x4b7r requesting resource cpu=10m on Node ip-10-0-1-23.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod calico-typha-autoscaler-54c8866496-tmj59 requesting resource cpu=20m on Node ip-10-0-1-23.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod coredns-58ffcc48df-2q4rv requesting resource cpu=100m on Node ip-10-0-1-23.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod kube-dns-autoscaler-f68f756b6-78kdn requesting resource cpu=20m on Node ip-10-0-1-23.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod kube-state-metrics-99bbfb4cd-g7vn6 requesting resource cpu=0m on Node ip-10-0-1-23.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod metrics-server-b7db9955-nwll9 requesting resource cpu=100m on Node ip-10-0-1-101.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod dashboard-metrics-scraper-6c57f89c7c-2vtv5 requesting resource cpu=0m on Node ip-10-0-1-23.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-1-101.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod sonobuoy-e2e-job-c6e00385bb4e4fb8 requesting resource cpu=0m on Node ip-10-0-1-126.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-6gxwx requesting resource cpu=0m on Node ip-10-0-1-31.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-ckdgn requesting resource cpu=0m on Node ip-10-0-1-126.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-g65bg requesting resource cpu=0m on Node ip-10-0-1-23.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c requesting resource cpu=0m on Node ip-10-0-1-101.us-west-2.compute.internal
    Aug 26 05:48:59.409: INFO: Pod sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-tpq9m requesting resource cpu=0m on Node ip-10-0-1-5.us-west-2.compute.internal
    STEP: Starting Pods to consume most of the cluster CPU. 08/26/23 05:48:59.409
    Aug 26 05:48:59.409: INFO: Creating a pod which consumes cpu=2548m on Node ip-10-0-1-101.us-west-2.compute.internal
    Aug 26 05:48:59.421: INFO: Creating a pod which consumes cpu=2618m on Node ip-10-0-1-126.us-west-2.compute.internal
    Aug 26 05:48:59.432: INFO: Creating a pod which consumes cpu=2519m on Node ip-10-0-1-23.us-west-2.compute.internal
    Aug 26 05:48:59.446: INFO: Creating a pod which consumes cpu=2625m on Node ip-10-0-1-31.us-west-2.compute.internal
    Aug 26 05:48:59.462: INFO: Creating a pod which consumes cpu=2625m on Node ip-10-0-1-5.us-west-2.compute.internal
    Aug 26 05:48:59.474: INFO: Waiting up to 5m0s for pod "filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db" in namespace "sched-pred-6517" to be "running"
    Aug 26 05:48:59.483: INFO: Pod "filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.727405ms
    Aug 26 05:49:01.488: INFO: Pod "filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db": Phase="Running", Reason="", readiness=true. Elapsed: 2.013309003s
    Aug 26 05:49:01.488: INFO: Pod "filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db" satisfied condition "running"
    Aug 26 05:49:01.488: INFO: Waiting up to 5m0s for pod "filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90" in namespace "sched-pred-6517" to be "running"
    Aug 26 05:49:01.491: INFO: Pod "filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90": Phase="Running", Reason="", readiness=true. Elapsed: 3.430165ms
    Aug 26 05:49:01.491: INFO: Pod "filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90" satisfied condition "running"
    Aug 26 05:49:01.491: INFO: Waiting up to 5m0s for pod "filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b" in namespace "sched-pred-6517" to be "running"
    Aug 26 05:49:01.494: INFO: Pod "filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b": Phase="Running", Reason="", readiness=true. Elapsed: 3.25647ms
    Aug 26 05:49:01.494: INFO: Pod "filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b" satisfied condition "running"
    Aug 26 05:49:01.494: INFO: Waiting up to 5m0s for pod "filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745" in namespace "sched-pred-6517" to be "running"
    Aug 26 05:49:01.503: INFO: Pod "filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745": Phase="Running", Reason="", readiness=true. Elapsed: 8.337987ms
    Aug 26 05:49:01.503: INFO: Pod "filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745" satisfied condition "running"
    Aug 26 05:49:01.503: INFO: Waiting up to 5m0s for pod "filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306" in namespace "sched-pred-6517" to be "running"
    Aug 26 05:49:01.508: INFO: Pod "filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306": Phase="Running", Reason="", readiness=true. Elapsed: 5.086098ms
    Aug 26 05:49:01.508: INFO: Pod "filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 08/26/23 05:49:01.508
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b.177ed8d9b32c694d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6517/filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b to ip-10-0-1-23.us-west-2.compute.internal] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b.177ed8d9dcd83100], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b.177ed8d9fa5f5876], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 495.350396ms (495.366855ms including waiting)] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b.177ed8d9fb7e796c], Reason = [Created], Message = [Created container filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b.177ed8da01e62dad], Reason = [Started], Message = [Started container filler-pod-2bbc57c7-8b29-44b6-87f3-c7f2d0b2709b] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90.177ed8d9b27e186d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6517/filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90 to ip-10-0-1-126.us-west-2.compute.internal] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90.177ed8d9e18f1702], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90.177ed8d9e2eb0e57], Reason = [Created], Message = [Created container filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90.177ed8d9ea88fd57], Reason = [Started], Message = [Started container filler-pod-5a393ced-6448-407e-88bb-d802d22a2a90] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306.177ed8d9b4757371], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6517/filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306 to ip-10-0-1-5.us-west-2.compute.internal] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306.177ed8d9e76b82a7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306.177ed8d9e8d5123b], Reason = [Created], Message = [Created container filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306.177ed8d9f23ae91b], Reason = [Started], Message = [Started container filler-pod-87c5fe9b-d459-44f0-8532-67ba4ca7b306] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db.177ed8d9b151eb08], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6517/filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db to ip-10-0-1-101.us-west-2.compute.internal] 08/26/23 05:49:01.514
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db.177ed8d9dc612ba0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/26/23 05:49:01.515
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db.177ed8d9de1cf95a], Reason = [Created], Message = [Created container filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db] 08/26/23 05:49:01.515
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db.177ed8d9e7c60a9b], Reason = [Started], Message = [Started container filler-pod-972fb5f1-bf6d-4805-8d6b-e4f55565e1db] 08/26/23 05:49:01.515
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745.177ed8d9b422e1b1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6517/filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745 to ip-10-0-1-31.us-west-2.compute.internal] 08/26/23 05:49:01.515
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745.177ed8d9e5e3eb05], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/26/23 05:49:01.515
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745.177ed8d9e727007b], Reason = [Created], Message = [Created container filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745] 08/26/23 05:49:01.515
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745.177ed8d9ed23d843], Reason = [Started], Message = [Started container filler-pod-c4d48187-4d5b-4de2-aa27-bafe90ede745] 08/26/23 05:49:01.515
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.177ed8da2ed34ba5], Reason = [FailedScheduling], Message = [0/8 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}, 5 Insufficient cpu. preemption: 0/8 nodes are available: 3 Preemption is not helpful for scheduling, 5 No preemption victims found for incoming pod..] 08/26/23 05:49:01.545
    STEP: removing the label node off the node ip-10-0-1-5.us-west-2.compute.internal 08/26/23 05:49:02.544
    STEP: verifying the node doesn't have the label node 08/26/23 05:49:02.564
    STEP: removing the label node off the node ip-10-0-1-101.us-west-2.compute.internal 08/26/23 05:49:02.569
    STEP: verifying the node doesn't have the label node 08/26/23 05:49:02.583
    STEP: removing the label node off the node ip-10-0-1-126.us-west-2.compute.internal 08/26/23 05:49:02.587
    STEP: verifying the node doesn't have the label node 08/26/23 05:49:02.602
    STEP: removing the label node off the node ip-10-0-1-23.us-west-2.compute.internal 08/26/23 05:49:02.606
    STEP: verifying the node doesn't have the label node 08/26/23 05:49:02.626
    STEP: removing the label node off the node ip-10-0-1-31.us-west-2.compute.internal 08/26/23 05:49:02.63
    STEP: verifying the node doesn't have the label node 08/26/23 05:49:02.644
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:02.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6517" for this suite. 08/26/23 05:49:02.659
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:02.667
Aug 26 05:49:02.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename dns 08/26/23 05:49:02.669
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:02.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:02.692
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1120.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1120.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 08/26/23 05:49:02.697
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1120.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1120.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 08/26/23 05:49:02.698
STEP: creating a pod to probe /etc/hosts 08/26/23 05:49:02.698
STEP: submitting the pod to kubernetes 08/26/23 05:49:02.698
Aug 26 05:49:02.712: INFO: Waiting up to 15m0s for pod "dns-test-017ad921-6f0d-4684-94fe-758c8eb31b3b" in namespace "dns-1120" to be "running"
Aug 26 05:49:02.715: INFO: Pod "dns-test-017ad921-6f0d-4684-94fe-758c8eb31b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338405ms
Aug 26 05:49:04.722: INFO: Pod "dns-test-017ad921-6f0d-4684-94fe-758c8eb31b3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00951793s
Aug 26 05:49:04.722: INFO: Pod "dns-test-017ad921-6f0d-4684-94fe-758c8eb31b3b" satisfied condition "running"
STEP: retrieving the pod 08/26/23 05:49:04.722
STEP: looking for the results for each expected name from probers 08/26/23 05:49:04.725
Aug 26 05:49:04.747: INFO: DNS probes using dns-1120/dns-test-017ad921-6f0d-4684-94fe-758c8eb31b3b succeeded

STEP: deleting the pod 08/26/23 05:49:04.747
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:04.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1120" for this suite. 08/26/23 05:49:04.771
------------------------------
• [2.111 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:02.667
    Aug 26 05:49:02.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename dns 08/26/23 05:49:02.669
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:02.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:02.692
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1120.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1120.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     08/26/23 05:49:02.697
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1120.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1120.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     08/26/23 05:49:02.698
    STEP: creating a pod to probe /etc/hosts 08/26/23 05:49:02.698
    STEP: submitting the pod to kubernetes 08/26/23 05:49:02.698
    Aug 26 05:49:02.712: INFO: Waiting up to 15m0s for pod "dns-test-017ad921-6f0d-4684-94fe-758c8eb31b3b" in namespace "dns-1120" to be "running"
    Aug 26 05:49:02.715: INFO: Pod "dns-test-017ad921-6f0d-4684-94fe-758c8eb31b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338405ms
    Aug 26 05:49:04.722: INFO: Pod "dns-test-017ad921-6f0d-4684-94fe-758c8eb31b3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00951793s
    Aug 26 05:49:04.722: INFO: Pod "dns-test-017ad921-6f0d-4684-94fe-758c8eb31b3b" satisfied condition "running"
    STEP: retrieving the pod 08/26/23 05:49:04.722
    STEP: looking for the results for each expected name from probers 08/26/23 05:49:04.725
    Aug 26 05:49:04.747: INFO: DNS probes using dns-1120/dns-test-017ad921-6f0d-4684-94fe-758c8eb31b3b succeeded

    STEP: deleting the pod 08/26/23 05:49:04.747
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:04.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1120" for this suite. 08/26/23 05:49:04.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:04.782
Aug 26 05:49:04.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 05:49:04.783
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:04.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:04.803
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 08/26/23 05:49:04.806
STEP: submitting the pod to kubernetes 08/26/23 05:49:04.806
STEP: verifying QOS class is set on the pod 08/26/23 05:49:04.816
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:04.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1023" for this suite. 08/26/23 05:49:04.829
------------------------------
• [0.058 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:04.782
    Aug 26 05:49:04.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 05:49:04.783
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:04.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:04.803
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 08/26/23 05:49:04.806
    STEP: submitting the pod to kubernetes 08/26/23 05:49:04.806
    STEP: verifying QOS class is set on the pod 08/26/23 05:49:04.816
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:04.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1023" for this suite. 08/26/23 05:49:04.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:04.841
Aug 26 05:49:04.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:49:04.842
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:04.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:04.868
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:49:04.871
Aug 26 05:49:04.882: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941" in namespace "downward-api-3498" to be "Succeeded or Failed"
Aug 26 05:49:04.884: INFO: Pod "downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724922ms
Aug 26 05:49:06.889: INFO: Pod "downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007499268s
Aug 26 05:49:08.889: INFO: Pod "downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007482486s
STEP: Saw pod success 08/26/23 05:49:08.889
Aug 26 05:49:08.889: INFO: Pod "downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941" satisfied condition "Succeeded or Failed"
Aug 26 05:49:08.893: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941 container client-container: <nil>
STEP: delete the pod 08/26/23 05:49:08.911
Aug 26 05:49:08.928: INFO: Waiting for pod downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941 to disappear
Aug 26 05:49:08.931: INFO: Pod downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:08.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3498" for this suite. 08/26/23 05:49:08.94
------------------------------
• [4.109 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:04.841
    Aug 26 05:49:04.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:49:04.842
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:04.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:04.868
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:49:04.871
    Aug 26 05:49:04.882: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941" in namespace "downward-api-3498" to be "Succeeded or Failed"
    Aug 26 05:49:04.884: INFO: Pod "downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724922ms
    Aug 26 05:49:06.889: INFO: Pod "downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007499268s
    Aug 26 05:49:08.889: INFO: Pod "downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007482486s
    STEP: Saw pod success 08/26/23 05:49:08.889
    Aug 26 05:49:08.889: INFO: Pod "downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941" satisfied condition "Succeeded or Failed"
    Aug 26 05:49:08.893: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941 container client-container: <nil>
    STEP: delete the pod 08/26/23 05:49:08.911
    Aug 26 05:49:08.928: INFO: Waiting for pod downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941 to disappear
    Aug 26 05:49:08.931: INFO: Pod downwardapi-volume-cdb16aa2-555c-444e-83cc-3904c0df8941 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:08.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3498" for this suite. 08/26/23 05:49:08.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:08.951
Aug 26 05:49:08.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:49:08.952
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:08.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:08.998
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:49:09.002
Aug 26 05:49:09.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587" in namespace "projected-1667" to be "Succeeded or Failed"
Aug 26 05:49:09.023: INFO: Pod "downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587": Phase="Pending", Reason="", readiness=false. Elapsed: 4.303262ms
Aug 26 05:49:11.027: INFO: Pod "downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008227781s
Aug 26 05:49:13.029: INFO: Pod "downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009971914s
STEP: Saw pod success 08/26/23 05:49:13.029
Aug 26 05:49:13.029: INFO: Pod "downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587" satisfied condition "Succeeded or Failed"
Aug 26 05:49:13.034: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587 container client-container: <nil>
STEP: delete the pod 08/26/23 05:49:13.059
Aug 26 05:49:13.082: INFO: Waiting for pod downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587 to disappear
Aug 26 05:49:13.089: INFO: Pod downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:13.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1667" for this suite. 08/26/23 05:49:13.098
------------------------------
• [4.156 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:08.951
    Aug 26 05:49:08.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:49:08.952
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:08.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:08.998
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:49:09.002
    Aug 26 05:49:09.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587" in namespace "projected-1667" to be "Succeeded or Failed"
    Aug 26 05:49:09.023: INFO: Pod "downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587": Phase="Pending", Reason="", readiness=false. Elapsed: 4.303262ms
    Aug 26 05:49:11.027: INFO: Pod "downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008227781s
    Aug 26 05:49:13.029: INFO: Pod "downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009971914s
    STEP: Saw pod success 08/26/23 05:49:13.029
    Aug 26 05:49:13.029: INFO: Pod "downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587" satisfied condition "Succeeded or Failed"
    Aug 26 05:49:13.034: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587 container client-container: <nil>
    STEP: delete the pod 08/26/23 05:49:13.059
    Aug 26 05:49:13.082: INFO: Waiting for pod downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587 to disappear
    Aug 26 05:49:13.089: INFO: Pod downwardapi-volume-e3fac034-708e-4898-a5c3-bece43a6b587 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:13.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1667" for this suite. 08/26/23 05:49:13.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:13.11
Aug 26 05:49:13.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename svcaccounts 08/26/23 05:49:13.11
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:13.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:13.132
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  08/26/23 05:49:13.135
Aug 26 05:49:13.149: INFO: Waiting up to 5m0s for pod "test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2" in namespace "svcaccounts-8120" to be "Succeeded or Failed"
Aug 26 05:49:13.155: INFO: Pod "test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.437973ms
Aug 26 05:49:15.165: INFO: Pod "test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01528477s
Aug 26 05:49:17.160: INFO: Pod "test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010686534s
STEP: Saw pod success 08/26/23 05:49:17.16
Aug 26 05:49:17.160: INFO: Pod "test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2" satisfied condition "Succeeded or Failed"
Aug 26 05:49:17.164: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2 container agnhost-container: <nil>
STEP: delete the pod 08/26/23 05:49:17.172
Aug 26 05:49:17.185: INFO: Waiting for pod test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2 to disappear
Aug 26 05:49:17.190: INFO: Pod test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:17.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8120" for this suite. 08/26/23 05:49:17.198
------------------------------
• [4.097 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:13.11
    Aug 26 05:49:13.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename svcaccounts 08/26/23 05:49:13.11
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:13.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:13.132
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  08/26/23 05:49:13.135
    Aug 26 05:49:13.149: INFO: Waiting up to 5m0s for pod "test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2" in namespace "svcaccounts-8120" to be "Succeeded or Failed"
    Aug 26 05:49:13.155: INFO: Pod "test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.437973ms
    Aug 26 05:49:15.165: INFO: Pod "test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01528477s
    Aug 26 05:49:17.160: INFO: Pod "test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010686534s
    STEP: Saw pod success 08/26/23 05:49:17.16
    Aug 26 05:49:17.160: INFO: Pod "test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2" satisfied condition "Succeeded or Failed"
    Aug 26 05:49:17.164: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2 container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 05:49:17.172
    Aug 26 05:49:17.185: INFO: Waiting for pod test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2 to disappear
    Aug 26 05:49:17.190: INFO: Pod test-pod-64ba3290-d116-49fb-b0df-46bf6b1b3ea2 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:17.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8120" for this suite. 08/26/23 05:49:17.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:17.208
Aug 26 05:49:17.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename containers 08/26/23 05:49:17.212
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:17.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:17.229
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Aug 26 05:49:17.241: INFO: Waiting up to 5m0s for pod "client-containers-7f2a53f5-ae35-4d16-9ac0-e372db22781b" in namespace "containers-9862" to be "running"
Aug 26 05:49:17.245: INFO: Pod "client-containers-7f2a53f5-ae35-4d16-9ac0-e372db22781b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.245936ms
Aug 26 05:49:19.249: INFO: Pod "client-containers-7f2a53f5-ae35-4d16-9ac0-e372db22781b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008539413s
Aug 26 05:49:19.249: INFO: Pod "client-containers-7f2a53f5-ae35-4d16-9ac0-e372db22781b" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9862" for this suite. 08/26/23 05:49:19.273
------------------------------
• [2.073 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:17.208
    Aug 26 05:49:17.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename containers 08/26/23 05:49:17.212
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:17.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:17.229
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Aug 26 05:49:17.241: INFO: Waiting up to 5m0s for pod "client-containers-7f2a53f5-ae35-4d16-9ac0-e372db22781b" in namespace "containers-9862" to be "running"
    Aug 26 05:49:17.245: INFO: Pod "client-containers-7f2a53f5-ae35-4d16-9ac0-e372db22781b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.245936ms
    Aug 26 05:49:19.249: INFO: Pod "client-containers-7f2a53f5-ae35-4d16-9ac0-e372db22781b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008539413s
    Aug 26 05:49:19.249: INFO: Pod "client-containers-7f2a53f5-ae35-4d16-9ac0-e372db22781b" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9862" for this suite. 08/26/23 05:49:19.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:19.281
Aug 26 05:49:19.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename events 08/26/23 05:49:19.283
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:19.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:19.307
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 08/26/23 05:49:19.309
STEP: listing all events in all namespaces 08/26/23 05:49:19.318
STEP: patching the test event 08/26/23 05:49:19.462
STEP: fetching the test event 08/26/23 05:49:19.47
STEP: updating the test event 08/26/23 05:49:19.473
STEP: getting the test event 08/26/23 05:49:19.484
STEP: deleting the test event 08/26/23 05:49:19.487
STEP: listing all events in all namespaces 08/26/23 05:49:19.495
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:19.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6841" for this suite. 08/26/23 05:49:19.539
------------------------------
• [0.264 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:19.281
    Aug 26 05:49:19.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename events 08/26/23 05:49:19.283
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:19.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:19.307
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 08/26/23 05:49:19.309
    STEP: listing all events in all namespaces 08/26/23 05:49:19.318
    STEP: patching the test event 08/26/23 05:49:19.462
    STEP: fetching the test event 08/26/23 05:49:19.47
    STEP: updating the test event 08/26/23 05:49:19.473
    STEP: getting the test event 08/26/23 05:49:19.484
    STEP: deleting the test event 08/26/23 05:49:19.487
    STEP: listing all events in all namespaces 08/26/23 05:49:19.495
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:19.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6841" for this suite. 08/26/23 05:49:19.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:19.548
Aug 26 05:49:19.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:49:19.549
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:19.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:19.566
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:49:19.582
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:49:20.045
STEP: Deploying the webhook pod 08/26/23 05:49:20.056
STEP: Wait for the deployment to be ready 08/26/23 05:49:20.074
Aug 26 05:49:20.083: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:49:22.182
STEP: Verifying the service has paired with the endpoint 08/26/23 05:49:22.194
Aug 26 05:49:23.195: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Aug 26 05:49:23.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9874-crds.webhook.example.com via the AdmissionRegistration API 08/26/23 05:49:23.712
STEP: Creating a custom resource that should be mutated by the webhook 08/26/23 05:49:23.733
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:26.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5954" for this suite. 08/26/23 05:49:26.364
STEP: Destroying namespace "webhook-5954-markers" for this suite. 08/26/23 05:49:26.38
------------------------------
• [SLOW TEST] [6.851 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:19.548
    Aug 26 05:49:19.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:49:19.549
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:19.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:19.566
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:49:19.582
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:49:20.045
    STEP: Deploying the webhook pod 08/26/23 05:49:20.056
    STEP: Wait for the deployment to be ready 08/26/23 05:49:20.074
    Aug 26 05:49:20.083: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:49:22.182
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:49:22.194
    Aug 26 05:49:23.195: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Aug 26 05:49:23.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9874-crds.webhook.example.com via the AdmissionRegistration API 08/26/23 05:49:23.712
    STEP: Creating a custom resource that should be mutated by the webhook 08/26/23 05:49:23.733
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:26.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5954" for this suite. 08/26/23 05:49:26.364
    STEP: Destroying namespace "webhook-5954-markers" for this suite. 08/26/23 05:49:26.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:26.4
Aug 26 05:49:26.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 05:49:26.402
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:26.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:26.434
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 08/26/23 05:49:26.438
Aug 26 05:49:26.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2185 cluster-info'
Aug 26 05:49:26.560: INFO: stderr: ""
Aug 26 05:49:26.560: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:26.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2185" for this suite. 08/26/23 05:49:26.574
------------------------------
• [0.180 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:26.4
    Aug 26 05:49:26.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 05:49:26.402
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:26.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:26.434
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 08/26/23 05:49:26.438
    Aug 26 05:49:26.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2185 cluster-info'
    Aug 26 05:49:26.560: INFO: stderr: ""
    Aug 26 05:49:26.560: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:26.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2185" for this suite. 08/26/23 05:49:26.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:26.582
Aug 26 05:49:26.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 05:49:26.583
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:26.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:26.609
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/26/23 05:49:26.616
Aug 26 05:49:26.630: INFO: Waiting up to 5m0s for pod "pod-85f15037-af86-4b40-be58-0b8d146b223e" in namespace "emptydir-3252" to be "Succeeded or Failed"
Aug 26 05:49:26.633: INFO: Pod "pod-85f15037-af86-4b40-be58-0b8d146b223e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226668ms
Aug 26 05:49:28.640: INFO: Pod "pod-85f15037-af86-4b40-be58-0b8d146b223e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010373294s
Aug 26 05:49:30.647: INFO: Pod "pod-85f15037-af86-4b40-be58-0b8d146b223e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017032219s
STEP: Saw pod success 08/26/23 05:49:30.647
Aug 26 05:49:30.647: INFO: Pod "pod-85f15037-af86-4b40-be58-0b8d146b223e" satisfied condition "Succeeded or Failed"
Aug 26 05:49:30.651: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-85f15037-af86-4b40-be58-0b8d146b223e container test-container: <nil>
STEP: delete the pod 08/26/23 05:49:30.66
Aug 26 05:49:30.675: INFO: Waiting for pod pod-85f15037-af86-4b40-be58-0b8d146b223e to disappear
Aug 26 05:49:30.678: INFO: Pod pod-85f15037-af86-4b40-be58-0b8d146b223e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:30.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3252" for this suite. 08/26/23 05:49:30.686
------------------------------
• [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:26.582
    Aug 26 05:49:26.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 05:49:26.583
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:26.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:26.609
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/26/23 05:49:26.616
    Aug 26 05:49:26.630: INFO: Waiting up to 5m0s for pod "pod-85f15037-af86-4b40-be58-0b8d146b223e" in namespace "emptydir-3252" to be "Succeeded or Failed"
    Aug 26 05:49:26.633: INFO: Pod "pod-85f15037-af86-4b40-be58-0b8d146b223e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226668ms
    Aug 26 05:49:28.640: INFO: Pod "pod-85f15037-af86-4b40-be58-0b8d146b223e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010373294s
    Aug 26 05:49:30.647: INFO: Pod "pod-85f15037-af86-4b40-be58-0b8d146b223e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017032219s
    STEP: Saw pod success 08/26/23 05:49:30.647
    Aug 26 05:49:30.647: INFO: Pod "pod-85f15037-af86-4b40-be58-0b8d146b223e" satisfied condition "Succeeded or Failed"
    Aug 26 05:49:30.651: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-85f15037-af86-4b40-be58-0b8d146b223e container test-container: <nil>
    STEP: delete the pod 08/26/23 05:49:30.66
    Aug 26 05:49:30.675: INFO: Waiting for pod pod-85f15037-af86-4b40-be58-0b8d146b223e to disappear
    Aug 26 05:49:30.678: INFO: Pod pod-85f15037-af86-4b40-be58-0b8d146b223e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:30.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3252" for this suite. 08/26/23 05:49:30.686
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:30.695
Aug 26 05:49:30.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename statefulset 08/26/23 05:49:30.704
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:30.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:30.726
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6279 08/26/23 05:49:30.729
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 08/26/23 05:49:30.734
STEP: Creating pod with conflicting port in namespace statefulset-6279 08/26/23 05:49:30.743
STEP: Waiting until pod test-pod will start running in namespace statefulset-6279 08/26/23 05:49:30.752
Aug 26 05:49:30.752: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6279" to be "running"
Aug 26 05:49:30.759: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.325586ms
Aug 26 05:49:32.764: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011825081s
Aug 26 05:49:32.764: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-6279 08/26/23 05:49:32.764
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6279 08/26/23 05:49:32.77
Aug 26 05:49:32.783: INFO: Observed stateful pod in namespace: statefulset-6279, name: ss-0, uid: 362927d0-0ce5-4bb4-a2b7-0acad20ac7b2, status phase: Pending. Waiting for statefulset controller to delete.
Aug 26 05:49:32.803: INFO: Observed stateful pod in namespace: statefulset-6279, name: ss-0, uid: 362927d0-0ce5-4bb4-a2b7-0acad20ac7b2, status phase: Failed. Waiting for statefulset controller to delete.
Aug 26 05:49:32.823: INFO: Observed stateful pod in namespace: statefulset-6279, name: ss-0, uid: 362927d0-0ce5-4bb4-a2b7-0acad20ac7b2, status phase: Failed. Waiting for statefulset controller to delete.
Aug 26 05:49:32.837: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6279
STEP: Removing pod with conflicting port in namespace statefulset-6279 08/26/23 05:49:32.837
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6279 and will be in running state 08/26/23 05:49:32.945
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 26 05:49:34.960: INFO: Deleting all statefulset in ns statefulset-6279
Aug 26 05:49:34.965: INFO: Scaling statefulset ss to 0
Aug 26 05:49:45.001: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 05:49:45.005: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 26 05:49:45.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6279" for this suite. 08/26/23 05:49:45.035
------------------------------
• [SLOW TEST] [14.357 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:30.695
    Aug 26 05:49:30.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename statefulset 08/26/23 05:49:30.704
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:30.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:30.726
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6279 08/26/23 05:49:30.729
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 08/26/23 05:49:30.734
    STEP: Creating pod with conflicting port in namespace statefulset-6279 08/26/23 05:49:30.743
    STEP: Waiting until pod test-pod will start running in namespace statefulset-6279 08/26/23 05:49:30.752
    Aug 26 05:49:30.752: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6279" to be "running"
    Aug 26 05:49:30.759: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.325586ms
    Aug 26 05:49:32.764: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011825081s
    Aug 26 05:49:32.764: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-6279 08/26/23 05:49:32.764
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6279 08/26/23 05:49:32.77
    Aug 26 05:49:32.783: INFO: Observed stateful pod in namespace: statefulset-6279, name: ss-0, uid: 362927d0-0ce5-4bb4-a2b7-0acad20ac7b2, status phase: Pending. Waiting for statefulset controller to delete.
    Aug 26 05:49:32.803: INFO: Observed stateful pod in namespace: statefulset-6279, name: ss-0, uid: 362927d0-0ce5-4bb4-a2b7-0acad20ac7b2, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 26 05:49:32.823: INFO: Observed stateful pod in namespace: statefulset-6279, name: ss-0, uid: 362927d0-0ce5-4bb4-a2b7-0acad20ac7b2, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 26 05:49:32.837: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6279
    STEP: Removing pod with conflicting port in namespace statefulset-6279 08/26/23 05:49:32.837
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6279 and will be in running state 08/26/23 05:49:32.945
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 26 05:49:34.960: INFO: Deleting all statefulset in ns statefulset-6279
    Aug 26 05:49:34.965: INFO: Scaling statefulset ss to 0
    Aug 26 05:49:45.001: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 05:49:45.005: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:49:45.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6279" for this suite. 08/26/23 05:49:45.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:49:45.053
Aug 26 05:49:45.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename deployment 08/26/23 05:49:45.054
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:45.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:45.075
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Aug 26 05:49:45.099: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 26 05:49:50.104: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/26/23 05:49:50.104
Aug 26 05:49:50.104: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 26 05:49:52.109: INFO: Creating deployment "test-rollover-deployment"
Aug 26 05:49:52.128: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 26 05:49:54.137: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 26 05:49:54.143: INFO: Ensure that both replica sets have 1 created replica
Aug 26 05:49:54.152: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 26 05:49:54.164: INFO: Updating deployment test-rollover-deployment
Aug 26 05:49:54.164: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 26 05:49:56.175: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 26 05:49:56.181: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 26 05:49:56.187: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 05:49:56.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:49:58.194: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 05:49:58.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:50:00.203: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 05:50:00.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:50:02.196: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 05:50:02.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:50:04.196: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 05:50:04.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 05:50:06.195: INFO: 
Aug 26 05:50:06.195: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 26 05:50:06.204: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2226  25e3be8b-3fd2-4514-a60e-08378c77176d 26286 2 2023-08-26 05:49:52 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:50:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c32ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-26 05:49:52 +0000 UTC,LastTransitionTime:2023-08-26 05:49:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-26 05:50:05 +0000 UTC,LastTransitionTime:2023-08-26 05:49:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 26 05:50:06.210: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2226  0e6637c2-8a35-434c-bbbf-4c4ccb027894 26276 2 2023-08-26 05:49:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 25e3be8b-3fd2-4514-a60e-08378c77176d 0xc003c33007 0xc003c33008}] [] [{kube-controller-manager Update apps/v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25e3be8b-3fd2-4514-a60e-08378c77176d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:50:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c330b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 26 05:50:06.210: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 26 05:50:06.210: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2226  78462ace-2f14-47c6-83dd-4e37851b2d93 26285 2 2023-08-26 05:49:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 25e3be8b-3fd2-4514-a60e-08378c77176d 0xc003c32ec7 0xc003c32ec8}] [] [{e2e.test Update apps/v1 2023-08-26 05:49:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:50:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25e3be8b-3fd2-4514-a60e-08378c77176d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:50:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c32f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 26 05:50:06.210: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2226  3044d228-0adb-49a9-823a-9e6823d2857d 26204 2 2023-08-26 05:49:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 25e3be8b-3fd2-4514-a60e-08378c77176d 0xc003c33127 0xc003c33128}] [] [{kube-controller-manager Update apps/v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25e3be8b-3fd2-4514-a60e-08378c77176d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c33208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 26 05:50:06.214: INFO: Pod "test-rollover-deployment-6c6df9974f-gqmlp" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-gqmlp test-rollover-deployment-6c6df9974f- deployment-2226  03222476-5708-4924-9a77-fd3098e5ac11 26228 0 2023-08-26 05:49:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:b4d0ffc31567222f3415b9f6f189993e3780d6463572474c35e940eddca4d82a cni.projectcalico.org/podIP:10.20.199.105/32 cni.projectcalico.org/podIPs:10.20.199.105/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 0e6637c2-8a35-434c-bbbf-4c4ccb027894 0xc003c338e7 0xc003c338e8}] [] [{calico Update v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e6637c2-8a35-434c-bbbf-4c4ccb027894\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 05:49:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4cvkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4cvkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:49:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:49:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:49:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:49:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:10.20.199.105,StartTime:2023-08-26 05:49:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 05:49:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://58153cd2b99910f59fab4ba1810be8a2776d192473924240ee47da94940ee15d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.199.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 26 05:50:06.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2226" for this suite. 08/26/23 05:50:06.223
------------------------------
• [SLOW TEST] [21.176 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:49:45.053
    Aug 26 05:49:45.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename deployment 08/26/23 05:49:45.054
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:49:45.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:49:45.075
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Aug 26 05:49:45.099: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Aug 26 05:49:50.104: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/26/23 05:49:50.104
    Aug 26 05:49:50.104: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Aug 26 05:49:52.109: INFO: Creating deployment "test-rollover-deployment"
    Aug 26 05:49:52.128: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Aug 26 05:49:54.137: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Aug 26 05:49:54.143: INFO: Ensure that both replica sets have 1 created replica
    Aug 26 05:49:54.152: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Aug 26 05:49:54.164: INFO: Updating deployment test-rollover-deployment
    Aug 26 05:49:54.164: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Aug 26 05:49:56.175: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Aug 26 05:49:56.181: INFO: Make sure deployment "test-rollover-deployment" is complete
    Aug 26 05:49:56.187: INFO: all replica sets need to contain the pod-template-hash label
    Aug 26 05:49:56.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:49:58.194: INFO: all replica sets need to contain the pod-template-hash label
    Aug 26 05:49:58.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:50:00.203: INFO: all replica sets need to contain the pod-template-hash label
    Aug 26 05:50:00.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:50:02.196: INFO: all replica sets need to contain the pod-template-hash label
    Aug 26 05:50:02.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:50:04.196: INFO: all replica sets need to contain the pod-template-hash label
    Aug 26 05:50:04.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 5, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 5, 49, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 05:50:06.195: INFO: 
    Aug 26 05:50:06.195: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 26 05:50:06.204: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2226  25e3be8b-3fd2-4514-a60e-08378c77176d 26286 2 2023-08-26 05:49:52 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:50:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c32ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-26 05:49:52 +0000 UTC,LastTransitionTime:2023-08-26 05:49:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-26 05:50:05 +0000 UTC,LastTransitionTime:2023-08-26 05:49:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 26 05:50:06.210: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2226  0e6637c2-8a35-434c-bbbf-4c4ccb027894 26276 2 2023-08-26 05:49:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 25e3be8b-3fd2-4514-a60e-08378c77176d 0xc003c33007 0xc003c33008}] [] [{kube-controller-manager Update apps/v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25e3be8b-3fd2-4514-a60e-08378c77176d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:50:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c330b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 05:50:06.210: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Aug 26 05:50:06.210: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2226  78462ace-2f14-47c6-83dd-4e37851b2d93 26285 2 2023-08-26 05:49:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 25e3be8b-3fd2-4514-a60e-08378c77176d 0xc003c32ec7 0xc003c32ec8}] [] [{e2e.test Update apps/v1 2023-08-26 05:49:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:50:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25e3be8b-3fd2-4514-a60e-08378c77176d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:50:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c32f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 05:50:06.210: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2226  3044d228-0adb-49a9-823a-9e6823d2857d 26204 2 2023-08-26 05:49:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 25e3be8b-3fd2-4514-a60e-08378c77176d 0xc003c33127 0xc003c33128}] [] [{kube-controller-manager Update apps/v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25e3be8b-3fd2-4514-a60e-08378c77176d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c33208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 05:50:06.214: INFO: Pod "test-rollover-deployment-6c6df9974f-gqmlp" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-gqmlp test-rollover-deployment-6c6df9974f- deployment-2226  03222476-5708-4924-9a77-fd3098e5ac11 26228 0 2023-08-26 05:49:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:b4d0ffc31567222f3415b9f6f189993e3780d6463572474c35e940eddca4d82a cni.projectcalico.org/podIP:10.20.199.105/32 cni.projectcalico.org/podIPs:10.20.199.105/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 0e6637c2-8a35-434c-bbbf-4c4ccb027894 0xc003c338e7 0xc003c338e8}] [] [{calico Update v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 05:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e6637c2-8a35-434c-bbbf-4c4ccb027894\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 05:49:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4cvkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4cvkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:49:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:49:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:49:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 05:49:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:10.20.199.105,StartTime:2023-08-26 05:49:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 05:49:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://58153cd2b99910f59fab4ba1810be8a2776d192473924240ee47da94940ee15d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.199.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:50:06.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2226" for this suite. 08/26/23 05:50:06.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:50:06.231
Aug 26 05:50:06.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 05:50:06.234
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:50:06.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:50:06.263
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 08/26/23 05:50:06.289
STEP: watching for Pod to be ready 08/26/23 05:50:06.303
Aug 26 05:50:06.304: INFO: observed Pod pod-test in namespace pods-3961 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 26 05:50:06.307: INFO: observed Pod pod-test in namespace pods-3961 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  }]
Aug 26 05:50:06.324: INFO: observed Pod pod-test in namespace pods-3961 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  }]
Aug 26 05:50:06.887: INFO: observed Pod pod-test in namespace pods-3961 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  }]
Aug 26 05:50:07.353: INFO: Found Pod pod-test in namespace pods-3961 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 08/26/23 05:50:07.36
STEP: getting the Pod and ensuring that it's patched 08/26/23 05:50:07.376
STEP: replacing the Pod's status Ready condition to False 08/26/23 05:50:07.384
STEP: check the Pod again to ensure its Ready conditions are False 08/26/23 05:50:07.409
STEP: deleting the Pod via a Collection with a LabelSelector 08/26/23 05:50:07.409
STEP: watching for the Pod to be deleted 08/26/23 05:50:07.422
Aug 26 05:50:07.424: INFO: observed event type MODIFIED
Aug 26 05:50:09.367: INFO: observed event type MODIFIED
Aug 26 05:50:09.592: INFO: observed event type MODIFIED
Aug 26 05:50:10.369: INFO: observed event type MODIFIED
Aug 26 05:50:10.380: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 26 05:50:10.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3961" for this suite. 08/26/23 05:50:10.405
------------------------------
• [4.181 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:50:06.231
    Aug 26 05:50:06.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 05:50:06.234
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:50:06.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:50:06.263
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 08/26/23 05:50:06.289
    STEP: watching for Pod to be ready 08/26/23 05:50:06.303
    Aug 26 05:50:06.304: INFO: observed Pod pod-test in namespace pods-3961 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Aug 26 05:50:06.307: INFO: observed Pod pod-test in namespace pods-3961 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  }]
    Aug 26 05:50:06.324: INFO: observed Pod pod-test in namespace pods-3961 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  }]
    Aug 26 05:50:06.887: INFO: observed Pod pod-test in namespace pods-3961 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  }]
    Aug 26 05:50:07.353: INFO: Found Pod pod-test in namespace pods-3961 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 05:50:06 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 08/26/23 05:50:07.36
    STEP: getting the Pod and ensuring that it's patched 08/26/23 05:50:07.376
    STEP: replacing the Pod's status Ready condition to False 08/26/23 05:50:07.384
    STEP: check the Pod again to ensure its Ready conditions are False 08/26/23 05:50:07.409
    STEP: deleting the Pod via a Collection with a LabelSelector 08/26/23 05:50:07.409
    STEP: watching for the Pod to be deleted 08/26/23 05:50:07.422
    Aug 26 05:50:07.424: INFO: observed event type MODIFIED
    Aug 26 05:50:09.367: INFO: observed event type MODIFIED
    Aug 26 05:50:09.592: INFO: observed event type MODIFIED
    Aug 26 05:50:10.369: INFO: observed event type MODIFIED
    Aug 26 05:50:10.380: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:50:10.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3961" for this suite. 08/26/23 05:50:10.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:50:10.415
Aug 26 05:50:10.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename security-context 08/26/23 05:50:10.417
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:50:10.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:50:10.441
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/26/23 05:50:10.444
Aug 26 05:50:10.453: INFO: Waiting up to 5m0s for pod "security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d" in namespace "security-context-5257" to be "Succeeded or Failed"
Aug 26 05:50:10.458: INFO: Pod "security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.417875ms
Aug 26 05:50:12.465: INFO: Pod "security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012561599s
Aug 26 05:50:14.464: INFO: Pod "security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011394007s
STEP: Saw pod success 08/26/23 05:50:14.464
Aug 26 05:50:14.464: INFO: Pod "security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d" satisfied condition "Succeeded or Failed"
Aug 26 05:50:14.468: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d container test-container: <nil>
STEP: delete the pod 08/26/23 05:50:14.484
Aug 26 05:50:14.504: INFO: Waiting for pod security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d to disappear
Aug 26 05:50:14.508: INFO: Pod security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 26 05:50:14.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5257" for this suite. 08/26/23 05:50:14.52
------------------------------
• [4.115 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:50:10.415
    Aug 26 05:50:10.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename security-context 08/26/23 05:50:10.417
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:50:10.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:50:10.441
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/26/23 05:50:10.444
    Aug 26 05:50:10.453: INFO: Waiting up to 5m0s for pod "security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d" in namespace "security-context-5257" to be "Succeeded or Failed"
    Aug 26 05:50:10.458: INFO: Pod "security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.417875ms
    Aug 26 05:50:12.465: INFO: Pod "security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012561599s
    Aug 26 05:50:14.464: INFO: Pod "security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011394007s
    STEP: Saw pod success 08/26/23 05:50:14.464
    Aug 26 05:50:14.464: INFO: Pod "security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d" satisfied condition "Succeeded or Failed"
    Aug 26 05:50:14.468: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d container test-container: <nil>
    STEP: delete the pod 08/26/23 05:50:14.484
    Aug 26 05:50:14.504: INFO: Waiting for pod security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d to disappear
    Aug 26 05:50:14.508: INFO: Pod security-context-75788e81-2d56-4b3a-9cf1-52ff88c7568d no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:50:14.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5257" for this suite. 08/26/23 05:50:14.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:50:14.532
Aug 26 05:50:14.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:50:14.533
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:50:14.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:50:14.559
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:50:14.562
Aug 26 05:50:14.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258" in namespace "projected-4283" to be "Succeeded or Failed"
Aug 26 05:50:14.577: INFO: Pod "downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258": Phase="Pending", Reason="", readiness=false. Elapsed: 3.208485ms
Aug 26 05:50:16.584: INFO: Pod "downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009735241s
Aug 26 05:50:18.583: INFO: Pod "downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008713967s
STEP: Saw pod success 08/26/23 05:50:18.583
Aug 26 05:50:18.583: INFO: Pod "downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258" satisfied condition "Succeeded or Failed"
Aug 26 05:50:18.587: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258 container client-container: <nil>
STEP: delete the pod 08/26/23 05:50:18.594
Aug 26 05:50:18.612: INFO: Waiting for pod downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258 to disappear
Aug 26 05:50:18.616: INFO: Pod downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 05:50:18.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4283" for this suite. 08/26/23 05:50:18.623
------------------------------
• [4.100 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:50:14.532
    Aug 26 05:50:14.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:50:14.533
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:50:14.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:50:14.559
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:50:14.562
    Aug 26 05:50:14.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258" in namespace "projected-4283" to be "Succeeded or Failed"
    Aug 26 05:50:14.577: INFO: Pod "downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258": Phase="Pending", Reason="", readiness=false. Elapsed: 3.208485ms
    Aug 26 05:50:16.584: INFO: Pod "downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009735241s
    Aug 26 05:50:18.583: INFO: Pod "downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008713967s
    STEP: Saw pod success 08/26/23 05:50:18.583
    Aug 26 05:50:18.583: INFO: Pod "downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258" satisfied condition "Succeeded or Failed"
    Aug 26 05:50:18.587: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258 container client-container: <nil>
    STEP: delete the pod 08/26/23 05:50:18.594
    Aug 26 05:50:18.612: INFO: Waiting for pod downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258 to disappear
    Aug 26 05:50:18.616: INFO: Pod downwardapi-volume-70dc16a6-4777-45c5-8da9-c50dd77ca258 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:50:18.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4283" for this suite. 08/26/23 05:50:18.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:50:18.632
Aug 26 05:50:18.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 05:50:18.633
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:50:18.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:50:18.655
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:50:18.661
Aug 26 05:50:18.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a" in namespace "projected-7254" to be "Succeeded or Failed"
Aug 26 05:50:18.687: INFO: Pod "downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.673896ms
Aug 26 05:50:20.692: INFO: Pod "downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019395231s
Aug 26 05:50:22.692: INFO: Pod "downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020021907s
STEP: Saw pod success 08/26/23 05:50:22.692
Aug 26 05:50:22.692: INFO: Pod "downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a" satisfied condition "Succeeded or Failed"
Aug 26 05:50:22.696: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a container client-container: <nil>
STEP: delete the pod 08/26/23 05:50:22.702
Aug 26 05:50:22.714: INFO: Waiting for pod downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a to disappear
Aug 26 05:50:22.717: INFO: Pod downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 05:50:22.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7254" for this suite. 08/26/23 05:50:22.724
------------------------------
• [4.098 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:50:18.632
    Aug 26 05:50:18.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 05:50:18.633
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:50:18.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:50:18.655
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:50:18.661
    Aug 26 05:50:18.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a" in namespace "projected-7254" to be "Succeeded or Failed"
    Aug 26 05:50:18.687: INFO: Pod "downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.673896ms
    Aug 26 05:50:20.692: INFO: Pod "downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019395231s
    Aug 26 05:50:22.692: INFO: Pod "downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020021907s
    STEP: Saw pod success 08/26/23 05:50:22.692
    Aug 26 05:50:22.692: INFO: Pod "downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a" satisfied condition "Succeeded or Failed"
    Aug 26 05:50:22.696: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a container client-container: <nil>
    STEP: delete the pod 08/26/23 05:50:22.702
    Aug 26 05:50:22.714: INFO: Waiting for pod downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a to disappear
    Aug 26 05:50:22.717: INFO: Pod downwardapi-volume-f638279e-ee78-484e-b31d-ca8c76ab6a8a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:50:22.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7254" for this suite. 08/26/23 05:50:22.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:50:22.732
Aug 26 05:50:22.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename cronjob 08/26/23 05:50:22.733
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:50:22.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:50:22.751
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 08/26/23 05:50:22.754
STEP: Ensuring no jobs are scheduled 08/26/23 05:50:22.762
STEP: Ensuring no job exists by listing jobs explicitly 08/26/23 05:55:22.77
STEP: Removing cronjob 08/26/23 05:55:22.773
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 26 05:55:22.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4879" for this suite. 08/26/23 05:55:22.788
------------------------------
• [SLOW TEST] [300.062 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:50:22.732
    Aug 26 05:50:22.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename cronjob 08/26/23 05:50:22.733
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:50:22.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:50:22.751
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 08/26/23 05:50:22.754
    STEP: Ensuring no jobs are scheduled 08/26/23 05:50:22.762
    STEP: Ensuring no job exists by listing jobs explicitly 08/26/23 05:55:22.77
    STEP: Removing cronjob 08/26/23 05:55:22.773
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:55:22.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4879" for this suite. 08/26/23 05:55:22.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:55:22.795
Aug 26 05:55:22.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:55:22.796
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:22.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:22.818
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 08/26/23 05:55:22.823
Aug 26 05:55:22.832: INFO: Waiting up to 5m0s for pod "downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64" in namespace "downward-api-901" to be "Succeeded or Failed"
Aug 26 05:55:22.836: INFO: Pod "downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.692406ms
Aug 26 05:55:24.841: INFO: Pod "downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00922376s
Aug 26 05:55:26.842: INFO: Pod "downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009953762s
STEP: Saw pod success 08/26/23 05:55:26.842
Aug 26 05:55:26.842: INFO: Pod "downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64" satisfied condition "Succeeded or Failed"
Aug 26 05:55:26.846: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64 container dapi-container: <nil>
STEP: delete the pod 08/26/23 05:55:26.868
Aug 26 05:55:26.884: INFO: Waiting for pod downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64 to disappear
Aug 26 05:55:26.888: INFO: Pod downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 26 05:55:26.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-901" for this suite. 08/26/23 05:55:26.904
------------------------------
• [4.117 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:55:22.795
    Aug 26 05:55:22.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:55:22.796
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:22.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:22.818
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 08/26/23 05:55:22.823
    Aug 26 05:55:22.832: INFO: Waiting up to 5m0s for pod "downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64" in namespace "downward-api-901" to be "Succeeded or Failed"
    Aug 26 05:55:22.836: INFO: Pod "downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.692406ms
    Aug 26 05:55:24.841: INFO: Pod "downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00922376s
    Aug 26 05:55:26.842: INFO: Pod "downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009953762s
    STEP: Saw pod success 08/26/23 05:55:26.842
    Aug 26 05:55:26.842: INFO: Pod "downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64" satisfied condition "Succeeded or Failed"
    Aug 26 05:55:26.846: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64 container dapi-container: <nil>
    STEP: delete the pod 08/26/23 05:55:26.868
    Aug 26 05:55:26.884: INFO: Waiting for pod downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64 to disappear
    Aug 26 05:55:26.888: INFO: Pod downward-api-f6b099ff-f9d6-44d7-8ae3-dcf1ba11cd64 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:55:26.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-901" for this suite. 08/26/23 05:55:26.904
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:55:26.912
Aug 26 05:55:26.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 05:55:26.917
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:26.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:26.936
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:55:26.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7642" for this suite. 08/26/23 05:55:26.989
------------------------------
• [0.085 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:55:26.912
    Aug 26 05:55:26.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 05:55:26.917
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:26.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:26.936
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:55:26.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7642" for this suite. 08/26/23 05:55:26.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:55:27.002
Aug 26 05:55:27.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 05:55:27.006
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:27.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:27.038
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-b50e1cb5-bf6a-4332-a57d-7f2805384f9a 08/26/23 05:55:27.043
STEP: Creating a pod to test consume configMaps 08/26/23 05:55:27.047
Aug 26 05:55:27.055: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874" in namespace "configmap-6811" to be "Succeeded or Failed"
Aug 26 05:55:27.058: INFO: Pod "pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901107ms
Aug 26 05:55:29.064: INFO: Pod "pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009306622s
Aug 26 05:55:31.063: INFO: Pod "pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007628531s
STEP: Saw pod success 08/26/23 05:55:31.063
Aug 26 05:55:31.063: INFO: Pod "pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874" satisfied condition "Succeeded or Failed"
Aug 26 05:55:31.066: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874 container agnhost-container: <nil>
STEP: delete the pod 08/26/23 05:55:31.073
Aug 26 05:55:31.086: INFO: Waiting for pod pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874 to disappear
Aug 26 05:55:31.092: INFO: Pod pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:55:31.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6811" for this suite. 08/26/23 05:55:31.099
------------------------------
• [4.104 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:55:27.002
    Aug 26 05:55:27.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 05:55:27.006
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:27.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:27.038
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-b50e1cb5-bf6a-4332-a57d-7f2805384f9a 08/26/23 05:55:27.043
    STEP: Creating a pod to test consume configMaps 08/26/23 05:55:27.047
    Aug 26 05:55:27.055: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874" in namespace "configmap-6811" to be "Succeeded or Failed"
    Aug 26 05:55:27.058: INFO: Pod "pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901107ms
    Aug 26 05:55:29.064: INFO: Pod "pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009306622s
    Aug 26 05:55:31.063: INFO: Pod "pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007628531s
    STEP: Saw pod success 08/26/23 05:55:31.063
    Aug 26 05:55:31.063: INFO: Pod "pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874" satisfied condition "Succeeded or Failed"
    Aug 26 05:55:31.066: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874 container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 05:55:31.073
    Aug 26 05:55:31.086: INFO: Waiting for pod pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874 to disappear
    Aug 26 05:55:31.092: INFO: Pod pod-configmaps-9d8b9927-64b2-40af-9043-9c0cc97ff874 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:55:31.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6811" for this suite. 08/26/23 05:55:31.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:55:31.106
Aug 26 05:55:31.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:55:31.107
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:31.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:31.132
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:55:31.156
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:55:31.755
STEP: Deploying the webhook pod 08/26/23 05:55:31.766
STEP: Wait for the deployment to be ready 08/26/23 05:55:31.78
Aug 26 05:55:31.793: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:55:33.805
STEP: Verifying the service has paired with the endpoint 08/26/23 05:55:33.816
Aug 26 05:55:34.817: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Aug 26 05:55:34.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/26/23 05:55:35.333
STEP: Creating a custom resource that should be denied by the webhook 08/26/23 05:55:35.353
STEP: Creating a custom resource whose deletion would be denied by the webhook 08/26/23 05:55:37.399
STEP: Updating the custom resource with disallowed data should be denied 08/26/23 05:55:37.405
STEP: Deleting the custom resource should be denied 08/26/23 05:55:37.414
STEP: Remove the offending key and value from the custom resource data 08/26/23 05:55:37.421
STEP: Deleting the updated custom resource should be successful 08/26/23 05:55:37.44
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:55:37.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2755" for this suite. 08/26/23 05:55:38.038
STEP: Destroying namespace "webhook-2755-markers" for this suite. 08/26/23 05:55:38.046
------------------------------
• [SLOW TEST] [6.953 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:55:31.106
    Aug 26 05:55:31.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:55:31.107
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:31.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:31.132
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:55:31.156
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:55:31.755
    STEP: Deploying the webhook pod 08/26/23 05:55:31.766
    STEP: Wait for the deployment to be ready 08/26/23 05:55:31.78
    Aug 26 05:55:31.793: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:55:33.805
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:55:33.816
    Aug 26 05:55:34.817: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Aug 26 05:55:34.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/26/23 05:55:35.333
    STEP: Creating a custom resource that should be denied by the webhook 08/26/23 05:55:35.353
    STEP: Creating a custom resource whose deletion would be denied by the webhook 08/26/23 05:55:37.399
    STEP: Updating the custom resource with disallowed data should be denied 08/26/23 05:55:37.405
    STEP: Deleting the custom resource should be denied 08/26/23 05:55:37.414
    STEP: Remove the offending key and value from the custom resource data 08/26/23 05:55:37.421
    STEP: Deleting the updated custom resource should be successful 08/26/23 05:55:37.44
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:55:37.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2755" for this suite. 08/26/23 05:55:38.038
    STEP: Destroying namespace "webhook-2755-markers" for this suite. 08/26/23 05:55:38.046
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:55:38.06
Aug 26 05:55:38.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 05:55:38.061
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:38.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:38.085
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-6bb70668-9346-4f79-bfa9-50c3b3d764e0 08/26/23 05:55:38.096
STEP: Creating configMap with name cm-test-opt-upd-ba13da7c-3b24-4d99-a0e7-38866088067a 08/26/23 05:55:38.101
STEP: Creating the pod 08/26/23 05:55:38.106
Aug 26 05:55:38.116: INFO: Waiting up to 5m0s for pod "pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668" in namespace "configmap-7389" to be "running and ready"
Aug 26 05:55:38.119: INFO: Pod "pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668": Phase="Pending", Reason="", readiness=false. Elapsed: 2.852936ms
Aug 26 05:55:38.119: INFO: The phase of Pod pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:55:40.128: INFO: Pod "pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668": Phase="Running", Reason="", readiness=true. Elapsed: 2.012309377s
Aug 26 05:55:40.128: INFO: The phase of Pod pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668 is Running (Ready = true)
Aug 26 05:55:40.128: INFO: Pod "pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-6bb70668-9346-4f79-bfa9-50c3b3d764e0 08/26/23 05:55:40.17
STEP: Updating configmap cm-test-opt-upd-ba13da7c-3b24-4d99-a0e7-38866088067a 08/26/23 05:55:40.176
STEP: Creating configMap with name cm-test-opt-create-80888ebe-d39d-42d7-ae71-f9196d25fc71 08/26/23 05:55:40.181
STEP: waiting to observe update in volume 08/26/23 05:55:40.184
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 05:55:42.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7389" for this suite. 08/26/23 05:55:42.223
------------------------------
• [4.169 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:55:38.06
    Aug 26 05:55:38.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 05:55:38.061
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:38.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:38.085
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-6bb70668-9346-4f79-bfa9-50c3b3d764e0 08/26/23 05:55:38.096
    STEP: Creating configMap with name cm-test-opt-upd-ba13da7c-3b24-4d99-a0e7-38866088067a 08/26/23 05:55:38.101
    STEP: Creating the pod 08/26/23 05:55:38.106
    Aug 26 05:55:38.116: INFO: Waiting up to 5m0s for pod "pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668" in namespace "configmap-7389" to be "running and ready"
    Aug 26 05:55:38.119: INFO: Pod "pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668": Phase="Pending", Reason="", readiness=false. Elapsed: 2.852936ms
    Aug 26 05:55:38.119: INFO: The phase of Pod pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:55:40.128: INFO: Pod "pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668": Phase="Running", Reason="", readiness=true. Elapsed: 2.012309377s
    Aug 26 05:55:40.128: INFO: The phase of Pod pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668 is Running (Ready = true)
    Aug 26 05:55:40.128: INFO: Pod "pod-configmaps-6dbeb1e4-151a-4f27-b158-a7172b1cc668" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-6bb70668-9346-4f79-bfa9-50c3b3d764e0 08/26/23 05:55:40.17
    STEP: Updating configmap cm-test-opt-upd-ba13da7c-3b24-4d99-a0e7-38866088067a 08/26/23 05:55:40.176
    STEP: Creating configMap with name cm-test-opt-create-80888ebe-d39d-42d7-ae71-f9196d25fc71 08/26/23 05:55:40.181
    STEP: waiting to observe update in volume 08/26/23 05:55:40.184
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:55:42.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7389" for this suite. 08/26/23 05:55:42.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:55:42.23
Aug 26 05:55:42.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:55:42.231
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:42.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:42.253
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:55:42.274
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:55:42.462
STEP: Deploying the webhook pod 08/26/23 05:55:42.468
STEP: Wait for the deployment to be ready 08/26/23 05:55:42.483
Aug 26 05:55:42.496: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/26/23 05:55:44.508
STEP: Verifying the service has paired with the endpoint 08/26/23 05:55:44.526
Aug 26 05:55:45.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 08/26/23 05:55:45.534
STEP: Creating a custom resource definition that should be denied by the webhook 08/26/23 05:55:45.563
Aug 26 05:55:45.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:55:45.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-374" for this suite. 08/26/23 05:55:45.68
STEP: Destroying namespace "webhook-374-markers" for this suite. 08/26/23 05:55:45.692
------------------------------
• [3.471 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:55:42.23
    Aug 26 05:55:42.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:55:42.231
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:42.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:42.253
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:55:42.274
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:55:42.462
    STEP: Deploying the webhook pod 08/26/23 05:55:42.468
    STEP: Wait for the deployment to be ready 08/26/23 05:55:42.483
    Aug 26 05:55:42.496: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/26/23 05:55:44.508
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:55:44.526
    Aug 26 05:55:45.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 08/26/23 05:55:45.534
    STEP: Creating a custom resource definition that should be denied by the webhook 08/26/23 05:55:45.563
    Aug 26 05:55:45.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:55:45.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-374" for this suite. 08/26/23 05:55:45.68
    STEP: Destroying namespace "webhook-374-markers" for this suite. 08/26/23 05:55:45.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:55:45.702
Aug 26 05:55:45.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename custom-resource-definition 08/26/23 05:55:45.704
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:45.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:45.722
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Aug 26 05:55:45.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:55:46.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3899" for this suite. 08/26/23 05:55:46.279
------------------------------
• [0.583 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:55:45.702
    Aug 26 05:55:45.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename custom-resource-definition 08/26/23 05:55:45.704
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:45.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:45.722
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Aug 26 05:55:45.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:55:46.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3899" for this suite. 08/26/23 05:55:46.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:55:46.287
Aug 26 05:55:46.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 05:55:46.288
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:46.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:46.302
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 05:55:46.317
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:55:46.72
STEP: Deploying the webhook pod 08/26/23 05:55:46.727
STEP: Wait for the deployment to be ready 08/26/23 05:55:46.74
Aug 26 05:55:46.752: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 05:55:48.766
STEP: Verifying the service has paired with the endpoint 08/26/23 05:55:48.778
Aug 26 05:55:49.778: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/26/23 05:55:49.783
STEP: Registering slow webhook via the AdmissionRegistration API 08/26/23 05:55:49.783
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/26/23 05:55:49.809
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/26/23 05:55:50.82
STEP: Registering slow webhook via the AdmissionRegistration API 08/26/23 05:55:50.821
STEP: Having no error when timeout is longer than webhook latency 08/26/23 05:55:51.857
STEP: Registering slow webhook via the AdmissionRegistration API 08/26/23 05:55:51.857
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/26/23 05:55:56.907
STEP: Registering slow webhook via the AdmissionRegistration API 08/26/23 05:55:56.907
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 05:56:01.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7622" for this suite. 08/26/23 05:56:02.06
STEP: Destroying namespace "webhook-7622-markers" for this suite. 08/26/23 05:56:02.07
------------------------------
• [SLOW TEST] [15.797 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:55:46.287
    Aug 26 05:55:46.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 05:55:46.288
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:55:46.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:55:46.302
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 05:55:46.317
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 05:55:46.72
    STEP: Deploying the webhook pod 08/26/23 05:55:46.727
    STEP: Wait for the deployment to be ready 08/26/23 05:55:46.74
    Aug 26 05:55:46.752: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 05:55:48.766
    STEP: Verifying the service has paired with the endpoint 08/26/23 05:55:48.778
    Aug 26 05:55:49.778: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/26/23 05:55:49.783
    STEP: Registering slow webhook via the AdmissionRegistration API 08/26/23 05:55:49.783
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/26/23 05:55:49.809
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/26/23 05:55:50.82
    STEP: Registering slow webhook via the AdmissionRegistration API 08/26/23 05:55:50.821
    STEP: Having no error when timeout is longer than webhook latency 08/26/23 05:55:51.857
    STEP: Registering slow webhook via the AdmissionRegistration API 08/26/23 05:55:51.857
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/26/23 05:55:56.907
    STEP: Registering slow webhook via the AdmissionRegistration API 08/26/23 05:55:56.907
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:56:01.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7622" for this suite. 08/26/23 05:56:02.06
    STEP: Destroying namespace "webhook-7622-markers" for this suite. 08/26/23 05:56:02.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:56:02.085
Aug 26 05:56:02.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename limitrange 08/26/23 05:56:02.086
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:02.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:02.107
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-4d9qx" in namespace "limitrange-7886" 08/26/23 05:56:02.11
STEP: Creating another limitRange in another namespace 08/26/23 05:56:02.117
Aug 26 05:56:02.150: INFO: Namespace "e2e-limitrange-4d9qx-3684" created
Aug 26 05:56:02.151: INFO: Creating LimitRange "e2e-limitrange-4d9qx" in namespace "e2e-limitrange-4d9qx-3684"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-4d9qx" 08/26/23 05:56:02.158
Aug 26 05:56:02.162: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-4d9qx" in "limitrange-7886" namespace 08/26/23 05:56:02.162
Aug 26 05:56:02.174: INFO: LimitRange "e2e-limitrange-4d9qx" has been patched
STEP: Delete LimitRange "e2e-limitrange-4d9qx" by Collection with labelSelector: "e2e-limitrange-4d9qx=patched" 08/26/23 05:56:02.174
STEP: Confirm that the limitRange "e2e-limitrange-4d9qx" has been deleted 08/26/23 05:56:02.183
Aug 26 05:56:02.183: INFO: Requesting list of LimitRange to confirm quantity
Aug 26 05:56:02.198: INFO: Found 0 LimitRange with label "e2e-limitrange-4d9qx=patched"
Aug 26 05:56:02.198: INFO: LimitRange "e2e-limitrange-4d9qx" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-4d9qx" 08/26/23 05:56:02.198
Aug 26 05:56:02.205: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 26 05:56:02.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7886" for this suite. 08/26/23 05:56:02.218
STEP: Destroying namespace "e2e-limitrange-4d9qx-3684" for this suite. 08/26/23 05:56:02.226
------------------------------
• [0.152 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:56:02.085
    Aug 26 05:56:02.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename limitrange 08/26/23 05:56:02.086
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:02.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:02.107
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-4d9qx" in namespace "limitrange-7886" 08/26/23 05:56:02.11
    STEP: Creating another limitRange in another namespace 08/26/23 05:56:02.117
    Aug 26 05:56:02.150: INFO: Namespace "e2e-limitrange-4d9qx-3684" created
    Aug 26 05:56:02.151: INFO: Creating LimitRange "e2e-limitrange-4d9qx" in namespace "e2e-limitrange-4d9qx-3684"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-4d9qx" 08/26/23 05:56:02.158
    Aug 26 05:56:02.162: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-4d9qx" in "limitrange-7886" namespace 08/26/23 05:56:02.162
    Aug 26 05:56:02.174: INFO: LimitRange "e2e-limitrange-4d9qx" has been patched
    STEP: Delete LimitRange "e2e-limitrange-4d9qx" by Collection with labelSelector: "e2e-limitrange-4d9qx=patched" 08/26/23 05:56:02.174
    STEP: Confirm that the limitRange "e2e-limitrange-4d9qx" has been deleted 08/26/23 05:56:02.183
    Aug 26 05:56:02.183: INFO: Requesting list of LimitRange to confirm quantity
    Aug 26 05:56:02.198: INFO: Found 0 LimitRange with label "e2e-limitrange-4d9qx=patched"
    Aug 26 05:56:02.198: INFO: LimitRange "e2e-limitrange-4d9qx" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-4d9qx" 08/26/23 05:56:02.198
    Aug 26 05:56:02.205: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:56:02.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7886" for this suite. 08/26/23 05:56:02.218
    STEP: Destroying namespace "e2e-limitrange-4d9qx-3684" for this suite. 08/26/23 05:56:02.226
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:56:02.237
Aug 26 05:56:02.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:56:02.238
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:02.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:02.262
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 08/26/23 05:56:02.265
Aug 26 05:56:02.277: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e" in namespace "downward-api-6688" to be "Succeeded or Failed"
Aug 26 05:56:02.285: INFO: Pod "downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.137594ms
Aug 26 05:56:04.290: INFO: Pod "downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013291144s
Aug 26 05:56:06.289: INFO: Pod "downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012398865s
STEP: Saw pod success 08/26/23 05:56:06.289
Aug 26 05:56:06.289: INFO: Pod "downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e" satisfied condition "Succeeded or Failed"
Aug 26 05:56:06.292: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e container client-container: <nil>
STEP: delete the pod 08/26/23 05:56:06.316
Aug 26 05:56:06.330: INFO: Waiting for pod downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e to disappear
Aug 26 05:56:06.333: INFO: Pod downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 05:56:06.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6688" for this suite. 08/26/23 05:56:06.342
------------------------------
• [4.113 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:56:02.237
    Aug 26 05:56:02.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:56:02.238
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:02.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:02.262
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 08/26/23 05:56:02.265
    Aug 26 05:56:02.277: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e" in namespace "downward-api-6688" to be "Succeeded or Failed"
    Aug 26 05:56:02.285: INFO: Pod "downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.137594ms
    Aug 26 05:56:04.290: INFO: Pod "downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013291144s
    Aug 26 05:56:06.289: INFO: Pod "downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012398865s
    STEP: Saw pod success 08/26/23 05:56:06.289
    Aug 26 05:56:06.289: INFO: Pod "downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e" satisfied condition "Succeeded or Failed"
    Aug 26 05:56:06.292: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e container client-container: <nil>
    STEP: delete the pod 08/26/23 05:56:06.316
    Aug 26 05:56:06.330: INFO: Waiting for pod downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e to disappear
    Aug 26 05:56:06.333: INFO: Pod downwardapi-volume-dda1f436-3b5b-4434-a889-bb64dbd9ab7e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:56:06.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6688" for this suite. 08/26/23 05:56:06.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:56:06.353
Aug 26 05:56:06.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 05:56:06.354
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:06.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:06.369
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Aug 26 05:56:06.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: creating the pod 08/26/23 05:56:06.373
STEP: submitting the pod to kubernetes 08/26/23 05:56:06.373
Aug 26 05:56:06.382: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705" in namespace "pods-3543" to be "running and ready"
Aug 26 05:56:06.385: INFO: Pod "pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369964ms
Aug 26 05:56:06.385: INFO: The phase of Pod pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:56:08.389: INFO: Pod "pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705": Phase="Running", Reason="", readiness=true. Elapsed: 2.007718215s
Aug 26 05:56:08.389: INFO: The phase of Pod pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705 is Running (Ready = true)
Aug 26 05:56:08.389: INFO: Pod "pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 26 05:56:08.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3543" for this suite. 08/26/23 05:56:08.424
------------------------------
• [2.077 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:56:06.353
    Aug 26 05:56:06.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 05:56:06.354
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:06.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:06.369
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Aug 26 05:56:06.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: creating the pod 08/26/23 05:56:06.373
    STEP: submitting the pod to kubernetes 08/26/23 05:56:06.373
    Aug 26 05:56:06.382: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705" in namespace "pods-3543" to be "running and ready"
    Aug 26 05:56:06.385: INFO: Pod "pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369964ms
    Aug 26 05:56:06.385: INFO: The phase of Pod pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:56:08.389: INFO: Pod "pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705": Phase="Running", Reason="", readiness=true. Elapsed: 2.007718215s
    Aug 26 05:56:08.389: INFO: The phase of Pod pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705 is Running (Ready = true)
    Aug 26 05:56:08.389: INFO: Pod "pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:56:08.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3543" for this suite. 08/26/23 05:56:08.424
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:56:08.431
Aug 26 05:56:08.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 05:56:08.432
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:08.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:08.449
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-4f65078e-af08-4f89-883b-e6b63218bd65 08/26/23 05:56:08.452
STEP: Creating a pod to test consume secrets 08/26/23 05:56:08.457
Aug 26 05:56:08.467: INFO: Waiting up to 5m0s for pod "pod-secrets-97382efd-21dc-4f42-b576-380e7f823012" in namespace "secrets-8102" to be "Succeeded or Failed"
Aug 26 05:56:08.470: INFO: Pod "pod-secrets-97382efd-21dc-4f42-b576-380e7f823012": Phase="Pending", Reason="", readiness=false. Elapsed: 3.093005ms
Aug 26 05:56:10.476: INFO: Pod "pod-secrets-97382efd-21dc-4f42-b576-380e7f823012": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008447497s
Aug 26 05:56:12.475: INFO: Pod "pod-secrets-97382efd-21dc-4f42-b576-380e7f823012": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008158681s
STEP: Saw pod success 08/26/23 05:56:12.475
Aug 26 05:56:12.475: INFO: Pod "pod-secrets-97382efd-21dc-4f42-b576-380e7f823012" satisfied condition "Succeeded or Failed"
Aug 26 05:56:12.479: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-secrets-97382efd-21dc-4f42-b576-380e7f823012 container secret-volume-test: <nil>
STEP: delete the pod 08/26/23 05:56:12.486
Aug 26 05:56:12.499: INFO: Waiting for pod pod-secrets-97382efd-21dc-4f42-b576-380e7f823012 to disappear
Aug 26 05:56:12.502: INFO: Pod pod-secrets-97382efd-21dc-4f42-b576-380e7f823012 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 05:56:12.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8102" for this suite. 08/26/23 05:56:12.51
------------------------------
• [4.087 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:56:08.431
    Aug 26 05:56:08.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 05:56:08.432
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:08.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:08.449
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-4f65078e-af08-4f89-883b-e6b63218bd65 08/26/23 05:56:08.452
    STEP: Creating a pod to test consume secrets 08/26/23 05:56:08.457
    Aug 26 05:56:08.467: INFO: Waiting up to 5m0s for pod "pod-secrets-97382efd-21dc-4f42-b576-380e7f823012" in namespace "secrets-8102" to be "Succeeded or Failed"
    Aug 26 05:56:08.470: INFO: Pod "pod-secrets-97382efd-21dc-4f42-b576-380e7f823012": Phase="Pending", Reason="", readiness=false. Elapsed: 3.093005ms
    Aug 26 05:56:10.476: INFO: Pod "pod-secrets-97382efd-21dc-4f42-b576-380e7f823012": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008447497s
    Aug 26 05:56:12.475: INFO: Pod "pod-secrets-97382efd-21dc-4f42-b576-380e7f823012": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008158681s
    STEP: Saw pod success 08/26/23 05:56:12.475
    Aug 26 05:56:12.475: INFO: Pod "pod-secrets-97382efd-21dc-4f42-b576-380e7f823012" satisfied condition "Succeeded or Failed"
    Aug 26 05:56:12.479: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod pod-secrets-97382efd-21dc-4f42-b576-380e7f823012 container secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 05:56:12.486
    Aug 26 05:56:12.499: INFO: Waiting for pod pod-secrets-97382efd-21dc-4f42-b576-380e7f823012 to disappear
    Aug 26 05:56:12.502: INFO: Pod pod-secrets-97382efd-21dc-4f42-b576-380e7f823012 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:56:12.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8102" for this suite. 08/26/23 05:56:12.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:56:12.519
Aug 26 05:56:12.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-lifecycle-hook 08/26/23 05:56:12.52
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:12.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:12.541
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/26/23 05:56:12.552
Aug 26 05:56:12.563: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3641" to be "running and ready"
Aug 26 05:56:12.572: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.209791ms
Aug 26 05:56:12.572: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:56:14.576: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013404685s
Aug 26 05:56:14.576: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 26 05:56:14.576: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 08/26/23 05:56:14.579
Aug 26 05:56:14.587: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3641" to be "running and ready"
Aug 26 05:56:14.590: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.376018ms
Aug 26 05:56:14.590: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 26 05:56:16.595: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008156862s
Aug 26 05:56:16.595: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Aug 26 05:56:16.595: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/26/23 05:56:16.598
STEP: delete the pod with lifecycle hook 08/26/23 05:56:16.619
Aug 26 05:56:16.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 26 05:56:16.647: INFO: Pod pod-with-poststart-http-hook still exists
Aug 26 05:56:18.648: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 26 05:56:18.652: INFO: Pod pod-with-poststart-http-hook still exists
Aug 26 05:56:20.648: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 26 05:56:20.652: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 26 05:56:20.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3641" for this suite. 08/26/23 05:56:20.664
------------------------------
• [SLOW TEST] [8.153 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:56:12.519
    Aug 26 05:56:12.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/26/23 05:56:12.52
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:12.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:12.541
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/26/23 05:56:12.552
    Aug 26 05:56:12.563: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3641" to be "running and ready"
    Aug 26 05:56:12.572: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.209791ms
    Aug 26 05:56:12.572: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:56:14.576: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013404685s
    Aug 26 05:56:14.576: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 26 05:56:14.576: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 08/26/23 05:56:14.579
    Aug 26 05:56:14.587: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3641" to be "running and ready"
    Aug 26 05:56:14.590: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.376018ms
    Aug 26 05:56:14.590: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 05:56:16.595: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008156862s
    Aug 26 05:56:16.595: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Aug 26 05:56:16.595: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/26/23 05:56:16.598
    STEP: delete the pod with lifecycle hook 08/26/23 05:56:16.619
    Aug 26 05:56:16.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 26 05:56:16.647: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 26 05:56:18.648: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 26 05:56:18.652: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 26 05:56:20.648: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 26 05:56:20.652: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:56:20.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3641" for this suite. 08/26/23 05:56:20.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:56:20.673
Aug 26 05:56:20.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 05:56:20.674
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:20.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:20.698
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 08/26/23 05:56:20.701
Aug 26 05:56:20.708: INFO: Waiting up to 5m0s for pod "downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7" in namespace "downward-api-2639" to be "Succeeded or Failed"
Aug 26 05:56:20.713: INFO: Pod "downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281587ms
Aug 26 05:56:22.718: INFO: Pod "downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009294323s
Aug 26 05:56:24.719: INFO: Pod "downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011058885s
STEP: Saw pod success 08/26/23 05:56:24.719
Aug 26 05:56:24.720: INFO: Pod "downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7" satisfied condition "Succeeded or Failed"
Aug 26 05:56:24.723: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7 container dapi-container: <nil>
STEP: delete the pod 08/26/23 05:56:24.752
Aug 26 05:56:24.773: INFO: Waiting for pod downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7 to disappear
Aug 26 05:56:24.777: INFO: Pod downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 26 05:56:24.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2639" for this suite. 08/26/23 05:56:24.786
------------------------------
• [4.121 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:56:20.673
    Aug 26 05:56:20.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 05:56:20.674
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:20.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:20.698
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 08/26/23 05:56:20.701
    Aug 26 05:56:20.708: INFO: Waiting up to 5m0s for pod "downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7" in namespace "downward-api-2639" to be "Succeeded or Failed"
    Aug 26 05:56:20.713: INFO: Pod "downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281587ms
    Aug 26 05:56:22.718: INFO: Pod "downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009294323s
    Aug 26 05:56:24.719: INFO: Pod "downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011058885s
    STEP: Saw pod success 08/26/23 05:56:24.719
    Aug 26 05:56:24.720: INFO: Pod "downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7" satisfied condition "Succeeded or Failed"
    Aug 26 05:56:24.723: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7 container dapi-container: <nil>
    STEP: delete the pod 08/26/23 05:56:24.752
    Aug 26 05:56:24.773: INFO: Waiting for pod downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7 to disappear
    Aug 26 05:56:24.777: INFO: Pod downward-api-32114b3d-e581-497a-afdc-da91c0f58ee7 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 26 05:56:24.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2639" for this suite. 08/26/23 05:56:24.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 05:56:24.795
Aug 26 05:56:24.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sched-pred 08/26/23 05:56:24.796
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:24.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:24.826
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 26 05:56:24.829: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 26 05:56:24.849: INFO: Waiting for terminating namespaces to be deleted...
Aug 26 05:56:24.856: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-101.us-west-2.compute.internal before test
Aug 26 05:56:24.886: INFO: calico-node-hgm7c from kube-system started at 2023-08-26 04:58:59 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.886: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 05:56:24.886: INFO: calico-typha-6dd9648c8f-85hjp from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.886: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 05:56:24.886: INFO: pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705 from pods-3543 started at 2023-08-26 05:56:06 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.886: INFO: 	Container main ready: true, restart count 0
Aug 26 05:56:24.886: INFO: sonobuoy from sonobuoy started at 2023-08-26 05:16:19 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.886: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 26 05:56:24.886: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:56:24.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:56:24.886: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 05:56:24.886: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-126.us-west-2.compute.internal before test
Aug 26 05:56:24.950: INFO: calico-node-8272m from kube-system started at 2023-08-26 04:58:47 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.950: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 05:56:24.950: INFO: calico-typha-6dd9648c8f-th65n from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.950: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 05:56:24.950: INFO: sonobuoy-e2e-job-c6e00385bb4e4fb8 from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:56:24.950: INFO: 	Container e2e ready: true, restart count 0
Aug 26 05:56:24.950: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:56:24.950: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-ckdgn from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:56:24.950: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:56:24.950: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 05:56:24.950: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-23.us-west-2.compute.internal before test
Aug 26 05:56:24.972: INFO: pod-handle-http-request from container-lifecycle-hook-3641 started at 2023-08-26 05:56:12 +0000 UTC (2 container statuses recorded)
Aug 26 05:56:24.972: INFO: 	Container container-handle-http-request ready: true, restart count 0
Aug 26 05:56:24.972: INFO: 	Container container-handle-https-request ready: true, restart count 0
Aug 26 05:56:24.972: INFO: calico-kube-controllers-76798f54cb-cdxwm from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.972: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 26 05:56:24.972: INFO: calico-node-trdnz from kube-system started at 2023-08-26 04:59:15 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.972: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 05:56:24.972: INFO: calico-typha-6dd9648c8f-x4b7r from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.972: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 05:56:24.972: INFO: calico-typha-autoscaler-54c8866496-tmj59 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.972: INFO: 	Container autoscaler ready: true, restart count 0
Aug 26 05:56:24.972: INFO: coredns-58ffcc48df-2q4rv from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.972: INFO: 	Container coredns ready: true, restart count 0
Aug 26 05:56:24.972: INFO: kube-dns-autoscaler-f68f756b6-78kdn from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.972: INFO: 	Container autoscaler ready: true, restart count 0
Aug 26 05:56:24.972: INFO: kube-state-metrics-99bbfb4cd-g7vn6 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.972: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 26 05:56:24.972: INFO: dashboard-metrics-scraper-6c57f89c7c-2vtv5 from kubernetes-dashboard started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:24.972: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 26 05:56:24.972: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-g65bg from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:56:24.972: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:56:24.972: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 05:56:24.972: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-31.us-west-2.compute.internal before test
Aug 26 05:56:25.009: INFO: calico-node-mxmcm from kube-system started at 2023-08-26 04:59:09 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:25.009: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 05:56:25.009: INFO: coredns-58ffcc48df-mvbsx from kube-system started at 2023-08-26 05:54:46 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:25.009: INFO: 	Container coredns ready: true, restart count 0
Aug 26 05:56:25.009: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-6gxwx from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:56:25.009: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:56:25.009: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 05:56:25.009: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-5.us-west-2.compute.internal before test
Aug 26 05:56:25.033: INFO: calico-node-jftpx from kube-system started at 2023-08-26 04:58:49 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:25.033: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 05:56:25.033: INFO: metrics-server-b7db9955-cln8d from kube-system started at 2023-08-26 05:53:20 +0000 UTC (1 container statuses recorded)
Aug 26 05:56:25.033: INFO: 	Container metrics-server ready: true, restart count 0
Aug 26 05:56:25.033: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-tpq9m from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 05:56:25.033: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 05:56:25.033: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/26/23 05:56:25.033
Aug 26 05:56:25.042: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3306" to be "running"
Aug 26 05:56:25.048: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201003ms
Aug 26 05:56:27.054: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011290386s
Aug 26 05:56:27.054: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/26/23 05:56:27.057
STEP: Trying to apply a random label on the found node. 08/26/23 05:56:27.078
STEP: verifying the node has the label kubernetes.io/e2e-9ed2eb99-b6ce-4e9a-aa42-c87ea056f213 95 08/26/23 05:56:27.09
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/26/23 05:56:27.1
Aug 26 05:56:27.110: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3306" to be "not pending"
Aug 26 05:56:27.116: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.666679ms
Aug 26 05:56:29.123: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.013319518s
Aug 26 05:56:29.123: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.1.31 on the node which pod4 resides and expect not scheduled 08/26/23 05:56:29.123
Aug 26 05:56:29.132: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3306" to be "not pending"
Aug 26 05:56:29.136: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.899089ms
Aug 26 05:56:31.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008968524s
Aug 26 05:56:33.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008465443s
Aug 26 05:56:35.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009306041s
Aug 26 05:56:37.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010045505s
Aug 26 05:56:39.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011036757s
Aug 26 05:56:41.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008279062s
Aug 26 05:56:43.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010176507s
Aug 26 05:56:45.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009616106s
Aug 26 05:56:47.145: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01327487s
Aug 26 05:56:49.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009356609s
Aug 26 05:56:51.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009893656s
Aug 26 05:56:53.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009705027s
Aug 26 05:56:55.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008122807s
Aug 26 05:56:57.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008512363s
Aug 26 05:56:59.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009148788s
Aug 26 05:57:01.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008867123s
Aug 26 05:57:03.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009690328s
Aug 26 05:57:05.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009606336s
Aug 26 05:57:07.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008994395s
Aug 26 05:57:09.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009348807s
Aug 26 05:57:11.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010260363s
Aug 26 05:57:13.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009063026s
Aug 26 05:57:15.145: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012927901s
Aug 26 05:57:17.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009277619s
Aug 26 05:57:19.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009499551s
Aug 26 05:57:21.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011159874s
Aug 26 05:57:23.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008371018s
Aug 26 05:57:25.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009841095s
Aug 26 05:57:27.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010796472s
Aug 26 05:57:29.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009439429s
Aug 26 05:57:31.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00835097s
Aug 26 05:57:33.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009746356s
Aug 26 05:57:35.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008509922s
Aug 26 05:57:37.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009887842s
Aug 26 05:57:39.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009740237s
Aug 26 05:57:41.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009170074s
Aug 26 05:57:43.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009726896s
Aug 26 05:57:45.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010821232s
Aug 26 05:57:47.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008533563s
Aug 26 05:57:49.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009603553s
Aug 26 05:57:51.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008744721s
Aug 26 05:57:53.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009901433s
Aug 26 05:57:55.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01104405s
Aug 26 05:57:57.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008546022s
Aug 26 05:57:59.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008436395s
Aug 26 05:58:01.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009818249s
Aug 26 05:58:03.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009164826s
Aug 26 05:58:05.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009157192s
Aug 26 05:58:07.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008481257s
Aug 26 05:58:09.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009103038s
Aug 26 05:58:11.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011397108s
Aug 26 05:58:13.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009208968s
Aug 26 05:58:15.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008588525s
Aug 26 05:58:17.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008293484s
Aug 26 05:58:19.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009657001s
Aug 26 05:58:21.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008882993s
Aug 26 05:58:23.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010044301s
Aug 26 05:58:25.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010084565s
Aug 26 05:58:27.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00889718s
Aug 26 05:58:29.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009637563s
Aug 26 05:58:31.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.008923838s
Aug 26 05:58:33.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009157843s
Aug 26 05:58:35.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.011816587s
Aug 26 05:58:37.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.011146388s
Aug 26 05:58:39.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.010186607s
Aug 26 05:58:41.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.008460582s
Aug 26 05:58:43.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.010228703s
Aug 26 05:58:45.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009520621s
Aug 26 05:58:47.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010003697s
Aug 26 05:58:49.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008758978s
Aug 26 05:58:51.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.010261198s
Aug 26 05:58:53.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.011119033s
Aug 26 05:58:55.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010456511s
Aug 26 05:58:57.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.00883854s
Aug 26 05:58:59.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.01097765s
Aug 26 05:59:01.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.010364477s
Aug 26 05:59:03.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.009733813s
Aug 26 05:59:05.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.010003394s
Aug 26 05:59:07.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008630497s
Aug 26 05:59:09.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.010448475s
Aug 26 05:59:11.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009160704s
Aug 26 05:59:13.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009713563s
Aug 26 05:59:15.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009698818s
Aug 26 05:59:17.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009444607s
Aug 26 05:59:19.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008469172s
Aug 26 05:59:21.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008595954s
Aug 26 05:59:23.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.00948385s
Aug 26 05:59:25.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008603579s
Aug 26 05:59:27.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008106242s
Aug 26 05:59:29.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.00933959s
Aug 26 05:59:31.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009568274s
Aug 26 05:59:33.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008355617s
Aug 26 05:59:35.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.011490504s
Aug 26 05:59:37.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009120905s
Aug 26 05:59:39.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009150204s
Aug 26 05:59:41.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008420073s
Aug 26 05:59:43.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008069112s
Aug 26 05:59:45.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009250274s
Aug 26 05:59:47.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009637162s
Aug 26 05:59:49.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008246038s
Aug 26 05:59:51.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.009122642s
Aug 26 05:59:53.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.010014423s
Aug 26 05:59:55.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009937771s
Aug 26 05:59:57.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008238756s
Aug 26 05:59:59.150: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.018225444s
Aug 26 06:00:01.145: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.01299929s
Aug 26 06:00:03.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.010092287s
Aug 26 06:00:05.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009493323s
Aug 26 06:00:07.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009479256s
Aug 26 06:00:09.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.009212891s
Aug 26 06:00:11.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.010314282s
Aug 26 06:00:13.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.010880904s
Aug 26 06:00:15.145: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.013140528s
Aug 26 06:00:17.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.008786396s
Aug 26 06:00:19.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.009440458s
Aug 26 06:00:21.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008297546s
Aug 26 06:00:23.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.009665035s
Aug 26 06:00:25.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009352821s
Aug 26 06:00:27.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009038313s
Aug 26 06:00:29.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009384567s
Aug 26 06:00:31.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008098254s
Aug 26 06:00:33.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009470715s
Aug 26 06:00:35.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009625824s
Aug 26 06:00:37.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.010910052s
Aug 26 06:00:39.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009698164s
Aug 26 06:00:41.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.012563148s
Aug 26 06:00:43.147: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.015539766s
Aug 26 06:00:45.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009528329s
Aug 26 06:00:47.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.010385666s
Aug 26 06:00:49.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008359314s
Aug 26 06:00:51.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009008042s
Aug 26 06:00:53.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009167771s
Aug 26 06:00:55.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009452113s
Aug 26 06:00:57.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009142075s
Aug 26 06:00:59.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.011295667s
Aug 26 06:01:01.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009651859s
Aug 26 06:01:03.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.010366929s
Aug 26 06:01:05.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.011068755s
Aug 26 06:01:07.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.008180139s
Aug 26 06:01:09.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.01012851s
Aug 26 06:01:11.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008916922s
Aug 26 06:01:13.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008415878s
Aug 26 06:01:15.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009363024s
Aug 26 06:01:17.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.01061765s
Aug 26 06:01:19.155: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.023566554s
Aug 26 06:01:21.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008219898s
Aug 26 06:01:23.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.009997454s
Aug 26 06:01:25.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009395366s
Aug 26 06:01:27.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009227142s
Aug 26 06:01:29.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009923651s
Aug 26 06:01:29.148: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.015874722s
STEP: removing the label kubernetes.io/e2e-9ed2eb99-b6ce-4e9a-aa42-c87ea056f213 off the node ip-10-0-1-31.us-west-2.compute.internal 08/26/23 06:01:29.148
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9ed2eb99-b6ce-4e9a-aa42-c87ea056f213 08/26/23 06:01:29.167
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:01:29.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3306" for this suite. 08/26/23 06:01:29.185
------------------------------
• [SLOW TEST] [304.399 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 05:56:24.795
    Aug 26 05:56:24.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sched-pred 08/26/23 05:56:24.796
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 05:56:24.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 05:56:24.826
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 26 05:56:24.829: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 26 05:56:24.849: INFO: Waiting for terminating namespaces to be deleted...
    Aug 26 05:56:24.856: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-101.us-west-2.compute.internal before test
    Aug 26 05:56:24.886: INFO: calico-node-hgm7c from kube-system started at 2023-08-26 04:58:59 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.886: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 05:56:24.886: INFO: calico-typha-6dd9648c8f-85hjp from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.886: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 05:56:24.886: INFO: pod-logs-websocket-bfeb1bde-a689-470b-a927-b719dfa7d705 from pods-3543 started at 2023-08-26 05:56:06 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.886: INFO: 	Container main ready: true, restart count 0
    Aug 26 05:56:24.886: INFO: sonobuoy from sonobuoy started at 2023-08-26 05:16:19 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.886: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 26 05:56:24.886: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:56:24.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:56:24.886: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 05:56:24.886: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-126.us-west-2.compute.internal before test
    Aug 26 05:56:24.950: INFO: calico-node-8272m from kube-system started at 2023-08-26 04:58:47 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.950: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 05:56:24.950: INFO: calico-typha-6dd9648c8f-th65n from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.950: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 05:56:24.950: INFO: sonobuoy-e2e-job-c6e00385bb4e4fb8 from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:56:24.950: INFO: 	Container e2e ready: true, restart count 0
    Aug 26 05:56:24.950: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:56:24.950: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-ckdgn from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:56:24.950: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:56:24.950: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 05:56:24.950: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-23.us-west-2.compute.internal before test
    Aug 26 05:56:24.972: INFO: pod-handle-http-request from container-lifecycle-hook-3641 started at 2023-08-26 05:56:12 +0000 UTC (2 container statuses recorded)
    Aug 26 05:56:24.972: INFO: 	Container container-handle-http-request ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: 	Container container-handle-https-request ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: calico-kube-controllers-76798f54cb-cdxwm from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.972: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: calico-node-trdnz from kube-system started at 2023-08-26 04:59:15 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.972: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: calico-typha-6dd9648c8f-x4b7r from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.972: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: calico-typha-autoscaler-54c8866496-tmj59 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.972: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: coredns-58ffcc48df-2q4rv from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.972: INFO: 	Container coredns ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: kube-dns-autoscaler-f68f756b6-78kdn from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.972: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: kube-state-metrics-99bbfb4cd-g7vn6 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.972: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: dashboard-metrics-scraper-6c57f89c7c-2vtv5 from kubernetes-dashboard started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:24.972: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-g65bg from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:56:24.972: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 05:56:24.972: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-31.us-west-2.compute.internal before test
    Aug 26 05:56:25.009: INFO: calico-node-mxmcm from kube-system started at 2023-08-26 04:59:09 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:25.009: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 05:56:25.009: INFO: coredns-58ffcc48df-mvbsx from kube-system started at 2023-08-26 05:54:46 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:25.009: INFO: 	Container coredns ready: true, restart count 0
    Aug 26 05:56:25.009: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-6gxwx from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:56:25.009: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:56:25.009: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 05:56:25.009: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-5.us-west-2.compute.internal before test
    Aug 26 05:56:25.033: INFO: calico-node-jftpx from kube-system started at 2023-08-26 04:58:49 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:25.033: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 05:56:25.033: INFO: metrics-server-b7db9955-cln8d from kube-system started at 2023-08-26 05:53:20 +0000 UTC (1 container statuses recorded)
    Aug 26 05:56:25.033: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 26 05:56:25.033: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-tpq9m from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 05:56:25.033: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 05:56:25.033: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/26/23 05:56:25.033
    Aug 26 05:56:25.042: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3306" to be "running"
    Aug 26 05:56:25.048: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201003ms
    Aug 26 05:56:27.054: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011290386s
    Aug 26 05:56:27.054: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/26/23 05:56:27.057
    STEP: Trying to apply a random label on the found node. 08/26/23 05:56:27.078
    STEP: verifying the node has the label kubernetes.io/e2e-9ed2eb99-b6ce-4e9a-aa42-c87ea056f213 95 08/26/23 05:56:27.09
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/26/23 05:56:27.1
    Aug 26 05:56:27.110: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3306" to be "not pending"
    Aug 26 05:56:27.116: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.666679ms
    Aug 26 05:56:29.123: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.013319518s
    Aug 26 05:56:29.123: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.1.31 on the node which pod4 resides and expect not scheduled 08/26/23 05:56:29.123
    Aug 26 05:56:29.132: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3306" to be "not pending"
    Aug 26 05:56:29.136: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.899089ms
    Aug 26 05:56:31.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008968524s
    Aug 26 05:56:33.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008465443s
    Aug 26 05:56:35.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009306041s
    Aug 26 05:56:37.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010045505s
    Aug 26 05:56:39.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011036757s
    Aug 26 05:56:41.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008279062s
    Aug 26 05:56:43.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010176507s
    Aug 26 05:56:45.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009616106s
    Aug 26 05:56:47.145: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01327487s
    Aug 26 05:56:49.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009356609s
    Aug 26 05:56:51.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009893656s
    Aug 26 05:56:53.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009705027s
    Aug 26 05:56:55.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008122807s
    Aug 26 05:56:57.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008512363s
    Aug 26 05:56:59.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009148788s
    Aug 26 05:57:01.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008867123s
    Aug 26 05:57:03.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009690328s
    Aug 26 05:57:05.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009606336s
    Aug 26 05:57:07.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008994395s
    Aug 26 05:57:09.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009348807s
    Aug 26 05:57:11.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010260363s
    Aug 26 05:57:13.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009063026s
    Aug 26 05:57:15.145: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012927901s
    Aug 26 05:57:17.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009277619s
    Aug 26 05:57:19.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009499551s
    Aug 26 05:57:21.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011159874s
    Aug 26 05:57:23.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008371018s
    Aug 26 05:57:25.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009841095s
    Aug 26 05:57:27.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010796472s
    Aug 26 05:57:29.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009439429s
    Aug 26 05:57:31.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00835097s
    Aug 26 05:57:33.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009746356s
    Aug 26 05:57:35.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008509922s
    Aug 26 05:57:37.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009887842s
    Aug 26 05:57:39.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009740237s
    Aug 26 05:57:41.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009170074s
    Aug 26 05:57:43.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009726896s
    Aug 26 05:57:45.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010821232s
    Aug 26 05:57:47.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008533563s
    Aug 26 05:57:49.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009603553s
    Aug 26 05:57:51.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008744721s
    Aug 26 05:57:53.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009901433s
    Aug 26 05:57:55.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01104405s
    Aug 26 05:57:57.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008546022s
    Aug 26 05:57:59.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008436395s
    Aug 26 05:58:01.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009818249s
    Aug 26 05:58:03.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009164826s
    Aug 26 05:58:05.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009157192s
    Aug 26 05:58:07.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008481257s
    Aug 26 05:58:09.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009103038s
    Aug 26 05:58:11.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011397108s
    Aug 26 05:58:13.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009208968s
    Aug 26 05:58:15.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008588525s
    Aug 26 05:58:17.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008293484s
    Aug 26 05:58:19.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009657001s
    Aug 26 05:58:21.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008882993s
    Aug 26 05:58:23.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010044301s
    Aug 26 05:58:25.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010084565s
    Aug 26 05:58:27.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00889718s
    Aug 26 05:58:29.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009637563s
    Aug 26 05:58:31.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.008923838s
    Aug 26 05:58:33.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009157843s
    Aug 26 05:58:35.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.011816587s
    Aug 26 05:58:37.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.011146388s
    Aug 26 05:58:39.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.010186607s
    Aug 26 05:58:41.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.008460582s
    Aug 26 05:58:43.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.010228703s
    Aug 26 05:58:45.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009520621s
    Aug 26 05:58:47.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010003697s
    Aug 26 05:58:49.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008758978s
    Aug 26 05:58:51.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.010261198s
    Aug 26 05:58:53.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.011119033s
    Aug 26 05:58:55.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010456511s
    Aug 26 05:58:57.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.00883854s
    Aug 26 05:58:59.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.01097765s
    Aug 26 05:59:01.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.010364477s
    Aug 26 05:59:03.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.009733813s
    Aug 26 05:59:05.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.010003394s
    Aug 26 05:59:07.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008630497s
    Aug 26 05:59:09.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.010448475s
    Aug 26 05:59:11.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009160704s
    Aug 26 05:59:13.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009713563s
    Aug 26 05:59:15.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009698818s
    Aug 26 05:59:17.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009444607s
    Aug 26 05:59:19.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008469172s
    Aug 26 05:59:21.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008595954s
    Aug 26 05:59:23.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.00948385s
    Aug 26 05:59:25.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008603579s
    Aug 26 05:59:27.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008106242s
    Aug 26 05:59:29.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.00933959s
    Aug 26 05:59:31.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009568274s
    Aug 26 05:59:33.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008355617s
    Aug 26 05:59:35.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.011490504s
    Aug 26 05:59:37.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009120905s
    Aug 26 05:59:39.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009150204s
    Aug 26 05:59:41.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008420073s
    Aug 26 05:59:43.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008069112s
    Aug 26 05:59:45.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009250274s
    Aug 26 05:59:47.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009637162s
    Aug 26 05:59:49.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008246038s
    Aug 26 05:59:51.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.009122642s
    Aug 26 05:59:53.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.010014423s
    Aug 26 05:59:55.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009937771s
    Aug 26 05:59:57.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008238756s
    Aug 26 05:59:59.150: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.018225444s
    Aug 26 06:00:01.145: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.01299929s
    Aug 26 06:00:03.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.010092287s
    Aug 26 06:00:05.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009493323s
    Aug 26 06:00:07.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009479256s
    Aug 26 06:00:09.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.009212891s
    Aug 26 06:00:11.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.010314282s
    Aug 26 06:00:13.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.010880904s
    Aug 26 06:00:15.145: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.013140528s
    Aug 26 06:00:17.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.008786396s
    Aug 26 06:00:19.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.009440458s
    Aug 26 06:00:21.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008297546s
    Aug 26 06:00:23.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.009665035s
    Aug 26 06:00:25.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009352821s
    Aug 26 06:00:27.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009038313s
    Aug 26 06:00:29.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009384567s
    Aug 26 06:00:31.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008098254s
    Aug 26 06:00:33.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009470715s
    Aug 26 06:00:35.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009625824s
    Aug 26 06:00:37.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.010910052s
    Aug 26 06:00:39.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009698164s
    Aug 26 06:00:41.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.012563148s
    Aug 26 06:00:43.147: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.015539766s
    Aug 26 06:00:45.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009528329s
    Aug 26 06:00:47.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.010385666s
    Aug 26 06:00:49.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008359314s
    Aug 26 06:00:51.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009008042s
    Aug 26 06:00:53.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009167771s
    Aug 26 06:00:55.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009452113s
    Aug 26 06:00:57.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009142075s
    Aug 26 06:00:59.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.011295667s
    Aug 26 06:01:01.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009651859s
    Aug 26 06:01:03.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.010366929s
    Aug 26 06:01:05.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.011068755s
    Aug 26 06:01:07.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.008180139s
    Aug 26 06:01:09.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.01012851s
    Aug 26 06:01:11.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008916922s
    Aug 26 06:01:13.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008415878s
    Aug 26 06:01:15.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009363024s
    Aug 26 06:01:17.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.01061765s
    Aug 26 06:01:19.155: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.023566554s
    Aug 26 06:01:21.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008219898s
    Aug 26 06:01:23.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.009997454s
    Aug 26 06:01:25.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009395366s
    Aug 26 06:01:27.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009227142s
    Aug 26 06:01:29.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009923651s
    Aug 26 06:01:29.148: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.015874722s
    STEP: removing the label kubernetes.io/e2e-9ed2eb99-b6ce-4e9a-aa42-c87ea056f213 off the node ip-10-0-1-31.us-west-2.compute.internal 08/26/23 06:01:29.148
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-9ed2eb99-b6ce-4e9a-aa42-c87ea056f213 08/26/23 06:01:29.167
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:01:29.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3306" for this suite. 08/26/23 06:01:29.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:01:29.195
Aug 26 06:01:29.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename daemonsets 08/26/23 06:01:29.196
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:01:29.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:01:29.328
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Aug 26 06:01:29.377: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 06:01:29.386
Aug 26 06:01:29.394: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:29.394: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:29.394: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:29.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:01:29.397: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:01:30.409: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:30.409: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:30.409: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:30.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 26 06:01:30.419: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:01:31.406: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:31.406: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:31.406: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:31.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 26 06:01:31.416: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Update daemon pods image. 08/26/23 06:01:31.434
STEP: Check that daemon pods images are updated. 08/26/23 06:01:31.457
Aug 26 06:01:31.467: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:31.467: INFO: Wrong image for pod: daemon-set-gbcrt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:31.467: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:31.467: INFO: Wrong image for pod: daemon-set-t6tp4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:31.467: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:31.477: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:31.477: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:31.477: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:32.486: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:32.486: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:32.486: INFO: Wrong image for pod: daemon-set-t6tp4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:32.486: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:32.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:32.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:32.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:33.485: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:33.485: INFO: Pod daemon-set-l54dg is not available
Aug 26 06:01:33.485: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:33.485: INFO: Wrong image for pod: daemon-set-t6tp4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:33.485: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:33.501: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:33.501: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:33.501: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:34.486: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:34.486: INFO: Pod daemon-set-l54dg is not available
Aug 26 06:01:34.486: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:34.486: INFO: Wrong image for pod: daemon-set-t6tp4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:34.486: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:34.502: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:34.502: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:34.502: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:35.485: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:35.485: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:35.485: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:35.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:35.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:35.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:36.483: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:36.483: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:36.483: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:36.483: INFO: Pod daemon-set-x5sbb is not available
Aug 26 06:01:36.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:36.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:36.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:37.484: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:37.484: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:37.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:37.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:37.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:38.484: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:38.484: INFO: Pod daemon-set-ktg7l is not available
Aug 26 06:01:38.484: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:38.495: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:38.495: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:38.495: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:39.484: INFO: Pod daemon-set-llr9q is not available
Aug 26 06:01:39.484: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 26 06:01:39.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:39.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:39.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:40.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:40.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:40.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:41.483: INFO: Pod daemon-set-c66q5 is not available
Aug 26 06:01:41.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:41.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:41.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 08/26/23 06:01:41.492
Aug 26 06:01:41.500: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:41.500: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:41.500: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:41.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 26 06:01:41.506: INFO: Node ip-10-0-1-31.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:01:42.529: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:42.529: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:42.529: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:01:42.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 26 06:01:42.535: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/26/23 06:01:42.567
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5921, will wait for the garbage collector to delete the pods 08/26/23 06:01:42.567
Aug 26 06:01:42.632: INFO: Deleting DaemonSet.extensions daemon-set took: 9.903082ms
Aug 26 06:01:42.733: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.991218ms
Aug 26 06:01:45.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:01:45.340: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 26 06:01:45.342: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29310"},"items":null}

Aug 26 06:01:45.345: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29310"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:01:45.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5921" for this suite. 08/26/23 06:01:45.378
------------------------------
• [SLOW TEST] [16.189 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:01:29.195
    Aug 26 06:01:29.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename daemonsets 08/26/23 06:01:29.196
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:01:29.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:01:29.328
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Aug 26 06:01:29.377: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 06:01:29.386
    Aug 26 06:01:29.394: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:29.394: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:29.394: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:29.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:01:29.397: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:01:30.409: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:30.409: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:30.409: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:30.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 26 06:01:30.419: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:01:31.406: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:31.406: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:31.406: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:31.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 26 06:01:31.416: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Update daemon pods image. 08/26/23 06:01:31.434
    STEP: Check that daemon pods images are updated. 08/26/23 06:01:31.457
    Aug 26 06:01:31.467: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:31.467: INFO: Wrong image for pod: daemon-set-gbcrt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:31.467: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:31.467: INFO: Wrong image for pod: daemon-set-t6tp4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:31.467: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:31.477: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:31.477: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:31.477: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:32.486: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:32.486: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:32.486: INFO: Wrong image for pod: daemon-set-t6tp4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:32.486: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:32.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:32.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:32.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:33.485: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:33.485: INFO: Pod daemon-set-l54dg is not available
    Aug 26 06:01:33.485: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:33.485: INFO: Wrong image for pod: daemon-set-t6tp4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:33.485: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:33.501: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:33.501: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:33.501: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:34.486: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:34.486: INFO: Pod daemon-set-l54dg is not available
    Aug 26 06:01:34.486: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:34.486: INFO: Wrong image for pod: daemon-set-t6tp4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:34.486: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:34.502: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:34.502: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:34.502: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:35.485: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:35.485: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:35.485: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:35.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:35.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:35.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:36.483: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:36.483: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:36.483: INFO: Wrong image for pod: daemon-set-tcml9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:36.483: INFO: Pod daemon-set-x5sbb is not available
    Aug 26 06:01:36.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:36.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:36.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:37.484: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:37.484: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:37.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:37.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:37.494: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:38.484: INFO: Wrong image for pod: daemon-set-bzxmv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:38.484: INFO: Pod daemon-set-ktg7l is not available
    Aug 26 06:01:38.484: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:38.495: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:38.495: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:38.495: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:39.484: INFO: Pod daemon-set-llr9q is not available
    Aug 26 06:01:39.484: INFO: Wrong image for pod: daemon-set-sr2rm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 26 06:01:39.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:39.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:39.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:40.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:40.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:40.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:41.483: INFO: Pod daemon-set-c66q5 is not available
    Aug 26 06:01:41.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:41.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:41.492: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 08/26/23 06:01:41.492
    Aug 26 06:01:41.500: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:41.500: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:41.500: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:41.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 26 06:01:41.506: INFO: Node ip-10-0-1-31.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:01:42.529: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:42.529: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:42.529: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:01:42.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 26 06:01:42.535: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/26/23 06:01:42.567
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5921, will wait for the garbage collector to delete the pods 08/26/23 06:01:42.567
    Aug 26 06:01:42.632: INFO: Deleting DaemonSet.extensions daemon-set took: 9.903082ms
    Aug 26 06:01:42.733: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.991218ms
    Aug 26 06:01:45.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:01:45.340: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 26 06:01:45.342: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29310"},"items":null}

    Aug 26 06:01:45.345: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29310"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:01:45.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5921" for this suite. 08/26/23 06:01:45.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:01:45.384
Aug 26 06:01:45.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 06:01:45.385
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:01:45.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:01:45.401
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 08/26/23 06:01:45.404
Aug 26 06:01:45.412: INFO: Waiting up to 5m0s for pod "annotationupdate06982042-8677-4eb3-be24-9fda15403d64" in namespace "downward-api-6007" to be "running and ready"
Aug 26 06:01:45.416: INFO: Pod "annotationupdate06982042-8677-4eb3-be24-9fda15403d64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.096828ms
Aug 26 06:01:45.416: INFO: The phase of Pod annotationupdate06982042-8677-4eb3-be24-9fda15403d64 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:01:47.420: INFO: Pod "annotationupdate06982042-8677-4eb3-be24-9fda15403d64": Phase="Running", Reason="", readiness=true. Elapsed: 2.007796047s
Aug 26 06:01:47.420: INFO: The phase of Pod annotationupdate06982042-8677-4eb3-be24-9fda15403d64 is Running (Ready = true)
Aug 26 06:01:47.420: INFO: Pod "annotationupdate06982042-8677-4eb3-be24-9fda15403d64" satisfied condition "running and ready"
Aug 26 06:01:47.976: INFO: Successfully updated pod "annotationupdate06982042-8677-4eb3-be24-9fda15403d64"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 06:01:52.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6007" for this suite. 08/26/23 06:01:52.015
------------------------------
• [SLOW TEST] [6.638 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:01:45.384
    Aug 26 06:01:45.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 06:01:45.385
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:01:45.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:01:45.401
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 08/26/23 06:01:45.404
    Aug 26 06:01:45.412: INFO: Waiting up to 5m0s for pod "annotationupdate06982042-8677-4eb3-be24-9fda15403d64" in namespace "downward-api-6007" to be "running and ready"
    Aug 26 06:01:45.416: INFO: Pod "annotationupdate06982042-8677-4eb3-be24-9fda15403d64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.096828ms
    Aug 26 06:01:45.416: INFO: The phase of Pod annotationupdate06982042-8677-4eb3-be24-9fda15403d64 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:01:47.420: INFO: Pod "annotationupdate06982042-8677-4eb3-be24-9fda15403d64": Phase="Running", Reason="", readiness=true. Elapsed: 2.007796047s
    Aug 26 06:01:47.420: INFO: The phase of Pod annotationupdate06982042-8677-4eb3-be24-9fda15403d64 is Running (Ready = true)
    Aug 26 06:01:47.420: INFO: Pod "annotationupdate06982042-8677-4eb3-be24-9fda15403d64" satisfied condition "running and ready"
    Aug 26 06:01:47.976: INFO: Successfully updated pod "annotationupdate06982042-8677-4eb3-be24-9fda15403d64"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:01:52.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6007" for this suite. 08/26/23 06:01:52.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:01:52.023
Aug 26 06:01:52.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 06:01:52.024
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:01:52.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:01:52.045
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 06:01:52.071
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:01:52.546
STEP: Deploying the webhook pod 08/26/23 06:01:52.553
STEP: Wait for the deployment to be ready 08/26/23 06:01:52.566
Aug 26 06:01:52.573: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 06:01:54.586
STEP: Verifying the service has paired with the endpoint 08/26/23 06:01:54.596
Aug 26 06:01:55.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 08/26/23 06:01:55.721
STEP: Creating a configMap that does not comply to the validation webhook rules 08/26/23 06:01:55.785
STEP: Deleting the collection of validation webhooks 08/26/23 06:01:55.823
STEP: Creating a configMap that does not comply to the validation webhook rules 08/26/23 06:01:55.888
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:01:55.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5448" for this suite. 08/26/23 06:01:55.96
STEP: Destroying namespace "webhook-5448-markers" for this suite. 08/26/23 06:01:55.97
------------------------------
• [3.958 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:01:52.023
    Aug 26 06:01:52.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 06:01:52.024
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:01:52.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:01:52.045
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 06:01:52.071
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:01:52.546
    STEP: Deploying the webhook pod 08/26/23 06:01:52.553
    STEP: Wait for the deployment to be ready 08/26/23 06:01:52.566
    Aug 26 06:01:52.573: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 06:01:54.586
    STEP: Verifying the service has paired with the endpoint 08/26/23 06:01:54.596
    Aug 26 06:01:55.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 08/26/23 06:01:55.721
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/26/23 06:01:55.785
    STEP: Deleting the collection of validation webhooks 08/26/23 06:01:55.823
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/26/23 06:01:55.888
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:01:55.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5448" for this suite. 08/26/23 06:01:55.96
    STEP: Destroying namespace "webhook-5448-markers" for this suite. 08/26/23 06:01:55.97
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:01:55.981
Aug 26 06:01:55.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:01:55.982
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:01:55.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:01:56
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Aug 26 06:01:56.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/26/23 06:01:58.176
Aug 26 06:01:58.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-2567 --namespace=crd-publish-openapi-2567 create -f -'
Aug 26 06:01:59.005: INFO: stderr: ""
Aug 26 06:01:59.006: INFO: stdout: "e2e-test-crd-publish-openapi-6615-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 26 06:01:59.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-2567 --namespace=crd-publish-openapi-2567 delete e2e-test-crd-publish-openapi-6615-crds test-cr'
Aug 26 06:01:59.167: INFO: stderr: ""
Aug 26 06:01:59.167: INFO: stdout: "e2e-test-crd-publish-openapi-6615-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 26 06:01:59.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-2567 --namespace=crd-publish-openapi-2567 apply -f -'
Aug 26 06:01:59.762: INFO: stderr: ""
Aug 26 06:01:59.762: INFO: stdout: "e2e-test-crd-publish-openapi-6615-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 26 06:01:59.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-2567 --namespace=crd-publish-openapi-2567 delete e2e-test-crd-publish-openapi-6615-crds test-cr'
Aug 26 06:01:59.838: INFO: stderr: ""
Aug 26 06:01:59.839: INFO: stdout: "e2e-test-crd-publish-openapi-6615-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 08/26/23 06:01:59.839
Aug 26 06:01:59.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-2567 explain e2e-test-crd-publish-openapi-6615-crds'
Aug 26 06:02:00.630: INFO: stderr: ""
Aug 26 06:02:00.630: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6615-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:02:02.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2567" for this suite. 08/26/23 06:02:02.914
------------------------------
• [SLOW TEST] [6.941 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:01:55.981
    Aug 26 06:01:55.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:01:55.982
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:01:55.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:01:56
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Aug 26 06:01:56.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/26/23 06:01:58.176
    Aug 26 06:01:58.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-2567 --namespace=crd-publish-openapi-2567 create -f -'
    Aug 26 06:01:59.005: INFO: stderr: ""
    Aug 26 06:01:59.006: INFO: stdout: "e2e-test-crd-publish-openapi-6615-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 26 06:01:59.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-2567 --namespace=crd-publish-openapi-2567 delete e2e-test-crd-publish-openapi-6615-crds test-cr'
    Aug 26 06:01:59.167: INFO: stderr: ""
    Aug 26 06:01:59.167: INFO: stdout: "e2e-test-crd-publish-openapi-6615-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Aug 26 06:01:59.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-2567 --namespace=crd-publish-openapi-2567 apply -f -'
    Aug 26 06:01:59.762: INFO: stderr: ""
    Aug 26 06:01:59.762: INFO: stdout: "e2e-test-crd-publish-openapi-6615-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 26 06:01:59.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-2567 --namespace=crd-publish-openapi-2567 delete e2e-test-crd-publish-openapi-6615-crds test-cr'
    Aug 26 06:01:59.838: INFO: stderr: ""
    Aug 26 06:01:59.839: INFO: stdout: "e2e-test-crd-publish-openapi-6615-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 08/26/23 06:01:59.839
    Aug 26 06:01:59.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-2567 explain e2e-test-crd-publish-openapi-6615-crds'
    Aug 26 06:02:00.630: INFO: stderr: ""
    Aug 26 06:02:00.630: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6615-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:02:02.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2567" for this suite. 08/26/23 06:02:02.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:02:02.922
Aug 26 06:02:02.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 06:02:02.923
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:02.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:02.947
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-852c9bf2-40c9-4935-a639-e8fe71dab051 08/26/23 06:02:02.969
STEP: Creating secret with name s-test-opt-upd-fb04fd7a-6873-4b66-884e-d51d416a5b02 08/26/23 06:02:02.974
STEP: Creating the pod 08/26/23 06:02:02.985
Aug 26 06:02:03.004: INFO: Waiting up to 5m0s for pod "pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e" in namespace "secrets-1894" to be "running and ready"
Aug 26 06:02:03.010: INFO: Pod "pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.496812ms
Aug 26 06:02:03.010: INFO: The phase of Pod pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:02:05.015: INFO: Pod "pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010808307s
Aug 26 06:02:05.015: INFO: The phase of Pod pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e is Running (Ready = true)
Aug 26 06:02:05.015: INFO: Pod "pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-852c9bf2-40c9-4935-a639-e8fe71dab051 08/26/23 06:02:05.059
STEP: Updating secret s-test-opt-upd-fb04fd7a-6873-4b66-884e-d51d416a5b02 08/26/23 06:02:05.066
STEP: Creating secret with name s-test-opt-create-27befd26-eaa1-4b26-b9a5-5dbc17022139 08/26/23 06:02:05.072
STEP: waiting to observe update in volume 08/26/23 06:02:05.078
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 06:02:07.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1894" for this suite. 08/26/23 06:02:07.121
------------------------------
• [4.211 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:02:02.922
    Aug 26 06:02:02.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 06:02:02.923
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:02.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:02.947
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-852c9bf2-40c9-4935-a639-e8fe71dab051 08/26/23 06:02:02.969
    STEP: Creating secret with name s-test-opt-upd-fb04fd7a-6873-4b66-884e-d51d416a5b02 08/26/23 06:02:02.974
    STEP: Creating the pod 08/26/23 06:02:02.985
    Aug 26 06:02:03.004: INFO: Waiting up to 5m0s for pod "pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e" in namespace "secrets-1894" to be "running and ready"
    Aug 26 06:02:03.010: INFO: Pod "pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.496812ms
    Aug 26 06:02:03.010: INFO: The phase of Pod pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:02:05.015: INFO: Pod "pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010808307s
    Aug 26 06:02:05.015: INFO: The phase of Pod pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e is Running (Ready = true)
    Aug 26 06:02:05.015: INFO: Pod "pod-secrets-6ad59b7b-40c7-46b7-ae83-3841b24ca01e" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-852c9bf2-40c9-4935-a639-e8fe71dab051 08/26/23 06:02:05.059
    STEP: Updating secret s-test-opt-upd-fb04fd7a-6873-4b66-884e-d51d416a5b02 08/26/23 06:02:05.066
    STEP: Creating secret with name s-test-opt-create-27befd26-eaa1-4b26-b9a5-5dbc17022139 08/26/23 06:02:05.072
    STEP: waiting to observe update in volume 08/26/23 06:02:05.078
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:02:07.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1894" for this suite. 08/26/23 06:02:07.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:02:07.134
Aug 26 06:02:07.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 06:02:07.135
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:07.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:07.164
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 08/26/23 06:02:07.169
Aug 26 06:02:07.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 create -f -'
Aug 26 06:02:07.963: INFO: stderr: ""
Aug 26 06:02:07.963: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/26/23 06:02:07.963
Aug 26 06:02:07.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 26 06:02:08.078: INFO: stderr: ""
Aug 26 06:02:08.078: INFO: stdout: "update-demo-nautilus-gf9cq update-demo-nautilus-pdllk "
Aug 26 06:02:08.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-gf9cq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 26 06:02:08.163: INFO: stderr: ""
Aug 26 06:02:08.163: INFO: stdout: ""
Aug 26 06:02:08.163: INFO: update-demo-nautilus-gf9cq is created but not running
Aug 26 06:02:13.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 26 06:02:13.275: INFO: stderr: ""
Aug 26 06:02:13.275: INFO: stdout: "update-demo-nautilus-gf9cq update-demo-nautilus-pdllk "
Aug 26 06:02:13.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-gf9cq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 26 06:02:13.399: INFO: stderr: ""
Aug 26 06:02:13.399: INFO: stdout: "true"
Aug 26 06:02:13.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-gf9cq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 26 06:02:13.493: INFO: stderr: ""
Aug 26 06:02:13.493: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 26 06:02:13.493: INFO: validating pod update-demo-nautilus-gf9cq
Aug 26 06:02:13.499: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 06:02:13.499: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 06:02:13.499: INFO: update-demo-nautilus-gf9cq is verified up and running
Aug 26 06:02:13.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 26 06:02:13.583: INFO: stderr: ""
Aug 26 06:02:13.583: INFO: stdout: "true"
Aug 26 06:02:13.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 26 06:02:13.652: INFO: stderr: ""
Aug 26 06:02:13.652: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 26 06:02:13.652: INFO: validating pod update-demo-nautilus-pdllk
Aug 26 06:02:13.659: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 06:02:13.660: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 06:02:13.660: INFO: update-demo-nautilus-pdllk is verified up and running
STEP: scaling down the replication controller 08/26/23 06:02:13.66
Aug 26 06:02:13.662: INFO: scanned /root for discovery docs: <nil>
Aug 26 06:02:13.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 26 06:02:14.815: INFO: stderr: ""
Aug 26 06:02:14.815: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/26/23 06:02:14.815
Aug 26 06:02:14.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 26 06:02:14.897: INFO: stderr: ""
Aug 26 06:02:14.897: INFO: stdout: "update-demo-nautilus-gf9cq update-demo-nautilus-pdllk "
STEP: Replicas for name=update-demo: expected=1 actual=2 08/26/23 06:02:14.897
Aug 26 06:02:19.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 26 06:02:19.969: INFO: stderr: ""
Aug 26 06:02:19.969: INFO: stdout: "update-demo-nautilus-pdllk "
Aug 26 06:02:19.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 26 06:02:20.043: INFO: stderr: ""
Aug 26 06:02:20.043: INFO: stdout: "true"
Aug 26 06:02:20.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 26 06:02:20.150: INFO: stderr: ""
Aug 26 06:02:20.150: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 26 06:02:20.150: INFO: validating pod update-demo-nautilus-pdllk
Aug 26 06:02:20.157: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 06:02:20.157: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 06:02:20.157: INFO: update-demo-nautilus-pdllk is verified up and running
STEP: scaling up the replication controller 08/26/23 06:02:20.157
Aug 26 06:02:20.159: INFO: scanned /root for discovery docs: <nil>
Aug 26 06:02:20.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 26 06:02:21.290: INFO: stderr: ""
Aug 26 06:02:21.290: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/26/23 06:02:21.29
Aug 26 06:02:21.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 26 06:02:21.372: INFO: stderr: ""
Aug 26 06:02:21.372: INFO: stdout: "update-demo-nautilus-b5kj2 update-demo-nautilus-pdllk "
Aug 26 06:02:21.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-b5kj2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 26 06:02:21.457: INFO: stderr: ""
Aug 26 06:02:21.457: INFO: stdout: "true"
Aug 26 06:02:21.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-b5kj2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 26 06:02:21.538: INFO: stderr: ""
Aug 26 06:02:21.538: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 26 06:02:21.538: INFO: validating pod update-demo-nautilus-b5kj2
Aug 26 06:02:21.543: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 06:02:21.543: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 06:02:21.543: INFO: update-demo-nautilus-b5kj2 is verified up and running
Aug 26 06:02:21.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 26 06:02:21.647: INFO: stderr: ""
Aug 26 06:02:21.647: INFO: stdout: "true"
Aug 26 06:02:21.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 26 06:02:21.740: INFO: stderr: ""
Aug 26 06:02:21.740: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 26 06:02:21.740: INFO: validating pod update-demo-nautilus-pdllk
Aug 26 06:02:21.749: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 06:02:21.749: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 06:02:21.749: INFO: update-demo-nautilus-pdllk is verified up and running
STEP: using delete to clean up resources 08/26/23 06:02:21.749
Aug 26 06:02:21.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 delete --grace-period=0 --force -f -'
Aug 26 06:02:21.854: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 06:02:21.854: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 26 06:02:21.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get rc,svc -l name=update-demo --no-headers'
Aug 26 06:02:21.958: INFO: stderr: "No resources found in kubectl-2245 namespace.\n"
Aug 26 06:02:21.958: INFO: stdout: ""
Aug 26 06:02:21.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 26 06:02:22.048: INFO: stderr: ""
Aug 26 06:02:22.048: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 06:02:22.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2245" for this suite. 08/26/23 06:02:22.056
------------------------------
• [SLOW TEST] [14.929 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:02:07.134
    Aug 26 06:02:07.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 06:02:07.135
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:07.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:07.164
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 08/26/23 06:02:07.169
    Aug 26 06:02:07.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 create -f -'
    Aug 26 06:02:07.963: INFO: stderr: ""
    Aug 26 06:02:07.963: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/26/23 06:02:07.963
    Aug 26 06:02:07.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 26 06:02:08.078: INFO: stderr: ""
    Aug 26 06:02:08.078: INFO: stdout: "update-demo-nautilus-gf9cq update-demo-nautilus-pdllk "
    Aug 26 06:02:08.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-gf9cq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 26 06:02:08.163: INFO: stderr: ""
    Aug 26 06:02:08.163: INFO: stdout: ""
    Aug 26 06:02:08.163: INFO: update-demo-nautilus-gf9cq is created but not running
    Aug 26 06:02:13.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 26 06:02:13.275: INFO: stderr: ""
    Aug 26 06:02:13.275: INFO: stdout: "update-demo-nautilus-gf9cq update-demo-nautilus-pdllk "
    Aug 26 06:02:13.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-gf9cq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 26 06:02:13.399: INFO: stderr: ""
    Aug 26 06:02:13.399: INFO: stdout: "true"
    Aug 26 06:02:13.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-gf9cq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 26 06:02:13.493: INFO: stderr: ""
    Aug 26 06:02:13.493: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 26 06:02:13.493: INFO: validating pod update-demo-nautilus-gf9cq
    Aug 26 06:02:13.499: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 26 06:02:13.499: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 26 06:02:13.499: INFO: update-demo-nautilus-gf9cq is verified up and running
    Aug 26 06:02:13.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 26 06:02:13.583: INFO: stderr: ""
    Aug 26 06:02:13.583: INFO: stdout: "true"
    Aug 26 06:02:13.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 26 06:02:13.652: INFO: stderr: ""
    Aug 26 06:02:13.652: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 26 06:02:13.652: INFO: validating pod update-demo-nautilus-pdllk
    Aug 26 06:02:13.659: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 26 06:02:13.660: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 26 06:02:13.660: INFO: update-demo-nautilus-pdllk is verified up and running
    STEP: scaling down the replication controller 08/26/23 06:02:13.66
    Aug 26 06:02:13.662: INFO: scanned /root for discovery docs: <nil>
    Aug 26 06:02:13.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Aug 26 06:02:14.815: INFO: stderr: ""
    Aug 26 06:02:14.815: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/26/23 06:02:14.815
    Aug 26 06:02:14.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 26 06:02:14.897: INFO: stderr: ""
    Aug 26 06:02:14.897: INFO: stdout: "update-demo-nautilus-gf9cq update-demo-nautilus-pdllk "
    STEP: Replicas for name=update-demo: expected=1 actual=2 08/26/23 06:02:14.897
    Aug 26 06:02:19.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 26 06:02:19.969: INFO: stderr: ""
    Aug 26 06:02:19.969: INFO: stdout: "update-demo-nautilus-pdllk "
    Aug 26 06:02:19.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 26 06:02:20.043: INFO: stderr: ""
    Aug 26 06:02:20.043: INFO: stdout: "true"
    Aug 26 06:02:20.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 26 06:02:20.150: INFO: stderr: ""
    Aug 26 06:02:20.150: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 26 06:02:20.150: INFO: validating pod update-demo-nautilus-pdllk
    Aug 26 06:02:20.157: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 26 06:02:20.157: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 26 06:02:20.157: INFO: update-demo-nautilus-pdllk is verified up and running
    STEP: scaling up the replication controller 08/26/23 06:02:20.157
    Aug 26 06:02:20.159: INFO: scanned /root for discovery docs: <nil>
    Aug 26 06:02:20.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Aug 26 06:02:21.290: INFO: stderr: ""
    Aug 26 06:02:21.290: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/26/23 06:02:21.29
    Aug 26 06:02:21.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 26 06:02:21.372: INFO: stderr: ""
    Aug 26 06:02:21.372: INFO: stdout: "update-demo-nautilus-b5kj2 update-demo-nautilus-pdllk "
    Aug 26 06:02:21.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-b5kj2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 26 06:02:21.457: INFO: stderr: ""
    Aug 26 06:02:21.457: INFO: stdout: "true"
    Aug 26 06:02:21.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-b5kj2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 26 06:02:21.538: INFO: stderr: ""
    Aug 26 06:02:21.538: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 26 06:02:21.538: INFO: validating pod update-demo-nautilus-b5kj2
    Aug 26 06:02:21.543: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 26 06:02:21.543: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 26 06:02:21.543: INFO: update-demo-nautilus-b5kj2 is verified up and running
    Aug 26 06:02:21.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 26 06:02:21.647: INFO: stderr: ""
    Aug 26 06:02:21.647: INFO: stdout: "true"
    Aug 26 06:02:21.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods update-demo-nautilus-pdllk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 26 06:02:21.740: INFO: stderr: ""
    Aug 26 06:02:21.740: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 26 06:02:21.740: INFO: validating pod update-demo-nautilus-pdllk
    Aug 26 06:02:21.749: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 26 06:02:21.749: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 26 06:02:21.749: INFO: update-demo-nautilus-pdllk is verified up and running
    STEP: using delete to clean up resources 08/26/23 06:02:21.749
    Aug 26 06:02:21.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 delete --grace-period=0 --force -f -'
    Aug 26 06:02:21.854: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 26 06:02:21.854: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 26 06:02:21.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get rc,svc -l name=update-demo --no-headers'
    Aug 26 06:02:21.958: INFO: stderr: "No resources found in kubectl-2245 namespace.\n"
    Aug 26 06:02:21.958: INFO: stdout: ""
    Aug 26 06:02:21.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2245 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 26 06:02:22.048: INFO: stderr: ""
    Aug 26 06:02:22.048: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:02:22.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2245" for this suite. 08/26/23 06:02:22.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:02:22.064
Aug 26 06:02:22.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 06:02:22.065
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:22.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:22.082
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 08/26/23 06:02:22.085
Aug 26 06:02:22.094: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457" in namespace "emptydir-4287" to be "running"
Aug 26 06:02:22.097: INFO: Pod "pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457": Phase="Pending", Reason="", readiness=false. Elapsed: 3.47888ms
Aug 26 06:02:24.102: INFO: Pod "pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457": Phase="Running", Reason="", readiness=false. Elapsed: 2.008294661s
Aug 26 06:02:24.102: INFO: Pod "pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457" satisfied condition "running"
STEP: Reading file content from the nginx-container 08/26/23 06:02:24.102
Aug 26 06:02:24.102: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4287 PodName:pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:02:24.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:02:24.103: INFO: ExecWithOptions: Clientset creation
Aug 26 06:02:24.103: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/emptydir-4287/pods/pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 26 06:02:24.175: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 06:02:24.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4287" for this suite. 08/26/23 06:02:24.187
------------------------------
• [2.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:02:22.064
    Aug 26 06:02:22.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 06:02:22.065
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:22.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:22.082
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 08/26/23 06:02:22.085
    Aug 26 06:02:22.094: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457" in namespace "emptydir-4287" to be "running"
    Aug 26 06:02:22.097: INFO: Pod "pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457": Phase="Pending", Reason="", readiness=false. Elapsed: 3.47888ms
    Aug 26 06:02:24.102: INFO: Pod "pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457": Phase="Running", Reason="", readiness=false. Elapsed: 2.008294661s
    Aug 26 06:02:24.102: INFO: Pod "pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457" satisfied condition "running"
    STEP: Reading file content from the nginx-container 08/26/23 06:02:24.102
    Aug 26 06:02:24.102: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4287 PodName:pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:02:24.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:02:24.103: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:02:24.103: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/emptydir-4287/pods/pod-sharedvolume-60c04f51-c74f-4f4d-870a-b0220825c457/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Aug 26 06:02:24.175: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:02:24.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4287" for this suite. 08/26/23 06:02:24.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:02:24.198
Aug 26 06:02:24.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replicaset 08/26/23 06:02:24.2
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:24.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:24.222
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 08/26/23 06:02:24.231
STEP: Verify that the required pods have come up. 08/26/23 06:02:24.238
Aug 26 06:02:24.242: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 26 06:02:29.253: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/26/23 06:02:29.253
STEP: Getting /status 08/26/23 06:02:29.253
Aug 26 06:02:29.257: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 08/26/23 06:02:29.257
Aug 26 06:02:29.270: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 08/26/23 06:02:29.27
Aug 26 06:02:29.273: INFO: Observed &ReplicaSet event: ADDED
Aug 26 06:02:29.273: INFO: Observed &ReplicaSet event: MODIFIED
Aug 26 06:02:29.274: INFO: Observed &ReplicaSet event: MODIFIED
Aug 26 06:02:29.274: INFO: Observed &ReplicaSet event: MODIFIED
Aug 26 06:02:29.274: INFO: Found replicaset test-rs in namespace replicaset-7817 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 26 06:02:29.274: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 08/26/23 06:02:29.274
Aug 26 06:02:29.275: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 26 06:02:29.283: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 08/26/23 06:02:29.283
Aug 26 06:02:29.286: INFO: Observed &ReplicaSet event: ADDED
Aug 26 06:02:29.287: INFO: Observed &ReplicaSet event: MODIFIED
Aug 26 06:02:29.287: INFO: Observed &ReplicaSet event: MODIFIED
Aug 26 06:02:29.287: INFO: Observed &ReplicaSet event: MODIFIED
Aug 26 06:02:29.287: INFO: Observed replicaset test-rs in namespace replicaset-7817 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 26 06:02:29.287: INFO: Observed &ReplicaSet event: MODIFIED
Aug 26 06:02:29.287: INFO: Found replicaset test-rs in namespace replicaset-7817 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 26 06:02:29.287: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 26 06:02:29.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7817" for this suite. 08/26/23 06:02:29.301
------------------------------
• [SLOW TEST] [5.113 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:02:24.198
    Aug 26 06:02:24.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replicaset 08/26/23 06:02:24.2
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:24.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:24.222
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 08/26/23 06:02:24.231
    STEP: Verify that the required pods have come up. 08/26/23 06:02:24.238
    Aug 26 06:02:24.242: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 26 06:02:29.253: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/26/23 06:02:29.253
    STEP: Getting /status 08/26/23 06:02:29.253
    Aug 26 06:02:29.257: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 08/26/23 06:02:29.257
    Aug 26 06:02:29.270: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 08/26/23 06:02:29.27
    Aug 26 06:02:29.273: INFO: Observed &ReplicaSet event: ADDED
    Aug 26 06:02:29.273: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 26 06:02:29.274: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 26 06:02:29.274: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 26 06:02:29.274: INFO: Found replicaset test-rs in namespace replicaset-7817 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 26 06:02:29.274: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 08/26/23 06:02:29.274
    Aug 26 06:02:29.275: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 26 06:02:29.283: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 08/26/23 06:02:29.283
    Aug 26 06:02:29.286: INFO: Observed &ReplicaSet event: ADDED
    Aug 26 06:02:29.287: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 26 06:02:29.287: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 26 06:02:29.287: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 26 06:02:29.287: INFO: Observed replicaset test-rs in namespace replicaset-7817 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 26 06:02:29.287: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 26 06:02:29.287: INFO: Found replicaset test-rs in namespace replicaset-7817 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Aug 26 06:02:29.287: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:02:29.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7817" for this suite. 08/26/23 06:02:29.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:02:29.313
Aug 26 06:02:29.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replicaset 08/26/23 06:02:29.314
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:29.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:29.335
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/26/23 06:02:29.34
Aug 26 06:02:29.348: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1568" to be "running and ready"
Aug 26 06:02:29.362: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 14.538661ms
Aug 26 06:02:29.363: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:02:31.368: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.019971241s
Aug 26 06:02:31.368: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Aug 26 06:02:31.368: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 08/26/23 06:02:31.373
STEP: Then the orphan pod is adopted 08/26/23 06:02:31.384
STEP: When the matched label of one of its pods change 08/26/23 06:02:32.408
Aug 26 06:02:32.417: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 08/26/23 06:02:32.452
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 26 06:02:32.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1568" for this suite. 08/26/23 06:02:32.491
------------------------------
• [3.194 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:02:29.313
    Aug 26 06:02:29.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replicaset 08/26/23 06:02:29.314
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:29.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:29.335
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/26/23 06:02:29.34
    Aug 26 06:02:29.348: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1568" to be "running and ready"
    Aug 26 06:02:29.362: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 14.538661ms
    Aug 26 06:02:29.363: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:02:31.368: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.019971241s
    Aug 26 06:02:31.368: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Aug 26 06:02:31.368: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 08/26/23 06:02:31.373
    STEP: Then the orphan pod is adopted 08/26/23 06:02:31.384
    STEP: When the matched label of one of its pods change 08/26/23 06:02:32.408
    Aug 26 06:02:32.417: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/26/23 06:02:32.452
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:02:32.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1568" for this suite. 08/26/23 06:02:32.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:02:32.508
Aug 26 06:02:32.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename namespaces 08/26/23 06:02:32.509
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:32.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:32.539
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-3099" 08/26/23 06:02:32.542
Aug 26 06:02:32.557: INFO: Namespace "namespaces-3099" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e53ca283-6d70-4e16-83be-49a5afe9d1bc", "kubernetes.io/metadata.name":"namespaces-3099", "namespaces-3099":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:02:32.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3099" for this suite. 08/26/23 06:02:32.569
------------------------------
• [0.068 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:02:32.508
    Aug 26 06:02:32.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename namespaces 08/26/23 06:02:32.509
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:32.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:32.539
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-3099" 08/26/23 06:02:32.542
    Aug 26 06:02:32.557: INFO: Namespace "namespaces-3099" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e53ca283-6d70-4e16-83be-49a5afe9d1bc", "kubernetes.io/metadata.name":"namespaces-3099", "namespaces-3099":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:02:32.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3099" for this suite. 08/26/23 06:02:32.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:02:32.577
Aug 26 06:02:32.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 06:02:32.578
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:32.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:32.598
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 08/26/23 06:02:32.601
Aug 26 06:02:32.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-5645 create -f -'
Aug 26 06:02:33.350: INFO: stderr: ""
Aug 26 06:02:33.350: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/26/23 06:02:33.35
Aug 26 06:02:34.357: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 26 06:02:34.357: INFO: Found 0 / 1
Aug 26 06:02:35.354: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 26 06:02:35.354: INFO: Found 1 / 1
Aug 26 06:02:35.354: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 08/26/23 06:02:35.354
Aug 26 06:02:35.358: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 26 06:02:35.358: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 26 06:02:35.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-5645 patch pod agnhost-primary-bdn6k -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 26 06:02:35.455: INFO: stderr: ""
Aug 26 06:02:35.456: INFO: stdout: "pod/agnhost-primary-bdn6k patched\n"
STEP: checking annotations 08/26/23 06:02:35.456
Aug 26 06:02:35.461: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 26 06:02:35.461: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 06:02:35.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5645" for this suite. 08/26/23 06:02:35.471
------------------------------
• [2.901 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:02:32.577
    Aug 26 06:02:32.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 06:02:32.578
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:32.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:32.598
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 08/26/23 06:02:32.601
    Aug 26 06:02:32.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-5645 create -f -'
    Aug 26 06:02:33.350: INFO: stderr: ""
    Aug 26 06:02:33.350: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/26/23 06:02:33.35
    Aug 26 06:02:34.357: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 26 06:02:34.357: INFO: Found 0 / 1
    Aug 26 06:02:35.354: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 26 06:02:35.354: INFO: Found 1 / 1
    Aug 26 06:02:35.354: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 08/26/23 06:02:35.354
    Aug 26 06:02:35.358: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 26 06:02:35.358: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 26 06:02:35.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-5645 patch pod agnhost-primary-bdn6k -p {"metadata":{"annotations":{"x":"y"}}}'
    Aug 26 06:02:35.455: INFO: stderr: ""
    Aug 26 06:02:35.456: INFO: stdout: "pod/agnhost-primary-bdn6k patched\n"
    STEP: checking annotations 08/26/23 06:02:35.456
    Aug 26 06:02:35.461: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 26 06:02:35.461: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:02:35.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5645" for this suite. 08/26/23 06:02:35.471
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:02:35.479
Aug 26 06:02:35.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename watch 08/26/23 06:02:35.48
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:35.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:35.5
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 08/26/23 06:02:35.503
STEP: creating a watch on configmaps with label B 08/26/23 06:02:35.505
STEP: creating a watch on configmaps with label A or B 08/26/23 06:02:35.508
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/26/23 06:02:35.509
Aug 26 06:02:35.515: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29927 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:02:35.515: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29927 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/26/23 06:02:35.515
Aug 26 06:02:35.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29928 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:02:35.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29928 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/26/23 06:02:35.523
Aug 26 06:02:35.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29929 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:02:35.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29929 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/26/23 06:02:35.53
Aug 26 06:02:35.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29930 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:02:35.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29930 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/26/23 06:02:35.535
Aug 26 06:02:35.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9245  e224d437-5df7-4984-bfff-a8abbaad94e6 29931 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:02:35.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9245  e224d437-5df7-4984-bfff-a8abbaad94e6 29931 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/26/23 06:02:45.541
Aug 26 06:02:45.548: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9245  e224d437-5df7-4984-bfff-a8abbaad94e6 30002 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:02:45.548: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9245  e224d437-5df7-4984-bfff-a8abbaad94e6 30002 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 26 06:02:55.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9245" for this suite. 08/26/23 06:02:55.557
------------------------------
• [SLOW TEST] [20.087 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:02:35.479
    Aug 26 06:02:35.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename watch 08/26/23 06:02:35.48
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:35.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:35.5
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 08/26/23 06:02:35.503
    STEP: creating a watch on configmaps with label B 08/26/23 06:02:35.505
    STEP: creating a watch on configmaps with label A or B 08/26/23 06:02:35.508
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/26/23 06:02:35.509
    Aug 26 06:02:35.515: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29927 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:02:35.515: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29927 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/26/23 06:02:35.515
    Aug 26 06:02:35.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29928 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:02:35.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29928 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/26/23 06:02:35.523
    Aug 26 06:02:35.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29929 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:02:35.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29929 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/26/23 06:02:35.53
    Aug 26 06:02:35.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29930 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:02:35.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9245  e2df1107-7b72-4d34-a8d3-7c70b1ab9bc1 29930 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/26/23 06:02:35.535
    Aug 26 06:02:35.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9245  e224d437-5df7-4984-bfff-a8abbaad94e6 29931 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:02:35.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9245  e224d437-5df7-4984-bfff-a8abbaad94e6 29931 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/26/23 06:02:45.541
    Aug 26 06:02:45.548: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9245  e224d437-5df7-4984-bfff-a8abbaad94e6 30002 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:02:45.548: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9245  e224d437-5df7-4984-bfff-a8abbaad94e6 30002 0 2023-08-26 06:02:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-26 06:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:02:55.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9245" for this suite. 08/26/23 06:02:55.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:02:55.567
Aug 26 06:02:55.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 06:02:55.568
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:55.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:55.597
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-2507/configmap-test-5500494b-4364-41f8-ac27-7e9c2e892f3c 08/26/23 06:02:55.599
STEP: Creating a pod to test consume configMaps 08/26/23 06:02:55.605
Aug 26 06:02:55.613: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1" in namespace "configmap-2507" to be "Succeeded or Failed"
Aug 26 06:02:55.617: INFO: Pod "pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.298013ms
Aug 26 06:02:57.623: INFO: Pod "pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1": Phase="Running", Reason="", readiness=false. Elapsed: 2.009614731s
Aug 26 06:02:59.621: INFO: Pod "pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007459202s
STEP: Saw pod success 08/26/23 06:02:59.621
Aug 26 06:02:59.621: INFO: Pod "pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1" satisfied condition "Succeeded or Failed"
Aug 26 06:02:59.625: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1 container env-test: <nil>
STEP: delete the pod 08/26/23 06:02:59.634
Aug 26 06:02:59.646: INFO: Waiting for pod pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1 to disappear
Aug 26 06:02:59.649: INFO: Pod pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:02:59.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2507" for this suite. 08/26/23 06:02:59.656
------------------------------
• [4.095 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:02:55.567
    Aug 26 06:02:55.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 06:02:55.568
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:55.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:55.597
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-2507/configmap-test-5500494b-4364-41f8-ac27-7e9c2e892f3c 08/26/23 06:02:55.599
    STEP: Creating a pod to test consume configMaps 08/26/23 06:02:55.605
    Aug 26 06:02:55.613: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1" in namespace "configmap-2507" to be "Succeeded or Failed"
    Aug 26 06:02:55.617: INFO: Pod "pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.298013ms
    Aug 26 06:02:57.623: INFO: Pod "pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1": Phase="Running", Reason="", readiness=false. Elapsed: 2.009614731s
    Aug 26 06:02:59.621: INFO: Pod "pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007459202s
    STEP: Saw pod success 08/26/23 06:02:59.621
    Aug 26 06:02:59.621: INFO: Pod "pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1" satisfied condition "Succeeded or Failed"
    Aug 26 06:02:59.625: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1 container env-test: <nil>
    STEP: delete the pod 08/26/23 06:02:59.634
    Aug 26 06:02:59.646: INFO: Waiting for pod pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1 to disappear
    Aug 26 06:02:59.649: INFO: Pod pod-configmaps-cf297cb9-bdca-49ab-994f-200f47594ad1 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:02:59.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2507" for this suite. 08/26/23 06:02:59.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:02:59.663
Aug 26 06:02:59.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-lifecycle-hook 08/26/23 06:02:59.664
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:59.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:59.694
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/26/23 06:02:59.709
Aug 26 06:02:59.727: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7470" to be "running and ready"
Aug 26 06:02:59.732: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.001112ms
Aug 26 06:02:59.732: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:03:01.736: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009260399s
Aug 26 06:03:01.736: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 26 06:03:01.736: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 08/26/23 06:03:01.746
Aug 26 06:03:01.753: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7470" to be "running and ready"
Aug 26 06:03:01.757: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066484ms
Aug 26 06:03:01.757: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:03:03.764: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010864289s
Aug 26 06:03:03.764: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Aug 26 06:03:03.764: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/26/23 06:03:03.772
Aug 26 06:03:03.781: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 26 06:03:03.785: INFO: Pod pod-with-prestop-http-hook still exists
Aug 26 06:03:05.785: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 26 06:03:05.790: INFO: Pod pod-with-prestop-http-hook still exists
Aug 26 06:03:07.786: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 26 06:03:07.790: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 08/26/23 06:03:07.79
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 26 06:03:07.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7470" for this suite. 08/26/23 06:03:07.822
------------------------------
• [SLOW TEST] [8.169 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:02:59.663
    Aug 26 06:02:59.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/26/23 06:02:59.664
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:02:59.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:02:59.694
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/26/23 06:02:59.709
    Aug 26 06:02:59.727: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7470" to be "running and ready"
    Aug 26 06:02:59.732: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.001112ms
    Aug 26 06:02:59.732: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:03:01.736: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009260399s
    Aug 26 06:03:01.736: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 26 06:03:01.736: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 08/26/23 06:03:01.746
    Aug 26 06:03:01.753: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7470" to be "running and ready"
    Aug 26 06:03:01.757: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066484ms
    Aug 26 06:03:01.757: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:03:03.764: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010864289s
    Aug 26 06:03:03.764: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Aug 26 06:03:03.764: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/26/23 06:03:03.772
    Aug 26 06:03:03.781: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 26 06:03:03.785: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 26 06:03:05.785: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 26 06:03:05.790: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 26 06:03:07.786: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 26 06:03:07.790: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 08/26/23 06:03:07.79
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:03:07.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7470" for this suite. 08/26/23 06:03:07.822
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:03:07.833
Aug 26 06:03:07.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 06:03:07.834
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:07.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:07.854
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 08/26/23 06:03:07.859
STEP: fetching the ConfigMap 08/26/23 06:03:07.867
STEP: patching the ConfigMap 08/26/23 06:03:07.873
STEP: listing all ConfigMaps in all namespaces with a label selector 08/26/23 06:03:07.882
STEP: deleting the ConfigMap by collection with a label selector 08/26/23 06:03:07.89
STEP: listing all ConfigMaps in test namespace 08/26/23 06:03:07.906
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:03:07.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3820" for this suite. 08/26/23 06:03:07.919
------------------------------
• [0.094 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:03:07.833
    Aug 26 06:03:07.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 06:03:07.834
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:07.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:07.854
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 08/26/23 06:03:07.859
    STEP: fetching the ConfigMap 08/26/23 06:03:07.867
    STEP: patching the ConfigMap 08/26/23 06:03:07.873
    STEP: listing all ConfigMaps in all namespaces with a label selector 08/26/23 06:03:07.882
    STEP: deleting the ConfigMap by collection with a label selector 08/26/23 06:03:07.89
    STEP: listing all ConfigMaps in test namespace 08/26/23 06:03:07.906
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:03:07.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3820" for this suite. 08/26/23 06:03:07.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:03:07.928
Aug 26 06:03:07.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename csiinlinevolumes 08/26/23 06:03:07.929
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:07.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:07.945
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 08/26/23 06:03:07.948
STEP: getting 08/26/23 06:03:07.968
STEP: listing in namespace 08/26/23 06:03:07.972
STEP: patching 08/26/23 06:03:07.976
STEP: deleting 08/26/23 06:03:07.992
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 26 06:03:08.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-1217" for this suite. 08/26/23 06:03:08.014
------------------------------
• [0.092 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:03:07.928
    Aug 26 06:03:07.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename csiinlinevolumes 08/26/23 06:03:07.929
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:07.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:07.945
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 08/26/23 06:03:07.948
    STEP: getting 08/26/23 06:03:07.968
    STEP: listing in namespace 08/26/23 06:03:07.972
    STEP: patching 08/26/23 06:03:07.976
    STEP: deleting 08/26/23 06:03:07.992
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:03:08.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-1217" for this suite. 08/26/23 06:03:08.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:03:08.022
Aug 26 06:03:08.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename runtimeclass 08/26/23 06:03:08.022
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:08.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:08.04
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Aug 26 06:03:08.058: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8636 to be scheduled
Aug 26 06:03:08.069: INFO: 1 pods are not scheduled: [runtimeclass-8636/test-runtimeclass-runtimeclass-8636-preconfigured-handler-2bdr2(eed7909e-7551-4b63-b165-e77046821263)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 26 06:03:10.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8636" for this suite. 08/26/23 06:03:10.088
------------------------------
• [2.077 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:03:08.022
    Aug 26 06:03:08.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename runtimeclass 08/26/23 06:03:08.022
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:08.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:08.04
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Aug 26 06:03:08.058: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8636 to be scheduled
    Aug 26 06:03:08.069: INFO: 1 pods are not scheduled: [runtimeclass-8636/test-runtimeclass-runtimeclass-8636-preconfigured-handler-2bdr2(eed7909e-7551-4b63-b165-e77046821263)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:03:10.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8636" for this suite. 08/26/23 06:03:10.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:03:10.1
Aug 26 06:03:10.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename deployment 08/26/23 06:03:10.101
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:10.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:10.121
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Aug 26 06:03:10.123: INFO: Creating simple deployment test-new-deployment
Aug 26 06:03:10.141: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 08/26/23 06:03:12.155
STEP: updating a scale subresource 08/26/23 06:03:12.158
STEP: verifying the deployment Spec.Replicas was modified 08/26/23 06:03:12.165
STEP: Patch a scale subresource 08/26/23 06:03:12.169
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 26 06:03:12.338: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3424  66879e4a-c9ca-4512-a2cb-c7e62efb3d34 30210 3 2023-08-26 06:03:10 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-26 06:03:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:03:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eb5b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-26 06:03:11 +0000 UTC,LastTransitionTime:2023-08-26 06:03:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-26 06:03:11 +0000 UTC,LastTransitionTime:2023-08-26 06:03:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 26 06:03:12.361: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-3424  0208c281-2f20-413e-b17f-d3db46e2e702 30213 2 2023-08-26 06:03:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 66879e4a-c9ca-4512-a2cb-c7e62efb3d34 0xc005238fc7 0xc005238fc8}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:03:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66879e4a-c9ca-4512-a2cb-c7e62efb3d34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:03:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005239058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 26 06:03:12.383: INFO: Pod "test-new-deployment-7f5969cbc7-jnlrn" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-jnlrn test-new-deployment-7f5969cbc7- deployment-3424  b70e78c9-9b5c-4cbd-9a11-48e10dcae9a1 30204 0 2023-08-26 06:03:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8717973a92756b360cff449da72bd531e9c9bed9d660ebb3630c9f4f35ff4edb cni.projectcalico.org/podIP:10.20.199.104/32 cni.projectcalico.org/podIPs:10.20.199.104/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0208c281-2f20-413e-b17f-d3db46e2e702 0xc005239467 0xc005239468}] [] [{calico Update v1 2023-08-26 06:03:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 06:03:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0208c281-2f20-413e-b17f-d3db46e2e702\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6c49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6c49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:10.20.199.104,StartTime:2023-08-26 06:03:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://231900b5dfb4aa61861dbf5a5ab01e590b5e4ee4844c9ca7bc1a38d1db52a64c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.199.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:12.384: INFO: Pod "test-new-deployment-7f5969cbc7-wtxjl" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-wtxjl test-new-deployment-7f5969cbc7- deployment-3424  d8f10757-e93b-4015-a784-21007d5015f5 30215 0 2023-08-26 06:03:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0208c281-2f20-413e-b17f-d3db46e2e702 0xc005239650 0xc005239651}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0208c281-2f20-413e-b17f-d3db46e2e702\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrbtx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrbtx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 26 06:03:12.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3424" for this suite. 08/26/23 06:03:12.4
------------------------------
• [2.341 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:03:10.1
    Aug 26 06:03:10.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename deployment 08/26/23 06:03:10.101
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:10.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:10.121
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Aug 26 06:03:10.123: INFO: Creating simple deployment test-new-deployment
    Aug 26 06:03:10.141: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 08/26/23 06:03:12.155
    STEP: updating a scale subresource 08/26/23 06:03:12.158
    STEP: verifying the deployment Spec.Replicas was modified 08/26/23 06:03:12.165
    STEP: Patch a scale subresource 08/26/23 06:03:12.169
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 26 06:03:12.338: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-3424  66879e4a-c9ca-4512-a2cb-c7e62efb3d34 30210 3 2023-08-26 06:03:10 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-26 06:03:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:03:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eb5b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-26 06:03:11 +0000 UTC,LastTransitionTime:2023-08-26 06:03:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-26 06:03:11 +0000 UTC,LastTransitionTime:2023-08-26 06:03:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 26 06:03:12.361: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-3424  0208c281-2f20-413e-b17f-d3db46e2e702 30213 2 2023-08-26 06:03:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 66879e4a-c9ca-4512-a2cb-c7e62efb3d34 0xc005238fc7 0xc005238fc8}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:03:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66879e4a-c9ca-4512-a2cb-c7e62efb3d34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:03:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005239058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 06:03:12.383: INFO: Pod "test-new-deployment-7f5969cbc7-jnlrn" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-jnlrn test-new-deployment-7f5969cbc7- deployment-3424  b70e78c9-9b5c-4cbd-9a11-48e10dcae9a1 30204 0 2023-08-26 06:03:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8717973a92756b360cff449da72bd531e9c9bed9d660ebb3630c9f4f35ff4edb cni.projectcalico.org/podIP:10.20.199.104/32 cni.projectcalico.org/podIPs:10.20.199.104/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0208c281-2f20-413e-b17f-d3db46e2e702 0xc005239467 0xc005239468}] [] [{calico Update v1 2023-08-26 06:03:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 06:03:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0208c281-2f20-413e-b17f-d3db46e2e702\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6c49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6c49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:10.20.199.104,StartTime:2023-08-26 06:03:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://231900b5dfb4aa61861dbf5a5ab01e590b5e4ee4844c9ca7bc1a38d1db52a64c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.199.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:12.384: INFO: Pod "test-new-deployment-7f5969cbc7-wtxjl" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-wtxjl test-new-deployment-7f5969cbc7- deployment-3424  d8f10757-e93b-4015-a784-21007d5015f5 30215 0 2023-08-26 06:03:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0208c281-2f20-413e-b17f-d3db46e2e702 0xc005239650 0xc005239651}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0208c281-2f20-413e-b17f-d3db46e2e702\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrbtx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrbtx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:03:12.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3424" for this suite. 08/26/23 06:03:12.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:03:12.441
Aug 26 06:03:12.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:03:12.442
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:12.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:12.48
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 08/26/23 06:03:12.484
Aug 26 06:03:12.499: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5" in namespace "projected-8497" to be "Succeeded or Failed"
Aug 26 06:03:12.504: INFO: Pod "downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.74057ms
Aug 26 06:03:14.508: INFO: Pod "downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009004535s
Aug 26 06:03:16.509: INFO: Pod "downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010169937s
STEP: Saw pod success 08/26/23 06:03:16.509
Aug 26 06:03:16.509: INFO: Pod "downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5" satisfied condition "Succeeded or Failed"
Aug 26 06:03:16.513: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5 container client-container: <nil>
STEP: delete the pod 08/26/23 06:03:16.531
Aug 26 06:03:16.547: INFO: Waiting for pod downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5 to disappear
Aug 26 06:03:16.550: INFO: Pod downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 06:03:16.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8497" for this suite. 08/26/23 06:03:16.558
------------------------------
• [4.122 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:03:12.441
    Aug 26 06:03:12.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:03:12.442
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:12.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:12.48
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 08/26/23 06:03:12.484
    Aug 26 06:03:12.499: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5" in namespace "projected-8497" to be "Succeeded or Failed"
    Aug 26 06:03:12.504: INFO: Pod "downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.74057ms
    Aug 26 06:03:14.508: INFO: Pod "downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009004535s
    Aug 26 06:03:16.509: INFO: Pod "downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010169937s
    STEP: Saw pod success 08/26/23 06:03:16.509
    Aug 26 06:03:16.509: INFO: Pod "downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5" satisfied condition "Succeeded or Failed"
    Aug 26 06:03:16.513: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5 container client-container: <nil>
    STEP: delete the pod 08/26/23 06:03:16.531
    Aug 26 06:03:16.547: INFO: Waiting for pod downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5 to disappear
    Aug 26 06:03:16.550: INFO: Pod downwardapi-volume-fcceb7a1-da5e-4980-8b5d-f532a657bfb5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:03:16.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8497" for this suite. 08/26/23 06:03:16.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:03:16.564
Aug 26 06:03:16.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename deployment 08/26/23 06:03:16.565
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:16.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:16.581
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Aug 26 06:03:16.584: INFO: Creating deployment "webserver-deployment"
Aug 26 06:03:16.590: INFO: Waiting for observed generation 1
Aug 26 06:03:18.598: INFO: Waiting for all required pods to come up
Aug 26 06:03:18.614: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 08/26/23 06:03:18.614
Aug 26 06:03:18.615: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-8b5vt" in namespace "deployment-5176" to be "running"
Aug 26 06:03:18.619: INFO: Pod "webserver-deployment-7f5969cbc7-8b5vt": Phase="Running", Reason="", readiness=true. Elapsed: 4.513283ms
Aug 26 06:03:18.619: INFO: Pod "webserver-deployment-7f5969cbc7-8b5vt" satisfied condition "running"
Aug 26 06:03:18.619: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 26 06:03:18.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:10, UpdatedReplicas:10, ReadyReplicas:9, AvailableReplicas:9, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"webserver-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 06:03:20.634: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 26 06:03:20.644: INFO: Updating deployment webserver-deployment
Aug 26 06:03:20.644: INFO: Waiting for observed generation 2
Aug 26 06:03:22.655: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 26 06:03:22.659: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 26 06:03:22.662: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 26 06:03:22.674: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 26 06:03:22.674: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 26 06:03:22.681: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 26 06:03:22.692: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 26 06:03:22.692: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 26 06:03:22.702: INFO: Updating deployment webserver-deployment
Aug 26 06:03:22.702: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 26 06:03:22.709: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 26 06:03:22.716: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 26 06:03:22.740: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5176  9abd5fb4-14a3-49d4-a2a6-87eaf5bad39e 30624 3 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005be5f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-26 06:03:20 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-26 06:03:22 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 26 06:03:22.755: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5176  5cf62f24-b778-497c-a05a-6954d9cd5d1f 30618 3 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9abd5fb4-14a3-49d4-a2a6-87eaf5bad39e 0xc0073abbe7 0xc0073abbe8}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9abd5fb4-14a3-49d4-a2a6-87eaf5bad39e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073abc88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 26 06:03:22.755: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 26 06:03:22.756: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5176  3fce32d3-b137-4172-9972-9b114c1295c3 30615 3 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9abd5fb4-14a3-49d4-a2a6-87eaf5bad39e 0xc0073abaf7 0xc0073abaf8}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9abd5fb4-14a3-49d4-a2a6-87eaf5bad39e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073abb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 26 06:03:22.767: INFO: Pod "webserver-deployment-7f5969cbc7-47gt2" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-47gt2 webserver-deployment-7f5969cbc7- deployment-5176  838d0f8d-a2ec-45fd-aab6-1a4dc00554f6 30630 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78157 0xc005e78158}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57qnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57qnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.768: INFO: Pod "webserver-deployment-7f5969cbc7-54769" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-54769 webserver-deployment-7f5969cbc7- deployment-5176  8e4f9676-d416-4296-a9df-f2b35eabbdee 30635 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78297 0xc005e78298}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fjch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fjch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.769: INFO: Pod "webserver-deployment-7f5969cbc7-87dtt" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-87dtt webserver-deployment-7f5969cbc7- deployment-5176  c76391c9-0525-416b-954b-5532065e7b18 30484 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:87de7c21ef24b52706184d183f3236124c2fd171346e434d643e5b2566aecb1d cni.projectcalico.org/podIP:10.20.50.235/32 cni.projectcalico.org/podIPs:10.20.50.235/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78410 0xc005e78411}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.50.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftgff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftgff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.101,PodIP:10.20.50.235,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f03cb9fca1fcab2b81fb578a7cff9b925abe8983b90b69f56ab0cf88bbeae33f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.50.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.769: INFO: Pod "webserver-deployment-7f5969cbc7-8b5vt" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8b5vt webserver-deployment-7f5969cbc7- deployment-5176  71692bee-9a4d-4f33-a1fb-390366714d09 30496 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:88e4cc774c91f29663287ef00ca2789baeedb8ff2e376e3af8ebfb6fcdf9a421 cni.projectcalico.org/podIP:10.20.193.197/32 cni.projectcalico.org/podIPs:10.20.193.197/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78620 0xc005e78621}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.193.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49cgr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49cgr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-126.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.126,PodIP:10.20.193.197,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b595be7e273d0de8d1a4e74cc5ebd537844d47e8623b881d4cc4108243b11977,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.193.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.770: INFO: Pod "webserver-deployment-7f5969cbc7-8z7bk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8z7bk webserver-deployment-7f5969cbc7- deployment-5176  1bc6e4ea-3290-44f4-96f6-2dec5c3bc0ef 30477 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a1dc1d0d8985ffe25b0624c68e7028614e0cd47850ce50fb90a13a8f5a79d196 cni.projectcalico.org/podIP:10.20.199.96/32 cni.projectcalico.org/podIPs:10.20.199.96/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78830 0xc005e78831}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4p4sz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4p4sz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:10.20.199.96,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://895c440dec4a74567a41b3fa76e56df83cec0c9fe9cc9db16af55dc4b20026b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.199.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.770: INFO: Pod "webserver-deployment-7f5969cbc7-dszjc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dszjc webserver-deployment-7f5969cbc7- deployment-5176  809b252b-76f5-444e-9bd4-e452f3c89f19 30481 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0c577adcc189062ac256556b9f7dd67666f53ff6f8a2f01f9e336d6112ac0ecd cni.projectcalico.org/podIP:10.20.50.234/32 cni.projectcalico.org/podIPs:10.20.50.234/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78a40 0xc005e78a41}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.50.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2s4xp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2s4xp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.101,PodIP:10.20.50.234,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f349b9be14e96904ca025805516d26dfd78f28dd3fe4341a7be1b578c2135c0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.50.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.770: INFO: Pod "webserver-deployment-7f5969cbc7-h2dp5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h2dp5 webserver-deployment-7f5969cbc7- deployment-5176  9362e778-ad2a-4842-b8c3-30120f3d9040 30471 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6a457cafffc367ce7271842c5c2c96d4d06ebad0b21282ce972cb87cdccef64b cni.projectcalico.org/podIP:10.20.62.189/32 cni.projectcalico.org/podIPs:10.20.62.189/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78c50 0xc005e78c51}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.62.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dwpb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dwpb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-23.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.23,PodIP:10.20.62.189,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://df585218149c78010ed7bb78d3797b599dbdcfa2afa8c30eeaeeb51c74a0b159,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.62.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.770: INFO: Pod "webserver-deployment-7f5969cbc7-jrnbl" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jrnbl webserver-deployment-7f5969cbc7- deployment-5176  90603765-a771-47b5-82e2-eb92b70e8f9d 30629 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78e40 0xc005e78e41}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tj7jm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tj7jm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.771: INFO: Pod "webserver-deployment-7f5969cbc7-jzvrh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jzvrh webserver-deployment-7f5969cbc7- deployment-5176  fb4234d5-733e-4257-ab2c-5c7785eae0db 30636 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78f77 0xc005e78f78}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fjswh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fjswh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.771: INFO: Pod "webserver-deployment-7f5969cbc7-mbdxw" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mbdxw webserver-deployment-7f5969cbc7- deployment-5176  05119b26-0d46-4bb2-ab01-06b2abbcb82f 30474 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6ecb7490b73b2c04de9c38609f6909efe91696781e8e3bb47c176dbf2d242a5b cni.projectcalico.org/podIP:10.20.199.97/32 cni.projectcalico.org/podIPs:10.20.199.97/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e790d7 0xc005e790d8}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lmjvj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lmjvj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:10.20.199.97,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fa99108359a2a8d6d7cea32218e3cd0f329c5f8fef699e5badeb47d0e855c004,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.199.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.772: INFO: Pod "webserver-deployment-7f5969cbc7-mtpjk" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mtpjk webserver-deployment-7f5969cbc7- deployment-5176  60611d6c-17c8-40dd-9f69-03c27f7730bc 30638 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e792d0 0xc005e792d1}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njgs9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njgs9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.31,PodIP:,StartTime:2023-08-26 06:03:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.772: INFO: Pod "webserver-deployment-7f5969cbc7-q5dvh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q5dvh webserver-deployment-7f5969cbc7- deployment-5176  55afd664-a101-422d-b1ff-763674e966bf 30632 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e79480 0xc005e79481}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6v52j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6v52j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.772: INFO: Pod "webserver-deployment-7f5969cbc7-scdvq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-scdvq webserver-deployment-7f5969cbc7- deployment-5176  267f202b-ada6-44c8-95de-e7ed38371768 30467 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:16cb78e540dc9316b243eaa666fd408dc2b53d06201a18a2acc626e73756b634 cni.projectcalico.org/podIP:10.20.62.190/32 cni.projectcalico.org/podIPs:10.20.62.190/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e795d7 0xc005e795d8}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.62.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wksw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wksw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-23.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.23,PodIP:10.20.62.190,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9af1e0bc7dd0aacfab1ab253010ed46f02614517b6be5bcbfabe8f42e56d422b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.62.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.772: INFO: Pod "webserver-deployment-7f5969cbc7-v7mbm" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-v7mbm webserver-deployment-7f5969cbc7- deployment-5176  f6b14ace-a027-4e15-9c08-2fd0f9db3222 30625 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e797d0 0xc005e797d1}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqqg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqqg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.773: INFO: Pod "webserver-deployment-7f5969cbc7-xnwsx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xnwsx webserver-deployment-7f5969cbc7- deployment-5176  145dd352-6b79-459b-8aba-17241366ae8d 30493 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e546f7d87344f29dfd8a6088f4d7be9a4a938687aedfdc6c0027ab0808ec6377 cni.projectcalico.org/podIP:10.20.193.194/32 cni.projectcalico.org/podIPs:10.20.193.194/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e79940 0xc005e79941}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.193.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzmhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzmhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-126.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.126,PodIP:10.20.193.194,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://832eaf76a1b275b065165e46ab77f545202a97b664ded0d5a13bac3d32e6afbb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.193.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.773: INFO: Pod "webserver-deployment-d9f79cb5-7cfmc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7cfmc webserver-deployment-d9f79cb5- deployment-5176  b6c0cccc-0ddd-43f3-b673-f7c75e3e763a 30634 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc005e79b1f 0xc005e79b30}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6vw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6vw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.773: INFO: Pod "webserver-deployment-d9f79cb5-7g68j" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7g68j webserver-deployment-d9f79cb5- deployment-5176  f76a316e-a3d7-4411-96c2-4a977506d0fe 30586 0 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:358098d32a17ac841aed4933b9c32636d811ab0c00c5cca671b8c5ecdeda623e cni.projectcalico.org/podIP:10.20.193.198/32 cni.projectcalico.org/podIPs:10.20.193.198/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc005e79c7f 0xc005e79cb0}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-26 06:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5gx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5gx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-126.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.126,PodIP:,StartTime:2023-08-26 06:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.773: INFO: Pod "webserver-deployment-d9f79cb5-chckt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-chckt webserver-deployment-d9f79cb5- deployment-5176  687a1a9a-de8e-4776-a5af-56e7750571b9 30587 0 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e8cdd8b861d0ab1b0136eec55a6ead309683a8e363542e6509717620acac9bbb cni.projectcalico.org/podIP:10.20.8.244/32 cni.projectcalico.org/podIPs:10.20.8.244/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc005e79e8f 0xc005e79ec0}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-26 06:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgkr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgkr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.31,PodIP:,StartTime:2023-08-26 06:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.774: INFO: Pod "webserver-deployment-d9f79cb5-j5rn6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j5rn6 webserver-deployment-d9f79cb5- deployment-5176  ecf23475-3085-433e-a2a3-410fb264abac 30592 0 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ca2a936f250a0482d6ed1c2c0ef9ffd55b3b80da67ecd0cac490452658c6d623 cni.projectcalico.org/podIP:10.20.199.109/32 cni.projectcalico.org/podIPs:10.20.199.109/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc0065d609f 0xc0065d60d0}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-26 06:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9rsn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9rsn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:,StartTime:2023-08-26 06:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.774: INFO: Pod "webserver-deployment-d9f79cb5-jztf2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jztf2 webserver-deployment-d9f79cb5- deployment-5176  b3da67b5-1b15-417e-90f3-28af8b06e3af 30640 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc0065d62af 0xc0065d62c0}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdggn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdggn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.774: INFO: Pod "webserver-deployment-d9f79cb5-kw477" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kw477 webserver-deployment-d9f79cb5- deployment-5176  56b3bf69-2db3-43d1-a36b-c798e2131943 30594 0 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7f2ec7e2877d423fd1b48156a24a750352eb4930948f7b99b7573fa7684d8703 cni.projectcalico.org/podIP:10.20.62.191/32 cni.projectcalico.org/podIPs:10.20.62.191/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc0065d640f 0xc0065d6440}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-26 06:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xsrtn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xsrtn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-23.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.23,PodIP:,StartTime:2023-08-26 06:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.775: INFO: Pod "webserver-deployment-d9f79cb5-kws4c" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kws4c webserver-deployment-d9f79cb5- deployment-5176  d0b4ea9c-0982-42fc-91a1-9d217afee7ae 30593 0 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ebfebada30e35fd1b1e0062fece0745284b5f30cc4090cb50753889dfc376694 cni.projectcalico.org/podIP:10.20.50.236/32 cni.projectcalico.org/podIPs:10.20.50.236/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc0065d661f 0xc0065d6650}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-26 06:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8xbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8xbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.101,PodIP:,StartTime:2023-08-26 06:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:03:22.775: INFO: Pod "webserver-deployment-d9f79cb5-qt2zb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qt2zb webserver-deployment-d9f79cb5- deployment-5176  bd51d306-855f-4dbc-8a2b-ce9e3142c6a6 30631 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc0065d682f 0xc0065d6840}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qn4xh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qn4xh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 26 06:03:22.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5176" for this suite. 08/26/23 06:03:22.786
------------------------------
• [SLOW TEST] [6.244 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:03:16.564
    Aug 26 06:03:16.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename deployment 08/26/23 06:03:16.565
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:16.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:16.581
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Aug 26 06:03:16.584: INFO: Creating deployment "webserver-deployment"
    Aug 26 06:03:16.590: INFO: Waiting for observed generation 1
    Aug 26 06:03:18.598: INFO: Waiting for all required pods to come up
    Aug 26 06:03:18.614: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 08/26/23 06:03:18.614
    Aug 26 06:03:18.615: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-8b5vt" in namespace "deployment-5176" to be "running"
    Aug 26 06:03:18.619: INFO: Pod "webserver-deployment-7f5969cbc7-8b5vt": Phase="Running", Reason="", readiness=true. Elapsed: 4.513283ms
    Aug 26 06:03:18.619: INFO: Pod "webserver-deployment-7f5969cbc7-8b5vt" satisfied condition "running"
    Aug 26 06:03:18.619: INFO: Waiting for deployment "webserver-deployment" to complete
    Aug 26 06:03:18.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:10, UpdatedReplicas:10, ReadyReplicas:9, AvailableReplicas:9, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"webserver-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 06:03:20.634: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Aug 26 06:03:20.644: INFO: Updating deployment webserver-deployment
    Aug 26 06:03:20.644: INFO: Waiting for observed generation 2
    Aug 26 06:03:22.655: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Aug 26 06:03:22.659: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Aug 26 06:03:22.662: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 26 06:03:22.674: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Aug 26 06:03:22.674: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Aug 26 06:03:22.681: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 26 06:03:22.692: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Aug 26 06:03:22.692: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Aug 26 06:03:22.702: INFO: Updating deployment webserver-deployment
    Aug 26 06:03:22.702: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Aug 26 06:03:22.709: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Aug 26 06:03:22.716: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 26 06:03:22.740: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-5176  9abd5fb4-14a3-49d4-a2a6-87eaf5bad39e 30624 3 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005be5f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-26 06:03:20 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-26 06:03:22 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Aug 26 06:03:22.755: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5176  5cf62f24-b778-497c-a05a-6954d9cd5d1f 30618 3 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9abd5fb4-14a3-49d4-a2a6-87eaf5bad39e 0xc0073abbe7 0xc0073abbe8}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9abd5fb4-14a3-49d4-a2a6-87eaf5bad39e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073abc88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 06:03:22.755: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Aug 26 06:03:22.756: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5176  3fce32d3-b137-4172-9972-9b114c1295c3 30615 3 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9abd5fb4-14a3-49d4-a2a6-87eaf5bad39e 0xc0073abaf7 0xc0073abaf8}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9abd5fb4-14a3-49d4-a2a6-87eaf5bad39e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073abb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 06:03:22.767: INFO: Pod "webserver-deployment-7f5969cbc7-47gt2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-47gt2 webserver-deployment-7f5969cbc7- deployment-5176  838d0f8d-a2ec-45fd-aab6-1a4dc00554f6 30630 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78157 0xc005e78158}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57qnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57qnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.768: INFO: Pod "webserver-deployment-7f5969cbc7-54769" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-54769 webserver-deployment-7f5969cbc7- deployment-5176  8e4f9676-d416-4296-a9df-f2b35eabbdee 30635 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78297 0xc005e78298}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fjch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fjch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.769: INFO: Pod "webserver-deployment-7f5969cbc7-87dtt" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-87dtt webserver-deployment-7f5969cbc7- deployment-5176  c76391c9-0525-416b-954b-5532065e7b18 30484 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:87de7c21ef24b52706184d183f3236124c2fd171346e434d643e5b2566aecb1d cni.projectcalico.org/podIP:10.20.50.235/32 cni.projectcalico.org/podIPs:10.20.50.235/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78410 0xc005e78411}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.50.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftgff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftgff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.101,PodIP:10.20.50.235,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f03cb9fca1fcab2b81fb578a7cff9b925abe8983b90b69f56ab0cf88bbeae33f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.50.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.769: INFO: Pod "webserver-deployment-7f5969cbc7-8b5vt" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8b5vt webserver-deployment-7f5969cbc7- deployment-5176  71692bee-9a4d-4f33-a1fb-390366714d09 30496 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:88e4cc774c91f29663287ef00ca2789baeedb8ff2e376e3af8ebfb6fcdf9a421 cni.projectcalico.org/podIP:10.20.193.197/32 cni.projectcalico.org/podIPs:10.20.193.197/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78620 0xc005e78621}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.193.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49cgr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49cgr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-126.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.126,PodIP:10.20.193.197,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b595be7e273d0de8d1a4e74cc5ebd537844d47e8623b881d4cc4108243b11977,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.193.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.770: INFO: Pod "webserver-deployment-7f5969cbc7-8z7bk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8z7bk webserver-deployment-7f5969cbc7- deployment-5176  1bc6e4ea-3290-44f4-96f6-2dec5c3bc0ef 30477 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a1dc1d0d8985ffe25b0624c68e7028614e0cd47850ce50fb90a13a8f5a79d196 cni.projectcalico.org/podIP:10.20.199.96/32 cni.projectcalico.org/podIPs:10.20.199.96/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78830 0xc005e78831}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4p4sz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4p4sz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:10.20.199.96,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://895c440dec4a74567a41b3fa76e56df83cec0c9fe9cc9db16af55dc4b20026b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.199.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.770: INFO: Pod "webserver-deployment-7f5969cbc7-dszjc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dszjc webserver-deployment-7f5969cbc7- deployment-5176  809b252b-76f5-444e-9bd4-e452f3c89f19 30481 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0c577adcc189062ac256556b9f7dd67666f53ff6f8a2f01f9e336d6112ac0ecd cni.projectcalico.org/podIP:10.20.50.234/32 cni.projectcalico.org/podIPs:10.20.50.234/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78a40 0xc005e78a41}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.50.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2s4xp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2s4xp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.101,PodIP:10.20.50.234,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f349b9be14e96904ca025805516d26dfd78f28dd3fe4341a7be1b578c2135c0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.50.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.770: INFO: Pod "webserver-deployment-7f5969cbc7-h2dp5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h2dp5 webserver-deployment-7f5969cbc7- deployment-5176  9362e778-ad2a-4842-b8c3-30120f3d9040 30471 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6a457cafffc367ce7271842c5c2c96d4d06ebad0b21282ce972cb87cdccef64b cni.projectcalico.org/podIP:10.20.62.189/32 cni.projectcalico.org/podIPs:10.20.62.189/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78c50 0xc005e78c51}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.62.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dwpb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dwpb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-23.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.23,PodIP:10.20.62.189,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://df585218149c78010ed7bb78d3797b599dbdcfa2afa8c30eeaeeb51c74a0b159,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.62.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.770: INFO: Pod "webserver-deployment-7f5969cbc7-jrnbl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jrnbl webserver-deployment-7f5969cbc7- deployment-5176  90603765-a771-47b5-82e2-eb92b70e8f9d 30629 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78e40 0xc005e78e41}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tj7jm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tj7jm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.771: INFO: Pod "webserver-deployment-7f5969cbc7-jzvrh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jzvrh webserver-deployment-7f5969cbc7- deployment-5176  fb4234d5-733e-4257-ab2c-5c7785eae0db 30636 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e78f77 0xc005e78f78}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fjswh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fjswh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.771: INFO: Pod "webserver-deployment-7f5969cbc7-mbdxw" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mbdxw webserver-deployment-7f5969cbc7- deployment-5176  05119b26-0d46-4bb2-ab01-06b2abbcb82f 30474 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6ecb7490b73b2c04de9c38609f6909efe91696781e8e3bb47c176dbf2d242a5b cni.projectcalico.org/podIP:10.20.199.97/32 cni.projectcalico.org/podIPs:10.20.199.97/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e790d7 0xc005e790d8}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lmjvj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lmjvj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:10.20.199.97,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fa99108359a2a8d6d7cea32218e3cd0f329c5f8fef699e5badeb47d0e855c004,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.199.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.772: INFO: Pod "webserver-deployment-7f5969cbc7-mtpjk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mtpjk webserver-deployment-7f5969cbc7- deployment-5176  60611d6c-17c8-40dd-9f69-03c27f7730bc 30638 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e792d0 0xc005e792d1}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njgs9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njgs9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.31,PodIP:,StartTime:2023-08-26 06:03:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.772: INFO: Pod "webserver-deployment-7f5969cbc7-q5dvh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q5dvh webserver-deployment-7f5969cbc7- deployment-5176  55afd664-a101-422d-b1ff-763674e966bf 30632 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e79480 0xc005e79481}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6v52j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6v52j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.772: INFO: Pod "webserver-deployment-7f5969cbc7-scdvq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-scdvq webserver-deployment-7f5969cbc7- deployment-5176  267f202b-ada6-44c8-95de-e7ed38371768 30467 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:16cb78e540dc9316b243eaa666fd408dc2b53d06201a18a2acc626e73756b634 cni.projectcalico.org/podIP:10.20.62.190/32 cni.projectcalico.org/podIPs:10.20.62.190/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e795d7 0xc005e795d8}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.62.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wksw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wksw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-23.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.23,PodIP:10.20.62.190,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9af1e0bc7dd0aacfab1ab253010ed46f02614517b6be5bcbfabe8f42e56d422b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.62.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.772: INFO: Pod "webserver-deployment-7f5969cbc7-v7mbm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-v7mbm webserver-deployment-7f5969cbc7- deployment-5176  f6b14ace-a027-4e15-9c08-2fd0f9db3222 30625 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e797d0 0xc005e797d1}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqqg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqqg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.773: INFO: Pod "webserver-deployment-7f5969cbc7-xnwsx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xnwsx webserver-deployment-7f5969cbc7- deployment-5176  145dd352-6b79-459b-8aba-17241366ae8d 30493 0 2023-08-26 06:03:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e546f7d87344f29dfd8a6088f4d7be9a4a938687aedfdc6c0027ab0808ec6377 cni.projectcalico.org/podIP:10.20.193.194/32 cni.projectcalico.org/podIPs:10.20.193.194/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 3fce32d3-b137-4172-9972-9b114c1295c3 0xc005e79940 0xc005e79941}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fce32d3-b137-4172-9972-9b114c1295c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:03:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.193.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzmhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzmhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-126.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.126,PodIP:10.20.193.194,StartTime:2023-08-26 06:03:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:03:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://832eaf76a1b275b065165e46ab77f545202a97b664ded0d5a13bac3d32e6afbb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.193.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.773: INFO: Pod "webserver-deployment-d9f79cb5-7cfmc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7cfmc webserver-deployment-d9f79cb5- deployment-5176  b6c0cccc-0ddd-43f3-b673-f7c75e3e763a 30634 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc005e79b1f 0xc005e79b30}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6vw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6vw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.773: INFO: Pod "webserver-deployment-d9f79cb5-7g68j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7g68j webserver-deployment-d9f79cb5- deployment-5176  f76a316e-a3d7-4411-96c2-4a977506d0fe 30586 0 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:358098d32a17ac841aed4933b9c32636d811ab0c00c5cca671b8c5ecdeda623e cni.projectcalico.org/podIP:10.20.193.198/32 cni.projectcalico.org/podIPs:10.20.193.198/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc005e79c7f 0xc005e79cb0}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-26 06:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5gx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5gx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-126.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.126,PodIP:,StartTime:2023-08-26 06:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.773: INFO: Pod "webserver-deployment-d9f79cb5-chckt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-chckt webserver-deployment-d9f79cb5- deployment-5176  687a1a9a-de8e-4776-a5af-56e7750571b9 30587 0 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e8cdd8b861d0ab1b0136eec55a6ead309683a8e363542e6509717620acac9bbb cni.projectcalico.org/podIP:10.20.8.244/32 cni.projectcalico.org/podIPs:10.20.8.244/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc005e79e8f 0xc005e79ec0}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-26 06:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgkr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgkr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.31,PodIP:,StartTime:2023-08-26 06:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.774: INFO: Pod "webserver-deployment-d9f79cb5-j5rn6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j5rn6 webserver-deployment-d9f79cb5- deployment-5176  ecf23475-3085-433e-a2a3-410fb264abac 30592 0 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ca2a936f250a0482d6ed1c2c0ef9ffd55b3b80da67ecd0cac490452658c6d623 cni.projectcalico.org/podIP:10.20.199.109/32 cni.projectcalico.org/podIPs:10.20.199.109/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc0065d609f 0xc0065d60d0}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-26 06:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9rsn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9rsn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:,StartTime:2023-08-26 06:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.774: INFO: Pod "webserver-deployment-d9f79cb5-jztf2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jztf2 webserver-deployment-d9f79cb5- deployment-5176  b3da67b5-1b15-417e-90f3-28af8b06e3af 30640 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc0065d62af 0xc0065d62c0}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdggn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdggn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.774: INFO: Pod "webserver-deployment-d9f79cb5-kw477" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kw477 webserver-deployment-d9f79cb5- deployment-5176  56b3bf69-2db3-43d1-a36b-c798e2131943 30594 0 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7f2ec7e2877d423fd1b48156a24a750352eb4930948f7b99b7573fa7684d8703 cni.projectcalico.org/podIP:10.20.62.191/32 cni.projectcalico.org/podIPs:10.20.62.191/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc0065d640f 0xc0065d6440}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-26 06:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xsrtn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xsrtn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-23.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.23,PodIP:,StartTime:2023-08-26 06:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.775: INFO: Pod "webserver-deployment-d9f79cb5-kws4c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kws4c webserver-deployment-d9f79cb5- deployment-5176  d0b4ea9c-0982-42fc-91a1-9d217afee7ae 30593 0 2023-08-26 06:03:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ebfebada30e35fd1b1e0062fece0745284b5f30cc4090cb50753889dfc376694 cni.projectcalico.org/podIP:10.20.50.236/32 cni.projectcalico.org/podIPs:10.20.50.236/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc0065d661f 0xc0065d6650}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:03:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-26 06:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8xbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8xbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.101,PodIP:,StartTime:2023-08-26 06:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:03:22.775: INFO: Pod "webserver-deployment-d9f79cb5-qt2zb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qt2zb webserver-deployment-d9f79cb5- deployment-5176  bd51d306-855f-4dbc-8a2b-ce9e3142c6a6 30631 0 2023-08-26 06:03:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 5cf62f24-b778-497c-a05a-6954d9cd5d1f 0xc0065d682f 0xc0065d6840}] [] [{kube-controller-manager Update v1 2023-08-26 06:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5cf62f24-b778-497c-a05a-6954d9cd5d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qn4xh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qn4xh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:03:22.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5176" for this suite. 08/26/23 06:03:22.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:03:22.81
Aug 26 06:03:22.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 06:03:22.816
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:22.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:22.875
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 06:03:22.892
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:03:23.36
STEP: Deploying the webhook pod 08/26/23 06:03:23.37
STEP: Wait for the deployment to be ready 08/26/23 06:03:23.384
Aug 26 06:03:23.398: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 26 06:03:25.412: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 06:03:27.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/26/23 06:03:29.417
STEP: Verifying the service has paired with the endpoint 08/26/23 06:03:29.427
Aug 26 06:03:30.428: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 08/26/23 06:03:30.433
STEP: create a pod that should be denied by the webhook 08/26/23 06:03:30.453
STEP: create a pod that causes the webhook to hang 08/26/23 06:03:30.468
STEP: create a configmap that should be denied by the webhook 08/26/23 06:03:40.476
STEP: create a configmap that should be admitted by the webhook 08/26/23 06:03:40.485
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/26/23 06:03:40.495
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/26/23 06:03:40.503
STEP: create a namespace that bypass the webhook 08/26/23 06:03:40.508
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/26/23 06:03:40.514
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:03:40.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1901" for this suite. 08/26/23 06:03:40.612
STEP: Destroying namespace "webhook-1901-markers" for this suite. 08/26/23 06:03:40.62
------------------------------
• [SLOW TEST] [17.820 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:03:22.81
    Aug 26 06:03:22.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 06:03:22.816
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:22.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:22.875
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 06:03:22.892
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:03:23.36
    STEP: Deploying the webhook pod 08/26/23 06:03:23.37
    STEP: Wait for the deployment to be ready 08/26/23 06:03:23.384
    Aug 26 06:03:23.398: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Aug 26 06:03:25.412: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 26 06:03:27.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 3, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/26/23 06:03:29.417
    STEP: Verifying the service has paired with the endpoint 08/26/23 06:03:29.427
    Aug 26 06:03:30.428: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 08/26/23 06:03:30.433
    STEP: create a pod that should be denied by the webhook 08/26/23 06:03:30.453
    STEP: create a pod that causes the webhook to hang 08/26/23 06:03:30.468
    STEP: create a configmap that should be denied by the webhook 08/26/23 06:03:40.476
    STEP: create a configmap that should be admitted by the webhook 08/26/23 06:03:40.485
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/26/23 06:03:40.495
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/26/23 06:03:40.503
    STEP: create a namespace that bypass the webhook 08/26/23 06:03:40.508
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/26/23 06:03:40.514
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:03:40.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1901" for this suite. 08/26/23 06:03:40.612
    STEP: Destroying namespace "webhook-1901-markers" for this suite. 08/26/23 06:03:40.62
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:03:40.63
Aug 26 06:03:40.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename svcaccounts 08/26/23 06:03:40.631
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:40.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:40.647
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 08/26/23 06:03:40.65
STEP: watching for the ServiceAccount to be added 08/26/23 06:03:40.657
STEP: patching the ServiceAccount 08/26/23 06:03:40.659
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/26/23 06:03:40.667
STEP: deleting the ServiceAccount 08/26/23 06:03:40.672
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 26 06:03:40.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7955" for this suite. 08/26/23 06:03:40.692
------------------------------
• [0.070 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:03:40.63
    Aug 26 06:03:40.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename svcaccounts 08/26/23 06:03:40.631
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:40.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:40.647
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 08/26/23 06:03:40.65
    STEP: watching for the ServiceAccount to be added 08/26/23 06:03:40.657
    STEP: patching the ServiceAccount 08/26/23 06:03:40.659
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/26/23 06:03:40.667
    STEP: deleting the ServiceAccount 08/26/23 06:03:40.672
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:03:40.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7955" for this suite. 08/26/23 06:03:40.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:03:40.702
Aug 26 06:03:40.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:03:40.703
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:40.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:40.72
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/26/23 06:03:40.722
Aug 26 06:03:40.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:03:42.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:03:50.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5435" for this suite. 08/26/23 06:03:50.806
------------------------------
• [SLOW TEST] [10.114 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:03:40.702
    Aug 26 06:03:40.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:03:40.703
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:40.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:40.72
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/26/23 06:03:40.722
    Aug 26 06:03:40.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:03:42.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:03:50.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5435" for this suite. 08/26/23 06:03:50.806
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:03:50.817
Aug 26 06:03:50.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 06:03:50.818
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:50.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:50.844
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 08/26/23 06:03:50.849
STEP: Creating a ResourceQuota 08/26/23 06:03:55.854
STEP: Ensuring resource quota status is calculated 08/26/23 06:03:55.86
STEP: Creating a ReplicationController 08/26/23 06:03:57.865
STEP: Ensuring resource quota status captures replication controller creation 08/26/23 06:03:57.881
STEP: Deleting a ReplicationController 08/26/23 06:03:59.889
STEP: Ensuring resource quota status released usage 08/26/23 06:03:59.896
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 06:04:01.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2558" for this suite. 08/26/23 06:04:01.919
------------------------------
• [SLOW TEST] [11.116 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:03:50.817
    Aug 26 06:03:50.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 06:03:50.818
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:03:50.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:03:50.844
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 08/26/23 06:03:50.849
    STEP: Creating a ResourceQuota 08/26/23 06:03:55.854
    STEP: Ensuring resource quota status is calculated 08/26/23 06:03:55.86
    STEP: Creating a ReplicationController 08/26/23 06:03:57.865
    STEP: Ensuring resource quota status captures replication controller creation 08/26/23 06:03:57.881
    STEP: Deleting a ReplicationController 08/26/23 06:03:59.889
    STEP: Ensuring resource quota status released usage 08/26/23 06:03:59.896
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:04:01.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2558" for this suite. 08/26/23 06:04:01.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:04:01.934
Aug 26 06:04:01.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sched-pred 08/26/23 06:04:01.935
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:04:01.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:04:01.966
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 26 06:04:01.969: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 26 06:04:01.993: INFO: Waiting for terminating namespaces to be deleted...
Aug 26 06:04:01.999: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-101.us-west-2.compute.internal before test
Aug 26 06:04:02.021: INFO: calico-node-hgm7c from kube-system started at 2023-08-26 04:58:59 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.021: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 06:04:02.021: INFO: calico-typha-6dd9648c8f-85hjp from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.021: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 06:04:02.021: INFO: sonobuoy from sonobuoy started at 2023-08-26 05:16:19 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.021: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 26 06:04:02.021: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:04:02.021: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:04:02.021: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 06:04:02.021: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-126.us-west-2.compute.internal before test
Aug 26 06:04:02.047: INFO: calico-node-8272m from kube-system started at 2023-08-26 04:58:47 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.047: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 06:04:02.047: INFO: calico-typha-6dd9648c8f-th65n from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.047: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 06:04:02.047: INFO: sonobuoy-e2e-job-c6e00385bb4e4fb8 from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:04:02.047: INFO: 	Container e2e ready: true, restart count 0
Aug 26 06:04:02.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:04:02.047: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-ckdgn from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:04:02.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:04:02.047: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 06:04:02.047: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-23.us-west-2.compute.internal before test
Aug 26 06:04:02.069: INFO: calico-kube-controllers-76798f54cb-cdxwm from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.069: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 26 06:04:02.069: INFO: calico-node-trdnz from kube-system started at 2023-08-26 04:59:15 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.069: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 06:04:02.069: INFO: calico-typha-6dd9648c8f-x4b7r from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.069: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 06:04:02.069: INFO: calico-typha-autoscaler-54c8866496-tmj59 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.069: INFO: 	Container autoscaler ready: true, restart count 0
Aug 26 06:04:02.069: INFO: coredns-58ffcc48df-2q4rv from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.069: INFO: 	Container coredns ready: true, restart count 0
Aug 26 06:04:02.069: INFO: kube-dns-autoscaler-f68f756b6-78kdn from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.069: INFO: 	Container autoscaler ready: true, restart count 0
Aug 26 06:04:02.069: INFO: kube-state-metrics-99bbfb4cd-g7vn6 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.069: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 26 06:04:02.069: INFO: dashboard-metrics-scraper-6c57f89c7c-2vtv5 from kubernetes-dashboard started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.069: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 26 06:04:02.069: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-g65bg from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:04:02.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:04:02.069: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 06:04:02.069: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-31.us-west-2.compute.internal before test
Aug 26 06:04:02.099: INFO: calico-node-mxmcm from kube-system started at 2023-08-26 04:59:09 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.100: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 06:04:02.100: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-6gxwx from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:04:02.100: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:04:02.100: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 06:04:02.100: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-5.us-west-2.compute.internal before test
Aug 26 06:04:02.121: INFO: calico-node-jftpx from kube-system started at 2023-08-26 04:58:49 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.121: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 06:04:02.121: INFO: metrics-server-b7db9955-ljgsw from kube-system started at 2023-08-26 06:03:25 +0000 UTC (1 container statuses recorded)
Aug 26 06:04:02.121: INFO: 	Container metrics-server ready: true, restart count 0
Aug 26 06:04:02.121: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-tpq9m from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:04:02.121: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:04:02.121: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 08/26/23 06:04:02.121
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.177ed9abe1d800b6], Reason = [FailedScheduling], Message = [0/8 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}, 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/8 nodes are available: 8 Preemption is not helpful for scheduling..] 08/26/23 06:04:02.201
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:04:03.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-922" for this suite. 08/26/23 06:04:03.222
------------------------------
• [1.303 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:04:01.934
    Aug 26 06:04:01.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sched-pred 08/26/23 06:04:01.935
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:04:01.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:04:01.966
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 26 06:04:01.969: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 26 06:04:01.993: INFO: Waiting for terminating namespaces to be deleted...
    Aug 26 06:04:01.999: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-101.us-west-2.compute.internal before test
    Aug 26 06:04:02.021: INFO: calico-node-hgm7c from kube-system started at 2023-08-26 04:58:59 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.021: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 06:04:02.021: INFO: calico-typha-6dd9648c8f-85hjp from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.021: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 06:04:02.021: INFO: sonobuoy from sonobuoy started at 2023-08-26 05:16:19 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.021: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 26 06:04:02.021: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:04:02.021: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:04:02.021: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 06:04:02.021: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-126.us-west-2.compute.internal before test
    Aug 26 06:04:02.047: INFO: calico-node-8272m from kube-system started at 2023-08-26 04:58:47 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.047: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 06:04:02.047: INFO: calico-typha-6dd9648c8f-th65n from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.047: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 06:04:02.047: INFO: sonobuoy-e2e-job-c6e00385bb4e4fb8 from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:04:02.047: INFO: 	Container e2e ready: true, restart count 0
    Aug 26 06:04:02.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:04:02.047: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-ckdgn from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:04:02.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:04:02.047: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 06:04:02.047: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-23.us-west-2.compute.internal before test
    Aug 26 06:04:02.069: INFO: calico-kube-controllers-76798f54cb-cdxwm from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.069: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 26 06:04:02.069: INFO: calico-node-trdnz from kube-system started at 2023-08-26 04:59:15 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.069: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 06:04:02.069: INFO: calico-typha-6dd9648c8f-x4b7r from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.069: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 06:04:02.069: INFO: calico-typha-autoscaler-54c8866496-tmj59 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.069: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 26 06:04:02.069: INFO: coredns-58ffcc48df-2q4rv from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.069: INFO: 	Container coredns ready: true, restart count 0
    Aug 26 06:04:02.069: INFO: kube-dns-autoscaler-f68f756b6-78kdn from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.069: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 26 06:04:02.069: INFO: kube-state-metrics-99bbfb4cd-g7vn6 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.069: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 26 06:04:02.069: INFO: dashboard-metrics-scraper-6c57f89c7c-2vtv5 from kubernetes-dashboard started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.069: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Aug 26 06:04:02.069: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-g65bg from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:04:02.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:04:02.069: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 06:04:02.069: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-31.us-west-2.compute.internal before test
    Aug 26 06:04:02.099: INFO: calico-node-mxmcm from kube-system started at 2023-08-26 04:59:09 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.100: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 06:04:02.100: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-6gxwx from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:04:02.100: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:04:02.100: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 06:04:02.100: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-5.us-west-2.compute.internal before test
    Aug 26 06:04:02.121: INFO: calico-node-jftpx from kube-system started at 2023-08-26 04:58:49 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.121: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 06:04:02.121: INFO: metrics-server-b7db9955-ljgsw from kube-system started at 2023-08-26 06:03:25 +0000 UTC (1 container statuses recorded)
    Aug 26 06:04:02.121: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 26 06:04:02.121: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-tpq9m from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:04:02.121: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:04:02.121: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 08/26/23 06:04:02.121
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.177ed9abe1d800b6], Reason = [FailedScheduling], Message = [0/8 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}, 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/8 nodes are available: 8 Preemption is not helpful for scheduling..] 08/26/23 06:04:02.201
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:04:03.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-922" for this suite. 08/26/23 06:04:03.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:04:03.276
Aug 26 06:04:03.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-probe 08/26/23 06:04:03.277
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:04:03.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:04:03.314
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-601a8395-eecd-47a0-8e80-a15e14f4543a in namespace container-probe-863 08/26/23 06:04:03.318
Aug 26 06:04:03.333: INFO: Waiting up to 5m0s for pod "busybox-601a8395-eecd-47a0-8e80-a15e14f4543a" in namespace "container-probe-863" to be "not pending"
Aug 26 06:04:03.338: INFO: Pod "busybox-601a8395-eecd-47a0-8e80-a15e14f4543a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.668563ms
Aug 26 06:04:05.343: INFO: Pod "busybox-601a8395-eecd-47a0-8e80-a15e14f4543a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009870965s
Aug 26 06:04:05.343: INFO: Pod "busybox-601a8395-eecd-47a0-8e80-a15e14f4543a" satisfied condition "not pending"
Aug 26 06:04:05.343: INFO: Started pod busybox-601a8395-eecd-47a0-8e80-a15e14f4543a in namespace container-probe-863
STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 06:04:05.343
Aug 26 06:04:05.351: INFO: Initial restart count of pod busybox-601a8395-eecd-47a0-8e80-a15e14f4543a is 0
STEP: deleting the pod 08/26/23 06:08:06.226
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 26 06:08:06.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-863" for this suite. 08/26/23 06:08:06.261
------------------------------
• [SLOW TEST] [242.998 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:04:03.276
    Aug 26 06:04:03.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-probe 08/26/23 06:04:03.277
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:04:03.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:04:03.314
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-601a8395-eecd-47a0-8e80-a15e14f4543a in namespace container-probe-863 08/26/23 06:04:03.318
    Aug 26 06:04:03.333: INFO: Waiting up to 5m0s for pod "busybox-601a8395-eecd-47a0-8e80-a15e14f4543a" in namespace "container-probe-863" to be "not pending"
    Aug 26 06:04:03.338: INFO: Pod "busybox-601a8395-eecd-47a0-8e80-a15e14f4543a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.668563ms
    Aug 26 06:04:05.343: INFO: Pod "busybox-601a8395-eecd-47a0-8e80-a15e14f4543a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009870965s
    Aug 26 06:04:05.343: INFO: Pod "busybox-601a8395-eecd-47a0-8e80-a15e14f4543a" satisfied condition "not pending"
    Aug 26 06:04:05.343: INFO: Started pod busybox-601a8395-eecd-47a0-8e80-a15e14f4543a in namespace container-probe-863
    STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 06:04:05.343
    Aug 26 06:04:05.351: INFO: Initial restart count of pod busybox-601a8395-eecd-47a0-8e80-a15e14f4543a is 0
    STEP: deleting the pod 08/26/23 06:08:06.226
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:08:06.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-863" for this suite. 08/26/23 06:08:06.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:08:06.276
Aug 26 06:08:06.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename deployment 08/26/23 06:08:06.276
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:08:06.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:08:06.304
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Aug 26 06:08:06.307: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 26 06:08:06.319: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 26 06:08:11.327: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/26/23 06:08:11.327
Aug 26 06:08:11.327: INFO: Creating deployment "test-rolling-update-deployment"
Aug 26 06:08:11.336: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 26 06:08:11.345: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 26 06:08:13.358: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 26 06:08:13.362: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 26 06:08:13.373: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2608  de7a4d29-e6b5-4947-9ede-2023aff37dae 31970 1 2023-08-26 06:08:11 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-26 06:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00285bee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-26 06:08:11 +0000 UTC,LastTransitionTime:2023-08-26 06:08:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-26 06:08:13 +0000 UTC,LastTransitionTime:2023-08-26 06:08:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 26 06:08:13.377: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2608  4057cdfc-a0d0-42d0-a1cc-a4963dd99e06 31960 1 2023-08-26 06:08:11 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment de7a4d29-e6b5-4947-9ede-2023aff37dae 0xc004363187 0xc004363188}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de7a4d29-e6b5-4947-9ede-2023aff37dae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:08:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004363238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 26 06:08:13.377: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 26 06:08:13.377: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2608  cd0f6c8b-4c27-4b8a-8602-0ac6ab111091 31969 2 2023-08-26 06:08:06 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment de7a4d29-e6b5-4947-9ede-2023aff37dae 0xc004363047 0xc004363048}] [] [{e2e.test Update apps/v1 2023-08-26 06:08:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de7a4d29-e6b5-4947-9ede-2023aff37dae\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:08:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004363118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 26 06:08:13.382: INFO: Pod "test-rolling-update-deployment-7549d9f46d-xq2xv" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-xq2xv test-rolling-update-deployment-7549d9f46d- deployment-2608  c3f54431-0d06-4e9a-bf9f-8e020cb79e55 31959 0 2023-08-26 06:08:11 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:37d229eea17cca685d6246d5c3db4872fbbeeaf51898c0910e8bad459e783cb1 cni.projectcalico.org/podIP:10.20.8.242/32 cni.projectcalico.org/podIPs:10.20.8.242/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 4057cdfc-a0d0-42d0-a1cc-a4963dd99e06 0xc0053b1fd7 0xc0053b1fd8}] [] [{kube-controller-manager Update v1 2023-08-26 06:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4057cdfc-a0d0-42d0-a1cc-a4963dd99e06\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:08:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.8.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6p8qz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6p8qz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:08:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:08:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:08:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:08:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.31,PodIP:10.20.8.242,StartTime:2023-08-26 06:08:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:08:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://f911778604e041d40d521e8b7e2d5d85c6aced93f1367eff5516b2b33ef89247,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.8.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 26 06:08:13.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2608" for this suite. 08/26/23 06:08:13.393
------------------------------
• [SLOW TEST] [7.127 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:08:06.276
    Aug 26 06:08:06.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename deployment 08/26/23 06:08:06.276
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:08:06.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:08:06.304
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Aug 26 06:08:06.307: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Aug 26 06:08:06.319: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 26 06:08:11.327: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/26/23 06:08:11.327
    Aug 26 06:08:11.327: INFO: Creating deployment "test-rolling-update-deployment"
    Aug 26 06:08:11.336: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Aug 26 06:08:11.345: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Aug 26 06:08:13.358: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Aug 26 06:08:13.362: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 26 06:08:13.373: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2608  de7a4d29-e6b5-4947-9ede-2023aff37dae 31970 1 2023-08-26 06:08:11 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-26 06:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00285bee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-26 06:08:11 +0000 UTC,LastTransitionTime:2023-08-26 06:08:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-26 06:08:13 +0000 UTC,LastTransitionTime:2023-08-26 06:08:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 26 06:08:13.377: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2608  4057cdfc-a0d0-42d0-a1cc-a4963dd99e06 31960 1 2023-08-26 06:08:11 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment de7a4d29-e6b5-4947-9ede-2023aff37dae 0xc004363187 0xc004363188}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de7a4d29-e6b5-4947-9ede-2023aff37dae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:08:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004363238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 06:08:13.377: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Aug 26 06:08:13.377: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2608  cd0f6c8b-4c27-4b8a-8602-0ac6ab111091 31969 2 2023-08-26 06:08:06 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment de7a4d29-e6b5-4947-9ede-2023aff37dae 0xc004363047 0xc004363048}] [] [{e2e.test Update apps/v1 2023-08-26 06:08:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de7a4d29-e6b5-4947-9ede-2023aff37dae\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:08:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004363118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 06:08:13.382: INFO: Pod "test-rolling-update-deployment-7549d9f46d-xq2xv" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-xq2xv test-rolling-update-deployment-7549d9f46d- deployment-2608  c3f54431-0d06-4e9a-bf9f-8e020cb79e55 31959 0 2023-08-26 06:08:11 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:37d229eea17cca685d6246d5c3db4872fbbeeaf51898c0910e8bad459e783cb1 cni.projectcalico.org/podIP:10.20.8.242/32 cni.projectcalico.org/podIPs:10.20.8.242/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 4057cdfc-a0d0-42d0-a1cc-a4963dd99e06 0xc0053b1fd7 0xc0053b1fd8}] [] [{kube-controller-manager Update v1 2023-08-26 06:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4057cdfc-a0d0-42d0-a1cc-a4963dd99e06\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:08:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.8.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6p8qz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6p8qz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:08:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:08:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:08:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:08:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.31,PodIP:10.20.8.242,StartTime:2023-08-26 06:08:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:08:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://f911778604e041d40d521e8b7e2d5d85c6aced93f1367eff5516b2b33ef89247,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.8.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:08:13.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2608" for this suite. 08/26/23 06:08:13.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:08:13.404
Aug 26 06:08:13.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename gc 08/26/23 06:08:13.405
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:08:13.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:08:13.435
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 08/26/23 06:08:13.438
STEP: delete the rc 08/26/23 06:08:18.451
STEP: wait for all pods to be garbage collected 08/26/23 06:08:18.469
STEP: Gathering metrics 08/26/23 06:08:23.48
W0826 06:08:23.506715      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 26 06:08:23.506: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 26 06:08:23.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9215" for this suite. 08/26/23 06:08:23.518
------------------------------
• [SLOW TEST] [10.127 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:08:13.404
    Aug 26 06:08:13.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename gc 08/26/23 06:08:13.405
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:08:13.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:08:13.435
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 08/26/23 06:08:13.438
    STEP: delete the rc 08/26/23 06:08:18.451
    STEP: wait for all pods to be garbage collected 08/26/23 06:08:18.469
    STEP: Gathering metrics 08/26/23 06:08:23.48
    W0826 06:08:23.506715      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 26 06:08:23.506: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:08:23.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9215" for this suite. 08/26/23 06:08:23.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:08:23.532
Aug 26 06:08:23.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 06:08:23.533
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:08:23.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:08:23.553
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7893 08/26/23 06:08:23.557
STEP: changing the ExternalName service to type=ClusterIP 08/26/23 06:08:23.562
STEP: creating replication controller externalname-service in namespace services-7893 08/26/23 06:08:23.59
I0826 06:08:23.600236      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7893, replica count: 2
I0826 06:08:26.650834      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 26 06:08:26.650: INFO: Creating new exec pod
Aug 26 06:08:26.662: INFO: Waiting up to 5m0s for pod "execpodrrz92" in namespace "services-7893" to be "running"
Aug 26 06:08:26.667: INFO: Pod "execpodrrz92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.246593ms
Aug 26 06:08:28.673: INFO: Pod "execpodrrz92": Phase="Running", Reason="", readiness=true. Elapsed: 2.010317697s
Aug 26 06:08:28.673: INFO: Pod "execpodrrz92" satisfied condition "running"
Aug 26 06:08:29.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7893 exec execpodrrz92 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 26 06:08:32.352: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 26 06:08:32.352: INFO: stdout: ""
Aug 26 06:08:32.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7893 exec execpodrrz92 -- /bin/sh -x -c nc -v -z -w 2 10.21.14.73 80'
Aug 26 06:08:32.550: INFO: stderr: "+ nc -v -z -w 2 10.21.14.73 80\nConnection to 10.21.14.73 80 port [tcp/http] succeeded!\n"
Aug 26 06:08:32.550: INFO: stdout: ""
Aug 26 06:08:32.550: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 06:08:32.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7893" for this suite. 08/26/23 06:08:32.614
------------------------------
• [SLOW TEST] [9.106 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:08:23.532
    Aug 26 06:08:23.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 06:08:23.533
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:08:23.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:08:23.553
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7893 08/26/23 06:08:23.557
    STEP: changing the ExternalName service to type=ClusterIP 08/26/23 06:08:23.562
    STEP: creating replication controller externalname-service in namespace services-7893 08/26/23 06:08:23.59
    I0826 06:08:23.600236      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7893, replica count: 2
    I0826 06:08:26.650834      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 26 06:08:26.650: INFO: Creating new exec pod
    Aug 26 06:08:26.662: INFO: Waiting up to 5m0s for pod "execpodrrz92" in namespace "services-7893" to be "running"
    Aug 26 06:08:26.667: INFO: Pod "execpodrrz92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.246593ms
    Aug 26 06:08:28.673: INFO: Pod "execpodrrz92": Phase="Running", Reason="", readiness=true. Elapsed: 2.010317697s
    Aug 26 06:08:28.673: INFO: Pod "execpodrrz92" satisfied condition "running"
    Aug 26 06:08:29.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7893 exec execpodrrz92 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 26 06:08:32.352: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 26 06:08:32.352: INFO: stdout: ""
    Aug 26 06:08:32.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-7893 exec execpodrrz92 -- /bin/sh -x -c nc -v -z -w 2 10.21.14.73 80'
    Aug 26 06:08:32.550: INFO: stderr: "+ nc -v -z -w 2 10.21.14.73 80\nConnection to 10.21.14.73 80 port [tcp/http] succeeded!\n"
    Aug 26 06:08:32.550: INFO: stdout: ""
    Aug 26 06:08:32.550: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:08:32.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7893" for this suite. 08/26/23 06:08:32.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:08:32.641
Aug 26 06:08:32.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 06:08:32.648
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:08:32.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:08:32.682
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 08/26/23 06:08:32.686
STEP: Getting a ResourceQuota 08/26/23 06:08:32.693
STEP: Updating a ResourceQuota 08/26/23 06:08:32.7
STEP: Verifying a ResourceQuota was modified 08/26/23 06:08:32.715
STEP: Deleting a ResourceQuota 08/26/23 06:08:32.722
STEP: Verifying the deleted ResourceQuota 08/26/23 06:08:32.739
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 06:08:32.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2161" for this suite. 08/26/23 06:08:32.763
------------------------------
• [0.136 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:08:32.641
    Aug 26 06:08:32.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 06:08:32.648
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:08:32.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:08:32.682
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 08/26/23 06:08:32.686
    STEP: Getting a ResourceQuota 08/26/23 06:08:32.693
    STEP: Updating a ResourceQuota 08/26/23 06:08:32.7
    STEP: Verifying a ResourceQuota was modified 08/26/23 06:08:32.715
    STEP: Deleting a ResourceQuota 08/26/23 06:08:32.722
    STEP: Verifying the deleted ResourceQuota 08/26/23 06:08:32.739
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:08:32.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2161" for this suite. 08/26/23 06:08:32.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:08:32.778
Aug 26 06:08:32.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename cronjob 08/26/23 06:08:32.781
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:08:32.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:08:32.827
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 08/26/23 06:08:32.83
STEP: Ensuring a job is scheduled 08/26/23 06:08:32.848
STEP: Ensuring exactly one is scheduled 08/26/23 06:09:00.855
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/26/23 06:09:00.862
STEP: Ensuring no more jobs are scheduled 08/26/23 06:09:00.878
STEP: Removing cronjob 08/26/23 06:14:00.889
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 26 06:14:00.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1806" for this suite. 08/26/23 06:14:00.91
------------------------------
• [SLOW TEST] [328.147 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:08:32.778
    Aug 26 06:08:32.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename cronjob 08/26/23 06:08:32.781
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:08:32.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:08:32.827
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 08/26/23 06:08:32.83
    STEP: Ensuring a job is scheduled 08/26/23 06:08:32.848
    STEP: Ensuring exactly one is scheduled 08/26/23 06:09:00.855
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/26/23 06:09:00.862
    STEP: Ensuring no more jobs are scheduled 08/26/23 06:09:00.878
    STEP: Removing cronjob 08/26/23 06:14:00.889
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:14:00.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1806" for this suite. 08/26/23 06:14:00.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:14:00.928
Aug 26 06:14:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 06:14:00.929
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:14:00.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:14:00.968
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-e5bb0aad-5aad-469c-9b23-527e959400a5 08/26/23 06:14:00.97
STEP: Creating a pod to test consume secrets 08/26/23 06:14:00.98
Aug 26 06:14:00.993: INFO: Waiting up to 5m0s for pod "pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e" in namespace "secrets-7535" to be "Succeeded or Failed"
Aug 26 06:14:01.000: INFO: Pod "pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.413906ms
Aug 26 06:14:03.011: INFO: Pod "pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017583138s
Aug 26 06:14:05.006: INFO: Pod "pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012423395s
STEP: Saw pod success 08/26/23 06:14:05.006
Aug 26 06:14:05.006: INFO: Pod "pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e" satisfied condition "Succeeded or Failed"
Aug 26 06:14:05.011: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e container secret-volume-test: <nil>
STEP: delete the pod 08/26/23 06:14:05.045
Aug 26 06:14:05.065: INFO: Waiting for pod pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e to disappear
Aug 26 06:14:05.071: INFO: Pod pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 06:14:05.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7535" for this suite. 08/26/23 06:14:05.082
------------------------------
• [4.166 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:14:00.928
    Aug 26 06:14:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 06:14:00.929
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:14:00.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:14:00.968
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-e5bb0aad-5aad-469c-9b23-527e959400a5 08/26/23 06:14:00.97
    STEP: Creating a pod to test consume secrets 08/26/23 06:14:00.98
    Aug 26 06:14:00.993: INFO: Waiting up to 5m0s for pod "pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e" in namespace "secrets-7535" to be "Succeeded or Failed"
    Aug 26 06:14:01.000: INFO: Pod "pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.413906ms
    Aug 26 06:14:03.011: INFO: Pod "pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017583138s
    Aug 26 06:14:05.006: INFO: Pod "pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012423395s
    STEP: Saw pod success 08/26/23 06:14:05.006
    Aug 26 06:14:05.006: INFO: Pod "pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e" satisfied condition "Succeeded or Failed"
    Aug 26 06:14:05.011: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e container secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 06:14:05.045
    Aug 26 06:14:05.065: INFO: Waiting for pod pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e to disappear
    Aug 26 06:14:05.071: INFO: Pod pod-secrets-15708cc6-43dc-440d-881f-6772c36cb55e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:14:05.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7535" for this suite. 08/26/23 06:14:05.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:14:05.096
Aug 26 06:14:05.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename statefulset 08/26/23 06:14:05.097
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:14:05.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:14:05.126
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8975 08/26/23 06:14:05.135
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 08/26/23 06:14:05.144
Aug 26 06:14:05.166: INFO: Found 0 stateful pods, waiting for 3
Aug 26 06:14:15.181: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 06:14:15.182: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 06:14:15.182: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/26/23 06:14:15.197
Aug 26 06:14:15.226: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/26/23 06:14:15.226
STEP: Not applying an update when the partition is greater than the number of replicas 08/26/23 06:14:25.263
STEP: Performing a canary update 08/26/23 06:14:25.263
Aug 26 06:14:25.286: INFO: Updating stateful set ss2
Aug 26 06:14:25.311: INFO: Waiting for Pod statefulset-8975/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 08/26/23 06:14:35.331
Aug 26 06:14:35.406: INFO: Found 2 stateful pods, waiting for 3
Aug 26 06:14:45.432: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 06:14:45.432: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 06:14:45.432: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 08/26/23 06:14:45.446
Aug 26 06:14:45.473: INFO: Updating stateful set ss2
Aug 26 06:14:45.494: INFO: Waiting for Pod statefulset-8975/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 26 06:14:55.535: INFO: Updating stateful set ss2
Aug 26 06:14:55.545: INFO: Waiting for StatefulSet statefulset-8975/ss2 to complete update
Aug 26 06:14:55.545: INFO: Waiting for Pod statefulset-8975/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 26 06:15:05.580: INFO: Deleting all statefulset in ns statefulset-8975
Aug 26 06:15:05.585: INFO: Scaling statefulset ss2 to 0
Aug 26 06:15:15.617: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 06:15:15.621: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 26 06:15:15.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8975" for this suite. 08/26/23 06:15:15.65
------------------------------
• [SLOW TEST] [70.574 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:14:05.096
    Aug 26 06:14:05.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename statefulset 08/26/23 06:14:05.097
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:14:05.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:14:05.126
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8975 08/26/23 06:14:05.135
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 08/26/23 06:14:05.144
    Aug 26 06:14:05.166: INFO: Found 0 stateful pods, waiting for 3
    Aug 26 06:14:15.181: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 06:14:15.182: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 06:14:15.182: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/26/23 06:14:15.197
    Aug 26 06:14:15.226: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/26/23 06:14:15.226
    STEP: Not applying an update when the partition is greater than the number of replicas 08/26/23 06:14:25.263
    STEP: Performing a canary update 08/26/23 06:14:25.263
    Aug 26 06:14:25.286: INFO: Updating stateful set ss2
    Aug 26 06:14:25.311: INFO: Waiting for Pod statefulset-8975/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 08/26/23 06:14:35.331
    Aug 26 06:14:35.406: INFO: Found 2 stateful pods, waiting for 3
    Aug 26 06:14:45.432: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 06:14:45.432: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 26 06:14:45.432: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 08/26/23 06:14:45.446
    Aug 26 06:14:45.473: INFO: Updating stateful set ss2
    Aug 26 06:14:45.494: INFO: Waiting for Pod statefulset-8975/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 26 06:14:55.535: INFO: Updating stateful set ss2
    Aug 26 06:14:55.545: INFO: Waiting for StatefulSet statefulset-8975/ss2 to complete update
    Aug 26 06:14:55.545: INFO: Waiting for Pod statefulset-8975/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 26 06:15:05.580: INFO: Deleting all statefulset in ns statefulset-8975
    Aug 26 06:15:05.585: INFO: Scaling statefulset ss2 to 0
    Aug 26 06:15:15.617: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 06:15:15.621: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:15:15.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8975" for this suite. 08/26/23 06:15:15.65
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:15:15.671
Aug 26 06:15:15.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 06:15:15.673
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:15:15.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:15:15.7
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 06:15:15.734
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:15:16.396
STEP: Deploying the webhook pod 08/26/23 06:15:16.406
STEP: Wait for the deployment to be ready 08/26/23 06:15:16.421
Aug 26 06:15:16.430: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 06:15:18.443
STEP: Verifying the service has paired with the endpoint 08/26/23 06:15:18.458
Aug 26 06:15:19.459: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 08/26/23 06:15:19.464
STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/26/23 06:15:19.489
STEP: Creating a configMap that should not be mutated 08/26/23 06:15:19.497
STEP: Patching a mutating webhook configuration's rules to include the create operation 08/26/23 06:15:19.512
STEP: Creating a configMap that should be mutated 08/26/23 06:15:19.521
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:15:19.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-184" for this suite. 08/26/23 06:15:19.654
STEP: Destroying namespace "webhook-184-markers" for this suite. 08/26/23 06:15:19.664
------------------------------
• [4.004 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:15:15.671
    Aug 26 06:15:15.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 06:15:15.673
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:15:15.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:15:15.7
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 06:15:15.734
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:15:16.396
    STEP: Deploying the webhook pod 08/26/23 06:15:16.406
    STEP: Wait for the deployment to be ready 08/26/23 06:15:16.421
    Aug 26 06:15:16.430: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 06:15:18.443
    STEP: Verifying the service has paired with the endpoint 08/26/23 06:15:18.458
    Aug 26 06:15:19.459: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 08/26/23 06:15:19.464
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/26/23 06:15:19.489
    STEP: Creating a configMap that should not be mutated 08/26/23 06:15:19.497
    STEP: Patching a mutating webhook configuration's rules to include the create operation 08/26/23 06:15:19.512
    STEP: Creating a configMap that should be mutated 08/26/23 06:15:19.521
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:15:19.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-184" for this suite. 08/26/23 06:15:19.654
    STEP: Destroying namespace "webhook-184-markers" for this suite. 08/26/23 06:15:19.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:15:19.675
Aug 26 06:15:19.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 06:15:19.678
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:15:19.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:15:19.705
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/26/23 06:15:19.71
Aug 26 06:15:19.726: INFO: Waiting up to 5m0s for pod "pod-12de5b7b-d766-47a6-917c-790beeb84fd3" in namespace "emptydir-3728" to be "Succeeded or Failed"
Aug 26 06:15:19.733: INFO: Pod "pod-12de5b7b-d766-47a6-917c-790beeb84fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516099ms
Aug 26 06:15:21.738: INFO: Pod "pod-12de5b7b-d766-47a6-917c-790beeb84fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01278051s
Aug 26 06:15:23.738: INFO: Pod "pod-12de5b7b-d766-47a6-917c-790beeb84fd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012550233s
STEP: Saw pod success 08/26/23 06:15:23.738
Aug 26 06:15:23.738: INFO: Pod "pod-12de5b7b-d766-47a6-917c-790beeb84fd3" satisfied condition "Succeeded or Failed"
Aug 26 06:15:23.743: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-12de5b7b-d766-47a6-917c-790beeb84fd3 container test-container: <nil>
STEP: delete the pod 08/26/23 06:15:23.751
Aug 26 06:15:23.764: INFO: Waiting for pod pod-12de5b7b-d766-47a6-917c-790beeb84fd3 to disappear
Aug 26 06:15:23.768: INFO: Pod pod-12de5b7b-d766-47a6-917c-790beeb84fd3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 06:15:23.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3728" for this suite. 08/26/23 06:15:23.778
------------------------------
• [4.114 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:15:19.675
    Aug 26 06:15:19.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 06:15:19.678
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:15:19.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:15:19.705
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/26/23 06:15:19.71
    Aug 26 06:15:19.726: INFO: Waiting up to 5m0s for pod "pod-12de5b7b-d766-47a6-917c-790beeb84fd3" in namespace "emptydir-3728" to be "Succeeded or Failed"
    Aug 26 06:15:19.733: INFO: Pod "pod-12de5b7b-d766-47a6-917c-790beeb84fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516099ms
    Aug 26 06:15:21.738: INFO: Pod "pod-12de5b7b-d766-47a6-917c-790beeb84fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01278051s
    Aug 26 06:15:23.738: INFO: Pod "pod-12de5b7b-d766-47a6-917c-790beeb84fd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012550233s
    STEP: Saw pod success 08/26/23 06:15:23.738
    Aug 26 06:15:23.738: INFO: Pod "pod-12de5b7b-d766-47a6-917c-790beeb84fd3" satisfied condition "Succeeded or Failed"
    Aug 26 06:15:23.743: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-12de5b7b-d766-47a6-917c-790beeb84fd3 container test-container: <nil>
    STEP: delete the pod 08/26/23 06:15:23.751
    Aug 26 06:15:23.764: INFO: Waiting for pod pod-12de5b7b-d766-47a6-917c-790beeb84fd3 to disappear
    Aug 26 06:15:23.768: INFO: Pod pod-12de5b7b-d766-47a6-917c-790beeb84fd3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:15:23.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3728" for this suite. 08/26/23 06:15:23.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:15:23.79
Aug 26 06:15:23.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename runtimeclass 08/26/23 06:15:23.791
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:15:23.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:15:23.813
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 08/26/23 06:15:23.816
STEP: getting /apis/node.k8s.io 08/26/23 06:15:23.818
STEP: getting /apis/node.k8s.io/v1 08/26/23 06:15:23.819
STEP: creating 08/26/23 06:15:23.82
STEP: watching 08/26/23 06:15:23.844
Aug 26 06:15:23.844: INFO: starting watch
STEP: getting 08/26/23 06:15:23.856
STEP: listing 08/26/23 06:15:23.861
STEP: patching 08/26/23 06:15:23.867
STEP: updating 08/26/23 06:15:23.874
Aug 26 06:15:23.884: INFO: waiting for watch events with expected annotations
STEP: deleting 08/26/23 06:15:23.884
STEP: deleting a collection 08/26/23 06:15:23.899
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 26 06:15:23.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8954" for this suite. 08/26/23 06:15:23.935
------------------------------
• [0.156 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:15:23.79
    Aug 26 06:15:23.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename runtimeclass 08/26/23 06:15:23.791
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:15:23.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:15:23.813
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 08/26/23 06:15:23.816
    STEP: getting /apis/node.k8s.io 08/26/23 06:15:23.818
    STEP: getting /apis/node.k8s.io/v1 08/26/23 06:15:23.819
    STEP: creating 08/26/23 06:15:23.82
    STEP: watching 08/26/23 06:15:23.844
    Aug 26 06:15:23.844: INFO: starting watch
    STEP: getting 08/26/23 06:15:23.856
    STEP: listing 08/26/23 06:15:23.861
    STEP: patching 08/26/23 06:15:23.867
    STEP: updating 08/26/23 06:15:23.874
    Aug 26 06:15:23.884: INFO: waiting for watch events with expected annotations
    STEP: deleting 08/26/23 06:15:23.884
    STEP: deleting a collection 08/26/23 06:15:23.899
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:15:23.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8954" for this suite. 08/26/23 06:15:23.935
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:15:23.947
Aug 26 06:15:23.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-watch 08/26/23 06:15:23.948
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:15:23.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:15:23.977
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Aug 26 06:15:23.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Creating first CR  08/26/23 06:15:26.561
Aug 26 06:15:26.567: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:26Z]] name:name1 resourceVersion:34035 uid:579ee58d-ea61-4b02-9579-1f8718ac819a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 08/26/23 06:15:36.568
Aug 26 06:15:36.581: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:36Z]] name:name2 resourceVersion:34071 uid:53537e6b-0371-411a-9534-219b9a5c68c3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 08/26/23 06:15:46.582
Aug 26 06:15:46.591: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:46Z]] name:name1 resourceVersion:34098 uid:579ee58d-ea61-4b02-9579-1f8718ac819a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 08/26/23 06:15:56.592
Aug 26 06:15:56.607: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:56Z]] name:name2 resourceVersion:34123 uid:53537e6b-0371-411a-9534-219b9a5c68c3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 08/26/23 06:16:06.607
Aug 26 06:16:06.620: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:46Z]] name:name1 resourceVersion:34148 uid:579ee58d-ea61-4b02-9579-1f8718ac819a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 08/26/23 06:16:16.621
Aug 26 06:16:16.631: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:56Z]] name:name2 resourceVersion:34174 uid:53537e6b-0371-411a-9534-219b9a5c68c3] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:16:27.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-2541" for this suite. 08/26/23 06:16:27.157
------------------------------
• [SLOW TEST] [63.218 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:15:23.947
    Aug 26 06:15:23.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-watch 08/26/23 06:15:23.948
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:15:23.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:15:23.977
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Aug 26 06:15:23.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Creating first CR  08/26/23 06:15:26.561
    Aug 26 06:15:26.567: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:26Z]] name:name1 resourceVersion:34035 uid:579ee58d-ea61-4b02-9579-1f8718ac819a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 08/26/23 06:15:36.568
    Aug 26 06:15:36.581: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:36Z]] name:name2 resourceVersion:34071 uid:53537e6b-0371-411a-9534-219b9a5c68c3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 08/26/23 06:15:46.582
    Aug 26 06:15:46.591: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:46Z]] name:name1 resourceVersion:34098 uid:579ee58d-ea61-4b02-9579-1f8718ac819a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 08/26/23 06:15:56.592
    Aug 26 06:15:56.607: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:56Z]] name:name2 resourceVersion:34123 uid:53537e6b-0371-411a-9534-219b9a5c68c3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 08/26/23 06:16:06.607
    Aug 26 06:16:06.620: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:46Z]] name:name1 resourceVersion:34148 uid:579ee58d-ea61-4b02-9579-1f8718ac819a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 08/26/23 06:16:16.621
    Aug 26 06:16:16.631: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-26T06:15:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-26T06:15:56Z]] name:name2 resourceVersion:34174 uid:53537e6b-0371-411a-9534-219b9a5c68c3] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:16:27.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-2541" for this suite. 08/26/23 06:16:27.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:16:27.167
Aug 26 06:16:27.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename subpath 08/26/23 06:16:27.168
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:16:27.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:16:27.197
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/26/23 06:16:27.199
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-27tz 08/26/23 06:16:27.214
STEP: Creating a pod to test atomic-volume-subpath 08/26/23 06:16:27.214
Aug 26 06:16:27.256: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-27tz" in namespace "subpath-3658" to be "Succeeded or Failed"
Aug 26 06:16:27.261: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438252ms
Aug 26 06:16:29.266: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 2.010201s
Aug 26 06:16:31.268: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 4.012010951s
Aug 26 06:16:33.267: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 6.010854384s
Aug 26 06:16:35.266: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 8.010569329s
Aug 26 06:16:37.267: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 10.011241489s
Aug 26 06:16:39.268: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 12.011889815s
Aug 26 06:16:41.269: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 14.013086863s
Aug 26 06:16:43.267: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 16.011497366s
Aug 26 06:16:45.276: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 18.020141947s
Aug 26 06:16:47.268: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 20.012173517s
Aug 26 06:16:49.267: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=false. Elapsed: 22.01158168s
Aug 26 06:16:51.267: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011112339s
STEP: Saw pod success 08/26/23 06:16:51.267
Aug 26 06:16:51.267: INFO: Pod "pod-subpath-test-configmap-27tz" satisfied condition "Succeeded or Failed"
Aug 26 06:16:51.271: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-subpath-test-configmap-27tz container test-container-subpath-configmap-27tz: <nil>
STEP: delete the pod 08/26/23 06:16:51.281
Aug 26 06:16:51.300: INFO: Waiting for pod pod-subpath-test-configmap-27tz to disappear
Aug 26 06:16:51.303: INFO: Pod pod-subpath-test-configmap-27tz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-27tz 08/26/23 06:16:51.303
Aug 26 06:16:51.303: INFO: Deleting pod "pod-subpath-test-configmap-27tz" in namespace "subpath-3658"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 26 06:16:51.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3658" for this suite. 08/26/23 06:16:51.318
------------------------------
• [SLOW TEST] [24.164 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:16:27.167
    Aug 26 06:16:27.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename subpath 08/26/23 06:16:27.168
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:16:27.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:16:27.197
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/26/23 06:16:27.199
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-27tz 08/26/23 06:16:27.214
    STEP: Creating a pod to test atomic-volume-subpath 08/26/23 06:16:27.214
    Aug 26 06:16:27.256: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-27tz" in namespace "subpath-3658" to be "Succeeded or Failed"
    Aug 26 06:16:27.261: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438252ms
    Aug 26 06:16:29.266: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 2.010201s
    Aug 26 06:16:31.268: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 4.012010951s
    Aug 26 06:16:33.267: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 6.010854384s
    Aug 26 06:16:35.266: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 8.010569329s
    Aug 26 06:16:37.267: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 10.011241489s
    Aug 26 06:16:39.268: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 12.011889815s
    Aug 26 06:16:41.269: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 14.013086863s
    Aug 26 06:16:43.267: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 16.011497366s
    Aug 26 06:16:45.276: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 18.020141947s
    Aug 26 06:16:47.268: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=true. Elapsed: 20.012173517s
    Aug 26 06:16:49.267: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Running", Reason="", readiness=false. Elapsed: 22.01158168s
    Aug 26 06:16:51.267: INFO: Pod "pod-subpath-test-configmap-27tz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011112339s
    STEP: Saw pod success 08/26/23 06:16:51.267
    Aug 26 06:16:51.267: INFO: Pod "pod-subpath-test-configmap-27tz" satisfied condition "Succeeded or Failed"
    Aug 26 06:16:51.271: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-subpath-test-configmap-27tz container test-container-subpath-configmap-27tz: <nil>
    STEP: delete the pod 08/26/23 06:16:51.281
    Aug 26 06:16:51.300: INFO: Waiting for pod pod-subpath-test-configmap-27tz to disappear
    Aug 26 06:16:51.303: INFO: Pod pod-subpath-test-configmap-27tz no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-27tz 08/26/23 06:16:51.303
    Aug 26 06:16:51.303: INFO: Deleting pod "pod-subpath-test-configmap-27tz" in namespace "subpath-3658"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:16:51.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3658" for this suite. 08/26/23 06:16:51.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:16:51.333
Aug 26 06:16:51.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:16:51.334
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:16:51.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:16:51.367
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/26/23 06:16:51.37
Aug 26 06:16:51.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/26/23 06:16:58.996
Aug 26 06:16:58.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:17:01.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:17:09.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8248" for this suite. 08/26/23 06:17:09.529
------------------------------
• [SLOW TEST] [18.203 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:16:51.333
    Aug 26 06:16:51.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:16:51.334
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:16:51.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:16:51.367
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/26/23 06:16:51.37
    Aug 26 06:16:51.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/26/23 06:16:58.996
    Aug 26 06:16:58.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:17:01.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:17:09.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8248" for this suite. 08/26/23 06:17:09.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:17:09.536
Aug 26 06:17:09.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 06:17:09.537
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:17:09.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:17:09.562
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 08/26/23 06:17:09.565
Aug 26 06:17:09.577: INFO: created test-pod-1
Aug 26 06:17:09.582: INFO: created test-pod-2
Aug 26 06:17:09.604: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 08/26/23 06:17:09.604
Aug 26 06:17:09.605: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9748' to be running and ready
Aug 26 06:17:09.629: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 26 06:17:09.629: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 26 06:17:09.629: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 26 06:17:09.629: INFO: 0 / 3 pods in namespace 'pods-9748' are running and ready (0 seconds elapsed)
Aug 26 06:17:09.629: INFO: expected 0 pod replicas in namespace 'pods-9748', 0 are Running and Ready.
Aug 26 06:17:09.629: INFO: POD         NODE                                     PHASE    GRACE  CONDITIONS
Aug 26 06:17:09.629: INFO: test-pod-1  ip-10-0-1-31.us-west-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC  }]
Aug 26 06:17:09.629: INFO: test-pod-2  ip-10-0-1-31.us-west-2.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC  }]
Aug 26 06:17:09.629: INFO: test-pod-3  ip-10-0-1-5.us-west-2.compute.internal   Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC  }]
Aug 26 06:17:09.629: INFO: 
Aug 26 06:17:11.640: INFO: 3 / 3 pods in namespace 'pods-9748' are running and ready (2 seconds elapsed)
Aug 26 06:17:11.640: INFO: expected 0 pod replicas in namespace 'pods-9748', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 08/26/23 06:17:11.672
Aug 26 06:17:11.675: INFO: Pod quantity 3 is different from expected quantity 0
Aug 26 06:17:12.680: INFO: Pod quantity 3 is different from expected quantity 0
Aug 26 06:17:13.681: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 26 06:17:14.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9748" for this suite. 08/26/23 06:17:14.685
------------------------------
• [SLOW TEST] [5.156 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:17:09.536
    Aug 26 06:17:09.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 06:17:09.537
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:17:09.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:17:09.562
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 08/26/23 06:17:09.565
    Aug 26 06:17:09.577: INFO: created test-pod-1
    Aug 26 06:17:09.582: INFO: created test-pod-2
    Aug 26 06:17:09.604: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 08/26/23 06:17:09.604
    Aug 26 06:17:09.605: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9748' to be running and ready
    Aug 26 06:17:09.629: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 26 06:17:09.629: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 26 06:17:09.629: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 26 06:17:09.629: INFO: 0 / 3 pods in namespace 'pods-9748' are running and ready (0 seconds elapsed)
    Aug 26 06:17:09.629: INFO: expected 0 pod replicas in namespace 'pods-9748', 0 are Running and Ready.
    Aug 26 06:17:09.629: INFO: POD         NODE                                     PHASE    GRACE  CONDITIONS
    Aug 26 06:17:09.629: INFO: test-pod-1  ip-10-0-1-31.us-west-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC  }]
    Aug 26 06:17:09.629: INFO: test-pod-2  ip-10-0-1-31.us-west-2.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC  }]
    Aug 26 06:17:09.629: INFO: test-pod-3  ip-10-0-1-5.us-west-2.compute.internal   Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-26 06:17:09 +0000 UTC  }]
    Aug 26 06:17:09.629: INFO: 
    Aug 26 06:17:11.640: INFO: 3 / 3 pods in namespace 'pods-9748' are running and ready (2 seconds elapsed)
    Aug 26 06:17:11.640: INFO: expected 0 pod replicas in namespace 'pods-9748', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 08/26/23 06:17:11.672
    Aug 26 06:17:11.675: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 26 06:17:12.680: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 26 06:17:13.681: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:17:14.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9748" for this suite. 08/26/23 06:17:14.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:17:14.694
Aug 26 06:17:14.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 06:17:14.695
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:17:14.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:17:14.722
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-4421/configmap-test-6075ae20-0ef9-43de-b578-8066b7c1a4a6 08/26/23 06:17:14.727
STEP: Creating a pod to test consume configMaps 08/26/23 06:17:14.733
Aug 26 06:17:14.743: INFO: Waiting up to 5m0s for pod "pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2" in namespace "configmap-4421" to be "Succeeded or Failed"
Aug 26 06:17:14.748: INFO: Pod "pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.272437ms
Aug 26 06:17:16.752: INFO: Pod "pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009480872s
Aug 26 06:17:18.757: INFO: Pod "pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013659494s
STEP: Saw pod success 08/26/23 06:17:18.757
Aug 26 06:17:18.757: INFO: Pod "pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2" satisfied condition "Succeeded or Failed"
Aug 26 06:17:18.760: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2 container env-test: <nil>
STEP: delete the pod 08/26/23 06:17:18.783
Aug 26 06:17:18.795: INFO: Waiting for pod pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2 to disappear
Aug 26 06:17:18.798: INFO: Pod pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:17:18.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4421" for this suite. 08/26/23 06:17:18.81
------------------------------
• [4.124 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:17:14.694
    Aug 26 06:17:14.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 06:17:14.695
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:17:14.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:17:14.722
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-4421/configmap-test-6075ae20-0ef9-43de-b578-8066b7c1a4a6 08/26/23 06:17:14.727
    STEP: Creating a pod to test consume configMaps 08/26/23 06:17:14.733
    Aug 26 06:17:14.743: INFO: Waiting up to 5m0s for pod "pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2" in namespace "configmap-4421" to be "Succeeded or Failed"
    Aug 26 06:17:14.748: INFO: Pod "pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.272437ms
    Aug 26 06:17:16.752: INFO: Pod "pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009480872s
    Aug 26 06:17:18.757: INFO: Pod "pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013659494s
    STEP: Saw pod success 08/26/23 06:17:18.757
    Aug 26 06:17:18.757: INFO: Pod "pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2" satisfied condition "Succeeded or Failed"
    Aug 26 06:17:18.760: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2 container env-test: <nil>
    STEP: delete the pod 08/26/23 06:17:18.783
    Aug 26 06:17:18.795: INFO: Waiting for pod pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2 to disappear
    Aug 26 06:17:18.798: INFO: Pod pod-configmaps-9493cc39-25d6-4fa3-bd27-b93e06603ea2 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:17:18.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4421" for this suite. 08/26/23 06:17:18.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:17:18.819
Aug 26 06:17:18.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename cronjob 08/26/23 06:17:18.819
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:17:18.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:17:18.838
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 08/26/23 06:17:18.843
STEP: Ensuring a job is scheduled 08/26/23 06:17:18.85
STEP: Ensuring exactly one is scheduled 08/26/23 06:18:00.854
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/26/23 06:18:00.858
STEP: Ensuring the job is replaced with a new one 08/26/23 06:18:00.861
STEP: Removing cronjob 08/26/23 06:19:00.866
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 26 06:19:00.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9732" for this suite. 08/26/23 06:19:00.888
------------------------------
• [SLOW TEST] [102.076 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:17:18.819
    Aug 26 06:17:18.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename cronjob 08/26/23 06:17:18.819
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:17:18.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:17:18.838
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 08/26/23 06:17:18.843
    STEP: Ensuring a job is scheduled 08/26/23 06:17:18.85
    STEP: Ensuring exactly one is scheduled 08/26/23 06:18:00.854
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/26/23 06:18:00.858
    STEP: Ensuring the job is replaced with a new one 08/26/23 06:18:00.861
    STEP: Removing cronjob 08/26/23 06:19:00.866
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:19:00.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9732" for this suite. 08/26/23 06:19:00.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:19:00.896
Aug 26 06:19:00.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sched-preemption 08/26/23 06:19:00.899
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:19:00.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:19:00.921
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 26 06:19:00.940: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 26 06:20:00.995: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:20:00.998
Aug 26 06:20:00.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sched-preemption-path 08/26/23 06:20:00.999
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:01.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:01.021
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Aug 26 06:20:01.039: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Aug 26 06:20:01.043: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Aug 26 06:20:01.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:20:01.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8075" for this suite. 08/26/23 06:20:01.177
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8894" for this suite. 08/26/23 06:20:01.186
------------------------------
• [SLOW TEST] [60.296 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:19:00.896
    Aug 26 06:19:00.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sched-preemption 08/26/23 06:19:00.899
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:19:00.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:19:00.921
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 26 06:19:00.940: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 26 06:20:00.995: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:20:00.998
    Aug 26 06:20:00.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sched-preemption-path 08/26/23 06:20:00.999
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:01.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:01.021
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Aug 26 06:20:01.039: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Aug 26 06:20:01.043: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:20:01.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:20:01.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8075" for this suite. 08/26/23 06:20:01.177
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8894" for this suite. 08/26/23 06:20:01.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:20:01.193
Aug 26 06:20:01.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 06:20:01.196
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:01.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:01.217
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/26/23 06:20:01.222
Aug 26 06:20:01.237: INFO: Waiting up to 5m0s for pod "pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed" in namespace "emptydir-4384" to be "Succeeded or Failed"
Aug 26 06:20:01.243: INFO: Pod "pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154851ms
Aug 26 06:20:03.250: INFO: Pod "pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012676778s
Aug 26 06:20:05.251: INFO: Pod "pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013400537s
STEP: Saw pod success 08/26/23 06:20:05.251
Aug 26 06:20:05.251: INFO: Pod "pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed" satisfied condition "Succeeded or Failed"
Aug 26 06:20:05.256: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed container test-container: <nil>
STEP: delete the pod 08/26/23 06:20:05.275
Aug 26 06:20:05.285: INFO: Waiting for pod pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed to disappear
Aug 26 06:20:05.288: INFO: Pod pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 06:20:05.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4384" for this suite. 08/26/23 06:20:05.295
------------------------------
• [4.107 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:20:01.193
    Aug 26 06:20:01.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 06:20:01.196
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:01.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:01.217
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/26/23 06:20:01.222
    Aug 26 06:20:01.237: INFO: Waiting up to 5m0s for pod "pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed" in namespace "emptydir-4384" to be "Succeeded or Failed"
    Aug 26 06:20:01.243: INFO: Pod "pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154851ms
    Aug 26 06:20:03.250: INFO: Pod "pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012676778s
    Aug 26 06:20:05.251: INFO: Pod "pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013400537s
    STEP: Saw pod success 08/26/23 06:20:05.251
    Aug 26 06:20:05.251: INFO: Pod "pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed" satisfied condition "Succeeded or Failed"
    Aug 26 06:20:05.256: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed container test-container: <nil>
    STEP: delete the pod 08/26/23 06:20:05.275
    Aug 26 06:20:05.285: INFO: Waiting for pod pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed to disappear
    Aug 26 06:20:05.288: INFO: Pod pod-cd29ee5d-8508-4486-b435-bf7bf0ca09ed no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:20:05.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4384" for this suite. 08/26/23 06:20:05.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:20:05.301
Aug 26 06:20:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename var-expansion 08/26/23 06:20:05.303
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:05.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:05.326
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 08/26/23 06:20:05.33
Aug 26 06:20:05.339: INFO: Waiting up to 5m0s for pod "var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6" in namespace "var-expansion-6842" to be "Succeeded or Failed"
Aug 26 06:20:05.343: INFO: Pod "var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.386891ms
Aug 26 06:20:07.348: INFO: Pod "var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009457661s
Aug 26 06:20:09.348: INFO: Pod "var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009174252s
STEP: Saw pod success 08/26/23 06:20:09.348
Aug 26 06:20:09.348: INFO: Pod "var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6" satisfied condition "Succeeded or Failed"
Aug 26 06:20:09.353: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6 container dapi-container: <nil>
STEP: delete the pod 08/26/23 06:20:09.363
Aug 26 06:20:09.378: INFO: Waiting for pod var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6 to disappear
Aug 26 06:20:09.380: INFO: Pod var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 26 06:20:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6842" for this suite. 08/26/23 06:20:09.387
------------------------------
• [4.090 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:20:05.301
    Aug 26 06:20:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename var-expansion 08/26/23 06:20:05.303
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:05.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:05.326
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 08/26/23 06:20:05.33
    Aug 26 06:20:05.339: INFO: Waiting up to 5m0s for pod "var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6" in namespace "var-expansion-6842" to be "Succeeded or Failed"
    Aug 26 06:20:05.343: INFO: Pod "var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.386891ms
    Aug 26 06:20:07.348: INFO: Pod "var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009457661s
    Aug 26 06:20:09.348: INFO: Pod "var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009174252s
    STEP: Saw pod success 08/26/23 06:20:09.348
    Aug 26 06:20:09.348: INFO: Pod "var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6" satisfied condition "Succeeded or Failed"
    Aug 26 06:20:09.353: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6 container dapi-container: <nil>
    STEP: delete the pod 08/26/23 06:20:09.363
    Aug 26 06:20:09.378: INFO: Waiting for pod var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6 to disappear
    Aug 26 06:20:09.380: INFO: Pod var-expansion-46e2d256-352f-42b9-989a-e6c3734a4eb6 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:20:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6842" for this suite. 08/26/23 06:20:09.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:20:09.393
Aug 26 06:20:09.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename subpath 08/26/23 06:20:09.394
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:09.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:09.411
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/26/23 06:20:09.414
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-mzn7 08/26/23 06:20:09.429
STEP: Creating a pod to test atomic-volume-subpath 08/26/23 06:20:09.429
Aug 26 06:20:09.442: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mzn7" in namespace "subpath-1202" to be "Succeeded or Failed"
Aug 26 06:20:09.450: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.684291ms
Aug 26 06:20:11.462: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 2.020455129s
Aug 26 06:20:13.454: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 4.012366837s
Aug 26 06:20:15.455: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 6.013505828s
Aug 26 06:20:17.454: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 8.012083105s
Aug 26 06:20:19.455: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 10.012787338s
Aug 26 06:20:21.454: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 12.012278069s
Aug 26 06:20:23.455: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 14.013061417s
Aug 26 06:20:25.456: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 16.013881074s
Aug 26 06:20:27.458: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 18.016003932s
Aug 26 06:20:29.456: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 20.013903137s
Aug 26 06:20:31.457: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=false. Elapsed: 22.014583146s
Aug 26 06:20:33.458: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016272681s
STEP: Saw pod success 08/26/23 06:20:33.458
Aug 26 06:20:33.458: INFO: Pod "pod-subpath-test-downwardapi-mzn7" satisfied condition "Succeeded or Failed"
Aug 26 06:20:33.464: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-subpath-test-downwardapi-mzn7 container test-container-subpath-downwardapi-mzn7: <nil>
STEP: delete the pod 08/26/23 06:20:33.471
Aug 26 06:20:33.490: INFO: Waiting for pod pod-subpath-test-downwardapi-mzn7 to disappear
Aug 26 06:20:33.494: INFO: Pod pod-subpath-test-downwardapi-mzn7 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mzn7 08/26/23 06:20:33.494
Aug 26 06:20:33.494: INFO: Deleting pod "pod-subpath-test-downwardapi-mzn7" in namespace "subpath-1202"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 26 06:20:33.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1202" for this suite. 08/26/23 06:20:33.51
------------------------------
• [SLOW TEST] [24.125 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:20:09.393
    Aug 26 06:20:09.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename subpath 08/26/23 06:20:09.394
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:09.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:09.411
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/26/23 06:20:09.414
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-mzn7 08/26/23 06:20:09.429
    STEP: Creating a pod to test atomic-volume-subpath 08/26/23 06:20:09.429
    Aug 26 06:20:09.442: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mzn7" in namespace "subpath-1202" to be "Succeeded or Failed"
    Aug 26 06:20:09.450: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.684291ms
    Aug 26 06:20:11.462: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 2.020455129s
    Aug 26 06:20:13.454: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 4.012366837s
    Aug 26 06:20:15.455: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 6.013505828s
    Aug 26 06:20:17.454: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 8.012083105s
    Aug 26 06:20:19.455: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 10.012787338s
    Aug 26 06:20:21.454: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 12.012278069s
    Aug 26 06:20:23.455: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 14.013061417s
    Aug 26 06:20:25.456: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 16.013881074s
    Aug 26 06:20:27.458: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 18.016003932s
    Aug 26 06:20:29.456: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=true. Elapsed: 20.013903137s
    Aug 26 06:20:31.457: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Running", Reason="", readiness=false. Elapsed: 22.014583146s
    Aug 26 06:20:33.458: INFO: Pod "pod-subpath-test-downwardapi-mzn7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016272681s
    STEP: Saw pod success 08/26/23 06:20:33.458
    Aug 26 06:20:33.458: INFO: Pod "pod-subpath-test-downwardapi-mzn7" satisfied condition "Succeeded or Failed"
    Aug 26 06:20:33.464: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-subpath-test-downwardapi-mzn7 container test-container-subpath-downwardapi-mzn7: <nil>
    STEP: delete the pod 08/26/23 06:20:33.471
    Aug 26 06:20:33.490: INFO: Waiting for pod pod-subpath-test-downwardapi-mzn7 to disappear
    Aug 26 06:20:33.494: INFO: Pod pod-subpath-test-downwardapi-mzn7 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-mzn7 08/26/23 06:20:33.494
    Aug 26 06:20:33.494: INFO: Deleting pod "pod-subpath-test-downwardapi-mzn7" in namespace "subpath-1202"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:20:33.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1202" for this suite. 08/26/23 06:20:33.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:20:33.52
Aug 26 06:20:33.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename containers 08/26/23 06:20:33.523
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:33.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:33.549
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 08/26/23 06:20:33.553
Aug 26 06:20:33.564: INFO: Waiting up to 5m0s for pod "client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a" in namespace "containers-6389" to be "Succeeded or Failed"
Aug 26 06:20:33.569: INFO: Pod "client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.491854ms
Aug 26 06:20:35.575: INFO: Pod "client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010898348s
Aug 26 06:20:37.575: INFO: Pod "client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010831672s
STEP: Saw pod success 08/26/23 06:20:37.575
Aug 26 06:20:37.575: INFO: Pod "client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a" satisfied condition "Succeeded or Failed"
Aug 26 06:20:37.578: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a container agnhost-container: <nil>
STEP: delete the pod 08/26/23 06:20:37.585
Aug 26 06:20:37.598: INFO: Waiting for pod client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a to disappear
Aug 26 06:20:37.601: INFO: Pod client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 26 06:20:37.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6389" for this suite. 08/26/23 06:20:37.609
------------------------------
• [4.098 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:20:33.52
    Aug 26 06:20:33.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename containers 08/26/23 06:20:33.523
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:33.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:33.549
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 08/26/23 06:20:33.553
    Aug 26 06:20:33.564: INFO: Waiting up to 5m0s for pod "client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a" in namespace "containers-6389" to be "Succeeded or Failed"
    Aug 26 06:20:33.569: INFO: Pod "client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.491854ms
    Aug 26 06:20:35.575: INFO: Pod "client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010898348s
    Aug 26 06:20:37.575: INFO: Pod "client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010831672s
    STEP: Saw pod success 08/26/23 06:20:37.575
    Aug 26 06:20:37.575: INFO: Pod "client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a" satisfied condition "Succeeded or Failed"
    Aug 26 06:20:37.578: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 06:20:37.585
    Aug 26 06:20:37.598: INFO: Waiting for pod client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a to disappear
    Aug 26 06:20:37.601: INFO: Pod client-containers-ed11eac9-ec45-4ac5-8a14-adebb9f8af5a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:20:37.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6389" for this suite. 08/26/23 06:20:37.609
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:20:37.618
Aug 26 06:20:37.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename job 08/26/23 06:20:37.619
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:37.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:37.636
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 08/26/23 06:20:37.638
STEP: Ensuring job reaches completions 08/26/23 06:20:37.646
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 26 06:20:49.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8033" for this suite. 08/26/23 06:20:49.657
------------------------------
• [SLOW TEST] [12.047 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:20:37.618
    Aug 26 06:20:37.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename job 08/26/23 06:20:37.619
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:37.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:37.636
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 08/26/23 06:20:37.638
    STEP: Ensuring job reaches completions 08/26/23 06:20:37.646
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:20:49.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8033" for this suite. 08/26/23 06:20:49.657
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:20:49.666
Aug 26 06:20:49.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 06:20:49.668
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:49.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:49.694
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 08/26/23 06:20:49.698
Aug 26 06:20:49.709: INFO: Waiting up to 5m0s for pod "pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123" in namespace "emptydir-7902" to be "Succeeded or Failed"
Aug 26 06:20:49.713: INFO: Pod "pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123": Phase="Pending", Reason="", readiness=false. Elapsed: 4.238851ms
Aug 26 06:20:51.717: INFO: Pod "pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008793897s
Aug 26 06:20:53.718: INFO: Pod "pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009710838s
STEP: Saw pod success 08/26/23 06:20:53.718
Aug 26 06:20:53.718: INFO: Pod "pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123" satisfied condition "Succeeded or Failed"
Aug 26 06:20:53.722: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123 container test-container: <nil>
STEP: delete the pod 08/26/23 06:20:53.731
Aug 26 06:20:53.746: INFO: Waiting for pod pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123 to disappear
Aug 26 06:20:53.749: INFO: Pod pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 06:20:53.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7902" for this suite. 08/26/23 06:20:53.759
------------------------------
• [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:20:49.666
    Aug 26 06:20:49.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 06:20:49.668
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:49.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:49.694
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/26/23 06:20:49.698
    Aug 26 06:20:49.709: INFO: Waiting up to 5m0s for pod "pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123" in namespace "emptydir-7902" to be "Succeeded or Failed"
    Aug 26 06:20:49.713: INFO: Pod "pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123": Phase="Pending", Reason="", readiness=false. Elapsed: 4.238851ms
    Aug 26 06:20:51.717: INFO: Pod "pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008793897s
    Aug 26 06:20:53.718: INFO: Pod "pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009710838s
    STEP: Saw pod success 08/26/23 06:20:53.718
    Aug 26 06:20:53.718: INFO: Pod "pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123" satisfied condition "Succeeded or Failed"
    Aug 26 06:20:53.722: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123 container test-container: <nil>
    STEP: delete the pod 08/26/23 06:20:53.731
    Aug 26 06:20:53.746: INFO: Waiting for pod pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123 to disappear
    Aug 26 06:20:53.749: INFO: Pod pod-5b0bb2ed-9b84-4ad2-b406-3c113bd92123 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:20:53.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7902" for this suite. 08/26/23 06:20:53.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:20:53.766
Aug 26 06:20:53.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:20:53.767
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:53.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:53.781
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-d29bf468-f443-4810-a354-b4a297925188 08/26/23 06:20:53.784
STEP: Creating a pod to test consume secrets 08/26/23 06:20:53.789
Aug 26 06:20:53.798: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69" in namespace "projected-2402" to be "Succeeded or Failed"
Aug 26 06:20:53.800: INFO: Pod "pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611105ms
Aug 26 06:20:55.805: INFO: Pod "pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007676651s
Aug 26 06:20:57.806: INFO: Pod "pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008079507s
STEP: Saw pod success 08/26/23 06:20:57.806
Aug 26 06:20:57.806: INFO: Pod "pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69" satisfied condition "Succeeded or Failed"
Aug 26 06:20:57.810: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/26/23 06:20:57.824
Aug 26 06:20:57.839: INFO: Waiting for pod pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69 to disappear
Aug 26 06:20:57.845: INFO: Pod pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 26 06:20:57.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2402" for this suite. 08/26/23 06:20:57.856
------------------------------
• [4.098 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:20:53.766
    Aug 26 06:20:53.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:20:53.767
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:53.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:53.781
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-d29bf468-f443-4810-a354-b4a297925188 08/26/23 06:20:53.784
    STEP: Creating a pod to test consume secrets 08/26/23 06:20:53.789
    Aug 26 06:20:53.798: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69" in namespace "projected-2402" to be "Succeeded or Failed"
    Aug 26 06:20:53.800: INFO: Pod "pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611105ms
    Aug 26 06:20:55.805: INFO: Pod "pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007676651s
    Aug 26 06:20:57.806: INFO: Pod "pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008079507s
    STEP: Saw pod success 08/26/23 06:20:57.806
    Aug 26 06:20:57.806: INFO: Pod "pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69" satisfied condition "Succeeded or Failed"
    Aug 26 06:20:57.810: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 06:20:57.824
    Aug 26 06:20:57.839: INFO: Waiting for pod pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69 to disappear
    Aug 26 06:20:57.845: INFO: Pod pod-projected-secrets-51434653-49b4-4527-a0dd-0091a22cfd69 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:20:57.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2402" for this suite. 08/26/23 06:20:57.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:20:57.866
Aug 26 06:20:57.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pod-network-test 08/26/23 06:20:57.867
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:57.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:57.886
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2905 08/26/23 06:20:57.889
STEP: creating a selector 08/26/23 06:20:57.889
STEP: Creating the service pods in kubernetes 08/26/23 06:20:57.889
Aug 26 06:20:57.889: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 26 06:20:57.960: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2905" to be "running and ready"
Aug 26 06:20:57.969: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.893802ms
Aug 26 06:20:57.969: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:20:59.974: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013533304s
Aug 26 06:20:59.974: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 06:21:01.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013026065s
Aug 26 06:21:01.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 06:21:03.975: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014684019s
Aug 26 06:21:03.975: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 06:21:05.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013200033s
Aug 26 06:21:05.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 06:21:07.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013282604s
Aug 26 06:21:07.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 26 06:21:09.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.013300847s
Aug 26 06:21:09.974: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 26 06:21:09.974: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 26 06:21:09.977: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2905" to be "running and ready"
Aug 26 06:21:09.980: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.507568ms
Aug 26 06:21:09.980: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 26 06:21:09.980: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 26 06:21:09.984: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2905" to be "running and ready"
Aug 26 06:21:09.987: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.166607ms
Aug 26 06:21:09.987: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 26 06:21:09.987: INFO: Pod "netserver-2" satisfied condition "running and ready"
Aug 26 06:21:09.990: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2905" to be "running and ready"
Aug 26 06:21:09.993: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.066295ms
Aug 26 06:21:09.993: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Aug 26 06:21:09.993: INFO: Pod "netserver-3" satisfied condition "running and ready"
Aug 26 06:21:09.997: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2905" to be "running and ready"
Aug 26 06:21:10.001: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.2221ms
Aug 26 06:21:10.001: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Aug 26 06:21:10.001: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 08/26/23 06:21:10.007
Aug 26 06:21:10.027: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2905" to be "running"
Aug 26 06:21:10.032: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.482856ms
Aug 26 06:21:12.037: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010367809s
Aug 26 06:21:12.037: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 26 06:21:12.041: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2905" to be "running"
Aug 26 06:21:12.044: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.046983ms
Aug 26 06:21:12.044: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 26 06:21:12.050: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Aug 26 06:21:12.050: INFO: Going to poll 10.20.50.250 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Aug 26 06:21:12.054: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.50.250:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2905 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:21:12.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:21:12.055: INFO: ExecWithOptions: Clientset creation
Aug 26 06:21:12.055: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.50.250%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 06:21:12.137: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 26 06:21:12.137: INFO: Going to poll 10.20.193.201 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Aug 26 06:21:12.143: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.193.201:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2905 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:21:12.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:21:12.143: INFO: ExecWithOptions: Clientset creation
Aug 26 06:21:12.143: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.193.201%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 06:21:12.246: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 26 06:21:12.246: INFO: Going to poll 10.20.62.133 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Aug 26 06:21:12.251: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.62.133:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2905 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:21:12.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:21:12.252: INFO: ExecWithOptions: Clientset creation
Aug 26 06:21:12.252: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.62.133%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 06:21:12.354: INFO: Found all 1 expected endpoints: [netserver-2]
Aug 26 06:21:12.354: INFO: Going to poll 10.20.8.198 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Aug 26 06:21:12.358: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.8.198:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2905 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:21:12.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:21:12.359: INFO: ExecWithOptions: Clientset creation
Aug 26 06:21:12.359: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.8.198%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 06:21:12.441: INFO: Found all 1 expected endpoints: [netserver-3]
Aug 26 06:21:12.441: INFO: Going to poll 10.20.199.69 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Aug 26 06:21:12.444: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.199.69:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2905 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:21:12.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:21:12.445: INFO: ExecWithOptions: Clientset creation
Aug 26 06:21:12.445: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.199.69%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 06:21:12.533: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 26 06:21:12.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2905" for this suite. 08/26/23 06:21:12.54
------------------------------
• [SLOW TEST] [14.681 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:20:57.866
    Aug 26 06:20:57.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pod-network-test 08/26/23 06:20:57.867
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:20:57.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:20:57.886
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2905 08/26/23 06:20:57.889
    STEP: creating a selector 08/26/23 06:20:57.889
    STEP: Creating the service pods in kubernetes 08/26/23 06:20:57.889
    Aug 26 06:20:57.889: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 26 06:20:57.960: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2905" to be "running and ready"
    Aug 26 06:20:57.969: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.893802ms
    Aug 26 06:20:57.969: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:20:59.974: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013533304s
    Aug 26 06:20:59.974: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 06:21:01.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013026065s
    Aug 26 06:21:01.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 06:21:03.975: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014684019s
    Aug 26 06:21:03.975: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 06:21:05.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013200033s
    Aug 26 06:21:05.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 06:21:07.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013282604s
    Aug 26 06:21:07.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 26 06:21:09.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.013300847s
    Aug 26 06:21:09.974: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 26 06:21:09.974: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 26 06:21:09.977: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2905" to be "running and ready"
    Aug 26 06:21:09.980: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.507568ms
    Aug 26 06:21:09.980: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 26 06:21:09.980: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 26 06:21:09.984: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2905" to be "running and ready"
    Aug 26 06:21:09.987: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.166607ms
    Aug 26 06:21:09.987: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 26 06:21:09.987: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Aug 26 06:21:09.990: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2905" to be "running and ready"
    Aug 26 06:21:09.993: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.066295ms
    Aug 26 06:21:09.993: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Aug 26 06:21:09.993: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Aug 26 06:21:09.997: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2905" to be "running and ready"
    Aug 26 06:21:10.001: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.2221ms
    Aug 26 06:21:10.001: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Aug 26 06:21:10.001: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 08/26/23 06:21:10.007
    Aug 26 06:21:10.027: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2905" to be "running"
    Aug 26 06:21:10.032: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.482856ms
    Aug 26 06:21:12.037: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010367809s
    Aug 26 06:21:12.037: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 26 06:21:12.041: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2905" to be "running"
    Aug 26 06:21:12.044: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.046983ms
    Aug 26 06:21:12.044: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 26 06:21:12.050: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Aug 26 06:21:12.050: INFO: Going to poll 10.20.50.250 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Aug 26 06:21:12.054: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.50.250:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2905 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:21:12.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:21:12.055: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:21:12.055: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.50.250%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 06:21:12.137: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 26 06:21:12.137: INFO: Going to poll 10.20.193.201 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Aug 26 06:21:12.143: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.193.201:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2905 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:21:12.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:21:12.143: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:21:12.143: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.193.201%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 06:21:12.246: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 26 06:21:12.246: INFO: Going to poll 10.20.62.133 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Aug 26 06:21:12.251: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.62.133:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2905 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:21:12.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:21:12.252: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:21:12.252: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.62.133%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 06:21:12.354: INFO: Found all 1 expected endpoints: [netserver-2]
    Aug 26 06:21:12.354: INFO: Going to poll 10.20.8.198 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Aug 26 06:21:12.358: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.8.198:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2905 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:21:12.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:21:12.359: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:21:12.359: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.8.198%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 06:21:12.441: INFO: Found all 1 expected endpoints: [netserver-3]
    Aug 26 06:21:12.441: INFO: Going to poll 10.20.199.69 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Aug 26 06:21:12.444: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.199.69:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2905 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:21:12.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:21:12.445: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:21:12.445: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.20.199.69%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 06:21:12.533: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:21:12.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2905" for this suite. 08/26/23 06:21:12.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:21:12.554
Aug 26 06:21:12.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 06:21:12.554
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:21:12.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:21:12.575
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-4634 08/26/23 06:21:12.578
STEP: creating service affinity-nodeport-transition in namespace services-4634 08/26/23 06:21:12.578
STEP: creating replication controller affinity-nodeport-transition in namespace services-4634 08/26/23 06:21:12.593
I0826 06:21:12.602161      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4634, replica count: 3
I0826 06:21:15.653672      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 26 06:21:15.683: INFO: Creating new exec pod
Aug 26 06:21:15.691: INFO: Waiting up to 5m0s for pod "execpod-affinity7dbqd" in namespace "services-4634" to be "running"
Aug 26 06:21:15.704: INFO: Pod "execpod-affinity7dbqd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.424767ms
Aug 26 06:21:17.708: INFO: Pod "execpod-affinity7dbqd": Phase="Running", Reason="", readiness=true. Elapsed: 2.016987293s
Aug 26 06:21:17.708: INFO: Pod "execpod-affinity7dbqd" satisfied condition "running"
Aug 26 06:21:18.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Aug 26 06:21:18.906: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 26 06:21:18.906: INFO: stdout: ""
Aug 26 06:21:18.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c nc -v -z -w 2 10.21.131.216 80'
Aug 26 06:21:19.081: INFO: stderr: "+ nc -v -z -w 2 10.21.131.216 80\nConnection to 10.21.131.216 80 port [tcp/http] succeeded!\n"
Aug 26 06:21:19.081: INFO: stdout: ""
Aug 26 06:21:19.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c nc -v -z -w 2 10.0.1.126 30030'
Aug 26 06:21:19.278: INFO: stderr: "+ nc -v -z -w 2 10.0.1.126 30030\nConnection to 10.0.1.126 30030 port [tcp/*] succeeded!\n"
Aug 26 06:21:19.278: INFO: stdout: ""
Aug 26 06:21:19.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c nc -v -z -w 2 10.0.1.101 30030'
Aug 26 06:21:19.431: INFO: stderr: "+ nc -v -z -w 2 10.0.1.101 30030\nConnection to 10.0.1.101 30030 port [tcp/*] succeeded!\n"
Aug 26 06:21:19.431: INFO: stdout: ""
Aug 26 06:21:19.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.101:30030/ ; done'
Aug 26 06:21:19.761: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n"
Aug 26 06:21:19.761: INFO: stdout: "\naffinity-nodeport-transition-7fgpt\naffinity-nodeport-transition-6clfb\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-7fgpt\naffinity-nodeport-transition-6clfb\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-7fgpt\naffinity-nodeport-transition-6clfb\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-7fgpt\naffinity-nodeport-transition-6clfb\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-7fgpt\naffinity-nodeport-transition-6clfb\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-7fgpt"
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-6clfb
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-6clfb
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-6clfb
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-6clfb
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-6clfb
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
Aug 26 06:21:19.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.101:30030/ ; done'
Aug 26 06:21:20.135: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n"
Aug 26 06:21:20.135: INFO: stdout: "\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8"
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
Aug 26 06:21:20.135: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4634, will wait for the garbage collector to delete the pods 08/26/23 06:21:20.152
Aug 26 06:21:20.215: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.064294ms
Aug 26 06:21:20.316: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.585481ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 06:21:22.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4634" for this suite. 08/26/23 06:21:22.159
------------------------------
• [SLOW TEST] [9.611 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:21:12.554
    Aug 26 06:21:12.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 06:21:12.554
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:21:12.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:21:12.575
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-4634 08/26/23 06:21:12.578
    STEP: creating service affinity-nodeport-transition in namespace services-4634 08/26/23 06:21:12.578
    STEP: creating replication controller affinity-nodeport-transition in namespace services-4634 08/26/23 06:21:12.593
    I0826 06:21:12.602161      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4634, replica count: 3
    I0826 06:21:15.653672      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 26 06:21:15.683: INFO: Creating new exec pod
    Aug 26 06:21:15.691: INFO: Waiting up to 5m0s for pod "execpod-affinity7dbqd" in namespace "services-4634" to be "running"
    Aug 26 06:21:15.704: INFO: Pod "execpod-affinity7dbqd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.424767ms
    Aug 26 06:21:17.708: INFO: Pod "execpod-affinity7dbqd": Phase="Running", Reason="", readiness=true. Elapsed: 2.016987293s
    Aug 26 06:21:17.708: INFO: Pod "execpod-affinity7dbqd" satisfied condition "running"
    Aug 26 06:21:18.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Aug 26 06:21:18.906: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Aug 26 06:21:18.906: INFO: stdout: ""
    Aug 26 06:21:18.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c nc -v -z -w 2 10.21.131.216 80'
    Aug 26 06:21:19.081: INFO: stderr: "+ nc -v -z -w 2 10.21.131.216 80\nConnection to 10.21.131.216 80 port [tcp/http] succeeded!\n"
    Aug 26 06:21:19.081: INFO: stdout: ""
    Aug 26 06:21:19.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c nc -v -z -w 2 10.0.1.126 30030'
    Aug 26 06:21:19.278: INFO: stderr: "+ nc -v -z -w 2 10.0.1.126 30030\nConnection to 10.0.1.126 30030 port [tcp/*] succeeded!\n"
    Aug 26 06:21:19.278: INFO: stdout: ""
    Aug 26 06:21:19.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c nc -v -z -w 2 10.0.1.101 30030'
    Aug 26 06:21:19.431: INFO: stderr: "+ nc -v -z -w 2 10.0.1.101 30030\nConnection to 10.0.1.101 30030 port [tcp/*] succeeded!\n"
    Aug 26 06:21:19.431: INFO: stdout: ""
    Aug 26 06:21:19.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.101:30030/ ; done'
    Aug 26 06:21:19.761: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n"
    Aug 26 06:21:19.761: INFO: stdout: "\naffinity-nodeport-transition-7fgpt\naffinity-nodeport-transition-6clfb\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-7fgpt\naffinity-nodeport-transition-6clfb\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-7fgpt\naffinity-nodeport-transition-6clfb\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-7fgpt\naffinity-nodeport-transition-6clfb\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-7fgpt\naffinity-nodeport-transition-6clfb\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-7fgpt"
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-6clfb
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-6clfb
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-6clfb
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-6clfb
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-6clfb
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:19.761: INFO: Received response from host: affinity-nodeport-transition-7fgpt
    Aug 26 06:21:19.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4634 exec execpod-affinity7dbqd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.101:30030/ ; done'
    Aug 26 06:21:20.135: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30030/\n"
    Aug 26 06:21:20.135: INFO: stdout: "\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8\naffinity-nodeport-transition-fxhh8"
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Received response from host: affinity-nodeport-transition-fxhh8
    Aug 26 06:21:20.135: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4634, will wait for the garbage collector to delete the pods 08/26/23 06:21:20.152
    Aug 26 06:21:20.215: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.064294ms
    Aug 26 06:21:20.316: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.585481ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:21:22.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4634" for this suite. 08/26/23 06:21:22.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:21:22.165
Aug 26 06:21:22.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 06:21:22.167
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:21:22.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:21:22.186
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/26/23 06:21:22.189
Aug 26 06:21:22.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9291 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 26 06:21:22.339: INFO: stderr: ""
Aug 26 06:21:22.339: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 08/26/23 06:21:22.339
Aug 26 06:21:22.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9291 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Aug 26 06:21:23.088: INFO: stderr: ""
Aug 26 06:21:23.088: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/26/23 06:21:23.088
Aug 26 06:21:23.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9291 delete pods e2e-test-httpd-pod'
Aug 26 06:21:26.156: INFO: stderr: ""
Aug 26 06:21:26.156: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 06:21:26.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9291" for this suite. 08/26/23 06:21:26.166
------------------------------
• [4.009 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:21:22.165
    Aug 26 06:21:22.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 06:21:22.167
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:21:22.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:21:22.186
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/26/23 06:21:22.189
    Aug 26 06:21:22.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9291 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 26 06:21:22.339: INFO: stderr: ""
    Aug 26 06:21:22.339: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 08/26/23 06:21:22.339
    Aug 26 06:21:22.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9291 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Aug 26 06:21:23.088: INFO: stderr: ""
    Aug 26 06:21:23.088: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/26/23 06:21:23.088
    Aug 26 06:21:23.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-9291 delete pods e2e-test-httpd-pod'
    Aug 26 06:21:26.156: INFO: stderr: ""
    Aug 26 06:21:26.156: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:21:26.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9291" for this suite. 08/26/23 06:21:26.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:21:26.176
Aug 26 06:21:26.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:21:26.178
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:21:26.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:21:26.194
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-61dcfbd6-7fb8-46c7-8a42-c5fa5141e201 08/26/23 06:21:26.197
STEP: Creating a pod to test consume configMaps 08/26/23 06:21:26.202
Aug 26 06:21:26.210: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6" in namespace "projected-6091" to be "Succeeded or Failed"
Aug 26 06:21:26.213: INFO: Pod "pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079217ms
Aug 26 06:21:28.219: INFO: Pod "pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009382257s
Aug 26 06:21:30.218: INFO: Pod "pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008302657s
STEP: Saw pod success 08/26/23 06:21:30.218
Aug 26 06:21:30.218: INFO: Pod "pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6" satisfied condition "Succeeded or Failed"
Aug 26 06:21:30.221: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6 container agnhost-container: <nil>
STEP: delete the pod 08/26/23 06:21:30.231
Aug 26 06:21:30.248: INFO: Waiting for pod pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6 to disappear
Aug 26 06:21:30.252: INFO: Pod pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:21:30.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6091" for this suite. 08/26/23 06:21:30.26
------------------------------
• [4.090 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:21:26.176
    Aug 26 06:21:26.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:21:26.178
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:21:26.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:21:26.194
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-61dcfbd6-7fb8-46c7-8a42-c5fa5141e201 08/26/23 06:21:26.197
    STEP: Creating a pod to test consume configMaps 08/26/23 06:21:26.202
    Aug 26 06:21:26.210: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6" in namespace "projected-6091" to be "Succeeded or Failed"
    Aug 26 06:21:26.213: INFO: Pod "pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079217ms
    Aug 26 06:21:28.219: INFO: Pod "pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009382257s
    Aug 26 06:21:30.218: INFO: Pod "pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008302657s
    STEP: Saw pod success 08/26/23 06:21:30.218
    Aug 26 06:21:30.218: INFO: Pod "pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6" satisfied condition "Succeeded or Failed"
    Aug 26 06:21:30.221: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6 container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 06:21:30.231
    Aug 26 06:21:30.248: INFO: Waiting for pod pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6 to disappear
    Aug 26 06:21:30.252: INFO: Pod pod-projected-configmaps-c02f2742-112e-4340-9c02-cab6df7488d6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:21:30.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6091" for this suite. 08/26/23 06:21:30.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:21:30.266
Aug 26 06:21:30.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 06:21:30.268
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:21:30.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:21:30.285
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 08/26/23 06:21:30.287
STEP: Ensuring ResourceQuota status is calculated 08/26/23 06:21:30.292
STEP: Creating a ResourceQuota with not best effort scope 08/26/23 06:21:32.298
STEP: Ensuring ResourceQuota status is calculated 08/26/23 06:21:32.319
STEP: Creating a best-effort pod 08/26/23 06:21:34.324
STEP: Ensuring resource quota with best effort scope captures the pod usage 08/26/23 06:21:34.352
STEP: Ensuring resource quota with not best effort ignored the pod usage 08/26/23 06:21:36.357
STEP: Deleting the pod 08/26/23 06:21:38.362
STEP: Ensuring resource quota status released the pod usage 08/26/23 06:21:38.38
STEP: Creating a not best-effort pod 08/26/23 06:21:40.384
STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/26/23 06:21:40.398
STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/26/23 06:21:42.404
STEP: Deleting the pod 08/26/23 06:21:44.409
STEP: Ensuring resource quota status released the pod usage 08/26/23 06:21:44.427
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 06:21:46.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8622" for this suite. 08/26/23 06:21:46.44
------------------------------
• [SLOW TEST] [16.180 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:21:30.266
    Aug 26 06:21:30.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 06:21:30.268
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:21:30.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:21:30.285
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 08/26/23 06:21:30.287
    STEP: Ensuring ResourceQuota status is calculated 08/26/23 06:21:30.292
    STEP: Creating a ResourceQuota with not best effort scope 08/26/23 06:21:32.298
    STEP: Ensuring ResourceQuota status is calculated 08/26/23 06:21:32.319
    STEP: Creating a best-effort pod 08/26/23 06:21:34.324
    STEP: Ensuring resource quota with best effort scope captures the pod usage 08/26/23 06:21:34.352
    STEP: Ensuring resource quota with not best effort ignored the pod usage 08/26/23 06:21:36.357
    STEP: Deleting the pod 08/26/23 06:21:38.362
    STEP: Ensuring resource quota status released the pod usage 08/26/23 06:21:38.38
    STEP: Creating a not best-effort pod 08/26/23 06:21:40.384
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/26/23 06:21:40.398
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/26/23 06:21:42.404
    STEP: Deleting the pod 08/26/23 06:21:44.409
    STEP: Ensuring resource quota status released the pod usage 08/26/23 06:21:44.427
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:21:46.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8622" for this suite. 08/26/23 06:21:46.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:21:46.45
Aug 26 06:21:46.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-probe 08/26/23 06:21:46.452
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:21:46.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:21:46.472
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 26 06:22:46.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-947" for this suite. 08/26/23 06:22:46.499
------------------------------
• [SLOW TEST] [60.058 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:21:46.45
    Aug 26 06:21:46.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-probe 08/26/23 06:21:46.452
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:21:46.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:21:46.472
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:22:46.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-947" for this suite. 08/26/23 06:22:46.499
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:22:46.507
Aug 26 06:22:46.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename endpointslicemirroring 08/26/23 06:22:46.509
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:22:46.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:22:46.532
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 08/26/23 06:22:46.548
Aug 26 06:22:46.558: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 08/26/23 06:22:48.563
Aug 26 06:22:48.570: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 08/26/23 06:22:50.575
Aug 26 06:22:50.586: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Aug 26 06:22:52.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-7698" for this suite. 08/26/23 06:22:52.599
------------------------------
• [SLOW TEST] [6.099 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:22:46.507
    Aug 26 06:22:46.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename endpointslicemirroring 08/26/23 06:22:46.509
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:22:46.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:22:46.532
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 08/26/23 06:22:46.548
    Aug 26 06:22:46.558: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 08/26/23 06:22:48.563
    Aug 26 06:22:48.570: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 08/26/23 06:22:50.575
    Aug 26 06:22:50.586: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:22:52.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-7698" for this suite. 08/26/23 06:22:52.599
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:22:52.607
Aug 26 06:22:52.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:22:52.608
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:22:52.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:22:52.634
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 08/26/23 06:22:52.648
Aug 26 06:22:52.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224" in namespace "projected-1317" to be "Succeeded or Failed"
Aug 26 06:22:52.663: INFO: Pod "downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033123ms
Aug 26 06:22:54.669: INFO: Pod "downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530743s
Aug 26 06:22:56.668: INFO: Pod "downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00750644s
STEP: Saw pod success 08/26/23 06:22:56.668
Aug 26 06:22:56.668: INFO: Pod "downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224" satisfied condition "Succeeded or Failed"
Aug 26 06:22:56.671: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224 container client-container: <nil>
STEP: delete the pod 08/26/23 06:22:56.688
Aug 26 06:22:56.702: INFO: Waiting for pod downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224 to disappear
Aug 26 06:22:56.705: INFO: Pod downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 06:22:56.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1317" for this suite. 08/26/23 06:22:56.714
------------------------------
• [4.115 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:22:52.607
    Aug 26 06:22:52.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:22:52.608
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:22:52.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:22:52.634
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 08/26/23 06:22:52.648
    Aug 26 06:22:52.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224" in namespace "projected-1317" to be "Succeeded or Failed"
    Aug 26 06:22:52.663: INFO: Pod "downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033123ms
    Aug 26 06:22:54.669: INFO: Pod "downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530743s
    Aug 26 06:22:56.668: INFO: Pod "downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00750644s
    STEP: Saw pod success 08/26/23 06:22:56.668
    Aug 26 06:22:56.668: INFO: Pod "downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224" satisfied condition "Succeeded or Failed"
    Aug 26 06:22:56.671: INFO: Trying to get logs from node ip-10-0-1-5.us-west-2.compute.internal pod downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224 container client-container: <nil>
    STEP: delete the pod 08/26/23 06:22:56.688
    Aug 26 06:22:56.702: INFO: Waiting for pod downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224 to disappear
    Aug 26 06:22:56.705: INFO: Pod downwardapi-volume-2c6444f2-a666-400c-96c8-3f3263f18224 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:22:56.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1317" for this suite. 08/26/23 06:22:56.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:22:56.724
Aug 26 06:22:56.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename daemonsets 08/26/23 06:22:56.725
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:22:56.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:22:56.752
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 08/26/23 06:22:56.794
STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 06:22:56.8
Aug 26 06:22:56.809: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:22:56.809: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:22:56.809: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:22:56.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:22:56.813: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:22:57.822: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:22:57.822: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:22:57.823: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:22:57.829: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:22:57.829: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:22:58.822: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:22:58.822: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:22:58.822: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:22:58.828: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 26 06:22:58.828: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: listing all DeamonSets 08/26/23 06:22:58.831
STEP: DeleteCollection of the DaemonSets 08/26/23 06:22:58.835
STEP: Verify that ReplicaSets have been deleted 08/26/23 06:22:58.849
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Aug 26 06:22:58.883: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36482"},"items":null}

Aug 26 06:22:58.892: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36486"},"items":[{"metadata":{"name":"daemon-set-k2vdw","generateName":"daemon-set-","namespace":"daemonsets-9018","uid":"a557c80b-3272-4702-8d65-46d2c11a158a","resourceVersion":"36476","creationTimestamp":"2023-08-26T06:22:56Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"69d7d34ee584a159c50b6aaa91249176d0196b568ae5a471122de1e560afe282","cni.projectcalico.org/podIP":"10.20.50.198/32","cni.projectcalico.org/podIPs":"10.20.50.198/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"70056da7-7559-45f6-bf96-5157cd060c0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70056da7-7559-45f6-bf96-5157cd060c0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.50.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-56k7l","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-56k7l","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-101.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-101.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"}],"hostIP":"10.0.1.101","podIP":"10.20.50.198","podIPs":[{"ip":"10.20.50.198"}],"startTime":"2023-08-26T06:22:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-26T06:22:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://60bcbd5e21c022c3abeb59c3d0e94c919e165797a2a0f971f13586887afcec08","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kqtrg","generateName":"daemon-set-","namespace":"daemonsets-9018","uid":"bb78400b-a133-4e7c-ae0a-c0cbb02ebc8d","resourceVersion":"36483","creationTimestamp":"2023-08-26T06:22:56Z","deletionTimestamp":"2023-08-26T06:23:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7fc9518769ea0d671c3437458d5525bc4dc2ebef4a92b52bdfe393da2c9c5943","cni.projectcalico.org/podIP":"10.20.62.136/32","cni.projectcalico.org/podIPs":"10.20.62.136/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"70056da7-7559-45f6-bf96-5157cd060c0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70056da7-7559-45f6-bf96-5157cd060c0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.62.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pksq9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pksq9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-23.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-23.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"}],"hostIP":"10.0.1.23","podIP":"10.20.62.136","podIPs":[{"ip":"10.20.62.136"}],"startTime":"2023-08-26T06:22:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-26T06:22:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e6cf47d1c03cc1890044d0d51f78dee2884daacf5428f47d7a231e263d8bd6f4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-stp4k","generateName":"daemon-set-","namespace":"daemonsets-9018","uid":"22f8298e-8b49-40c4-a636-d70416957795","resourceVersion":"36485","creationTimestamp":"2023-08-26T06:22:56Z","deletionTimestamp":"2023-08-26T06:23:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b6591d75b8cc2910613b1e8067bd4270d7c2c78ce216819a7f74ab505213fe1f","cni.projectcalico.org/podIP":"10.20.193.203/32","cni.projectcalico.org/podIPs":"10.20.193.203/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"70056da7-7559-45f6-bf96-5157cd060c0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70056da7-7559-45f6-bf96-5157cd060c0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.193.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6cksh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6cksh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-126.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-126.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"}],"hostIP":"10.0.1.126","podIP":"10.20.193.203","podIPs":[{"ip":"10.20.193.203"}],"startTime":"2023-08-26T06:22:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-26T06:22:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a92b0402097520391e7bc5435532e052182df034e2b5d4180ab4bf032032d3f4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vw4zh","generateName":"daemon-set-","namespace":"daemonsets-9018","uid":"9eefef8a-c6b1-45f6-96c3-e2e04bf41589","resourceVersion":"36486","creationTimestamp":"2023-08-26T06:22:56Z","deletionTimestamp":"2023-08-26T06:23:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"f22f220c3fe97024744cd53f890598cad959bc6626eb2aec4cac3b68c5abe665","cni.projectcalico.org/podIP":"10.20.199.65/32","cni.projectcalico.org/podIPs":"10.20.199.65/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"70056da7-7559-45f6-bf96-5157cd060c0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70056da7-7559-45f6-bf96-5157cd060c0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wdblr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wdblr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-5.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-5.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"}],"hostIP":"10.0.1.5","podIP":"10.20.199.65","podIPs":[{"ip":"10.20.199.65"}],"startTime":"2023-08-26T06:22:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-26T06:22:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d70638cbe00a1d79d9b8c2c5da97747592cb0a4129bb8126855a259865506468","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x46x2","generateName":"daemon-set-","namespace":"daemonsets-9018","uid":"8a97b033-814a-4240-8c99-98dc17c78f7c","resourceVersion":"36484","creationTimestamp":"2023-08-26T06:22:56Z","deletionTimestamp":"2023-08-26T06:23:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1656b1ada4aa55d99524c5cdfa1c25db2b585b3d2fb42c472b8e6c04efab043d","cni.projectcalico.org/podIP":"10.20.8.219/32","cni.projectcalico.org/podIPs":"10.20.8.219/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"70056da7-7559-45f6-bf96-5157cd060c0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70056da7-7559-45f6-bf96-5157cd060c0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.8.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fjbsw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fjbsw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-31.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-31.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"}],"hostIP":"10.0.1.31","podIP":"10.20.8.219","podIPs":[{"ip":"10.20.8.219"}],"startTime":"2023-08-26T06:22:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-26T06:22:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://6c1e36b48fa48d4203a1337193dc18461cf9d7fe89ccf181826472e545fb5cbd","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:22:58.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9018" for this suite. 08/26/23 06:22:58.967
------------------------------
• [2.282 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:22:56.724
    Aug 26 06:22:56.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename daemonsets 08/26/23 06:22:56.725
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:22:56.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:22:56.752
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 08/26/23 06:22:56.794
    STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 06:22:56.8
    Aug 26 06:22:56.809: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:22:56.809: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:22:56.809: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:22:56.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:22:56.813: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:22:57.822: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:22:57.822: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:22:57.823: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:22:57.829: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:22:57.829: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:22:58.822: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:22:58.822: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:22:58.822: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:22:58.828: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 26 06:22:58.828: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: listing all DeamonSets 08/26/23 06:22:58.831
    STEP: DeleteCollection of the DaemonSets 08/26/23 06:22:58.835
    STEP: Verify that ReplicaSets have been deleted 08/26/23 06:22:58.849
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Aug 26 06:22:58.883: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36482"},"items":null}

    Aug 26 06:22:58.892: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36486"},"items":[{"metadata":{"name":"daemon-set-k2vdw","generateName":"daemon-set-","namespace":"daemonsets-9018","uid":"a557c80b-3272-4702-8d65-46d2c11a158a","resourceVersion":"36476","creationTimestamp":"2023-08-26T06:22:56Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"69d7d34ee584a159c50b6aaa91249176d0196b568ae5a471122de1e560afe282","cni.projectcalico.org/podIP":"10.20.50.198/32","cni.projectcalico.org/podIPs":"10.20.50.198/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"70056da7-7559-45f6-bf96-5157cd060c0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70056da7-7559-45f6-bf96-5157cd060c0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.50.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-56k7l","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-56k7l","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-101.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-101.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"}],"hostIP":"10.0.1.101","podIP":"10.20.50.198","podIPs":[{"ip":"10.20.50.198"}],"startTime":"2023-08-26T06:22:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-26T06:22:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://60bcbd5e21c022c3abeb59c3d0e94c919e165797a2a0f971f13586887afcec08","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kqtrg","generateName":"daemon-set-","namespace":"daemonsets-9018","uid":"bb78400b-a133-4e7c-ae0a-c0cbb02ebc8d","resourceVersion":"36483","creationTimestamp":"2023-08-26T06:22:56Z","deletionTimestamp":"2023-08-26T06:23:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7fc9518769ea0d671c3437458d5525bc4dc2ebef4a92b52bdfe393da2c9c5943","cni.projectcalico.org/podIP":"10.20.62.136/32","cni.projectcalico.org/podIPs":"10.20.62.136/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"70056da7-7559-45f6-bf96-5157cd060c0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70056da7-7559-45f6-bf96-5157cd060c0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.62.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pksq9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pksq9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-23.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-23.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"}],"hostIP":"10.0.1.23","podIP":"10.20.62.136","podIPs":[{"ip":"10.20.62.136"}],"startTime":"2023-08-26T06:22:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-26T06:22:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e6cf47d1c03cc1890044d0d51f78dee2884daacf5428f47d7a231e263d8bd6f4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-stp4k","generateName":"daemon-set-","namespace":"daemonsets-9018","uid":"22f8298e-8b49-40c4-a636-d70416957795","resourceVersion":"36485","creationTimestamp":"2023-08-26T06:22:56Z","deletionTimestamp":"2023-08-26T06:23:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b6591d75b8cc2910613b1e8067bd4270d7c2c78ce216819a7f74ab505213fe1f","cni.projectcalico.org/podIP":"10.20.193.203/32","cni.projectcalico.org/podIPs":"10.20.193.203/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"70056da7-7559-45f6-bf96-5157cd060c0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70056da7-7559-45f6-bf96-5157cd060c0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.193.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6cksh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6cksh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-126.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-126.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"}],"hostIP":"10.0.1.126","podIP":"10.20.193.203","podIPs":[{"ip":"10.20.193.203"}],"startTime":"2023-08-26T06:22:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-26T06:22:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a92b0402097520391e7bc5435532e052182df034e2b5d4180ab4bf032032d3f4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vw4zh","generateName":"daemon-set-","namespace":"daemonsets-9018","uid":"9eefef8a-c6b1-45f6-96c3-e2e04bf41589","resourceVersion":"36486","creationTimestamp":"2023-08-26T06:22:56Z","deletionTimestamp":"2023-08-26T06:23:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"f22f220c3fe97024744cd53f890598cad959bc6626eb2aec4cac3b68c5abe665","cni.projectcalico.org/podIP":"10.20.199.65/32","cni.projectcalico.org/podIPs":"10.20.199.65/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"70056da7-7559-45f6-bf96-5157cd060c0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70056da7-7559-45f6-bf96-5157cd060c0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wdblr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wdblr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-5.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-5.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"}],"hostIP":"10.0.1.5","podIP":"10.20.199.65","podIPs":[{"ip":"10.20.199.65"}],"startTime":"2023-08-26T06:22:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-26T06:22:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d70638cbe00a1d79d9b8c2c5da97747592cb0a4129bb8126855a259865506468","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x46x2","generateName":"daemon-set-","namespace":"daemonsets-9018","uid":"8a97b033-814a-4240-8c99-98dc17c78f7c","resourceVersion":"36484","creationTimestamp":"2023-08-26T06:22:56Z","deletionTimestamp":"2023-08-26T06:23:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1656b1ada4aa55d99524c5cdfa1c25db2b585b3d2fb42c472b8e6c04efab043d","cni.projectcalico.org/podIP":"10.20.8.219/32","cni.projectcalico.org/podIPs":"10.20.8.219/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"70056da7-7559-45f6-bf96-5157cd060c0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70056da7-7559-45f6-bf96-5157cd060c0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-26T06:22:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.8.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fjbsw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fjbsw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-31.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-31.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-26T06:22:56Z"}],"hostIP":"10.0.1.31","podIP":"10.20.8.219","podIPs":[{"ip":"10.20.8.219"}],"startTime":"2023-08-26T06:22:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-26T06:22:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://6c1e36b48fa48d4203a1337193dc18461cf9d7fe89ccf181826472e545fb5cbd","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:22:58.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9018" for this suite. 08/26/23 06:22:58.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:22:59.007
Aug 26 06:22:59.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:22:59.008
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:22:59.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:22:59.026
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-5b2b9b32-798e-48f7-87a5-231bec3d95a3 08/26/23 06:22:59.029
STEP: Creating a pod to test consume secrets 08/26/23 06:22:59.034
Aug 26 06:22:59.044: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40" in namespace "projected-3419" to be "Succeeded or Failed"
Aug 26 06:22:59.048: INFO: Pod "pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.996943ms
Aug 26 06:23:01.053: INFO: Pod "pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009062403s
Aug 26 06:23:03.055: INFO: Pod "pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011220062s
STEP: Saw pod success 08/26/23 06:23:03.055
Aug 26 06:23:03.055: INFO: Pod "pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40" satisfied condition "Succeeded or Failed"
Aug 26 06:23:03.061: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/26/23 06:23:03.085
Aug 26 06:23:03.104: INFO: Waiting for pod pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40 to disappear
Aug 26 06:23:03.107: INFO: Pod pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:03.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3419" for this suite. 08/26/23 06:23:03.115
------------------------------
• [4.116 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:22:59.007
    Aug 26 06:22:59.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:22:59.008
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:22:59.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:22:59.026
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-5b2b9b32-798e-48f7-87a5-231bec3d95a3 08/26/23 06:22:59.029
    STEP: Creating a pod to test consume secrets 08/26/23 06:22:59.034
    Aug 26 06:22:59.044: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40" in namespace "projected-3419" to be "Succeeded or Failed"
    Aug 26 06:22:59.048: INFO: Pod "pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.996943ms
    Aug 26 06:23:01.053: INFO: Pod "pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009062403s
    Aug 26 06:23:03.055: INFO: Pod "pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011220062s
    STEP: Saw pod success 08/26/23 06:23:03.055
    Aug 26 06:23:03.055: INFO: Pod "pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40" satisfied condition "Succeeded or Failed"
    Aug 26 06:23:03.061: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 06:23:03.085
    Aug 26 06:23:03.104: INFO: Waiting for pod pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40 to disappear
    Aug 26 06:23:03.107: INFO: Pod pod-projected-secrets-e4d9fab8-0adc-47b6-b88b-0fb309defb40 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:03.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3419" for this suite. 08/26/23 06:23:03.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:03.125
Aug 26 06:23:03.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename csistoragecapacity 08/26/23 06:23:03.126
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:03.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:03.149
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 08/26/23 06:23:03.152
STEP: getting /apis/storage.k8s.io 08/26/23 06:23:03.157
STEP: getting /apis/storage.k8s.io/v1 08/26/23 06:23:03.158
STEP: creating 08/26/23 06:23:03.161
STEP: watching 08/26/23 06:23:03.18
Aug 26 06:23:03.180: INFO: starting watch
STEP: getting 08/26/23 06:23:03.192
STEP: listing in namespace 08/26/23 06:23:03.196
STEP: listing across namespaces 08/26/23 06:23:03.201
STEP: patching 08/26/23 06:23:03.206
STEP: updating 08/26/23 06:23:03.212
Aug 26 06:23:03.218: INFO: waiting for watch events with expected annotations in namespace
Aug 26 06:23:03.218: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 08/26/23 06:23:03.218
STEP: deleting a collection 08/26/23 06:23:03.231
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:03.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-457" for this suite. 08/26/23 06:23:03.253
------------------------------
• [0.135 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:03.125
    Aug 26 06:23:03.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename csistoragecapacity 08/26/23 06:23:03.126
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:03.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:03.149
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 08/26/23 06:23:03.152
    STEP: getting /apis/storage.k8s.io 08/26/23 06:23:03.157
    STEP: getting /apis/storage.k8s.io/v1 08/26/23 06:23:03.158
    STEP: creating 08/26/23 06:23:03.161
    STEP: watching 08/26/23 06:23:03.18
    Aug 26 06:23:03.180: INFO: starting watch
    STEP: getting 08/26/23 06:23:03.192
    STEP: listing in namespace 08/26/23 06:23:03.196
    STEP: listing across namespaces 08/26/23 06:23:03.201
    STEP: patching 08/26/23 06:23:03.206
    STEP: updating 08/26/23 06:23:03.212
    Aug 26 06:23:03.218: INFO: waiting for watch events with expected annotations in namespace
    Aug 26 06:23:03.218: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 08/26/23 06:23:03.218
    STEP: deleting a collection 08/26/23 06:23:03.231
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:03.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-457" for this suite. 08/26/23 06:23:03.253
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:03.262
Aug 26 06:23:03.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:23:03.263
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:03.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:03.283
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-e6baa37c-e2f7-422c-9b7a-cc066ca92f1e 08/26/23 06:23:03.287
STEP: Creating a pod to test consume configMaps 08/26/23 06:23:03.293
Aug 26 06:23:03.302: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db" in namespace "projected-3799" to be "Succeeded or Failed"
Aug 26 06:23:03.306: INFO: Pod "pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.446179ms
Aug 26 06:23:05.310: INFO: Pod "pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008290878s
Aug 26 06:23:07.312: INFO: Pod "pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00984311s
STEP: Saw pod success 08/26/23 06:23:07.312
Aug 26 06:23:07.312: INFO: Pod "pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db" satisfied condition "Succeeded or Failed"
Aug 26 06:23:07.316: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db container projected-configmap-volume-test: <nil>
STEP: delete the pod 08/26/23 06:23:07.322
Aug 26 06:23:07.340: INFO: Waiting for pod pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db to disappear
Aug 26 06:23:07.343: INFO: Pod pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:07.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3799" for this suite. 08/26/23 06:23:07.35
------------------------------
• [4.100 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:03.262
    Aug 26 06:23:03.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:23:03.263
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:03.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:03.283
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-e6baa37c-e2f7-422c-9b7a-cc066ca92f1e 08/26/23 06:23:03.287
    STEP: Creating a pod to test consume configMaps 08/26/23 06:23:03.293
    Aug 26 06:23:03.302: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db" in namespace "projected-3799" to be "Succeeded or Failed"
    Aug 26 06:23:03.306: INFO: Pod "pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.446179ms
    Aug 26 06:23:05.310: INFO: Pod "pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008290878s
    Aug 26 06:23:07.312: INFO: Pod "pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00984311s
    STEP: Saw pod success 08/26/23 06:23:07.312
    Aug 26 06:23:07.312: INFO: Pod "pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db" satisfied condition "Succeeded or Failed"
    Aug 26 06:23:07.316: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db container projected-configmap-volume-test: <nil>
    STEP: delete the pod 08/26/23 06:23:07.322
    Aug 26 06:23:07.340: INFO: Waiting for pod pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db to disappear
    Aug 26 06:23:07.343: INFO: Pod pod-projected-configmaps-0cb609d2-4eef-4630-8b93-a6b0abe765db no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:07.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3799" for this suite. 08/26/23 06:23:07.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:07.362
Aug 26 06:23:07.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename prestop 08/26/23 06:23:07.363
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:07.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:07.395
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-857 08/26/23 06:23:07.399
STEP: Waiting for pods to come up. 08/26/23 06:23:07.413
Aug 26 06:23:07.413: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-857" to be "running"
Aug 26 06:23:07.417: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.666308ms
Aug 26 06:23:09.422: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008732469s
Aug 26 06:23:09.422: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-857 08/26/23 06:23:09.425
Aug 26 06:23:09.433: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-857" to be "running"
Aug 26 06:23:09.439: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.472663ms
Aug 26 06:23:11.443: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.009600611s
Aug 26 06:23:11.443: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 08/26/23 06:23:11.443
Aug 26 06:23:16.457: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 08/26/23 06:23:16.457
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:16.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-857" for this suite. 08/26/23 06:23:16.484
------------------------------
• [SLOW TEST] [9.128 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:07.362
    Aug 26 06:23:07.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename prestop 08/26/23 06:23:07.363
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:07.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:07.395
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-857 08/26/23 06:23:07.399
    STEP: Waiting for pods to come up. 08/26/23 06:23:07.413
    Aug 26 06:23:07.413: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-857" to be "running"
    Aug 26 06:23:07.417: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.666308ms
    Aug 26 06:23:09.422: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008732469s
    Aug 26 06:23:09.422: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-857 08/26/23 06:23:09.425
    Aug 26 06:23:09.433: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-857" to be "running"
    Aug 26 06:23:09.439: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.472663ms
    Aug 26 06:23:11.443: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.009600611s
    Aug 26 06:23:11.443: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 08/26/23 06:23:11.443
    Aug 26 06:23:16.457: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 08/26/23 06:23:16.457
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:16.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-857" for this suite. 08/26/23 06:23:16.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:16.491
Aug 26 06:23:16.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 06:23:16.492
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:16.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:16.517
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:16.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-451" for this suite. 08/26/23 06:23:16.541
------------------------------
• [0.059 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:16.491
    Aug 26 06:23:16.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 06:23:16.492
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:16.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:16.517
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:16.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-451" for this suite. 08/26/23 06:23:16.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:16.551
Aug 26 06:23:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 06:23:16.552
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:16.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:16.575
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 08/26/23 06:23:16.578
Aug 26 06:23:16.589: INFO: Waiting up to 5m0s for pod "pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c" in namespace "emptydir-6681" to be "Succeeded or Failed"
Aug 26 06:23:16.592: INFO: Pod "pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119599ms
Aug 26 06:23:18.596: INFO: Pod "pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007317039s
Aug 26 06:23:20.598: INFO: Pod "pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008946319s
STEP: Saw pod success 08/26/23 06:23:20.598
Aug 26 06:23:20.598: INFO: Pod "pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c" satisfied condition "Succeeded or Failed"
Aug 26 06:23:20.602: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c container test-container: <nil>
STEP: delete the pod 08/26/23 06:23:20.609
Aug 26 06:23:20.624: INFO: Waiting for pod pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c to disappear
Aug 26 06:23:20.627: INFO: Pod pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:20.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6681" for this suite. 08/26/23 06:23:20.634
------------------------------
• [4.093 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:16.551
    Aug 26 06:23:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 06:23:16.552
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:16.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:16.575
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/26/23 06:23:16.578
    Aug 26 06:23:16.589: INFO: Waiting up to 5m0s for pod "pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c" in namespace "emptydir-6681" to be "Succeeded or Failed"
    Aug 26 06:23:16.592: INFO: Pod "pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119599ms
    Aug 26 06:23:18.596: INFO: Pod "pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007317039s
    Aug 26 06:23:20.598: INFO: Pod "pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008946319s
    STEP: Saw pod success 08/26/23 06:23:20.598
    Aug 26 06:23:20.598: INFO: Pod "pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c" satisfied condition "Succeeded or Failed"
    Aug 26 06:23:20.602: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c container test-container: <nil>
    STEP: delete the pod 08/26/23 06:23:20.609
    Aug 26 06:23:20.624: INFO: Waiting for pod pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c to disappear
    Aug 26 06:23:20.627: INFO: Pod pod-86ae73d5-9c66-4e6c-afa3-4fbb6bbb384c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:20.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6681" for this suite. 08/26/23 06:23:20.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:20.646
Aug 26 06:23:20.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:23:20.647
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:20.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:20.67
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Aug 26 06:23:20.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/26/23 06:23:22.967
Aug 26 06:23:22.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-502 --namespace=crd-publish-openapi-502 create -f -'
Aug 26 06:23:23.729: INFO: stderr: ""
Aug 26 06:23:23.729: INFO: stdout: "e2e-test-crd-publish-openapi-8995-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 26 06:23:23.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-502 --namespace=crd-publish-openapi-502 delete e2e-test-crd-publish-openapi-8995-crds test-cr'
Aug 26 06:23:23.807: INFO: stderr: ""
Aug 26 06:23:23.807: INFO: stdout: "e2e-test-crd-publish-openapi-8995-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 26 06:23:23.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-502 --namespace=crd-publish-openapi-502 apply -f -'
Aug 26 06:23:24.649: INFO: stderr: ""
Aug 26 06:23:24.649: INFO: stdout: "e2e-test-crd-publish-openapi-8995-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 26 06:23:24.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-502 --namespace=crd-publish-openapi-502 delete e2e-test-crd-publish-openapi-8995-crds test-cr'
Aug 26 06:23:24.750: INFO: stderr: ""
Aug 26 06:23:24.750: INFO: stdout: "e2e-test-crd-publish-openapi-8995-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/26/23 06:23:24.75
Aug 26 06:23:24.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-502 explain e2e-test-crd-publish-openapi-8995-crds'
Aug 26 06:23:25.314: INFO: stderr: ""
Aug 26 06:23:25.314: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8995-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:27.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-502" for this suite. 08/26/23 06:23:27.547
------------------------------
• [SLOW TEST] [6.918 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:20.646
    Aug 26 06:23:20.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:23:20.647
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:20.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:20.67
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Aug 26 06:23:20.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/26/23 06:23:22.967
    Aug 26 06:23:22.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-502 --namespace=crd-publish-openapi-502 create -f -'
    Aug 26 06:23:23.729: INFO: stderr: ""
    Aug 26 06:23:23.729: INFO: stdout: "e2e-test-crd-publish-openapi-8995-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 26 06:23:23.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-502 --namespace=crd-publish-openapi-502 delete e2e-test-crd-publish-openapi-8995-crds test-cr'
    Aug 26 06:23:23.807: INFO: stderr: ""
    Aug 26 06:23:23.807: INFO: stdout: "e2e-test-crd-publish-openapi-8995-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Aug 26 06:23:23.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-502 --namespace=crd-publish-openapi-502 apply -f -'
    Aug 26 06:23:24.649: INFO: stderr: ""
    Aug 26 06:23:24.649: INFO: stdout: "e2e-test-crd-publish-openapi-8995-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 26 06:23:24.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-502 --namespace=crd-publish-openapi-502 delete e2e-test-crd-publish-openapi-8995-crds test-cr'
    Aug 26 06:23:24.750: INFO: stderr: ""
    Aug 26 06:23:24.750: INFO: stdout: "e2e-test-crd-publish-openapi-8995-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/26/23 06:23:24.75
    Aug 26 06:23:24.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-502 explain e2e-test-crd-publish-openapi-8995-crds'
    Aug 26 06:23:25.314: INFO: stderr: ""
    Aug 26 06:23:25.314: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8995-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:27.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-502" for this suite. 08/26/23 06:23:27.547
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:27.565
Aug 26 06:23:27.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename downward-api 08/26/23 06:23:27.566
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:27.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:27.595
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 08/26/23 06:23:27.601
Aug 26 06:23:27.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892" in namespace "downward-api-9361" to be "Succeeded or Failed"
Aug 26 06:23:27.624: INFO: Pod "downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009906ms
Aug 26 06:23:29.630: INFO: Pod "downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014031711s
Aug 26 06:23:31.632: INFO: Pod "downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015706152s
STEP: Saw pod success 08/26/23 06:23:31.632
Aug 26 06:23:31.632: INFO: Pod "downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892" satisfied condition "Succeeded or Failed"
Aug 26 06:23:31.639: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892 container client-container: <nil>
STEP: delete the pod 08/26/23 06:23:31.668
Aug 26 06:23:31.693: INFO: Waiting for pod downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892 to disappear
Aug 26 06:23:31.699: INFO: Pod downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:31.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9361" for this suite. 08/26/23 06:23:31.709
------------------------------
• [4.153 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:27.565
    Aug 26 06:23:27.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename downward-api 08/26/23 06:23:27.566
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:27.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:27.595
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 08/26/23 06:23:27.601
    Aug 26 06:23:27.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892" in namespace "downward-api-9361" to be "Succeeded or Failed"
    Aug 26 06:23:27.624: INFO: Pod "downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009906ms
    Aug 26 06:23:29.630: INFO: Pod "downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014031711s
    Aug 26 06:23:31.632: INFO: Pod "downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015706152s
    STEP: Saw pod success 08/26/23 06:23:31.632
    Aug 26 06:23:31.632: INFO: Pod "downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892" satisfied condition "Succeeded or Failed"
    Aug 26 06:23:31.639: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892 container client-container: <nil>
    STEP: delete the pod 08/26/23 06:23:31.668
    Aug 26 06:23:31.693: INFO: Waiting for pod downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892 to disappear
    Aug 26 06:23:31.699: INFO: Pod downwardapi-volume-24cb4dd8-dcc1-4d91-80f6-5b16e074d892 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:31.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9361" for this suite. 08/26/23 06:23:31.709
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:31.718
Aug 26 06:23:31.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename disruption 08/26/23 06:23:31.719
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:31.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:31.752
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 08/26/23 06:23:31.755
STEP: Waiting for the pdb to be processed 08/26/23 06:23:31.761
STEP: First trying to evict a pod which shouldn't be evictable 08/26/23 06:23:33.781
STEP: Waiting for all pods to be running 08/26/23 06:23:33.781
Aug 26 06:23:33.789: INFO: pods: 1 < 3
STEP: locating a running pod 08/26/23 06:23:35.802
STEP: Updating the pdb to allow a pod to be evicted 08/26/23 06:23:35.826
STEP: Waiting for the pdb to be processed 08/26/23 06:23:35.842
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/26/23 06:23:37.856
STEP: Waiting for all pods to be running 08/26/23 06:23:37.856
STEP: Waiting for the pdb to observed all healthy pods 08/26/23 06:23:37.867
STEP: Patching the pdb to disallow a pod to be evicted 08/26/23 06:23:37.915
STEP: Waiting for the pdb to be processed 08/26/23 06:23:37.943
STEP: Waiting for all pods to be running 08/26/23 06:23:39.954
STEP: locating a running pod 08/26/23 06:23:39.96
STEP: Deleting the pdb to allow a pod to be evicted 08/26/23 06:23:39.971
STEP: Waiting for the pdb to be deleted 08/26/23 06:23:39.979
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/26/23 06:23:39.983
STEP: Waiting for all pods to be running 08/26/23 06:23:39.983
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:40.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8819" for this suite. 08/26/23 06:23:40.033
------------------------------
• [SLOW TEST] [8.342 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:31.718
    Aug 26 06:23:31.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename disruption 08/26/23 06:23:31.719
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:31.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:31.752
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 08/26/23 06:23:31.755
    STEP: Waiting for the pdb to be processed 08/26/23 06:23:31.761
    STEP: First trying to evict a pod which shouldn't be evictable 08/26/23 06:23:33.781
    STEP: Waiting for all pods to be running 08/26/23 06:23:33.781
    Aug 26 06:23:33.789: INFO: pods: 1 < 3
    STEP: locating a running pod 08/26/23 06:23:35.802
    STEP: Updating the pdb to allow a pod to be evicted 08/26/23 06:23:35.826
    STEP: Waiting for the pdb to be processed 08/26/23 06:23:35.842
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/26/23 06:23:37.856
    STEP: Waiting for all pods to be running 08/26/23 06:23:37.856
    STEP: Waiting for the pdb to observed all healthy pods 08/26/23 06:23:37.867
    STEP: Patching the pdb to disallow a pod to be evicted 08/26/23 06:23:37.915
    STEP: Waiting for the pdb to be processed 08/26/23 06:23:37.943
    STEP: Waiting for all pods to be running 08/26/23 06:23:39.954
    STEP: locating a running pod 08/26/23 06:23:39.96
    STEP: Deleting the pdb to allow a pod to be evicted 08/26/23 06:23:39.971
    STEP: Waiting for the pdb to be deleted 08/26/23 06:23:39.979
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/26/23 06:23:39.983
    STEP: Waiting for all pods to be running 08/26/23 06:23:39.983
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:40.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8819" for this suite. 08/26/23 06:23:40.033
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:40.06
Aug 26 06:23:40.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 06:23:40.062
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:40.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:40.085
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-7e0d1670-cc98-4d2a-8d6c-ef9ca58ce041 08/26/23 06:23:40.087
STEP: Creating a pod to test consume configMaps 08/26/23 06:23:40.095
Aug 26 06:23:40.105: INFO: Waiting up to 5m0s for pod "pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8" in namespace "configmap-1805" to be "Succeeded or Failed"
Aug 26 06:23:40.113: INFO: Pod "pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.532447ms
Aug 26 06:23:42.121: INFO: Pod "pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016250095s
Aug 26 06:23:44.121: INFO: Pod "pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015670641s
STEP: Saw pod success 08/26/23 06:23:44.121
Aug 26 06:23:44.121: INFO: Pod "pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8" satisfied condition "Succeeded or Failed"
Aug 26 06:23:44.125: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8 container agnhost-container: <nil>
STEP: delete the pod 08/26/23 06:23:44.135
Aug 26 06:23:44.152: INFO: Waiting for pod pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8 to disappear
Aug 26 06:23:44.157: INFO: Pod pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:44.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1805" for this suite. 08/26/23 06:23:44.179
------------------------------
• [4.127 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:40.06
    Aug 26 06:23:40.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 06:23:40.062
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:40.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:40.085
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-7e0d1670-cc98-4d2a-8d6c-ef9ca58ce041 08/26/23 06:23:40.087
    STEP: Creating a pod to test consume configMaps 08/26/23 06:23:40.095
    Aug 26 06:23:40.105: INFO: Waiting up to 5m0s for pod "pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8" in namespace "configmap-1805" to be "Succeeded or Failed"
    Aug 26 06:23:40.113: INFO: Pod "pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.532447ms
    Aug 26 06:23:42.121: INFO: Pod "pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016250095s
    Aug 26 06:23:44.121: INFO: Pod "pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015670641s
    STEP: Saw pod success 08/26/23 06:23:44.121
    Aug 26 06:23:44.121: INFO: Pod "pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8" satisfied condition "Succeeded or Failed"
    Aug 26 06:23:44.125: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8 container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 06:23:44.135
    Aug 26 06:23:44.152: INFO: Waiting for pod pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8 to disappear
    Aug 26 06:23:44.157: INFO: Pod pod-configmaps-a51cb41b-a962-4933-837c-25b04123b4e8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:44.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1805" for this suite. 08/26/23 06:23:44.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:44.189
Aug 26 06:23:44.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sysctl 08/26/23 06:23:44.19
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:44.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:44.215
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 08/26/23 06:23:44.218
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:44.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-346" for this suite. 08/26/23 06:23:44.232
------------------------------
• [0.056 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:44.189
    Aug 26 06:23:44.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sysctl 08/26/23 06:23:44.19
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:44.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:44.215
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 08/26/23 06:23:44.218
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:44.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-346" for this suite. 08/26/23 06:23:44.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:44.245
Aug 26 06:23:44.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-runtime 08/26/23 06:23:44.246
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:44.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:44.273
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 08/26/23 06:23:44.276
STEP: wait for the container to reach Succeeded 08/26/23 06:23:44.288
STEP: get the container status 08/26/23 06:23:48.324
STEP: the container should be terminated 08/26/23 06:23:48.329
STEP: the termination message should be set 08/26/23 06:23:48.329
Aug 26 06:23:48.329: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 08/26/23 06:23:48.329
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:48.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2483" for this suite. 08/26/23 06:23:48.367
------------------------------
• [4.134 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:44.245
    Aug 26 06:23:44.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-runtime 08/26/23 06:23:44.246
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:44.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:44.273
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 08/26/23 06:23:44.276
    STEP: wait for the container to reach Succeeded 08/26/23 06:23:44.288
    STEP: get the container status 08/26/23 06:23:48.324
    STEP: the container should be terminated 08/26/23 06:23:48.329
    STEP: the termination message should be set 08/26/23 06:23:48.329
    Aug 26 06:23:48.329: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 08/26/23 06:23:48.329
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:48.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2483" for this suite. 08/26/23 06:23:48.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:48.389
Aug 26 06:23:48.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 06:23:48.39
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:48.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:48.418
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 08/26/23 06:23:48.421
Aug 26 06:23:48.421: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1 proxy --unix-socket=/tmp/kubectl-proxy-unix1437421825/test'
STEP: retrieving proxy /api/ output 08/26/23 06:23:48.51
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:48.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1" for this suite. 08/26/23 06:23:48.535
------------------------------
• [0.160 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:48.389
    Aug 26 06:23:48.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 06:23:48.39
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:48.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:48.418
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 08/26/23 06:23:48.421
    Aug 26 06:23:48.421: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-1 proxy --unix-socket=/tmp/kubectl-proxy-unix1437421825/test'
    STEP: retrieving proxy /api/ output 08/26/23 06:23:48.51
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:48.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1" for this suite. 08/26/23 06:23:48.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:48.55
Aug 26 06:23:48.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename dns 08/26/23 06:23:48.551
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:48.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:48.574
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/26/23 06:23:48.577
Aug 26 06:23:48.587: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-722  86acf112-3ad3-4f81-88cd-9f1109aba872 37180 0 2023-08-26 06:23:48 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-26 06:23:48 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wqf8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wqf8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:23:48.587: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-722" to be "running and ready"
Aug 26 06:23:48.592: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.823141ms
Aug 26 06:23:48.592: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:23:50.598: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.010599124s
Aug 26 06:23:50.598: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Aug 26 06:23:50.598: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 08/26/23 06:23:50.598
Aug 26 06:23:50.598: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-722 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:23:50.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:23:50.599: INFO: ExecWithOptions: Clientset creation
Aug 26 06:23:50.599: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/dns-722/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 08/26/23 06:23:50.72
Aug 26 06:23:50.721: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-722 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:23:50.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:23:50.721: INFO: ExecWithOptions: Clientset creation
Aug 26 06:23:50.721: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/dns-722/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 26 06:23:50.810: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:50.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-722" for this suite. 08/26/23 06:23:50.836
------------------------------
• [2.298 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:48.55
    Aug 26 06:23:48.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename dns 08/26/23 06:23:48.551
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:48.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:48.574
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/26/23 06:23:48.577
    Aug 26 06:23:48.587: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-722  86acf112-3ad3-4f81-88cd-9f1109aba872 37180 0 2023-08-26 06:23:48 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-26 06:23:48 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wqf8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wqf8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:23:48.587: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-722" to be "running and ready"
    Aug 26 06:23:48.592: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.823141ms
    Aug 26 06:23:48.592: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:23:50.598: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.010599124s
    Aug 26 06:23:50.598: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Aug 26 06:23:50.598: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 08/26/23 06:23:50.598
    Aug 26 06:23:50.598: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-722 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:23:50.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:23:50.599: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:23:50.599: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/dns-722/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 08/26/23 06:23:50.72
    Aug 26 06:23:50.721: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-722 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:23:50.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:23:50.721: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:23:50.721: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/dns-722/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 26 06:23:50.810: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:50.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-722" for this suite. 08/26/23 06:23:50.836
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:50.848
Aug 26 06:23:50.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename discovery 08/26/23 06:23:50.85
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:50.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:50.872
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 08/26/23 06:23:50.879
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Aug 26 06:23:51.103: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 26 06:23:51.104: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 26 06:23:51.104: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 26 06:23:51.104: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 26 06:23:51.104: INFO: Checking APIGroup: apps
Aug 26 06:23:51.105: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 26 06:23:51.105: INFO: Versions found [{apps/v1 v1}]
Aug 26 06:23:51.105: INFO: apps/v1 matches apps/v1
Aug 26 06:23:51.105: INFO: Checking APIGroup: events.k8s.io
Aug 26 06:23:51.106: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 26 06:23:51.106: INFO: Versions found [{events.k8s.io/v1 v1}]
Aug 26 06:23:51.107: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 26 06:23:51.107: INFO: Checking APIGroup: authentication.k8s.io
Aug 26 06:23:51.108: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 26 06:23:51.108: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 26 06:23:51.108: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 26 06:23:51.108: INFO: Checking APIGroup: authorization.k8s.io
Aug 26 06:23:51.109: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 26 06:23:51.109: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 26 06:23:51.109: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 26 06:23:51.109: INFO: Checking APIGroup: autoscaling
Aug 26 06:23:51.110: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 26 06:23:51.110: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Aug 26 06:23:51.110: INFO: autoscaling/v2 matches autoscaling/v2
Aug 26 06:23:51.110: INFO: Checking APIGroup: batch
Aug 26 06:23:51.112: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 26 06:23:51.112: INFO: Versions found [{batch/v1 v1}]
Aug 26 06:23:51.112: INFO: batch/v1 matches batch/v1
Aug 26 06:23:51.112: INFO: Checking APIGroup: certificates.k8s.io
Aug 26 06:23:51.113: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 26 06:23:51.113: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 26 06:23:51.113: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 26 06:23:51.113: INFO: Checking APIGroup: networking.k8s.io
Aug 26 06:23:51.114: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 26 06:23:51.114: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 26 06:23:51.114: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 26 06:23:51.114: INFO: Checking APIGroup: policy
Aug 26 06:23:51.115: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 26 06:23:51.115: INFO: Versions found [{policy/v1 v1}]
Aug 26 06:23:51.115: INFO: policy/v1 matches policy/v1
Aug 26 06:23:51.115: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 26 06:23:51.116: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 26 06:23:51.116: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 26 06:23:51.116: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 26 06:23:51.116: INFO: Checking APIGroup: storage.k8s.io
Aug 26 06:23:51.117: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 26 06:23:51.117: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 26 06:23:51.117: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 26 06:23:51.117: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 26 06:23:51.118: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 26 06:23:51.118: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 26 06:23:51.118: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 26 06:23:51.118: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 26 06:23:51.118: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 26 06:23:51.118: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 26 06:23:51.118: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 26 06:23:51.118: INFO: Checking APIGroup: scheduling.k8s.io
Aug 26 06:23:51.119: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 26 06:23:51.119: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 26 06:23:51.119: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 26 06:23:51.119: INFO: Checking APIGroup: coordination.k8s.io
Aug 26 06:23:51.120: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 26 06:23:51.120: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 26 06:23:51.120: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 26 06:23:51.120: INFO: Checking APIGroup: node.k8s.io
Aug 26 06:23:51.121: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 26 06:23:51.121: INFO: Versions found [{node.k8s.io/v1 v1}]
Aug 26 06:23:51.121: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 26 06:23:51.121: INFO: Checking APIGroup: discovery.k8s.io
Aug 26 06:23:51.122: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 26 06:23:51.122: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Aug 26 06:23:51.122: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 26 06:23:51.122: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 26 06:23:51.122: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Aug 26 06:23:51.122: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Aug 26 06:23:51.122: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Aug 26 06:23:51.122: INFO: Checking APIGroup: agent.pf9.io
Aug 26 06:23:51.123: INFO: PreferredVersion.GroupVersion: agent.pf9.io/v1
Aug 26 06:23:51.123: INFO: Versions found [{agent.pf9.io/v1 v1}]
Aug 26 06:23:51.123: INFO: agent.pf9.io/v1 matches agent.pf9.io/v1
Aug 26 06:23:51.123: INFO: Checking APIGroup: crd.projectcalico.org
Aug 26 06:23:51.124: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 26 06:23:51.124: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 26 06:23:51.124: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 26 06:23:51.124: INFO: Checking APIGroup: metrics.k8s.io
Aug 26 06:23:51.125: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 26 06:23:51.125: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 26 06:23:51.125: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Aug 26 06:23:51.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-8484" for this suite. 08/26/23 06:23:51.133
------------------------------
• [0.297 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:50.848
    Aug 26 06:23:50.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename discovery 08/26/23 06:23:50.85
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:50.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:50.872
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 08/26/23 06:23:50.879
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Aug 26 06:23:51.103: INFO: Checking APIGroup: apiregistration.k8s.io
    Aug 26 06:23:51.104: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Aug 26 06:23:51.104: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Aug 26 06:23:51.104: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Aug 26 06:23:51.104: INFO: Checking APIGroup: apps
    Aug 26 06:23:51.105: INFO: PreferredVersion.GroupVersion: apps/v1
    Aug 26 06:23:51.105: INFO: Versions found [{apps/v1 v1}]
    Aug 26 06:23:51.105: INFO: apps/v1 matches apps/v1
    Aug 26 06:23:51.105: INFO: Checking APIGroup: events.k8s.io
    Aug 26 06:23:51.106: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Aug 26 06:23:51.106: INFO: Versions found [{events.k8s.io/v1 v1}]
    Aug 26 06:23:51.107: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Aug 26 06:23:51.107: INFO: Checking APIGroup: authentication.k8s.io
    Aug 26 06:23:51.108: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Aug 26 06:23:51.108: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Aug 26 06:23:51.108: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Aug 26 06:23:51.108: INFO: Checking APIGroup: authorization.k8s.io
    Aug 26 06:23:51.109: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Aug 26 06:23:51.109: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Aug 26 06:23:51.109: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Aug 26 06:23:51.109: INFO: Checking APIGroup: autoscaling
    Aug 26 06:23:51.110: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Aug 26 06:23:51.110: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Aug 26 06:23:51.110: INFO: autoscaling/v2 matches autoscaling/v2
    Aug 26 06:23:51.110: INFO: Checking APIGroup: batch
    Aug 26 06:23:51.112: INFO: PreferredVersion.GroupVersion: batch/v1
    Aug 26 06:23:51.112: INFO: Versions found [{batch/v1 v1}]
    Aug 26 06:23:51.112: INFO: batch/v1 matches batch/v1
    Aug 26 06:23:51.112: INFO: Checking APIGroup: certificates.k8s.io
    Aug 26 06:23:51.113: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Aug 26 06:23:51.113: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Aug 26 06:23:51.113: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Aug 26 06:23:51.113: INFO: Checking APIGroup: networking.k8s.io
    Aug 26 06:23:51.114: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Aug 26 06:23:51.114: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Aug 26 06:23:51.114: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Aug 26 06:23:51.114: INFO: Checking APIGroup: policy
    Aug 26 06:23:51.115: INFO: PreferredVersion.GroupVersion: policy/v1
    Aug 26 06:23:51.115: INFO: Versions found [{policy/v1 v1}]
    Aug 26 06:23:51.115: INFO: policy/v1 matches policy/v1
    Aug 26 06:23:51.115: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Aug 26 06:23:51.116: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Aug 26 06:23:51.116: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Aug 26 06:23:51.116: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Aug 26 06:23:51.116: INFO: Checking APIGroup: storage.k8s.io
    Aug 26 06:23:51.117: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Aug 26 06:23:51.117: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Aug 26 06:23:51.117: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Aug 26 06:23:51.117: INFO: Checking APIGroup: admissionregistration.k8s.io
    Aug 26 06:23:51.118: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Aug 26 06:23:51.118: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Aug 26 06:23:51.118: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Aug 26 06:23:51.118: INFO: Checking APIGroup: apiextensions.k8s.io
    Aug 26 06:23:51.118: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Aug 26 06:23:51.118: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Aug 26 06:23:51.118: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Aug 26 06:23:51.118: INFO: Checking APIGroup: scheduling.k8s.io
    Aug 26 06:23:51.119: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Aug 26 06:23:51.119: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Aug 26 06:23:51.119: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Aug 26 06:23:51.119: INFO: Checking APIGroup: coordination.k8s.io
    Aug 26 06:23:51.120: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Aug 26 06:23:51.120: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Aug 26 06:23:51.120: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Aug 26 06:23:51.120: INFO: Checking APIGroup: node.k8s.io
    Aug 26 06:23:51.121: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Aug 26 06:23:51.121: INFO: Versions found [{node.k8s.io/v1 v1}]
    Aug 26 06:23:51.121: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Aug 26 06:23:51.121: INFO: Checking APIGroup: discovery.k8s.io
    Aug 26 06:23:51.122: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Aug 26 06:23:51.122: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Aug 26 06:23:51.122: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Aug 26 06:23:51.122: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Aug 26 06:23:51.122: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Aug 26 06:23:51.122: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Aug 26 06:23:51.122: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Aug 26 06:23:51.122: INFO: Checking APIGroup: agent.pf9.io
    Aug 26 06:23:51.123: INFO: PreferredVersion.GroupVersion: agent.pf9.io/v1
    Aug 26 06:23:51.123: INFO: Versions found [{agent.pf9.io/v1 v1}]
    Aug 26 06:23:51.123: INFO: agent.pf9.io/v1 matches agent.pf9.io/v1
    Aug 26 06:23:51.123: INFO: Checking APIGroup: crd.projectcalico.org
    Aug 26 06:23:51.124: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Aug 26 06:23:51.124: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Aug 26 06:23:51.124: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Aug 26 06:23:51.124: INFO: Checking APIGroup: metrics.k8s.io
    Aug 26 06:23:51.125: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Aug 26 06:23:51.125: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Aug 26 06:23:51.125: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:23:51.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-8484" for this suite. 08/26/23 06:23:51.133
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:23:51.146
Aug 26 06:23:51.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename var-expansion 08/26/23 06:23:51.147
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:51.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:51.169
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 08/26/23 06:23:51.173
Aug 26 06:23:51.185: INFO: Waiting up to 2m0s for pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5" in namespace "var-expansion-6818" to be "running"
Aug 26 06:23:51.191: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.780687ms
Aug 26 06:23:53.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013444311s
Aug 26 06:23:55.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011844473s
Aug 26 06:23:57.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014021924s
Aug 26 06:23:59.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011055281s
Aug 26 06:24:01.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012099258s
Aug 26 06:24:03.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013052918s
Aug 26 06:24:05.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012875059s
Aug 26 06:24:07.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010941648s
Aug 26 06:24:09.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012789699s
Aug 26 06:24:11.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012092766s
Aug 26 06:24:13.210: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.025257259s
Aug 26 06:24:15.200: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015106051s
Aug 26 06:24:17.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011040006s
Aug 26 06:24:19.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011501203s
Aug 26 06:24:21.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011688429s
Aug 26 06:24:23.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011397576s
Aug 26 06:24:25.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012216282s
Aug 26 06:24:27.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011691237s
Aug 26 06:24:29.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011999758s
Aug 26 06:24:31.202: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.016598808s
Aug 26 06:24:33.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012540439s
Aug 26 06:24:35.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012667357s
Aug 26 06:24:37.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012447662s
Aug 26 06:24:39.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012291213s
Aug 26 06:24:41.203: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.017706148s
Aug 26 06:24:43.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012472094s
Aug 26 06:24:45.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.013262995s
Aug 26 06:24:47.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012034678s
Aug 26 06:24:49.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.011113666s
Aug 26 06:24:51.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012032441s
Aug 26 06:24:53.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012480223s
Aug 26 06:24:55.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012297308s
Aug 26 06:24:57.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012447532s
Aug 26 06:24:59.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010889336s
Aug 26 06:25:01.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.013637831s
Aug 26 06:25:03.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013093941s
Aug 26 06:25:05.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012666219s
Aug 26 06:25:07.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011065495s
Aug 26 06:25:09.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012790727s
Aug 26 06:25:11.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012610544s
Aug 26 06:25:13.200: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.014966055s
Aug 26 06:25:15.210: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.024533547s
Aug 26 06:25:17.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01244639s
Aug 26 06:25:19.200: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014707094s
Aug 26 06:25:21.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.012214926s
Aug 26 06:25:23.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01147555s
Aug 26 06:25:25.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011159736s
Aug 26 06:25:27.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.012560723s
Aug 26 06:25:29.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.012345153s
Aug 26 06:25:31.195: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010201068s
Aug 26 06:25:33.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012314864s
Aug 26 06:25:35.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010489065s
Aug 26 06:25:37.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011555837s
Aug 26 06:25:39.204: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.018670124s
Aug 26 06:25:41.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011238164s
Aug 26 06:25:43.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.013672357s
Aug 26 06:25:45.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013793112s
Aug 26 06:25:47.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011220704s
Aug 26 06:25:49.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01369191s
Aug 26 06:25:51.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013997635s
Aug 26 06:25:51.208: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022454505s
STEP: updating the pod 08/26/23 06:25:51.208
Aug 26 06:25:51.727: INFO: Successfully updated pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5"
STEP: waiting for pod running 08/26/23 06:25:51.727
Aug 26 06:25:51.727: INFO: Waiting up to 2m0s for pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5" in namespace "var-expansion-6818" to be "running"
Aug 26 06:25:51.739: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.201707ms
Aug 26 06:25:53.746: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.019459917s
Aug 26 06:25:53.746: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5" satisfied condition "running"
STEP: deleting the pod gracefully 08/26/23 06:25:53.747
Aug 26 06:25:53.747: INFO: Deleting pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5" in namespace "var-expansion-6818"
Aug 26 06:25:53.764: INFO: Wait up to 5m0s for pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 26 06:26:25.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6818" for this suite. 08/26/23 06:26:25.787
------------------------------
• [SLOW TEST] [154.650 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:23:51.146
    Aug 26 06:23:51.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename var-expansion 08/26/23 06:23:51.147
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:23:51.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:23:51.169
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 08/26/23 06:23:51.173
    Aug 26 06:23:51.185: INFO: Waiting up to 2m0s for pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5" in namespace "var-expansion-6818" to be "running"
    Aug 26 06:23:51.191: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.780687ms
    Aug 26 06:23:53.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013444311s
    Aug 26 06:23:55.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011844473s
    Aug 26 06:23:57.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014021924s
    Aug 26 06:23:59.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011055281s
    Aug 26 06:24:01.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012099258s
    Aug 26 06:24:03.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013052918s
    Aug 26 06:24:05.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012875059s
    Aug 26 06:24:07.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010941648s
    Aug 26 06:24:09.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012789699s
    Aug 26 06:24:11.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012092766s
    Aug 26 06:24:13.210: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.025257259s
    Aug 26 06:24:15.200: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015106051s
    Aug 26 06:24:17.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011040006s
    Aug 26 06:24:19.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011501203s
    Aug 26 06:24:21.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011688429s
    Aug 26 06:24:23.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011397576s
    Aug 26 06:24:25.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012216282s
    Aug 26 06:24:27.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011691237s
    Aug 26 06:24:29.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011999758s
    Aug 26 06:24:31.202: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.016598808s
    Aug 26 06:24:33.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012540439s
    Aug 26 06:24:35.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012667357s
    Aug 26 06:24:37.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012447662s
    Aug 26 06:24:39.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012291213s
    Aug 26 06:24:41.203: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.017706148s
    Aug 26 06:24:43.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012472094s
    Aug 26 06:24:45.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.013262995s
    Aug 26 06:24:47.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012034678s
    Aug 26 06:24:49.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.011113666s
    Aug 26 06:24:51.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012032441s
    Aug 26 06:24:53.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012480223s
    Aug 26 06:24:55.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012297308s
    Aug 26 06:24:57.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012447532s
    Aug 26 06:24:59.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010889336s
    Aug 26 06:25:01.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.013637831s
    Aug 26 06:25:03.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013093941s
    Aug 26 06:25:05.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012666219s
    Aug 26 06:25:07.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011065495s
    Aug 26 06:25:09.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012790727s
    Aug 26 06:25:11.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012610544s
    Aug 26 06:25:13.200: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.014966055s
    Aug 26 06:25:15.210: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.024533547s
    Aug 26 06:25:17.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01244639s
    Aug 26 06:25:19.200: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014707094s
    Aug 26 06:25:21.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.012214926s
    Aug 26 06:25:23.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01147555s
    Aug 26 06:25:25.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011159736s
    Aug 26 06:25:27.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.012560723s
    Aug 26 06:25:29.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.012345153s
    Aug 26 06:25:31.195: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010201068s
    Aug 26 06:25:33.198: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012314864s
    Aug 26 06:25:35.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010489065s
    Aug 26 06:25:37.197: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011555837s
    Aug 26 06:25:39.204: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.018670124s
    Aug 26 06:25:41.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011238164s
    Aug 26 06:25:43.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.013672357s
    Aug 26 06:25:45.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013793112s
    Aug 26 06:25:47.196: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011220704s
    Aug 26 06:25:49.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01369191s
    Aug 26 06:25:51.199: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013997635s
    Aug 26 06:25:51.208: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022454505s
    STEP: updating the pod 08/26/23 06:25:51.208
    Aug 26 06:25:51.727: INFO: Successfully updated pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5"
    STEP: waiting for pod running 08/26/23 06:25:51.727
    Aug 26 06:25:51.727: INFO: Waiting up to 2m0s for pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5" in namespace "var-expansion-6818" to be "running"
    Aug 26 06:25:51.739: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.201707ms
    Aug 26 06:25:53.746: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.019459917s
    Aug 26 06:25:53.746: INFO: Pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5" satisfied condition "running"
    STEP: deleting the pod gracefully 08/26/23 06:25:53.747
    Aug 26 06:25:53.747: INFO: Deleting pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5" in namespace "var-expansion-6818"
    Aug 26 06:25:53.764: INFO: Wait up to 5m0s for pod "var-expansion-d1752830-c68d-431a-84d5-484f7890a0b5" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:26:25.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6818" for this suite. 08/26/23 06:26:25.787
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:26:25.797
Aug 26 06:26:25.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename watch 08/26/23 06:26:25.801
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:25.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:25.832
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 08/26/23 06:26:25.838
STEP: creating a new configmap 08/26/23 06:26:25.839
STEP: modifying the configmap once 08/26/23 06:26:25.85
STEP: closing the watch once it receives two notifications 08/26/23 06:26:25.862
Aug 26 06:26:25.862: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6751  e047a579-f295-4816-b236-89d781683f98 37722 0 2023-08-26 06:26:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-26 06:26:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:26:25.863: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6751  e047a579-f295-4816-b236-89d781683f98 37723 0 2023-08-26 06:26:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-26 06:26:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 08/26/23 06:26:25.863
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/26/23 06:26:25.876
STEP: deleting the configmap 08/26/23 06:26:25.879
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/26/23 06:26:25.887
Aug 26 06:26:25.887: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6751  e047a579-f295-4816-b236-89d781683f98 37724 0 2023-08-26 06:26:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-26 06:26:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:26:25.890: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6751  e047a579-f295-4816-b236-89d781683f98 37725 0 2023-08-26 06:26:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-26 06:26:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 26 06:26:25.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6751" for this suite. 08/26/23 06:26:25.908
------------------------------
• [0.125 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:26:25.797
    Aug 26 06:26:25.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename watch 08/26/23 06:26:25.801
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:25.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:25.832
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 08/26/23 06:26:25.838
    STEP: creating a new configmap 08/26/23 06:26:25.839
    STEP: modifying the configmap once 08/26/23 06:26:25.85
    STEP: closing the watch once it receives two notifications 08/26/23 06:26:25.862
    Aug 26 06:26:25.862: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6751  e047a579-f295-4816-b236-89d781683f98 37722 0 2023-08-26 06:26:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-26 06:26:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:26:25.863: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6751  e047a579-f295-4816-b236-89d781683f98 37723 0 2023-08-26 06:26:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-26 06:26:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 08/26/23 06:26:25.863
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/26/23 06:26:25.876
    STEP: deleting the configmap 08/26/23 06:26:25.879
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/26/23 06:26:25.887
    Aug 26 06:26:25.887: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6751  e047a579-f295-4816-b236-89d781683f98 37724 0 2023-08-26 06:26:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-26 06:26:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:26:25.890: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6751  e047a579-f295-4816-b236-89d781683f98 37725 0 2023-08-26 06:26:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-26 06:26:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:26:25.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6751" for this suite. 08/26/23 06:26:25.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:26:25.928
Aug 26 06:26:25.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 06:26:25.931
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:25.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:25.958
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 08/26/23 06:26:25.962
STEP: listing secrets in all namespaces to ensure that there are more than zero 08/26/23 06:26:25.967
STEP: patching the secret 08/26/23 06:26:25.973
STEP: deleting the secret using a LabelSelector 08/26/23 06:26:25.992
STEP: listing secrets in all namespaces, searching for label name and value in patch 08/26/23 06:26:26.004
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 06:26:26.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6907" for this suite. 08/26/23 06:26:26.018
------------------------------
• [0.100 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:26:25.928
    Aug 26 06:26:25.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 06:26:25.931
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:25.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:25.958
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 08/26/23 06:26:25.962
    STEP: listing secrets in all namespaces to ensure that there are more than zero 08/26/23 06:26:25.967
    STEP: patching the secret 08/26/23 06:26:25.973
    STEP: deleting the secret using a LabelSelector 08/26/23 06:26:25.992
    STEP: listing secrets in all namespaces, searching for label name and value in patch 08/26/23 06:26:26.004
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:26:26.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6907" for this suite. 08/26/23 06:26:26.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:26:26.029
Aug 26 06:26:26.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 06:26:26.03
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:26.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:26.055
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-4712 08/26/23 06:26:26.059
STEP: creating replication controller nodeport-test in namespace services-4712 08/26/23 06:26:26.084
I0826 06:26:26.098265      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4712, replica count: 2
I0826 06:26:29.149853      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 26 06:26:29.149: INFO: Creating new exec pod
Aug 26 06:26:29.163: INFO: Waiting up to 5m0s for pod "execpodb7fhf" in namespace "services-4712" to be "running"
Aug 26 06:26:29.168: INFO: Pod "execpodb7fhf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.6044ms
Aug 26 06:26:31.174: INFO: Pod "execpodb7fhf": Phase="Running", Reason="", readiness=true. Elapsed: 2.011110735s
Aug 26 06:26:31.174: INFO: Pod "execpodb7fhf" satisfied condition "running"
Aug 26 06:26:32.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4712 exec execpodb7fhf -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Aug 26 06:26:32.456: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 26 06:26:32.456: INFO: stdout: ""
Aug 26 06:26:32.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4712 exec execpodb7fhf -- /bin/sh -x -c nc -v -z -w 2 10.21.21.177 80'
Aug 26 06:26:32.695: INFO: stderr: "+ nc -v -z -w 2 10.21.21.177 80\nConnection to 10.21.21.177 80 port [tcp/http] succeeded!\n"
Aug 26 06:26:32.695: INFO: stdout: ""
Aug 26 06:26:32.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4712 exec execpodb7fhf -- /bin/sh -x -c nc -v -z -w 2 10.0.1.5 31927'
Aug 26 06:26:32.872: INFO: stderr: "+ nc -v -z -w 2 10.0.1.5 31927\nConnection to 10.0.1.5 31927 port [tcp/*] succeeded!\n"
Aug 26 06:26:32.872: INFO: stdout: ""
Aug 26 06:26:32.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4712 exec execpodb7fhf -- /bin/sh -x -c nc -v -z -w 2 10.0.1.31 31927'
Aug 26 06:26:33.094: INFO: stderr: "+ nc -v -z -w 2 10.0.1.31 31927\nConnection to 10.0.1.31 31927 port [tcp/*] succeeded!\n"
Aug 26 06:26:33.094: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 06:26:33.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4712" for this suite. 08/26/23 06:26:33.108
------------------------------
• [SLOW TEST] [7.096 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:26:26.029
    Aug 26 06:26:26.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 06:26:26.03
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:26.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:26.055
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-4712 08/26/23 06:26:26.059
    STEP: creating replication controller nodeport-test in namespace services-4712 08/26/23 06:26:26.084
    I0826 06:26:26.098265      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4712, replica count: 2
    I0826 06:26:29.149853      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 26 06:26:29.149: INFO: Creating new exec pod
    Aug 26 06:26:29.163: INFO: Waiting up to 5m0s for pod "execpodb7fhf" in namespace "services-4712" to be "running"
    Aug 26 06:26:29.168: INFO: Pod "execpodb7fhf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.6044ms
    Aug 26 06:26:31.174: INFO: Pod "execpodb7fhf": Phase="Running", Reason="", readiness=true. Elapsed: 2.011110735s
    Aug 26 06:26:31.174: INFO: Pod "execpodb7fhf" satisfied condition "running"
    Aug 26 06:26:32.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4712 exec execpodb7fhf -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Aug 26 06:26:32.456: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 26 06:26:32.456: INFO: stdout: ""
    Aug 26 06:26:32.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4712 exec execpodb7fhf -- /bin/sh -x -c nc -v -z -w 2 10.21.21.177 80'
    Aug 26 06:26:32.695: INFO: stderr: "+ nc -v -z -w 2 10.21.21.177 80\nConnection to 10.21.21.177 80 port [tcp/http] succeeded!\n"
    Aug 26 06:26:32.695: INFO: stdout: ""
    Aug 26 06:26:32.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4712 exec execpodb7fhf -- /bin/sh -x -c nc -v -z -w 2 10.0.1.5 31927'
    Aug 26 06:26:32.872: INFO: stderr: "+ nc -v -z -w 2 10.0.1.5 31927\nConnection to 10.0.1.5 31927 port [tcp/*] succeeded!\n"
    Aug 26 06:26:32.872: INFO: stdout: ""
    Aug 26 06:26:32.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-4712 exec execpodb7fhf -- /bin/sh -x -c nc -v -z -w 2 10.0.1.31 31927'
    Aug 26 06:26:33.094: INFO: stderr: "+ nc -v -z -w 2 10.0.1.31 31927\nConnection to 10.0.1.31 31927 port [tcp/*] succeeded!\n"
    Aug 26 06:26:33.094: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:26:33.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4712" for this suite. 08/26/23 06:26:33.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:26:33.126
Aug 26 06:26:33.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 06:26:33.127
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:33.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:33.158
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-a73a6632-ef69-4652-859c-0a0878fa4f0e 08/26/23 06:26:33.166
STEP: Creating a pod to test consume secrets 08/26/23 06:26:33.174
Aug 26 06:26:33.189: INFO: Waiting up to 5m0s for pod "pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8" in namespace "secrets-6388" to be "Succeeded or Failed"
Aug 26 06:26:33.195: INFO: Pod "pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.883306ms
Aug 26 06:26:35.203: INFO: Pod "pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014098983s
Aug 26 06:26:37.201: INFO: Pod "pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012393793s
STEP: Saw pod success 08/26/23 06:26:37.201
Aug 26 06:26:37.201: INFO: Pod "pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8" satisfied condition "Succeeded or Failed"
Aug 26 06:26:37.206: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8 container secret-volume-test: <nil>
STEP: delete the pod 08/26/23 06:26:37.241
Aug 26 06:26:37.260: INFO: Waiting for pod pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8 to disappear
Aug 26 06:26:37.264: INFO: Pod pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 06:26:37.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6388" for this suite. 08/26/23 06:26:37.273
------------------------------
• [4.158 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:26:33.126
    Aug 26 06:26:33.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 06:26:33.127
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:33.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:33.158
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-a73a6632-ef69-4652-859c-0a0878fa4f0e 08/26/23 06:26:33.166
    STEP: Creating a pod to test consume secrets 08/26/23 06:26:33.174
    Aug 26 06:26:33.189: INFO: Waiting up to 5m0s for pod "pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8" in namespace "secrets-6388" to be "Succeeded or Failed"
    Aug 26 06:26:33.195: INFO: Pod "pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.883306ms
    Aug 26 06:26:35.203: INFO: Pod "pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014098983s
    Aug 26 06:26:37.201: INFO: Pod "pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012393793s
    STEP: Saw pod success 08/26/23 06:26:37.201
    Aug 26 06:26:37.201: INFO: Pod "pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8" satisfied condition "Succeeded or Failed"
    Aug 26 06:26:37.206: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8 container secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 06:26:37.241
    Aug 26 06:26:37.260: INFO: Waiting for pod pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8 to disappear
    Aug 26 06:26:37.264: INFO: Pod pod-secrets-fe57c4f3-63fc-40ee-a031-5f0fca0be0f8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:26:37.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6388" for this suite. 08/26/23 06:26:37.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:26:37.286
Aug 26 06:26:37.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename daemonsets 08/26/23 06:26:37.293
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:37.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:37.324
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 08/26/23 06:26:37.381
STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 06:26:37.389
Aug 26 06:26:37.399: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:37.399: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:37.399: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:37.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:26:37.406: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:26:38.435: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:38.435: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:38.435: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:38.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:26:38.443: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:26:39.429: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:39.429: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:39.429: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:39.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 26 06:26:39.437: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 08/26/23 06:26:39.443
Aug 26 06:26:39.471: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:39.471: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:39.471: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:39.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 26 06:26:39.480: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:26:40.503: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:40.503: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:40.503: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:40.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 26 06:26:40.509: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:26:41.489: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:41.489: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:41.489: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:41.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 26 06:26:41.495: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:26:42.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:42.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:42.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:42.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 26 06:26:42.496: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:26:43.493: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:43.493: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:43.493: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:26:43.503: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 26 06:26:43.503: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/26/23 06:26:43.508
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-279, will wait for the garbage collector to delete the pods 08/26/23 06:26:43.508
Aug 26 06:26:43.579: INFO: Deleting DaemonSet.extensions daemon-set took: 14.807539ms
Aug 26 06:26:43.680: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.441568ms
Aug 26 06:26:45.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:26:45.991: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 26 06:26:45.995: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38077"},"items":null}

Aug 26 06:26:45.998: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38077"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:26:46.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-279" for this suite. 08/26/23 06:26:46.056
------------------------------
• [SLOW TEST] [8.780 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:26:37.286
    Aug 26 06:26:37.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename daemonsets 08/26/23 06:26:37.293
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:37.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:37.324
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 08/26/23 06:26:37.381
    STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 06:26:37.389
    Aug 26 06:26:37.399: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:37.399: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:37.399: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:37.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:26:37.406: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:26:38.435: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:38.435: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:38.435: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:38.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:26:38.443: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:26:39.429: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:39.429: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:39.429: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:39.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 26 06:26:39.437: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 08/26/23 06:26:39.443
    Aug 26 06:26:39.471: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:39.471: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:39.471: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:39.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 26 06:26:39.480: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:26:40.503: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:40.503: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:40.503: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:40.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 26 06:26:40.509: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:26:41.489: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:41.489: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:41.489: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:41.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 26 06:26:41.495: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:26:42.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:42.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:42.491: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:42.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 26 06:26:42.496: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:26:43.493: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:43.493: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:43.493: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:26:43.503: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 26 06:26:43.503: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/26/23 06:26:43.508
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-279, will wait for the garbage collector to delete the pods 08/26/23 06:26:43.508
    Aug 26 06:26:43.579: INFO: Deleting DaemonSet.extensions daemon-set took: 14.807539ms
    Aug 26 06:26:43.680: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.441568ms
    Aug 26 06:26:45.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:26:45.991: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 26 06:26:45.995: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38077"},"items":null}

    Aug 26 06:26:45.998: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38077"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:26:46.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-279" for this suite. 08/26/23 06:26:46.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:26:46.073
Aug 26 06:26:46.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 06:26:46.074
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:46.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:46.097
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 08/26/23 06:26:46.102
STEP: Creating a ResourceQuota 08/26/23 06:26:51.108
STEP: Ensuring resource quota status is calculated 08/26/23 06:26:51.117
STEP: Creating a ReplicaSet 08/26/23 06:26:53.123
STEP: Ensuring resource quota status captures replicaset creation 08/26/23 06:26:53.141
STEP: Deleting a ReplicaSet 08/26/23 06:26:55.149
STEP: Ensuring resource quota status released usage 08/26/23 06:26:55.158
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 06:26:57.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5853" for this suite. 08/26/23 06:26:57.175
------------------------------
• [SLOW TEST] [11.122 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:26:46.073
    Aug 26 06:26:46.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 06:26:46.074
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:46.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:46.097
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 08/26/23 06:26:46.102
    STEP: Creating a ResourceQuota 08/26/23 06:26:51.108
    STEP: Ensuring resource quota status is calculated 08/26/23 06:26:51.117
    STEP: Creating a ReplicaSet 08/26/23 06:26:53.123
    STEP: Ensuring resource quota status captures replicaset creation 08/26/23 06:26:53.141
    STEP: Deleting a ReplicaSet 08/26/23 06:26:55.149
    STEP: Ensuring resource quota status released usage 08/26/23 06:26:55.158
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:26:57.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5853" for this suite. 08/26/23 06:26:57.175
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:26:57.202
Aug 26 06:26:57.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:26:57.203
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:57.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:57.236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Aug 26 06:26:57.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/26/23 06:26:59.589
Aug 26 06:26:59.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-8740 --namespace=crd-publish-openapi-8740 create -f -'
Aug 26 06:27:00.415: INFO: stderr: ""
Aug 26 06:27:00.415: INFO: stdout: "e2e-test-crd-publish-openapi-5565-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 26 06:27:00.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-8740 --namespace=crd-publish-openapi-8740 delete e2e-test-crd-publish-openapi-5565-crds test-cr'
Aug 26 06:27:00.542: INFO: stderr: ""
Aug 26 06:27:00.542: INFO: stdout: "e2e-test-crd-publish-openapi-5565-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 26 06:27:00.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-8740 --namespace=crd-publish-openapi-8740 apply -f -'
Aug 26 06:27:01.323: INFO: stderr: ""
Aug 26 06:27:01.323: INFO: stdout: "e2e-test-crd-publish-openapi-5565-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 26 06:27:01.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-8740 --namespace=crd-publish-openapi-8740 delete e2e-test-crd-publish-openapi-5565-crds test-cr'
Aug 26 06:27:01.456: INFO: stderr: ""
Aug 26 06:27:01.456: INFO: stdout: "e2e-test-crd-publish-openapi-5565-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/26/23 06:27:01.456
Aug 26 06:27:01.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-8740 explain e2e-test-crd-publish-openapi-5565-crds'
Aug 26 06:27:02.115: INFO: stderr: ""
Aug 26 06:27:02.115: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5565-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:27:04.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8740" for this suite. 08/26/23 06:27:04.937
------------------------------
• [SLOW TEST] [7.742 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:26:57.202
    Aug 26 06:26:57.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:26:57.203
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:26:57.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:26:57.236
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Aug 26 06:26:57.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/26/23 06:26:59.589
    Aug 26 06:26:59.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-8740 --namespace=crd-publish-openapi-8740 create -f -'
    Aug 26 06:27:00.415: INFO: stderr: ""
    Aug 26 06:27:00.415: INFO: stdout: "e2e-test-crd-publish-openapi-5565-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 26 06:27:00.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-8740 --namespace=crd-publish-openapi-8740 delete e2e-test-crd-publish-openapi-5565-crds test-cr'
    Aug 26 06:27:00.542: INFO: stderr: ""
    Aug 26 06:27:00.542: INFO: stdout: "e2e-test-crd-publish-openapi-5565-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Aug 26 06:27:00.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-8740 --namespace=crd-publish-openapi-8740 apply -f -'
    Aug 26 06:27:01.323: INFO: stderr: ""
    Aug 26 06:27:01.323: INFO: stdout: "e2e-test-crd-publish-openapi-5565-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 26 06:27:01.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-8740 --namespace=crd-publish-openapi-8740 delete e2e-test-crd-publish-openapi-5565-crds test-cr'
    Aug 26 06:27:01.456: INFO: stderr: ""
    Aug 26 06:27:01.456: INFO: stdout: "e2e-test-crd-publish-openapi-5565-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/26/23 06:27:01.456
    Aug 26 06:27:01.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=crd-publish-openapi-8740 explain e2e-test-crd-publish-openapi-5565-crds'
    Aug 26 06:27:02.115: INFO: stderr: ""
    Aug 26 06:27:02.115: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5565-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:27:04.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8740" for this suite. 08/26/23 06:27:04.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:27:04.944
Aug 26 06:27:04.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 06:27:04.945
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:27:04.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:27:04.965
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/26/23 06:27:04.967
Aug 26 06:27:04.984: INFO: Waiting up to 5m0s for pod "pod-4f300bcb-343e-4765-82d1-5a3f5c302254" in namespace "emptydir-8151" to be "Succeeded or Failed"
Aug 26 06:27:04.999: INFO: Pod "pod-4f300bcb-343e-4765-82d1-5a3f5c302254": Phase="Pending", Reason="", readiness=false. Elapsed: 15.033946ms
Aug 26 06:27:07.005: INFO: Pod "pod-4f300bcb-343e-4765-82d1-5a3f5c302254": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021228366s
Aug 26 06:27:09.006: INFO: Pod "pod-4f300bcb-343e-4765-82d1-5a3f5c302254": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0218254s
STEP: Saw pod success 08/26/23 06:27:09.006
Aug 26 06:27:09.006: INFO: Pod "pod-4f300bcb-343e-4765-82d1-5a3f5c302254" satisfied condition "Succeeded or Failed"
Aug 26 06:27:09.010: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-4f300bcb-343e-4765-82d1-5a3f5c302254 container test-container: <nil>
STEP: delete the pod 08/26/23 06:27:09.031
Aug 26 06:27:09.054: INFO: Waiting for pod pod-4f300bcb-343e-4765-82d1-5a3f5c302254 to disappear
Aug 26 06:27:09.064: INFO: Pod pod-4f300bcb-343e-4765-82d1-5a3f5c302254 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 06:27:09.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8151" for this suite. 08/26/23 06:27:09.073
------------------------------
• [4.139 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:27:04.944
    Aug 26 06:27:04.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 06:27:04.945
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:27:04.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:27:04.965
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/26/23 06:27:04.967
    Aug 26 06:27:04.984: INFO: Waiting up to 5m0s for pod "pod-4f300bcb-343e-4765-82d1-5a3f5c302254" in namespace "emptydir-8151" to be "Succeeded or Failed"
    Aug 26 06:27:04.999: INFO: Pod "pod-4f300bcb-343e-4765-82d1-5a3f5c302254": Phase="Pending", Reason="", readiness=false. Elapsed: 15.033946ms
    Aug 26 06:27:07.005: INFO: Pod "pod-4f300bcb-343e-4765-82d1-5a3f5c302254": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021228366s
    Aug 26 06:27:09.006: INFO: Pod "pod-4f300bcb-343e-4765-82d1-5a3f5c302254": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0218254s
    STEP: Saw pod success 08/26/23 06:27:09.006
    Aug 26 06:27:09.006: INFO: Pod "pod-4f300bcb-343e-4765-82d1-5a3f5c302254" satisfied condition "Succeeded or Failed"
    Aug 26 06:27:09.010: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-4f300bcb-343e-4765-82d1-5a3f5c302254 container test-container: <nil>
    STEP: delete the pod 08/26/23 06:27:09.031
    Aug 26 06:27:09.054: INFO: Waiting for pod pod-4f300bcb-343e-4765-82d1-5a3f5c302254 to disappear
    Aug 26 06:27:09.064: INFO: Pod pod-4f300bcb-343e-4765-82d1-5a3f5c302254 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:27:09.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8151" for this suite. 08/26/23 06:27:09.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:27:09.085
Aug 26 06:27:09.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 06:27:09.086
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:27:09.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:27:09.11
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-9f6acffc-07be-4842-8d14-2d5fdcd3ca0b 08/26/23 06:27:09.114
STEP: Creating a pod to test consume secrets 08/26/23 06:27:09.127
Aug 26 06:27:09.136: INFO: Waiting up to 5m0s for pod "pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4" in namespace "secrets-8868" to be "Succeeded or Failed"
Aug 26 06:27:09.139: INFO: Pod "pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369541ms
Aug 26 06:27:11.145: INFO: Pod "pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009064648s
Aug 26 06:27:13.144: INFO: Pod "pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008343726s
STEP: Saw pod success 08/26/23 06:27:13.144
Aug 26 06:27:13.145: INFO: Pod "pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4" satisfied condition "Succeeded or Failed"
Aug 26 06:27:13.156: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4 container secret-env-test: <nil>
STEP: delete the pod 08/26/23 06:27:13.167
Aug 26 06:27:13.186: INFO: Waiting for pod pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4 to disappear
Aug 26 06:27:13.189: INFO: Pod pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 06:27:13.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8868" for this suite. 08/26/23 06:27:13.197
------------------------------
• [4.125 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:27:09.085
    Aug 26 06:27:09.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 06:27:09.086
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:27:09.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:27:09.11
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-9f6acffc-07be-4842-8d14-2d5fdcd3ca0b 08/26/23 06:27:09.114
    STEP: Creating a pod to test consume secrets 08/26/23 06:27:09.127
    Aug 26 06:27:09.136: INFO: Waiting up to 5m0s for pod "pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4" in namespace "secrets-8868" to be "Succeeded or Failed"
    Aug 26 06:27:09.139: INFO: Pod "pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369541ms
    Aug 26 06:27:11.145: INFO: Pod "pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009064648s
    Aug 26 06:27:13.144: INFO: Pod "pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008343726s
    STEP: Saw pod success 08/26/23 06:27:13.144
    Aug 26 06:27:13.145: INFO: Pod "pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4" satisfied condition "Succeeded or Failed"
    Aug 26 06:27:13.156: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4 container secret-env-test: <nil>
    STEP: delete the pod 08/26/23 06:27:13.167
    Aug 26 06:27:13.186: INFO: Waiting for pod pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4 to disappear
    Aug 26 06:27:13.189: INFO: Pod pod-secrets-6565f694-2d59-4151-8b75-02d80e37aee4 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:27:13.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8868" for this suite. 08/26/23 06:27:13.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:27:13.211
Aug 26 06:27:13.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:27:13.212
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:27:13.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:27:13.236
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 08/26/23 06:27:13.238
Aug 26 06:27:13.249: INFO: Waiting up to 5m0s for pod "annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c" in namespace "projected-4913" to be "running and ready"
Aug 26 06:27:13.253: INFO: Pod "annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.988026ms
Aug 26 06:27:13.253: INFO: The phase of Pod annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:27:15.258: INFO: Pod "annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008843664s
Aug 26 06:27:15.258: INFO: The phase of Pod annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c is Running (Ready = true)
Aug 26 06:27:15.258: INFO: Pod "annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c" satisfied condition "running and ready"
Aug 26 06:27:15.787: INFO: Successfully updated pod "annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 06:27:19.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4913" for this suite. 08/26/23 06:27:19.826
------------------------------
• [SLOW TEST] [6.627 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:27:13.211
    Aug 26 06:27:13.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:27:13.212
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:27:13.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:27:13.236
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 08/26/23 06:27:13.238
    Aug 26 06:27:13.249: INFO: Waiting up to 5m0s for pod "annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c" in namespace "projected-4913" to be "running and ready"
    Aug 26 06:27:13.253: INFO: Pod "annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.988026ms
    Aug 26 06:27:13.253: INFO: The phase of Pod annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:27:15.258: INFO: Pod "annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008843664s
    Aug 26 06:27:15.258: INFO: The phase of Pod annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c is Running (Ready = true)
    Aug 26 06:27:15.258: INFO: Pod "annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c" satisfied condition "running and ready"
    Aug 26 06:27:15.787: INFO: Successfully updated pod "annotationupdate423a7049-e26b-423b-ae83-f3ae2016859c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:27:19.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4913" for this suite. 08/26/23 06:27:19.826
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:27:19.839
Aug 26 06:27:19.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-probe 08/26/23 06:27:19.843
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:27:19.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:27:19.876
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-9a8afe20-e81c-4339-b57e-a169e95bab29 in namespace container-probe-1728 08/26/23 06:27:19.882
Aug 26 06:27:19.903: INFO: Waiting up to 5m0s for pod "liveness-9a8afe20-e81c-4339-b57e-a169e95bab29" in namespace "container-probe-1728" to be "not pending"
Aug 26 06:27:19.920: INFO: Pod "liveness-9a8afe20-e81c-4339-b57e-a169e95bab29": Phase="Pending", Reason="", readiness=false. Elapsed: 16.30107ms
Aug 26 06:27:21.926: INFO: Pod "liveness-9a8afe20-e81c-4339-b57e-a169e95bab29": Phase="Running", Reason="", readiness=true. Elapsed: 2.022731086s
Aug 26 06:27:21.926: INFO: Pod "liveness-9a8afe20-e81c-4339-b57e-a169e95bab29" satisfied condition "not pending"
Aug 26 06:27:21.926: INFO: Started pod liveness-9a8afe20-e81c-4339-b57e-a169e95bab29 in namespace container-probe-1728
STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 06:27:21.926
Aug 26 06:27:21.930: INFO: Initial restart count of pod liveness-9a8afe20-e81c-4339-b57e-a169e95bab29 is 0
STEP: deleting the pod 08/26/23 06:31:22.702
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 26 06:31:22.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1728" for this suite. 08/26/23 06:31:22.745
------------------------------
• [SLOW TEST] [242.914 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:27:19.839
    Aug 26 06:27:19.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-probe 08/26/23 06:27:19.843
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:27:19.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:27:19.876
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-9a8afe20-e81c-4339-b57e-a169e95bab29 in namespace container-probe-1728 08/26/23 06:27:19.882
    Aug 26 06:27:19.903: INFO: Waiting up to 5m0s for pod "liveness-9a8afe20-e81c-4339-b57e-a169e95bab29" in namespace "container-probe-1728" to be "not pending"
    Aug 26 06:27:19.920: INFO: Pod "liveness-9a8afe20-e81c-4339-b57e-a169e95bab29": Phase="Pending", Reason="", readiness=false. Elapsed: 16.30107ms
    Aug 26 06:27:21.926: INFO: Pod "liveness-9a8afe20-e81c-4339-b57e-a169e95bab29": Phase="Running", Reason="", readiness=true. Elapsed: 2.022731086s
    Aug 26 06:27:21.926: INFO: Pod "liveness-9a8afe20-e81c-4339-b57e-a169e95bab29" satisfied condition "not pending"
    Aug 26 06:27:21.926: INFO: Started pod liveness-9a8afe20-e81c-4339-b57e-a169e95bab29 in namespace container-probe-1728
    STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 06:27:21.926
    Aug 26 06:27:21.930: INFO: Initial restart count of pod liveness-9a8afe20-e81c-4339-b57e-a169e95bab29 is 0
    STEP: deleting the pod 08/26/23 06:31:22.702
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:31:22.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1728" for this suite. 08/26/23 06:31:22.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:31:22.754
Aug 26 06:31:22.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename custom-resource-definition 08/26/23 06:31:22.755
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:22.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:22.781
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 08/26/23 06:31:22.784
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/26/23 06:31:22.786
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/26/23 06:31:22.786
STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/26/23 06:31:22.786
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/26/23 06:31:22.787
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/26/23 06:31:22.787
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/26/23 06:31:22.794
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:31:22.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-510" for this suite. 08/26/23 06:31:22.813
------------------------------
• [0.070 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:31:22.754
    Aug 26 06:31:22.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename custom-resource-definition 08/26/23 06:31:22.755
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:22.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:22.781
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 08/26/23 06:31:22.784
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/26/23 06:31:22.786
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/26/23 06:31:22.786
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/26/23 06:31:22.786
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/26/23 06:31:22.787
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/26/23 06:31:22.787
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/26/23 06:31:22.794
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:31:22.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-510" for this suite. 08/26/23 06:31:22.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:31:22.827
Aug 26 06:31:22.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 06:31:22.829
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:22.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:22.863
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-1897 08/26/23 06:31:22.866
STEP: creating service affinity-nodeport in namespace services-1897 08/26/23 06:31:22.867
STEP: creating replication controller affinity-nodeport in namespace services-1897 08/26/23 06:31:22.896
I0826 06:31:22.910091      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1897, replica count: 3
I0826 06:31:25.961163      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 26 06:31:25.978: INFO: Creating new exec pod
Aug 26 06:31:25.995: INFO: Waiting up to 5m0s for pod "execpod-affinity47knc" in namespace "services-1897" to be "running"
Aug 26 06:31:26.002: INFO: Pod "execpod-affinity47knc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.255893ms
Aug 26 06:31:28.029: INFO: Pod "execpod-affinity47knc": Phase="Running", Reason="", readiness=true. Elapsed: 2.033555858s
Aug 26 06:31:28.029: INFO: Pod "execpod-affinity47knc" satisfied condition "running"
Aug 26 06:31:29.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-1897 exec execpod-affinity47knc -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Aug 26 06:31:29.236: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 26 06:31:29.236: INFO: stdout: ""
Aug 26 06:31:29.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-1897 exec execpod-affinity47knc -- /bin/sh -x -c nc -v -z -w 2 10.21.1.183 80'
Aug 26 06:31:29.395: INFO: stderr: "+ nc -v -z -w 2 10.21.1.183 80\nConnection to 10.21.1.183 80 port [tcp/http] succeeded!\n"
Aug 26 06:31:29.395: INFO: stdout: ""
Aug 26 06:31:29.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-1897 exec execpod-affinity47knc -- /bin/sh -x -c nc -v -z -w 2 10.0.1.5 30126'
Aug 26 06:31:29.550: INFO: stderr: "+ nc -v -z -w 2 10.0.1.5 30126\nConnection to 10.0.1.5 30126 port [tcp/*] succeeded!\n"
Aug 26 06:31:29.550: INFO: stdout: ""
Aug 26 06:31:29.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-1897 exec execpod-affinity47knc -- /bin/sh -x -c nc -v -z -w 2 10.0.1.31 30126'
Aug 26 06:31:29.725: INFO: stderr: "+ nc -v -z -w 2 10.0.1.31 30126\nConnection to 10.0.1.31 30126 port [tcp/*] succeeded!\n"
Aug 26 06:31:29.725: INFO: stdout: ""
Aug 26 06:31:29.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-1897 exec execpod-affinity47knc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.101:30126/ ; done'
Aug 26 06:31:29.980: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n"
Aug 26 06:31:29.980: INFO: stdout: "\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6"
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
Aug 26 06:31:29.980: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1897, will wait for the garbage collector to delete the pods 08/26/23 06:31:29.999
Aug 26 06:31:30.076: INFO: Deleting ReplicationController affinity-nodeport took: 8.90432ms
Aug 26 06:31:30.177: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.056302ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 06:31:32.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1897" for this suite. 08/26/23 06:31:32.512
------------------------------
• [SLOW TEST] [9.696 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:31:22.827
    Aug 26 06:31:22.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 06:31:22.829
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:22.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:22.863
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-1897 08/26/23 06:31:22.866
    STEP: creating service affinity-nodeport in namespace services-1897 08/26/23 06:31:22.867
    STEP: creating replication controller affinity-nodeport in namespace services-1897 08/26/23 06:31:22.896
    I0826 06:31:22.910091      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1897, replica count: 3
    I0826 06:31:25.961163      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 26 06:31:25.978: INFO: Creating new exec pod
    Aug 26 06:31:25.995: INFO: Waiting up to 5m0s for pod "execpod-affinity47knc" in namespace "services-1897" to be "running"
    Aug 26 06:31:26.002: INFO: Pod "execpod-affinity47knc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.255893ms
    Aug 26 06:31:28.029: INFO: Pod "execpod-affinity47knc": Phase="Running", Reason="", readiness=true. Elapsed: 2.033555858s
    Aug 26 06:31:28.029: INFO: Pod "execpod-affinity47knc" satisfied condition "running"
    Aug 26 06:31:29.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-1897 exec execpod-affinity47knc -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Aug 26 06:31:29.236: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Aug 26 06:31:29.236: INFO: stdout: ""
    Aug 26 06:31:29.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-1897 exec execpod-affinity47knc -- /bin/sh -x -c nc -v -z -w 2 10.21.1.183 80'
    Aug 26 06:31:29.395: INFO: stderr: "+ nc -v -z -w 2 10.21.1.183 80\nConnection to 10.21.1.183 80 port [tcp/http] succeeded!\n"
    Aug 26 06:31:29.395: INFO: stdout: ""
    Aug 26 06:31:29.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-1897 exec execpod-affinity47knc -- /bin/sh -x -c nc -v -z -w 2 10.0.1.5 30126'
    Aug 26 06:31:29.550: INFO: stderr: "+ nc -v -z -w 2 10.0.1.5 30126\nConnection to 10.0.1.5 30126 port [tcp/*] succeeded!\n"
    Aug 26 06:31:29.550: INFO: stdout: ""
    Aug 26 06:31:29.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-1897 exec execpod-affinity47knc -- /bin/sh -x -c nc -v -z -w 2 10.0.1.31 30126'
    Aug 26 06:31:29.725: INFO: stderr: "+ nc -v -z -w 2 10.0.1.31 30126\nConnection to 10.0.1.31 30126 port [tcp/*] succeeded!\n"
    Aug 26 06:31:29.725: INFO: stdout: ""
    Aug 26 06:31:29.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-1897 exec execpod-affinity47knc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.101:30126/ ; done'
    Aug 26 06:31:29.980: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.101:30126/\n"
    Aug 26 06:31:29.980: INFO: stdout: "\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6\naffinity-nodeport-459q6"
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Received response from host: affinity-nodeport-459q6
    Aug 26 06:31:29.980: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1897, will wait for the garbage collector to delete the pods 08/26/23 06:31:29.999
    Aug 26 06:31:30.076: INFO: Deleting ReplicationController affinity-nodeport took: 8.90432ms
    Aug 26 06:31:30.177: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.056302ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:31:32.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1897" for this suite. 08/26/23 06:31:32.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:31:32.522
Aug 26 06:31:32.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:31:32.523
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:32.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:32.542
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/26/23 06:31:32.545
Aug 26 06:31:32.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:31:34.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:31:43.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2109" for this suite. 08/26/23 06:31:43.363
------------------------------
• [SLOW TEST] [10.847 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:31:32.522
    Aug 26 06:31:32.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:31:32.523
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:32.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:32.542
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/26/23 06:31:32.545
    Aug 26 06:31:32.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:31:34.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:31:43.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2109" for this suite. 08/26/23 06:31:43.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:31:43.37
Aug 26 06:31:43.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename events 08/26/23 06:31:43.371
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:43.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:43.396
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 08/26/23 06:31:43.4
Aug 26 06:31:43.411: INFO: created test-event-1
Aug 26 06:31:43.419: INFO: created test-event-2
Aug 26 06:31:43.427: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 08/26/23 06:31:43.427
STEP: delete collection of events 08/26/23 06:31:43.431
Aug 26 06:31:43.431: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/26/23 06:31:43.459
Aug 26 06:31:43.460: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 26 06:31:43.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4722" for this suite. 08/26/23 06:31:43.471
------------------------------
• [0.106 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:31:43.37
    Aug 26 06:31:43.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename events 08/26/23 06:31:43.371
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:43.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:43.396
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 08/26/23 06:31:43.4
    Aug 26 06:31:43.411: INFO: created test-event-1
    Aug 26 06:31:43.419: INFO: created test-event-2
    Aug 26 06:31:43.427: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 08/26/23 06:31:43.427
    STEP: delete collection of events 08/26/23 06:31:43.431
    Aug 26 06:31:43.431: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/26/23 06:31:43.459
    Aug 26 06:31:43.460: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:31:43.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4722" for this suite. 08/26/23 06:31:43.471
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:31:43.477
Aug 26 06:31:43.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-runtime 08/26/23 06:31:43.478
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:43.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:43.496
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 08/26/23 06:31:43.498
STEP: wait for the container to reach Succeeded 08/26/23 06:31:43.507
STEP: get the container status 08/26/23 06:31:47.528
STEP: the container should be terminated 08/26/23 06:31:47.535
STEP: the termination message should be set 08/26/23 06:31:47.535
Aug 26 06:31:47.535: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 08/26/23 06:31:47.535
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 26 06:31:47.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2884" for this suite. 08/26/23 06:31:47.563
------------------------------
• [4.094 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:31:43.477
    Aug 26 06:31:43.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-runtime 08/26/23 06:31:43.478
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:43.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:43.496
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 08/26/23 06:31:43.498
    STEP: wait for the container to reach Succeeded 08/26/23 06:31:43.507
    STEP: get the container status 08/26/23 06:31:47.528
    STEP: the container should be terminated 08/26/23 06:31:47.535
    STEP: the termination message should be set 08/26/23 06:31:47.535
    Aug 26 06:31:47.535: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 08/26/23 06:31:47.535
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:31:47.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2884" for this suite. 08/26/23 06:31:47.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:31:47.577
Aug 26 06:31:47.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 06:31:47.58
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:47.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:47.6
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-8911 08/26/23 06:31:47.603
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8911 to expose endpoints map[] 08/26/23 06:31:47.615
Aug 26 06:31:47.625: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Aug 26 06:31:48.632: INFO: successfully validated that service endpoint-test2 in namespace services-8911 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8911 08/26/23 06:31:48.632
Aug 26 06:31:48.646: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8911" to be "running and ready"
Aug 26 06:31:48.651: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617485ms
Aug 26 06:31:48.651: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:31:50.657: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010704694s
Aug 26 06:31:50.657: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 26 06:31:50.657: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8911 to expose endpoints map[pod1:[80]] 08/26/23 06:31:50.661
Aug 26 06:31:50.675: INFO: successfully validated that service endpoint-test2 in namespace services-8911 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 08/26/23 06:31:50.675
Aug 26 06:31:50.675: INFO: Creating new exec pod
Aug 26 06:31:50.681: INFO: Waiting up to 5m0s for pod "execpod96wbd" in namespace "services-8911" to be "running"
Aug 26 06:31:50.685: INFO: Pod "execpod96wbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278897ms
Aug 26 06:31:52.691: INFO: Pod "execpod96wbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010330721s
Aug 26 06:31:52.691: INFO: Pod "execpod96wbd" satisfied condition "running"
Aug 26 06:31:53.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 26 06:31:53.854: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 26 06:31:53.854: INFO: stdout: ""
Aug 26 06:31:53.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 10.21.234.28 80'
Aug 26 06:31:54.034: INFO: stderr: "+ nc -v -z -w 2 10.21.234.28 80\nConnection to 10.21.234.28 80 port [tcp/http] succeeded!\n"
Aug 26 06:31:54.034: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-8911 08/26/23 06:31:54.034
Aug 26 06:31:54.044: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8911" to be "running and ready"
Aug 26 06:31:54.051: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.09481ms
Aug 26 06:31:54.051: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:31:56.058: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013780377s
Aug 26 06:31:56.058: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 26 06:31:56.058: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8911 to expose endpoints map[pod1:[80] pod2:[80]] 08/26/23 06:31:56.067
Aug 26 06:31:56.089: INFO: successfully validated that service endpoint-test2 in namespace services-8911 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 08/26/23 06:31:56.089
Aug 26 06:31:57.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 26 06:31:59.775: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 26 06:31:59.775: INFO: stdout: ""
Aug 26 06:31:59.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 10.21.234.28 80'
Aug 26 06:31:59.965: INFO: stderr: "+ nc -v -z -w 2 10.21.234.28 80\nConnection to 10.21.234.28 80 port [tcp/http] succeeded!\n"
Aug 26 06:31:59.965: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-8911 08/26/23 06:31:59.965
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8911 to expose endpoints map[pod2:[80]] 08/26/23 06:31:59.981
Aug 26 06:31:59.995: INFO: successfully validated that service endpoint-test2 in namespace services-8911 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 08/26/23 06:31:59.996
Aug 26 06:32:00.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 26 06:32:01.253: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 26 06:32:01.253: INFO: stdout: ""
Aug 26 06:32:01.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 10.21.234.28 80'
Aug 26 06:32:01.435: INFO: stderr: "+ nc -v -z -w 2 10.21.234.28 80\nConnection to 10.21.234.28 80 port [tcp/http] succeeded!\n"
Aug 26 06:32:01.435: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-8911 08/26/23 06:32:01.435
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8911 to expose endpoints map[] 08/26/23 06:32:01.464
Aug 26 06:32:01.477: INFO: successfully validated that service endpoint-test2 in namespace services-8911 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 06:32:01.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8911" for this suite. 08/26/23 06:32:01.528
------------------------------
• [SLOW TEST] [13.958 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:31:47.577
    Aug 26 06:31:47.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 06:31:47.58
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:31:47.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:31:47.6
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-8911 08/26/23 06:31:47.603
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8911 to expose endpoints map[] 08/26/23 06:31:47.615
    Aug 26 06:31:47.625: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Aug 26 06:31:48.632: INFO: successfully validated that service endpoint-test2 in namespace services-8911 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8911 08/26/23 06:31:48.632
    Aug 26 06:31:48.646: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8911" to be "running and ready"
    Aug 26 06:31:48.651: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617485ms
    Aug 26 06:31:48.651: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:31:50.657: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010704694s
    Aug 26 06:31:50.657: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 26 06:31:50.657: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8911 to expose endpoints map[pod1:[80]] 08/26/23 06:31:50.661
    Aug 26 06:31:50.675: INFO: successfully validated that service endpoint-test2 in namespace services-8911 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 08/26/23 06:31:50.675
    Aug 26 06:31:50.675: INFO: Creating new exec pod
    Aug 26 06:31:50.681: INFO: Waiting up to 5m0s for pod "execpod96wbd" in namespace "services-8911" to be "running"
    Aug 26 06:31:50.685: INFO: Pod "execpod96wbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278897ms
    Aug 26 06:31:52.691: INFO: Pod "execpod96wbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010330721s
    Aug 26 06:31:52.691: INFO: Pod "execpod96wbd" satisfied condition "running"
    Aug 26 06:31:53.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 26 06:31:53.854: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 26 06:31:53.854: INFO: stdout: ""
    Aug 26 06:31:53.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 10.21.234.28 80'
    Aug 26 06:31:54.034: INFO: stderr: "+ nc -v -z -w 2 10.21.234.28 80\nConnection to 10.21.234.28 80 port [tcp/http] succeeded!\n"
    Aug 26 06:31:54.034: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-8911 08/26/23 06:31:54.034
    Aug 26 06:31:54.044: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8911" to be "running and ready"
    Aug 26 06:31:54.051: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.09481ms
    Aug 26 06:31:54.051: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:31:56.058: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013780377s
    Aug 26 06:31:56.058: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 26 06:31:56.058: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8911 to expose endpoints map[pod1:[80] pod2:[80]] 08/26/23 06:31:56.067
    Aug 26 06:31:56.089: INFO: successfully validated that service endpoint-test2 in namespace services-8911 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 08/26/23 06:31:56.089
    Aug 26 06:31:57.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 26 06:31:59.775: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 26 06:31:59.775: INFO: stdout: ""
    Aug 26 06:31:59.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 10.21.234.28 80'
    Aug 26 06:31:59.965: INFO: stderr: "+ nc -v -z -w 2 10.21.234.28 80\nConnection to 10.21.234.28 80 port [tcp/http] succeeded!\n"
    Aug 26 06:31:59.965: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-8911 08/26/23 06:31:59.965
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8911 to expose endpoints map[pod2:[80]] 08/26/23 06:31:59.981
    Aug 26 06:31:59.995: INFO: successfully validated that service endpoint-test2 in namespace services-8911 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 08/26/23 06:31:59.996
    Aug 26 06:32:00.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 26 06:32:01.253: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 26 06:32:01.253: INFO: stdout: ""
    Aug 26 06:32:01.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8911 exec execpod96wbd -- /bin/sh -x -c nc -v -z -w 2 10.21.234.28 80'
    Aug 26 06:32:01.435: INFO: stderr: "+ nc -v -z -w 2 10.21.234.28 80\nConnection to 10.21.234.28 80 port [tcp/http] succeeded!\n"
    Aug 26 06:32:01.435: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-8911 08/26/23 06:32:01.435
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8911 to expose endpoints map[] 08/26/23 06:32:01.464
    Aug 26 06:32:01.477: INFO: successfully validated that service endpoint-test2 in namespace services-8911 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:32:01.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8911" for this suite. 08/26/23 06:32:01.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:32:01.535
Aug 26 06:32:01.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename var-expansion 08/26/23 06:32:01.537
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:01.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:01.553
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 08/26/23 06:32:01.556
STEP: waiting for pod running 08/26/23 06:32:01.575
Aug 26 06:32:01.575: INFO: Waiting up to 2m0s for pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" in namespace "var-expansion-2535" to be "running"
Aug 26 06:32:01.584: INFO: Pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.44626ms
Aug 26 06:32:03.589: INFO: Pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014174595s
Aug 26 06:32:03.589: INFO: Pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" satisfied condition "running"
STEP: creating a file in subpath 08/26/23 06:32:03.589
Aug 26 06:32:03.594: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2535 PodName:var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:32:03.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:32:03.595: INFO: ExecWithOptions: Clientset creation
Aug 26 06:32:03.595: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/var-expansion-2535/pods/var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 08/26/23 06:32:03.7
Aug 26 06:32:03.705: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2535 PodName:var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:32:03.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:32:03.705: INFO: ExecWithOptions: Clientset creation
Aug 26 06:32:03.706: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/var-expansion-2535/pods/var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 08/26/23 06:32:03.801
Aug 26 06:32:04.315: INFO: Successfully updated pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1"
STEP: waiting for annotated pod running 08/26/23 06:32:04.315
Aug 26 06:32:04.315: INFO: Waiting up to 2m0s for pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" in namespace "var-expansion-2535" to be "running"
Aug 26 06:32:04.318: INFO: Pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.924201ms
Aug 26 06:32:04.318: INFO: Pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" satisfied condition "running"
STEP: deleting the pod gracefully 08/26/23 06:32:04.318
Aug 26 06:32:04.319: INFO: Deleting pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" in namespace "var-expansion-2535"
Aug 26 06:32:04.329: INFO: Wait up to 5m0s for pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 26 06:32:38.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2535" for this suite. 08/26/23 06:32:38.359
------------------------------
• [SLOW TEST] [36.832 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:32:01.535
    Aug 26 06:32:01.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename var-expansion 08/26/23 06:32:01.537
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:01.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:01.553
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 08/26/23 06:32:01.556
    STEP: waiting for pod running 08/26/23 06:32:01.575
    Aug 26 06:32:01.575: INFO: Waiting up to 2m0s for pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" in namespace "var-expansion-2535" to be "running"
    Aug 26 06:32:01.584: INFO: Pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.44626ms
    Aug 26 06:32:03.589: INFO: Pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014174595s
    Aug 26 06:32:03.589: INFO: Pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" satisfied condition "running"
    STEP: creating a file in subpath 08/26/23 06:32:03.589
    Aug 26 06:32:03.594: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2535 PodName:var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:32:03.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:32:03.595: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:32:03.595: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/var-expansion-2535/pods/var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 08/26/23 06:32:03.7
    Aug 26 06:32:03.705: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2535 PodName:var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:32:03.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:32:03.705: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:32:03.706: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/var-expansion-2535/pods/var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 08/26/23 06:32:03.801
    Aug 26 06:32:04.315: INFO: Successfully updated pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1"
    STEP: waiting for annotated pod running 08/26/23 06:32:04.315
    Aug 26 06:32:04.315: INFO: Waiting up to 2m0s for pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" in namespace "var-expansion-2535" to be "running"
    Aug 26 06:32:04.318: INFO: Pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.924201ms
    Aug 26 06:32:04.318: INFO: Pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" satisfied condition "running"
    STEP: deleting the pod gracefully 08/26/23 06:32:04.318
    Aug 26 06:32:04.319: INFO: Deleting pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" in namespace "var-expansion-2535"
    Aug 26 06:32:04.329: INFO: Wait up to 5m0s for pod "var-expansion-a9ac82cb-8fac-4cd7-9b18-e91a49debdd1" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:32:38.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2535" for this suite. 08/26/23 06:32:38.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:32:38.387
Aug 26 06:32:38.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename var-expansion 08/26/23 06:32:38.39
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:38.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:38.423
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 08/26/23 06:32:38.425
Aug 26 06:32:38.434: INFO: Waiting up to 5m0s for pod "var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe" in namespace "var-expansion-7145" to be "Succeeded or Failed"
Aug 26 06:32:38.436: INFO: Pod "var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.909285ms
Aug 26 06:32:40.441: INFO: Pod "var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007145084s
Aug 26 06:32:42.443: INFO: Pod "var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009349804s
STEP: Saw pod success 08/26/23 06:32:42.443
Aug 26 06:32:42.443: INFO: Pod "var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe" satisfied condition "Succeeded or Failed"
Aug 26 06:32:42.447: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe container dapi-container: <nil>
STEP: delete the pod 08/26/23 06:32:42.472
Aug 26 06:32:42.491: INFO: Waiting for pod var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe to disappear
Aug 26 06:32:42.494: INFO: Pod var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 26 06:32:42.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7145" for this suite. 08/26/23 06:32:42.504
------------------------------
• [4.125 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:32:38.387
    Aug 26 06:32:38.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename var-expansion 08/26/23 06:32:38.39
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:38.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:38.423
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 08/26/23 06:32:38.425
    Aug 26 06:32:38.434: INFO: Waiting up to 5m0s for pod "var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe" in namespace "var-expansion-7145" to be "Succeeded or Failed"
    Aug 26 06:32:38.436: INFO: Pod "var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.909285ms
    Aug 26 06:32:40.441: INFO: Pod "var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007145084s
    Aug 26 06:32:42.443: INFO: Pod "var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009349804s
    STEP: Saw pod success 08/26/23 06:32:42.443
    Aug 26 06:32:42.443: INFO: Pod "var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe" satisfied condition "Succeeded or Failed"
    Aug 26 06:32:42.447: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe container dapi-container: <nil>
    STEP: delete the pod 08/26/23 06:32:42.472
    Aug 26 06:32:42.491: INFO: Waiting for pod var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe to disappear
    Aug 26 06:32:42.494: INFO: Pod var-expansion-899abeca-5fa4-4fec-9ab1-79ec6be33dbe no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:32:42.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7145" for this suite. 08/26/23 06:32:42.504
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:32:42.512
Aug 26 06:32:42.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir 08/26/23 06:32:42.514
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:42.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:42.54
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/26/23 06:32:42.543
Aug 26 06:32:42.555: INFO: Waiting up to 5m0s for pod "pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b" in namespace "emptydir-8917" to be "Succeeded or Failed"
Aug 26 06:32:42.560: INFO: Pod "pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.342647ms
Aug 26 06:32:44.564: INFO: Pod "pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009428637s
Aug 26 06:32:46.565: INFO: Pod "pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009877819s
STEP: Saw pod success 08/26/23 06:32:46.565
Aug 26 06:32:46.565: INFO: Pod "pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b" satisfied condition "Succeeded or Failed"
Aug 26 06:32:46.568: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b container test-container: <nil>
STEP: delete the pod 08/26/23 06:32:46.578
Aug 26 06:32:46.593: INFO: Waiting for pod pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b to disappear
Aug 26 06:32:46.599: INFO: Pod pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 06:32:46.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8917" for this suite. 08/26/23 06:32:46.605
------------------------------
• [4.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:32:42.512
    Aug 26 06:32:42.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir 08/26/23 06:32:42.514
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:42.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:42.54
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/26/23 06:32:42.543
    Aug 26 06:32:42.555: INFO: Waiting up to 5m0s for pod "pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b" in namespace "emptydir-8917" to be "Succeeded or Failed"
    Aug 26 06:32:42.560: INFO: Pod "pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.342647ms
    Aug 26 06:32:44.564: INFO: Pod "pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009428637s
    Aug 26 06:32:46.565: INFO: Pod "pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009877819s
    STEP: Saw pod success 08/26/23 06:32:46.565
    Aug 26 06:32:46.565: INFO: Pod "pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b" satisfied condition "Succeeded or Failed"
    Aug 26 06:32:46.568: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b container test-container: <nil>
    STEP: delete the pod 08/26/23 06:32:46.578
    Aug 26 06:32:46.593: INFO: Waiting for pod pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b to disappear
    Aug 26 06:32:46.599: INFO: Pod pod-e5f2f41f-b104-45a6-ae6b-87f8a7556f0b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:32:46.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8917" for this suite. 08/26/23 06:32:46.605
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:32:46.614
Aug 26 06:32:46.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename limitrange 08/26/23 06:32:46.615
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:46.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:46.633
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 08/26/23 06:32:46.636
STEP: Setting up watch 08/26/23 06:32:46.636
STEP: Submitting a LimitRange 08/26/23 06:32:46.743
STEP: Verifying LimitRange creation was observed 08/26/23 06:32:46.748
STEP: Fetching the LimitRange to ensure it has proper values 08/26/23 06:32:46.748
Aug 26 06:32:46.751: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 26 06:32:46.751: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 08/26/23 06:32:46.751
STEP: Ensuring Pod has resource requirements applied from LimitRange 08/26/23 06:32:46.758
Aug 26 06:32:46.762: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 26 06:32:46.762: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 08/26/23 06:32:46.762
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/26/23 06:32:46.776
Aug 26 06:32:46.780: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 26 06:32:46.780: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 08/26/23 06:32:46.78
STEP: Failing to create a Pod with more than max resources 08/26/23 06:32:46.782
STEP: Updating a LimitRange 08/26/23 06:32:46.784
STEP: Verifying LimitRange updating is effective 08/26/23 06:32:46.79
STEP: Creating a Pod with less than former min resources 08/26/23 06:32:48.794
STEP: Failing to create a Pod with more than max resources 08/26/23 06:32:48.8
STEP: Deleting a LimitRange 08/26/23 06:32:48.803
STEP: Verifying the LimitRange was deleted 08/26/23 06:32:48.819
Aug 26 06:32:53.823: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 08/26/23 06:32:53.823
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 26 06:32:53.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-6293" for this suite. 08/26/23 06:32:53.854
------------------------------
• [SLOW TEST] [7.247 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:32:46.614
    Aug 26 06:32:46.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename limitrange 08/26/23 06:32:46.615
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:46.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:46.633
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 08/26/23 06:32:46.636
    STEP: Setting up watch 08/26/23 06:32:46.636
    STEP: Submitting a LimitRange 08/26/23 06:32:46.743
    STEP: Verifying LimitRange creation was observed 08/26/23 06:32:46.748
    STEP: Fetching the LimitRange to ensure it has proper values 08/26/23 06:32:46.748
    Aug 26 06:32:46.751: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 26 06:32:46.751: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 08/26/23 06:32:46.751
    STEP: Ensuring Pod has resource requirements applied from LimitRange 08/26/23 06:32:46.758
    Aug 26 06:32:46.762: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 26 06:32:46.762: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 08/26/23 06:32:46.762
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/26/23 06:32:46.776
    Aug 26 06:32:46.780: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Aug 26 06:32:46.780: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 08/26/23 06:32:46.78
    STEP: Failing to create a Pod with more than max resources 08/26/23 06:32:46.782
    STEP: Updating a LimitRange 08/26/23 06:32:46.784
    STEP: Verifying LimitRange updating is effective 08/26/23 06:32:46.79
    STEP: Creating a Pod with less than former min resources 08/26/23 06:32:48.794
    STEP: Failing to create a Pod with more than max resources 08/26/23 06:32:48.8
    STEP: Deleting a LimitRange 08/26/23 06:32:48.803
    STEP: Verifying the LimitRange was deleted 08/26/23 06:32:48.819
    Aug 26 06:32:53.823: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 08/26/23 06:32:53.823
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:32:53.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-6293" for this suite. 08/26/23 06:32:53.854
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:32:53.862
Aug 26 06:32:53.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 06:32:53.863
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:53.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:53.895
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-8d32ae52-34c4-49b5-8f58-7d3a4e0e8248 08/26/23 06:32:53.899
STEP: Creating a pod to test consume secrets 08/26/23 06:32:53.905
Aug 26 06:32:53.914: INFO: Waiting up to 5m0s for pod "pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9" in namespace "secrets-7637" to be "Succeeded or Failed"
Aug 26 06:32:53.917: INFO: Pod "pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.171101ms
Aug 26 06:32:55.922: INFO: Pod "pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007638839s
Aug 26 06:32:57.925: INFO: Pod "pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010393711s
STEP: Saw pod success 08/26/23 06:32:57.925
Aug 26 06:32:57.925: INFO: Pod "pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9" satisfied condition "Succeeded or Failed"
Aug 26 06:32:57.929: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9 container secret-volume-test: <nil>
STEP: delete the pod 08/26/23 06:32:57.937
Aug 26 06:32:57.949: INFO: Waiting for pod pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9 to disappear
Aug 26 06:32:57.952: INFO: Pod pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 06:32:57.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7637" for this suite. 08/26/23 06:32:57.959
------------------------------
• [4.105 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:32:53.862
    Aug 26 06:32:53.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 06:32:53.863
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:53.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:53.895
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-8d32ae52-34c4-49b5-8f58-7d3a4e0e8248 08/26/23 06:32:53.899
    STEP: Creating a pod to test consume secrets 08/26/23 06:32:53.905
    Aug 26 06:32:53.914: INFO: Waiting up to 5m0s for pod "pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9" in namespace "secrets-7637" to be "Succeeded or Failed"
    Aug 26 06:32:53.917: INFO: Pod "pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.171101ms
    Aug 26 06:32:55.922: INFO: Pod "pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007638839s
    Aug 26 06:32:57.925: INFO: Pod "pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010393711s
    STEP: Saw pod success 08/26/23 06:32:57.925
    Aug 26 06:32:57.925: INFO: Pod "pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9" satisfied condition "Succeeded or Failed"
    Aug 26 06:32:57.929: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9 container secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 06:32:57.937
    Aug 26 06:32:57.949: INFO: Waiting for pod pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9 to disappear
    Aug 26 06:32:57.952: INFO: Pod pod-secrets-5289ae90-626f-4a3a-aba0-1d8cec045cd9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:32:57.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7637" for this suite. 08/26/23 06:32:57.959
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:32:57.97
Aug 26 06:32:57.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename containers 08/26/23 06:32:57.971
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:57.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:57.989
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 08/26/23 06:32:57.995
Aug 26 06:32:58.011: INFO: Waiting up to 5m0s for pod "client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d" in namespace "containers-4876" to be "Succeeded or Failed"
Aug 26 06:32:58.015: INFO: Pod "client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.787728ms
Aug 26 06:33:00.020: INFO: Pod "client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d": Phase="Running", Reason="", readiness=false. Elapsed: 2.009660302s
Aug 26 06:33:02.021: INFO: Pod "client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010800311s
STEP: Saw pod success 08/26/23 06:33:02.021
Aug 26 06:33:02.022: INFO: Pod "client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d" satisfied condition "Succeeded or Failed"
Aug 26 06:33:02.025: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d container agnhost-container: <nil>
STEP: delete the pod 08/26/23 06:33:02.034
Aug 26 06:33:02.050: INFO: Waiting for pod client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d to disappear
Aug 26 06:33:02.054: INFO: Pod client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 26 06:33:02.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4876" for this suite. 08/26/23 06:33:02.064
------------------------------
• [4.108 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:32:57.97
    Aug 26 06:32:57.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename containers 08/26/23 06:32:57.971
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:32:57.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:32:57.989
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 08/26/23 06:32:57.995
    Aug 26 06:32:58.011: INFO: Waiting up to 5m0s for pod "client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d" in namespace "containers-4876" to be "Succeeded or Failed"
    Aug 26 06:32:58.015: INFO: Pod "client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.787728ms
    Aug 26 06:33:00.020: INFO: Pod "client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d": Phase="Running", Reason="", readiness=false. Elapsed: 2.009660302s
    Aug 26 06:33:02.021: INFO: Pod "client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010800311s
    STEP: Saw pod success 08/26/23 06:33:02.021
    Aug 26 06:33:02.022: INFO: Pod "client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d" satisfied condition "Succeeded or Failed"
    Aug 26 06:33:02.025: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 06:33:02.034
    Aug 26 06:33:02.050: INFO: Waiting for pod client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d to disappear
    Aug 26 06:33:02.054: INFO: Pod client-containers-1eb8c9e9-469b-4e97-baa4-184aec08241d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:33:02.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4876" for this suite. 08/26/23 06:33:02.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:33:02.078
Aug 26 06:33:02.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-probe 08/26/23 06:33:02.079
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:33:02.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:33:02.098
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194 in namespace container-probe-9180 08/26/23 06:33:02.101
Aug 26 06:33:02.111: INFO: Waiting up to 5m0s for pod "busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194" in namespace "container-probe-9180" to be "not pending"
Aug 26 06:33:02.114: INFO: Pod "busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396723ms
Aug 26 06:33:04.118: INFO: Pod "busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194": Phase="Running", Reason="", readiness=true. Elapsed: 2.007808437s
Aug 26 06:33:04.118: INFO: Pod "busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194" satisfied condition "not pending"
Aug 26 06:33:04.118: INFO: Started pod busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194 in namespace container-probe-9180
STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 06:33:04.118
Aug 26 06:33:04.122: INFO: Initial restart count of pod busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194 is 0
Aug 26 06:33:54.254: INFO: Restart count of pod container-probe-9180/busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194 is now 1 (50.132237367s elapsed)
STEP: deleting the pod 08/26/23 06:33:54.254
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 26 06:33:54.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9180" for this suite. 08/26/23 06:33:54.275
------------------------------
• [SLOW TEST] [52.203 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:33:02.078
    Aug 26 06:33:02.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-probe 08/26/23 06:33:02.079
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:33:02.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:33:02.098
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194 in namespace container-probe-9180 08/26/23 06:33:02.101
    Aug 26 06:33:02.111: INFO: Waiting up to 5m0s for pod "busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194" in namespace "container-probe-9180" to be "not pending"
    Aug 26 06:33:02.114: INFO: Pod "busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396723ms
    Aug 26 06:33:04.118: INFO: Pod "busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194": Phase="Running", Reason="", readiness=true. Elapsed: 2.007808437s
    Aug 26 06:33:04.118: INFO: Pod "busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194" satisfied condition "not pending"
    Aug 26 06:33:04.118: INFO: Started pod busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194 in namespace container-probe-9180
    STEP: checking the pod's current state and verifying that restartCount is present 08/26/23 06:33:04.118
    Aug 26 06:33:04.122: INFO: Initial restart count of pod busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194 is 0
    Aug 26 06:33:54.254: INFO: Restart count of pod container-probe-9180/busybox-a0a7637d-68bf-4e2e-a0f7-c08b455a3194 is now 1 (50.132237367s elapsed)
    STEP: deleting the pod 08/26/23 06:33:54.254
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:33:54.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9180" for this suite. 08/26/23 06:33:54.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:33:54.284
Aug 26 06:33:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replicaset 08/26/23 06:33:54.286
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:33:54.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:33:54.303
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 08/26/23 06:33:54.305
STEP: Verify that the required pods have come up 08/26/23 06:33:54.31
Aug 26 06:33:54.313: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 26 06:33:59.320: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 08/26/23 06:33:59.32
Aug 26 06:33:59.323: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 08/26/23 06:33:59.323
STEP: DeleteCollection of the ReplicaSets 08/26/23 06:33:59.33
STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/26/23 06:33:59.34
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 26 06:33:59.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6201" for this suite. 08/26/23 06:33:59.375
------------------------------
• [SLOW TEST] [5.104 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:33:54.284
    Aug 26 06:33:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replicaset 08/26/23 06:33:54.286
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:33:54.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:33:54.303
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 08/26/23 06:33:54.305
    STEP: Verify that the required pods have come up 08/26/23 06:33:54.31
    Aug 26 06:33:54.313: INFO: Pod name sample-pod: Found 0 pods out of 3
    Aug 26 06:33:59.320: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 08/26/23 06:33:59.32
    Aug 26 06:33:59.323: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 08/26/23 06:33:59.323
    STEP: DeleteCollection of the ReplicaSets 08/26/23 06:33:59.33
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/26/23 06:33:59.34
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:33:59.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6201" for this suite. 08/26/23 06:33:59.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:33:59.389
Aug 26 06:33:59.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename gc 08/26/23 06:33:59.39
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:33:59.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:33:59.416
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 08/26/23 06:33:59.419
STEP: Wait for the Deployment to create new ReplicaSet 08/26/23 06:33:59.425
STEP: delete the deployment 08/26/23 06:33:59.936
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/26/23 06:33:59.96
STEP: Gathering metrics 08/26/23 06:34:00.497
W0826 06:34:00.529786      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 26 06:34:00.529: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:00.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9185" for this suite. 08/26/23 06:34:00.543
------------------------------
• [1.165 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:33:59.389
    Aug 26 06:33:59.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename gc 08/26/23 06:33:59.39
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:33:59.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:33:59.416
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 08/26/23 06:33:59.419
    STEP: Wait for the Deployment to create new ReplicaSet 08/26/23 06:33:59.425
    STEP: delete the deployment 08/26/23 06:33:59.936
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/26/23 06:33:59.96
    STEP: Gathering metrics 08/26/23 06:34:00.497
    W0826 06:34:00.529786      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 26 06:34:00.529: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:00.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9185" for this suite. 08/26/23 06:34:00.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:00.556
Aug 26 06:34:00.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename custom-resource-definition 08/26/23 06:34:00.557
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:00.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:00.601
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Aug 26 06:34:00.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:01.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7756" for this suite. 08/26/23 06:34:01.65
------------------------------
• [1.105 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:00.556
    Aug 26 06:34:00.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename custom-resource-definition 08/26/23 06:34:00.557
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:00.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:00.601
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Aug 26 06:34:00.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:01.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7756" for this suite. 08/26/23 06:34:01.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:01.664
Aug 26 06:34:01.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename disruption 08/26/23 06:34:01.665
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:01.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:01.685
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 08/26/23 06:34:01.69
STEP: Waiting for the pdb to be processed 08/26/23 06:34:01.696
STEP: updating the pdb 08/26/23 06:34:03.704
STEP: Waiting for the pdb to be processed 08/26/23 06:34:03.712
STEP: patching the pdb 08/26/23 06:34:05.72
STEP: Waiting for the pdb to be processed 08/26/23 06:34:05.73
STEP: Waiting for the pdb to be deleted 08/26/23 06:34:05.753
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:05.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2310" for this suite. 08/26/23 06:34:05.765
------------------------------
• [4.110 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:01.664
    Aug 26 06:34:01.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename disruption 08/26/23 06:34:01.665
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:01.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:01.685
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 08/26/23 06:34:01.69
    STEP: Waiting for the pdb to be processed 08/26/23 06:34:01.696
    STEP: updating the pdb 08/26/23 06:34:03.704
    STEP: Waiting for the pdb to be processed 08/26/23 06:34:03.712
    STEP: patching the pdb 08/26/23 06:34:05.72
    STEP: Waiting for the pdb to be processed 08/26/23 06:34:05.73
    STEP: Waiting for the pdb to be deleted 08/26/23 06:34:05.753
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:05.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2310" for this suite. 08/26/23 06:34:05.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:05.775
Aug 26 06:34:05.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:34:05.776
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:05.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:05.79
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2d302a40-3983-4f61-bee1-9fcfcfbb49af 08/26/23 06:34:05.802
STEP: Creating the pod 08/26/23 06:34:05.807
Aug 26 06:34:05.817: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c" in namespace "projected-4103" to be "running and ready"
Aug 26 06:34:05.821: INFO: Pod "pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016651ms
Aug 26 06:34:05.821: INFO: The phase of Pod pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:34:07.826: INFO: Pod "pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00983398s
Aug 26 06:34:07.826: INFO: The phase of Pod pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c is Running (Ready = true)
Aug 26 06:34:07.826: INFO: Pod "pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-2d302a40-3983-4f61-bee1-9fcfcfbb49af 08/26/23 06:34:07.849
STEP: waiting to observe update in volume 08/26/23 06:34:07.854
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:09.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4103" for this suite. 08/26/23 06:34:09.878
------------------------------
• [4.111 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:05.775
    Aug 26 06:34:05.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:34:05.776
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:05.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:05.79
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-2d302a40-3983-4f61-bee1-9fcfcfbb49af 08/26/23 06:34:05.802
    STEP: Creating the pod 08/26/23 06:34:05.807
    Aug 26 06:34:05.817: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c" in namespace "projected-4103" to be "running and ready"
    Aug 26 06:34:05.821: INFO: Pod "pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016651ms
    Aug 26 06:34:05.821: INFO: The phase of Pod pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:34:07.826: INFO: Pod "pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00983398s
    Aug 26 06:34:07.826: INFO: The phase of Pod pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c is Running (Ready = true)
    Aug 26 06:34:07.826: INFO: Pod "pod-projected-configmaps-e95f3e3c-8553-4688-9517-90be64b27c6c" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-2d302a40-3983-4f61-bee1-9fcfcfbb49af 08/26/23 06:34:07.849
    STEP: waiting to observe update in volume 08/26/23 06:34:07.854
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:09.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4103" for this suite. 08/26/23 06:34:09.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:09.888
Aug 26 06:34:09.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename dns 08/26/23 06:34:09.889
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:09.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:09.91
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/26/23 06:34:09.913
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/26/23 06:34:09.913
STEP: creating a pod to probe DNS 08/26/23 06:34:09.913
STEP: submitting the pod to kubernetes 08/26/23 06:34:09.913
Aug 26 06:34:09.926: INFO: Waiting up to 15m0s for pod "dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23" in namespace "dns-8541" to be "running"
Aug 26 06:34:09.929: INFO: Pod "dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.528453ms
Aug 26 06:34:11.933: INFO: Pod "dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007543283s
Aug 26 06:34:13.935: INFO: Pod "dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23": Phase="Running", Reason="", readiness=true. Elapsed: 4.009584529s
Aug 26 06:34:13.936: INFO: Pod "dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23" satisfied condition "running"
STEP: retrieving the pod 08/26/23 06:34:13.936
STEP: looking for the results for each expected name from probers 08/26/23 06:34:13.941
Aug 26 06:34:13.962: INFO: DNS probes using dns-8541/dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23 succeeded

STEP: deleting the pod 08/26/23 06:34:13.962
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:13.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8541" for this suite. 08/26/23 06:34:14.001
------------------------------
• [4.123 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:09.888
    Aug 26 06:34:09.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename dns 08/26/23 06:34:09.889
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:09.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:09.91
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/26/23 06:34:09.913
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/26/23 06:34:09.913
    STEP: creating a pod to probe DNS 08/26/23 06:34:09.913
    STEP: submitting the pod to kubernetes 08/26/23 06:34:09.913
    Aug 26 06:34:09.926: INFO: Waiting up to 15m0s for pod "dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23" in namespace "dns-8541" to be "running"
    Aug 26 06:34:09.929: INFO: Pod "dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.528453ms
    Aug 26 06:34:11.933: INFO: Pod "dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007543283s
    Aug 26 06:34:13.935: INFO: Pod "dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23": Phase="Running", Reason="", readiness=true. Elapsed: 4.009584529s
    Aug 26 06:34:13.936: INFO: Pod "dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23" satisfied condition "running"
    STEP: retrieving the pod 08/26/23 06:34:13.936
    STEP: looking for the results for each expected name from probers 08/26/23 06:34:13.941
    Aug 26 06:34:13.962: INFO: DNS probes using dns-8541/dns-test-5fb66c31-44c5-45b8-bf4e-9c08a485dd23 succeeded

    STEP: deleting the pod 08/26/23 06:34:13.962
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:13.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8541" for this suite. 08/26/23 06:34:14.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:14.014
Aug 26 06:34:14.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename gc 08/26/23 06:34:14.016
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:14.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:14.032
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Aug 26 06:34:14.073: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d584ffc0-b077-44ba-abae-5f21117038f2", Controller:(*bool)(0xc00471d166), BlockOwnerDeletion:(*bool)(0xc00471d167)}}
Aug 26 06:34:14.086: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"efd58966-4b4e-4aa3-9f04-e66b2b99d0bb", Controller:(*bool)(0xc003f0dd4e), BlockOwnerDeletion:(*bool)(0xc003f0dd4f)}}
Aug 26 06:34:14.102: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6722a79b-9125-4971-ae85-f155d1860bcd", Controller:(*bool)(0xc00471d386), BlockOwnerDeletion:(*bool)(0xc00471d387)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:19.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6428" for this suite. 08/26/23 06:34:19.127
------------------------------
• [SLOW TEST] [5.122 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:14.014
    Aug 26 06:34:14.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename gc 08/26/23 06:34:14.016
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:14.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:14.032
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Aug 26 06:34:14.073: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d584ffc0-b077-44ba-abae-5f21117038f2", Controller:(*bool)(0xc00471d166), BlockOwnerDeletion:(*bool)(0xc00471d167)}}
    Aug 26 06:34:14.086: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"efd58966-4b4e-4aa3-9f04-e66b2b99d0bb", Controller:(*bool)(0xc003f0dd4e), BlockOwnerDeletion:(*bool)(0xc003f0dd4f)}}
    Aug 26 06:34:14.102: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6722a79b-9125-4971-ae85-f155d1860bcd", Controller:(*bool)(0xc00471d386), BlockOwnerDeletion:(*bool)(0xc00471d387)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:19.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6428" for this suite. 08/26/23 06:34:19.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:19.137
Aug 26 06:34:19.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename daemonsets 08/26/23 06:34:19.139
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:19.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:19.154
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Aug 26 06:34:19.191: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 08/26/23 06:34:19.198
Aug 26 06:34:19.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:34:19.204: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 08/26/23 06:34:19.204
Aug 26 06:34:19.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:34:19.235: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:34:20.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 26 06:34:20.241: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 08/26/23 06:34:20.248
Aug 26 06:34:20.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 26 06:34:20.270: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 26 06:34:21.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:34:21.275: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/26/23 06:34:21.275
Aug 26 06:34:21.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:34:21.292: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:34:22.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:34:22.301: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:34:23.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:34:23.298: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:34:24.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 26 06:34:24.296: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/26/23 06:34:24.306
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3670, will wait for the garbage collector to delete the pods 08/26/23 06:34:24.306
Aug 26 06:34:24.367: INFO: Deleting DaemonSet.extensions daemon-set took: 6.994874ms
Aug 26 06:34:24.468: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.337783ms
Aug 26 06:34:27.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:34:27.172: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 26 06:34:27.178: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40529"},"items":null}

Aug 26 06:34:27.181: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40530"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:27.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3670" for this suite. 08/26/23 06:34:27.221
------------------------------
• [SLOW TEST] [8.091 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:19.137
    Aug 26 06:34:19.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename daemonsets 08/26/23 06:34:19.139
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:19.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:19.154
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Aug 26 06:34:19.191: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 08/26/23 06:34:19.198
    Aug 26 06:34:19.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:34:19.204: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 08/26/23 06:34:19.204
    Aug 26 06:34:19.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:34:19.235: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:34:20.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 26 06:34:20.241: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 08/26/23 06:34:20.248
    Aug 26 06:34:20.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 26 06:34:20.270: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Aug 26 06:34:21.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:34:21.275: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/26/23 06:34:21.275
    Aug 26 06:34:21.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:34:21.292: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:34:22.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:34:22.301: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:34:23.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:34:23.298: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:34:24.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 26 06:34:24.296: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/26/23 06:34:24.306
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3670, will wait for the garbage collector to delete the pods 08/26/23 06:34:24.306
    Aug 26 06:34:24.367: INFO: Deleting DaemonSet.extensions daemon-set took: 6.994874ms
    Aug 26 06:34:24.468: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.337783ms
    Aug 26 06:34:27.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:34:27.172: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 26 06:34:27.178: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40529"},"items":null}

    Aug 26 06:34:27.181: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40530"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:27.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3670" for this suite. 08/26/23 06:34:27.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:27.228
Aug 26 06:34:27.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename deployment 08/26/23 06:34:27.229
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:27.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:27.244
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 08/26/23 06:34:27.251
Aug 26 06:34:27.252: INFO: Creating simple deployment test-deployment-zw282
Aug 26 06:34:27.265: INFO: deployment "test-deployment-zw282" doesn't have the required revision set
STEP: Getting /status 08/26/23 06:34:29.277
Aug 26 06:34:29.280: INFO: Deployment test-deployment-zw282 has Conditions: [{Available True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zw282-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 08/26/23 06:34:29.28
Aug 26 06:34:29.290: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 34, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 34, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 34, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 34, 27, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-zw282-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 08/26/23 06:34:29.29
Aug 26 06:34:29.292: INFO: Observed &Deployment event: ADDED
Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zw282-54bc444df"}
Aug 26 06:34:29.292: INFO: Observed &Deployment event: MODIFIED
Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zw282-54bc444df"}
Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 26 06:34:29.292: INFO: Observed &Deployment event: MODIFIED
Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zw282-54bc444df" is progressing.}
Aug 26 06:34:29.292: INFO: Observed &Deployment event: MODIFIED
Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zw282-54bc444df" has successfully progressed.}
Aug 26 06:34:29.292: INFO: Observed &Deployment event: MODIFIED
Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zw282-54bc444df" has successfully progressed.}
Aug 26 06:34:29.292: INFO: Found Deployment test-deployment-zw282 in namespace deployment-7482 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 26 06:34:29.292: INFO: Deployment test-deployment-zw282 has an updated status
STEP: patching the Statefulset Status 08/26/23 06:34:29.292
Aug 26 06:34:29.293: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 26 06:34:29.300: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 08/26/23 06:34:29.3
Aug 26 06:34:29.302: INFO: Observed &Deployment event: ADDED
Aug 26 06:34:29.302: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zw282-54bc444df"}
Aug 26 06:34:29.302: INFO: Observed &Deployment event: MODIFIED
Aug 26 06:34:29.302: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zw282-54bc444df"}
Aug 26 06:34:29.302: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 26 06:34:29.302: INFO: Observed &Deployment event: MODIFIED
Aug 26 06:34:29.302: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 26 06:34:29.302: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zw282-54bc444df" is progressing.}
Aug 26 06:34:29.303: INFO: Observed &Deployment event: MODIFIED
Aug 26 06:34:29.303: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 26 06:34:29.303: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zw282-54bc444df" has successfully progressed.}
Aug 26 06:34:29.303: INFO: Observed &Deployment event: MODIFIED
Aug 26 06:34:29.303: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 26 06:34:29.303: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zw282-54bc444df" has successfully progressed.}
Aug 26 06:34:29.303: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 26 06:34:29.303: INFO: Observed &Deployment event: MODIFIED
Aug 26 06:34:29.303: INFO: Found deployment test-deployment-zw282 in namespace deployment-7482 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 26 06:34:29.303: INFO: Deployment test-deployment-zw282 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 26 06:34:29.306: INFO: Deployment "test-deployment-zw282":
&Deployment{ObjectMeta:{test-deployment-zw282  deployment-7482  b98de610-7690-4e1c-ac80-0e23d16f5208 40565 1 2023-08-26 06:34:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-26 06:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:34:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-26 06:34:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a04a968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 26 06:34:29.309: INFO: New ReplicaSet "test-deployment-zw282-54bc444df" of Deployment "test-deployment-zw282":
&ReplicaSet{ObjectMeta:{test-deployment-zw282-54bc444df  deployment-7482  12b02095-1a37-449d-8ba3-4088a1d2a1fc 40560 1 2023-08-26 06:34:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-zw282 b98de610-7690-4e1c-ac80-0e23d16f5208 0xc006259210 0xc006259211}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b98de610-7690-4e1c-ac80-0e23d16f5208\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:34:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062592b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 26 06:34:29.313: INFO: Pod "test-deployment-zw282-54bc444df-mcg8s" is available:
&Pod{ObjectMeta:{test-deployment-zw282-54bc444df-mcg8s test-deployment-zw282-54bc444df- deployment-7482  8f02680b-ce10-4966-bc8b-4fdbafc980e4 40559 0 2023-08-26 06:34:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:1e177627acf3f29bc6634f5b02ea3ba4345cab45b87f3a1715b8220e447ff579 cni.projectcalico.org/podIP:10.20.8.215/32 cni.projectcalico.org/podIPs:10.20.8.215/32] [{apps/v1 ReplicaSet test-deployment-zw282-54bc444df 12b02095-1a37-449d-8ba3-4088a1d2a1fc 0xc006259680 0xc006259681}] [] [{calico Update v1 2023-08-26 06:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 06:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b02095-1a37-449d-8ba3-4088a1d2a1fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.8.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggfhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggfhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:34:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.31,PodIP:10.20.8.215,StartTime:2023-08-26 06:34:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c2285a1eea51f5462297694acfe22231824b2f0e93a44e6195073a1407de8eb0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.8.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:29.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7482" for this suite. 08/26/23 06:34:29.322
------------------------------
• [2.101 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:27.228
    Aug 26 06:34:27.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename deployment 08/26/23 06:34:27.229
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:27.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:27.244
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 08/26/23 06:34:27.251
    Aug 26 06:34:27.252: INFO: Creating simple deployment test-deployment-zw282
    Aug 26 06:34:27.265: INFO: deployment "test-deployment-zw282" doesn't have the required revision set
    STEP: Getting /status 08/26/23 06:34:29.277
    Aug 26 06:34:29.280: INFO: Deployment test-deployment-zw282 has Conditions: [{Available True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zw282-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 08/26/23 06:34:29.28
    Aug 26 06:34:29.290: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 34, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 34, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 34, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 34, 27, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-zw282-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 08/26/23 06:34:29.29
    Aug 26 06:34:29.292: INFO: Observed &Deployment event: ADDED
    Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zw282-54bc444df"}
    Aug 26 06:34:29.292: INFO: Observed &Deployment event: MODIFIED
    Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zw282-54bc444df"}
    Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 26 06:34:29.292: INFO: Observed &Deployment event: MODIFIED
    Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zw282-54bc444df" is progressing.}
    Aug 26 06:34:29.292: INFO: Observed &Deployment event: MODIFIED
    Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zw282-54bc444df" has successfully progressed.}
    Aug 26 06:34:29.292: INFO: Observed &Deployment event: MODIFIED
    Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 26 06:34:29.292: INFO: Observed Deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zw282-54bc444df" has successfully progressed.}
    Aug 26 06:34:29.292: INFO: Found Deployment test-deployment-zw282 in namespace deployment-7482 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 26 06:34:29.292: INFO: Deployment test-deployment-zw282 has an updated status
    STEP: patching the Statefulset Status 08/26/23 06:34:29.292
    Aug 26 06:34:29.293: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 26 06:34:29.300: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 08/26/23 06:34:29.3
    Aug 26 06:34:29.302: INFO: Observed &Deployment event: ADDED
    Aug 26 06:34:29.302: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zw282-54bc444df"}
    Aug 26 06:34:29.302: INFO: Observed &Deployment event: MODIFIED
    Aug 26 06:34:29.302: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zw282-54bc444df"}
    Aug 26 06:34:29.302: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 26 06:34:29.302: INFO: Observed &Deployment event: MODIFIED
    Aug 26 06:34:29.302: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 26 06:34:29.302: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:27 +0000 UTC 2023-08-26 06:34:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zw282-54bc444df" is progressing.}
    Aug 26 06:34:29.303: INFO: Observed &Deployment event: MODIFIED
    Aug 26 06:34:29.303: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 26 06:34:29.303: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zw282-54bc444df" has successfully progressed.}
    Aug 26 06:34:29.303: INFO: Observed &Deployment event: MODIFIED
    Aug 26 06:34:29.303: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 26 06:34:29.303: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-26 06:34:28 +0000 UTC 2023-08-26 06:34:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zw282-54bc444df" has successfully progressed.}
    Aug 26 06:34:29.303: INFO: Observed deployment test-deployment-zw282 in namespace deployment-7482 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 26 06:34:29.303: INFO: Observed &Deployment event: MODIFIED
    Aug 26 06:34:29.303: INFO: Found deployment test-deployment-zw282 in namespace deployment-7482 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Aug 26 06:34:29.303: INFO: Deployment test-deployment-zw282 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 26 06:34:29.306: INFO: Deployment "test-deployment-zw282":
    &Deployment{ObjectMeta:{test-deployment-zw282  deployment-7482  b98de610-7690-4e1c-ac80-0e23d16f5208 40565 1 2023-08-26 06:34:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-26 06:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:34:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-26 06:34:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a04a968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 26 06:34:29.309: INFO: New ReplicaSet "test-deployment-zw282-54bc444df" of Deployment "test-deployment-zw282":
    &ReplicaSet{ObjectMeta:{test-deployment-zw282-54bc444df  deployment-7482  12b02095-1a37-449d-8ba3-4088a1d2a1fc 40560 1 2023-08-26 06:34:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-zw282 b98de610-7690-4e1c-ac80-0e23d16f5208 0xc006259210 0xc006259211}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b98de610-7690-4e1c-ac80-0e23d16f5208\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:34:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062592b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 06:34:29.313: INFO: Pod "test-deployment-zw282-54bc444df-mcg8s" is available:
    &Pod{ObjectMeta:{test-deployment-zw282-54bc444df-mcg8s test-deployment-zw282-54bc444df- deployment-7482  8f02680b-ce10-4966-bc8b-4fdbafc980e4 40559 0 2023-08-26 06:34:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:1e177627acf3f29bc6634f5b02ea3ba4345cab45b87f3a1715b8220e447ff579 cni.projectcalico.org/podIP:10.20.8.215/32 cni.projectcalico.org/podIPs:10.20.8.215/32] [{apps/v1 ReplicaSet test-deployment-zw282-54bc444df 12b02095-1a37-449d-8ba3-4088a1d2a1fc 0xc006259680 0xc006259681}] [] [{calico Update v1 2023-08-26 06:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 06:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b02095-1a37-449d-8ba3-4088a1d2a1fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.8.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggfhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggfhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:34:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.31,PodIP:10.20.8.215,StartTime:2023-08-26 06:34:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c2285a1eea51f5462297694acfe22231824b2f0e93a44e6195073a1407de8eb0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.8.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:29.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7482" for this suite. 08/26/23 06:34:29.322
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:29.33
Aug 26 06:34:29.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename runtimeclass 08/26/23 06:34:29.331
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:29.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:29.348
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3848-delete-me 08/26/23 06:34:29.356
STEP: Waiting for the RuntimeClass to disappear 08/26/23 06:34:29.363
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:29.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3848" for this suite. 08/26/23 06:34:29.385
------------------------------
• [0.067 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:29.33
    Aug 26 06:34:29.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename runtimeclass 08/26/23 06:34:29.331
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:29.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:29.348
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3848-delete-me 08/26/23 06:34:29.356
    STEP: Waiting for the RuntimeClass to disappear 08/26/23 06:34:29.363
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:29.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3848" for this suite. 08/26/23 06:34:29.385
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:29.397
Aug 26 06:34:29.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:34:29.398
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:29.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:29.427
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-2a67e127-1d84-4ca1-87e8-1f123be89d09 08/26/23 06:34:29.44
STEP: Creating a pod to test consume secrets 08/26/23 06:34:29.447
Aug 26 06:34:29.468: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7" in namespace "projected-6796" to be "Succeeded or Failed"
Aug 26 06:34:29.472: INFO: Pod "pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.831865ms
Aug 26 06:34:31.478: INFO: Pod "pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009519538s
Aug 26 06:34:33.477: INFO: Pod "pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008768861s
STEP: Saw pod success 08/26/23 06:34:33.477
Aug 26 06:34:33.477: INFO: Pod "pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7" satisfied condition "Succeeded or Failed"
Aug 26 06:34:33.483: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/26/23 06:34:33.49
Aug 26 06:34:33.503: INFO: Waiting for pod pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7 to disappear
Aug 26 06:34:33.506: INFO: Pod pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:33.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6796" for this suite. 08/26/23 06:34:33.513
------------------------------
• [4.125 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:29.397
    Aug 26 06:34:29.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:34:29.398
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:29.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:29.427
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-2a67e127-1d84-4ca1-87e8-1f123be89d09 08/26/23 06:34:29.44
    STEP: Creating a pod to test consume secrets 08/26/23 06:34:29.447
    Aug 26 06:34:29.468: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7" in namespace "projected-6796" to be "Succeeded or Failed"
    Aug 26 06:34:29.472: INFO: Pod "pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.831865ms
    Aug 26 06:34:31.478: INFO: Pod "pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009519538s
    Aug 26 06:34:33.477: INFO: Pod "pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008768861s
    STEP: Saw pod success 08/26/23 06:34:33.477
    Aug 26 06:34:33.477: INFO: Pod "pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7" satisfied condition "Succeeded or Failed"
    Aug 26 06:34:33.483: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/26/23 06:34:33.49
    Aug 26 06:34:33.503: INFO: Waiting for pod pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7 to disappear
    Aug 26 06:34:33.506: INFO: Pod pod-projected-secrets-331045e2-7a29-4648-8a23-f9a3d22670b7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:33.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6796" for this suite. 08/26/23 06:34:33.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:33.523
Aug 26 06:34:33.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename ingress 08/26/23 06:34:33.524
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:33.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:33.541
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 08/26/23 06:34:33.544
STEP: getting /apis/networking.k8s.io 08/26/23 06:34:33.546
STEP: getting /apis/networking.k8s.iov1 08/26/23 06:34:33.547
STEP: creating 08/26/23 06:34:33.549
STEP: getting 08/26/23 06:34:33.566
STEP: listing 08/26/23 06:34:33.572
STEP: watching 08/26/23 06:34:33.582
Aug 26 06:34:33.582: INFO: starting watch
STEP: cluster-wide listing 08/26/23 06:34:33.583
STEP: cluster-wide watching 08/26/23 06:34:33.586
Aug 26 06:34:33.586: INFO: starting watch
STEP: patching 08/26/23 06:34:33.587
STEP: updating 08/26/23 06:34:33.594
Aug 26 06:34:33.608: INFO: waiting for watch events with expected annotations
Aug 26 06:34:33.608: INFO: saw patched and updated annotations
STEP: patching /status 08/26/23 06:34:33.608
STEP: updating /status 08/26/23 06:34:33.615
STEP: get /status 08/26/23 06:34:33.627
STEP: deleting 08/26/23 06:34:33.63
STEP: deleting a collection 08/26/23 06:34:33.642
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:33.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-2806" for this suite. 08/26/23 06:34:33.666
------------------------------
• [0.150 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:33.523
    Aug 26 06:34:33.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename ingress 08/26/23 06:34:33.524
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:33.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:33.541
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 08/26/23 06:34:33.544
    STEP: getting /apis/networking.k8s.io 08/26/23 06:34:33.546
    STEP: getting /apis/networking.k8s.iov1 08/26/23 06:34:33.547
    STEP: creating 08/26/23 06:34:33.549
    STEP: getting 08/26/23 06:34:33.566
    STEP: listing 08/26/23 06:34:33.572
    STEP: watching 08/26/23 06:34:33.582
    Aug 26 06:34:33.582: INFO: starting watch
    STEP: cluster-wide listing 08/26/23 06:34:33.583
    STEP: cluster-wide watching 08/26/23 06:34:33.586
    Aug 26 06:34:33.586: INFO: starting watch
    STEP: patching 08/26/23 06:34:33.587
    STEP: updating 08/26/23 06:34:33.594
    Aug 26 06:34:33.608: INFO: waiting for watch events with expected annotations
    Aug 26 06:34:33.608: INFO: saw patched and updated annotations
    STEP: patching /status 08/26/23 06:34:33.608
    STEP: updating /status 08/26/23 06:34:33.615
    STEP: get /status 08/26/23 06:34:33.627
    STEP: deleting 08/26/23 06:34:33.63
    STEP: deleting a collection 08/26/23 06:34:33.642
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:33.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-2806" for this suite. 08/26/23 06:34:33.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:33.673
Aug 26 06:34:33.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename statefulset 08/26/23 06:34:33.674
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:33.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:33.691
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6802 08/26/23 06:34:33.693
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-6802 08/26/23 06:34:33.703
Aug 26 06:34:33.718: INFO: Found 0 stateful pods, waiting for 1
Aug 26 06:34:43.723: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 08/26/23 06:34:43.729
STEP: Getting /status 08/26/23 06:34:43.739
Aug 26 06:34:43.743: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 08/26/23 06:34:43.743
Aug 26 06:34:43.751: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 08/26/23 06:34:43.751
Aug 26 06:34:43.754: INFO: Observed &StatefulSet event: ADDED
Aug 26 06:34:43.754: INFO: Found Statefulset ss in namespace statefulset-6802 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 26 06:34:43.754: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 08/26/23 06:34:43.754
Aug 26 06:34:43.754: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 26 06:34:43.762: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 08/26/23 06:34:43.762
Aug 26 06:34:43.764: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 26 06:34:43.765: INFO: Deleting all statefulset in ns statefulset-6802
Aug 26 06:34:43.767: INFO: Scaling statefulset ss to 0
Aug 26 06:34:53.788: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 06:34:53.792: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:53.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6802" for this suite. 08/26/23 06:34:53.819
------------------------------
• [SLOW TEST] [20.155 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:33.673
    Aug 26 06:34:33.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename statefulset 08/26/23 06:34:33.674
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:33.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:33.691
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6802 08/26/23 06:34:33.693
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-6802 08/26/23 06:34:33.703
    Aug 26 06:34:33.718: INFO: Found 0 stateful pods, waiting for 1
    Aug 26 06:34:43.723: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 08/26/23 06:34:43.729
    STEP: Getting /status 08/26/23 06:34:43.739
    Aug 26 06:34:43.743: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 08/26/23 06:34:43.743
    Aug 26 06:34:43.751: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 08/26/23 06:34:43.751
    Aug 26 06:34:43.754: INFO: Observed &StatefulSet event: ADDED
    Aug 26 06:34:43.754: INFO: Found Statefulset ss in namespace statefulset-6802 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 26 06:34:43.754: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 08/26/23 06:34:43.754
    Aug 26 06:34:43.754: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 26 06:34:43.762: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 08/26/23 06:34:43.762
    Aug 26 06:34:43.764: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 26 06:34:43.765: INFO: Deleting all statefulset in ns statefulset-6802
    Aug 26 06:34:43.767: INFO: Scaling statefulset ss to 0
    Aug 26 06:34:53.788: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 26 06:34:53.792: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:53.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6802" for this suite. 08/26/23 06:34:53.819
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:53.829
Aug 26 06:34:53.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:34:53.83
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:53.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:53.851
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-67786ffe-3812-4f6c-85bf-6f77455f9ea3 08/26/23 06:34:53.855
STEP: Creating a pod to test consume configMaps 08/26/23 06:34:53.863
Aug 26 06:34:53.874: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d" in namespace "projected-8659" to be "Succeeded or Failed"
Aug 26 06:34:53.885: INFO: Pod "pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.852708ms
Aug 26 06:34:55.889: INFO: Pod "pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015077695s
Aug 26 06:34:57.890: INFO: Pod "pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015347359s
STEP: Saw pod success 08/26/23 06:34:57.89
Aug 26 06:34:57.890: INFO: Pod "pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d" satisfied condition "Succeeded or Failed"
Aug 26 06:34:57.893: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d container agnhost-container: <nil>
STEP: delete the pod 08/26/23 06:34:57.91
Aug 26 06:34:57.934: INFO: Waiting for pod pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d to disappear
Aug 26 06:34:57.938: INFO: Pod pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:34:57.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8659" for this suite. 08/26/23 06:34:57.949
------------------------------
• [4.128 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:53.829
    Aug 26 06:34:53.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:34:53.83
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:53.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:53.851
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-67786ffe-3812-4f6c-85bf-6f77455f9ea3 08/26/23 06:34:53.855
    STEP: Creating a pod to test consume configMaps 08/26/23 06:34:53.863
    Aug 26 06:34:53.874: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d" in namespace "projected-8659" to be "Succeeded or Failed"
    Aug 26 06:34:53.885: INFO: Pod "pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.852708ms
    Aug 26 06:34:55.889: INFO: Pod "pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015077695s
    Aug 26 06:34:57.890: INFO: Pod "pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015347359s
    STEP: Saw pod success 08/26/23 06:34:57.89
    Aug 26 06:34:57.890: INFO: Pod "pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d" satisfied condition "Succeeded or Failed"
    Aug 26 06:34:57.893: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 06:34:57.91
    Aug 26 06:34:57.934: INFO: Waiting for pod pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d to disappear
    Aug 26 06:34:57.938: INFO: Pod pod-projected-configmaps-986ad1c7-6a1d-417c-a254-4dd78b37255d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:34:57.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8659" for this suite. 08/26/23 06:34:57.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:34:57.961
Aug 26 06:34:57.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename dns 08/26/23 06:34:57.962
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:57.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:57.979
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 08/26/23 06:34:57.982
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6643.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6643.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 08/26/23 06:34:57.988
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6643.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6643.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 08/26/23 06:34:57.988
STEP: creating a pod to probe DNS 08/26/23 06:34:57.988
STEP: submitting the pod to kubernetes 08/26/23 06:34:57.989
Aug 26 06:34:58.005: INFO: Waiting up to 15m0s for pod "dns-test-94aa8cf3-0a21-426c-93d9-100f202a5630" in namespace "dns-6643" to be "running"
Aug 26 06:34:58.009: INFO: Pod "dns-test-94aa8cf3-0a21-426c-93d9-100f202a5630": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769592ms
Aug 26 06:35:00.019: INFO: Pod "dns-test-94aa8cf3-0a21-426c-93d9-100f202a5630": Phase="Running", Reason="", readiness=true. Elapsed: 2.013540694s
Aug 26 06:35:00.019: INFO: Pod "dns-test-94aa8cf3-0a21-426c-93d9-100f202a5630" satisfied condition "running"
STEP: retrieving the pod 08/26/23 06:35:00.019
STEP: looking for the results for each expected name from probers 08/26/23 06:35:00.024
Aug 26 06:35:00.047: INFO: DNS probes using dns-6643/dns-test-94aa8cf3-0a21-426c-93d9-100f202a5630 succeeded

STEP: deleting the pod 08/26/23 06:35:00.047
STEP: deleting the test headless service 08/26/23 06:35:00.067
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:00.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6643" for this suite. 08/26/23 06:35:00.099
------------------------------
• [2.151 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:34:57.961
    Aug 26 06:34:57.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename dns 08/26/23 06:34:57.962
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:34:57.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:34:57.979
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 08/26/23 06:34:57.982
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6643.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6643.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     08/26/23 06:34:57.988
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6643.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6643.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     08/26/23 06:34:57.988
    STEP: creating a pod to probe DNS 08/26/23 06:34:57.988
    STEP: submitting the pod to kubernetes 08/26/23 06:34:57.989
    Aug 26 06:34:58.005: INFO: Waiting up to 15m0s for pod "dns-test-94aa8cf3-0a21-426c-93d9-100f202a5630" in namespace "dns-6643" to be "running"
    Aug 26 06:34:58.009: INFO: Pod "dns-test-94aa8cf3-0a21-426c-93d9-100f202a5630": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769592ms
    Aug 26 06:35:00.019: INFO: Pod "dns-test-94aa8cf3-0a21-426c-93d9-100f202a5630": Phase="Running", Reason="", readiness=true. Elapsed: 2.013540694s
    Aug 26 06:35:00.019: INFO: Pod "dns-test-94aa8cf3-0a21-426c-93d9-100f202a5630" satisfied condition "running"
    STEP: retrieving the pod 08/26/23 06:35:00.019
    STEP: looking for the results for each expected name from probers 08/26/23 06:35:00.024
    Aug 26 06:35:00.047: INFO: DNS probes using dns-6643/dns-test-94aa8cf3-0a21-426c-93d9-100f202a5630 succeeded

    STEP: deleting the pod 08/26/23 06:35:00.047
    STEP: deleting the test headless service 08/26/23 06:35:00.067
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:00.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6643" for this suite. 08/26/23 06:35:00.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:00.117
Aug 26 06:35:00.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename namespaces 08/26/23 06:35:00.118
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:00.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:00.142
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 08/26/23 06:35:00.147
Aug 26 06:35:00.152: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 08/26/23 06:35:00.152
Aug 26 06:35:00.160: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 08/26/23 06:35:00.16
Aug 26 06:35:00.171: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:00.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5186" for this suite. 08/26/23 06:35:00.179
------------------------------
• [0.071 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:00.117
    Aug 26 06:35:00.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename namespaces 08/26/23 06:35:00.118
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:00.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:00.142
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 08/26/23 06:35:00.147
    Aug 26 06:35:00.152: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 08/26/23 06:35:00.152
    Aug 26 06:35:00.160: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 08/26/23 06:35:00.16
    Aug 26 06:35:00.171: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:00.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5186" for this suite. 08/26/23 06:35:00.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:00.198
Aug 26 06:35:00.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 06:35:00.2
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:00.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:00.223
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 08/26/23 06:35:00.227
Aug 26 06:35:00.227: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2362 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 08/26/23 06:35:00.314
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:00.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2362" for this suite. 08/26/23 06:35:00.332
------------------------------
• [0.143 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:00.198
    Aug 26 06:35:00.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 06:35:00.2
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:00.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:00.223
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 08/26/23 06:35:00.227
    Aug 26 06:35:00.227: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-2362 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 08/26/23 06:35:00.314
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:00.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2362" for this suite. 08/26/23 06:35:00.332
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:00.341
Aug 26 06:35:00.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename replicaset 08/26/23 06:35:00.342
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:00.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:00.361
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/26/23 06:35:00.364
Aug 26 06:35:00.373: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 26 06:35:05.382: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/26/23 06:35:05.382
STEP: getting scale subresource 08/26/23 06:35:05.382
STEP: updating a scale subresource 08/26/23 06:35:05.387
STEP: verifying the replicaset Spec.Replicas was modified 08/26/23 06:35:05.398
STEP: Patch a scale subresource 08/26/23 06:35:05.419
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:05.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8623" for this suite. 08/26/23 06:35:05.486
------------------------------
• [SLOW TEST] [5.172 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:00.341
    Aug 26 06:35:00.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename replicaset 08/26/23 06:35:00.342
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:00.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:00.361
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/26/23 06:35:00.364
    Aug 26 06:35:00.373: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 26 06:35:05.382: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/26/23 06:35:05.382
    STEP: getting scale subresource 08/26/23 06:35:05.382
    STEP: updating a scale subresource 08/26/23 06:35:05.387
    STEP: verifying the replicaset Spec.Replicas was modified 08/26/23 06:35:05.398
    STEP: Patch a scale subresource 08/26/23 06:35:05.419
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:05.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8623" for this suite. 08/26/23 06:35:05.486
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:05.514
Aug 26 06:35:05.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename deployment 08/26/23 06:35:05.515
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:05.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:05.571
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 08/26/23 06:35:05.586
STEP: waiting for Deployment to be created 08/26/23 06:35:05.595
STEP: waiting for all Replicas to be Ready 08/26/23 06:35:05.603
Aug 26 06:35:05.606: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 26 06:35:05.606: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 26 06:35:05.625: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 26 06:35:05.625: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 26 06:35:05.659: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 26 06:35:05.659: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 26 06:35:05.768: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 26 06:35:05.768: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 26 06:35:07.074: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 26 06:35:07.074: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 26 06:35:07.242: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 08/26/23 06:35:07.242
W0826 06:35:07.252676      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 26 06:35:07.255: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 08/26/23 06:35:07.256
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:07.265: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:07.266: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:07.290: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:07.290: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:07.309: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:07.309: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:07.320: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:07.320: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:08.931: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:08.932: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:08.969: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
STEP: listing Deployments 08/26/23 06:35:08.969
Aug 26 06:35:08.988: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 08/26/23 06:35:08.988
Aug 26 06:35:09.012: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 08/26/23 06:35:09.012
Aug 26 06:35:09.021: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 26 06:35:09.025: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 26 06:35:09.045: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 26 06:35:09.081: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 26 06:35:09.093: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 26 06:35:10.317: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 26 06:35:10.350: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 26 06:35:10.385: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 26 06:35:10.404: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 26 06:35:12.077: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 08/26/23 06:35:12.102
STEP: fetching the DeploymentStatus 08/26/23 06:35:12.11
Aug 26 06:35:12.115: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:12.115: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:12.115: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 3
STEP: deleting the Deployment 08/26/23 06:35:12.116
Aug 26 06:35:12.126: INFO: observed event type MODIFIED
Aug 26 06:35:12.126: INFO: observed event type MODIFIED
Aug 26 06:35:12.126: INFO: observed event type MODIFIED
Aug 26 06:35:12.126: INFO: observed event type MODIFIED
Aug 26 06:35:12.126: INFO: observed event type MODIFIED
Aug 26 06:35:12.126: INFO: observed event type MODIFIED
Aug 26 06:35:12.126: INFO: observed event type MODIFIED
Aug 26 06:35:12.126: INFO: observed event type MODIFIED
Aug 26 06:35:12.126: INFO: observed event type MODIFIED
Aug 26 06:35:12.126: INFO: observed event type MODIFIED
Aug 26 06:35:12.127: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 26 06:35:12.130: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 26 06:35:12.134: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-1833  f3076d5d-b9fd-4765-84dc-7a59d935e9d5 41194 2 2023-08-26 06:35:09 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6664f713-0a40-40e6-90b3-fabbfbcab2fb 0xc0046dd357 0xc0046dd358}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:35:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6664f713-0a40-40e6-90b3-fabbfbcab2fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:35:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046dd3e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Aug 26 06:35:12.139: INFO: pod: "test-deployment-7b7876f9d6-9d5pf":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-9d5pf test-deployment-7b7876f9d6- deployment-1833  cc87e0e1-2871-4d12-9a80-4c17b7f27088 41193 0 2023-08-26 06:35:10 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:571a73e55102add06467f674ac7954f3e475c036062e4962d2f15e68f436f0f9 cni.projectcalico.org/podIP:10.20.8.221/32 cni.projectcalico.org/podIPs:10.20.8.221/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f3076d5d-b9fd-4765-84dc-7a59d935e9d5 0xc00468ccd7 0xc00468ccd8}] [] [{kube-controller-manager Update v1 2023-08-26 06:35:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3076d5d-b9fd-4765-84dc-7a59d935e9d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:35:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.8.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pw9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pw9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.31,PodIP:10.20.8.221,StartTime:2023-08-26 06:35:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:35:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aa1f7a027532ce346fb0907710fee9c95f563972e2dd48666235ae6ef800f176,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.8.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 26 06:35:12.139: INFO: pod: "test-deployment-7b7876f9d6-x7d2h":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-x7d2h test-deployment-7b7876f9d6- deployment-1833  c94ed35e-aca2-47eb-9b87-445ce69821c5 41130 0 2023-08-26 06:35:09 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:ad0f23cfc7f6ea96c49242b84ef524f88d5f19899e5150d4f5f3dc438a70a6f3 cni.projectcalico.org/podIP:10.20.50.217/32 cni.projectcalico.org/podIPs:10.20.50.217/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f3076d5d-b9fd-4765-84dc-7a59d935e9d5 0xc00468cfc7 0xc00468cfc8}] [] [{calico Update v1 2023-08-26 06:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 06:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3076d5d-b9fd-4765-84dc-7a59d935e9d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:35:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.50.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8vwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8vwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.101,PodIP:10.20.50.217,StartTime:2023-08-26 06:35:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:35:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3b407ae1569a69a7191907ef59db05f99a10ad9f699352db0a4acff35b792975,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.50.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 26 06:35:12.139: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-1833  52d56c8a-e59c-4926-9606-ca55dc2a6722 41202 4 2023-08-26 06:35:07 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6664f713-0a40-40e6-90b3-fabbfbcab2fb 0xc0046dd447 0xc0046dd448}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:35:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6664f713-0a40-40e6-90b3-fabbfbcab2fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:35:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046dd4d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 26 06:35:12.144: INFO: pod: "test-deployment-7df74c55ff-nk58w":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-nk58w test-deployment-7df74c55ff- deployment-1833  476a5270-4433-4e47-82a0-75210e18d660 41182 0 2023-08-26 06:35:09 +0000 UTC 2023-08-26 06:35:11 +0000 UTC 0xc001025028 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:e6ae81a3e8da5370f5217344b8a5a91cca99eb73a5a0c0861d5ffaada37a37b3 cni.projectcalico.org/podIP:10.20.193.206/32 cni.projectcalico.org/podIPs:10.20.193.206/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 52d56c8a-e59c-4926-9606-ca55dc2a6722 0xc001025077 0xc001025078}] [] [{calico Update v1 2023-08-26 06:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 06:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d56c8a-e59c-4926-9606-ca55dc2a6722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:35:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.193.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8x8j6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8x8j6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-126.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.126,PodIP:10.20.193.206,StartTime:2023-08-26 06:35:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:35:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://3075baf97e74e812fb302e89fb574f4adda9f221cffd1e6aa31a45e4cc6c9201,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.193.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 26 06:35:12.144: INFO: pod: "test-deployment-7df74c55ff-nz6c2":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-nz6c2 test-deployment-7df74c55ff- deployment-1833  a009d32b-7c88-46ce-af7e-7bf2e388a740 41198 0 2023-08-26 06:35:07 +0000 UTC 2023-08-26 06:35:13 +0000 UTC 0xc001025570 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:b12b4d1dabcc25c3865c4f97aeaaf656c313d9f2271e7a3ce3afa44888b96d8d cni.projectcalico.org/podIP:10.20.199.84/32 cni.projectcalico.org/podIPs:10.20.199.84/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 52d56c8a-e59c-4926-9606-ca55dc2a6722 0xc0010255c7 0xc0010255c8}] [] [{calico Update v1 2023-08-26 06:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 06:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d56c8a-e59c-4926-9606-ca55dc2a6722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gt7np,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gt7np,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:10.20.199.84,StartTime:2023-08-26 06:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://2575af55db5582c99091f590526faee803c847867f713f0030991048da57e34a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.199.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 26 06:35:12.144: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-1833  e5730799-f4a5-4b0e-b026-5c0541fc6fdb 41072 3 2023-08-26 06:35:05 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6664f713-0a40-40e6-90b3-fabbfbcab2fb 0xc0046dd537 0xc0046dd538}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6664f713-0a40-40e6-90b3-fabbfbcab2fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:35:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046dd5c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:12.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1833" for this suite. 08/26/23 06:35:12.162
------------------------------
• [SLOW TEST] [6.655 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:05.514
    Aug 26 06:35:05.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename deployment 08/26/23 06:35:05.515
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:05.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:05.571
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 08/26/23 06:35:05.586
    STEP: waiting for Deployment to be created 08/26/23 06:35:05.595
    STEP: waiting for all Replicas to be Ready 08/26/23 06:35:05.603
    Aug 26 06:35:05.606: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 26 06:35:05.606: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 26 06:35:05.625: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 26 06:35:05.625: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 26 06:35:05.659: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 26 06:35:05.659: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 26 06:35:05.768: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 26 06:35:05.768: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 26 06:35:07.074: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 26 06:35:07.074: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 26 06:35:07.242: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 08/26/23 06:35:07.242
    W0826 06:35:07.252676      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 26 06:35:07.255: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 08/26/23 06:35:07.256
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 0
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:07.258: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:07.265: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:07.266: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:07.290: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:07.290: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:07.309: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:07.309: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:07.320: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:07.320: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:08.931: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:08.932: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:08.969: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    STEP: listing Deployments 08/26/23 06:35:08.969
    Aug 26 06:35:08.988: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 08/26/23 06:35:08.988
    Aug 26 06:35:09.012: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 08/26/23 06:35:09.012
    Aug 26 06:35:09.021: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 26 06:35:09.025: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 26 06:35:09.045: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 26 06:35:09.081: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 26 06:35:09.093: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 26 06:35:10.317: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 26 06:35:10.350: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 26 06:35:10.385: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 26 06:35:10.404: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 26 06:35:12.077: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 08/26/23 06:35:12.102
    STEP: fetching the DeploymentStatus 08/26/23 06:35:12.11
    Aug 26 06:35:12.115: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:12.115: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:12.115: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 1
    Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 2
    Aug 26 06:35:12.116: INFO: observed Deployment test-deployment in namespace deployment-1833 with ReadyReplicas 3
    STEP: deleting the Deployment 08/26/23 06:35:12.116
    Aug 26 06:35:12.126: INFO: observed event type MODIFIED
    Aug 26 06:35:12.126: INFO: observed event type MODIFIED
    Aug 26 06:35:12.126: INFO: observed event type MODIFIED
    Aug 26 06:35:12.126: INFO: observed event type MODIFIED
    Aug 26 06:35:12.126: INFO: observed event type MODIFIED
    Aug 26 06:35:12.126: INFO: observed event type MODIFIED
    Aug 26 06:35:12.126: INFO: observed event type MODIFIED
    Aug 26 06:35:12.126: INFO: observed event type MODIFIED
    Aug 26 06:35:12.126: INFO: observed event type MODIFIED
    Aug 26 06:35:12.126: INFO: observed event type MODIFIED
    Aug 26 06:35:12.127: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 26 06:35:12.130: INFO: Log out all the ReplicaSets if there is no deployment created
    Aug 26 06:35:12.134: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-1833  f3076d5d-b9fd-4765-84dc-7a59d935e9d5 41194 2 2023-08-26 06:35:09 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6664f713-0a40-40e6-90b3-fabbfbcab2fb 0xc0046dd357 0xc0046dd358}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:35:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6664f713-0a40-40e6-90b3-fabbfbcab2fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:35:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046dd3e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Aug 26 06:35:12.139: INFO: pod: "test-deployment-7b7876f9d6-9d5pf":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-9d5pf test-deployment-7b7876f9d6- deployment-1833  cc87e0e1-2871-4d12-9a80-4c17b7f27088 41193 0 2023-08-26 06:35:10 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:571a73e55102add06467f674ac7954f3e475c036062e4962d2f15e68f436f0f9 cni.projectcalico.org/podIP:10.20.8.221/32 cni.projectcalico.org/podIPs:10.20.8.221/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f3076d5d-b9fd-4765-84dc-7a59d935e9d5 0xc00468ccd7 0xc00468ccd8}] [] [{kube-controller-manager Update v1 2023-08-26 06:35:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3076d5d-b9fd-4765-84dc-7a59d935e9d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:35:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.8.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pw9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pw9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-31.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.31,PodIP:10.20.8.221,StartTime:2023-08-26 06:35:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:35:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aa1f7a027532ce346fb0907710fee9c95f563972e2dd48666235ae6ef800f176,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.8.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 26 06:35:12.139: INFO: pod: "test-deployment-7b7876f9d6-x7d2h":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-x7d2h test-deployment-7b7876f9d6- deployment-1833  c94ed35e-aca2-47eb-9b87-445ce69821c5 41130 0 2023-08-26 06:35:09 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:ad0f23cfc7f6ea96c49242b84ef524f88d5f19899e5150d4f5f3dc438a70a6f3 cni.projectcalico.org/podIP:10.20.50.217/32 cni.projectcalico.org/podIPs:10.20.50.217/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f3076d5d-b9fd-4765-84dc-7a59d935e9d5 0xc00468cfc7 0xc00468cfc8}] [] [{calico Update v1 2023-08-26 06:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 06:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3076d5d-b9fd-4765-84dc-7a59d935e9d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:35:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.50.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8vwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8vwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.101,PodIP:10.20.50.217,StartTime:2023-08-26 06:35:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:35:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3b407ae1569a69a7191907ef59db05f99a10ad9f699352db0a4acff35b792975,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.50.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 26 06:35:12.139: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-1833  52d56c8a-e59c-4926-9606-ca55dc2a6722 41202 4 2023-08-26 06:35:07 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6664f713-0a40-40e6-90b3-fabbfbcab2fb 0xc0046dd447 0xc0046dd448}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:35:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6664f713-0a40-40e6-90b3-fabbfbcab2fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:35:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046dd4d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Aug 26 06:35:12.144: INFO: pod: "test-deployment-7df74c55ff-nk58w":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-nk58w test-deployment-7df74c55ff- deployment-1833  476a5270-4433-4e47-82a0-75210e18d660 41182 0 2023-08-26 06:35:09 +0000 UTC 2023-08-26 06:35:11 +0000 UTC 0xc001025028 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:e6ae81a3e8da5370f5217344b8a5a91cca99eb73a5a0c0861d5ffaada37a37b3 cni.projectcalico.org/podIP:10.20.193.206/32 cni.projectcalico.org/podIPs:10.20.193.206/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 52d56c8a-e59c-4926-9606-ca55dc2a6722 0xc001025077 0xc001025078}] [] [{calico Update v1 2023-08-26 06:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 06:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d56c8a-e59c-4926-9606-ca55dc2a6722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:35:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.193.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8x8j6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8x8j6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-126.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.126,PodIP:10.20.193.206,StartTime:2023-08-26 06:35:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:35:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://3075baf97e74e812fb302e89fb574f4adda9f221cffd1e6aa31a45e4cc6c9201,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.193.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 26 06:35:12.144: INFO: pod: "test-deployment-7df74c55ff-nz6c2":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-nz6c2 test-deployment-7df74c55ff- deployment-1833  a009d32b-7c88-46ce-af7e-7bf2e388a740 41198 0 2023-08-26 06:35:07 +0000 UTC 2023-08-26 06:35:13 +0000 UTC 0xc001025570 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:b12b4d1dabcc25c3865c4f97aeaaf656c313d9f2271e7a3ce3afa44888b96d8d cni.projectcalico.org/podIP:10.20.199.84/32 cni.projectcalico.org/podIPs:10.20.199.84/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 52d56c8a-e59c-4926-9606-ca55dc2a6722 0xc0010255c7 0xc0010255c8}] [] [{calico Update v1 2023-08-26 06:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-26 06:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52d56c8a-e59c-4926-9606-ca55dc2a6722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-26 06:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.199.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gt7np,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gt7np,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-5.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.5,PodIP:10.20.199.84,StartTime:2023-08-26 06:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://2575af55db5582c99091f590526faee803c847867f713f0030991048da57e34a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.199.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 26 06:35:12.144: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-1833  e5730799-f4a5-4b0e-b026-5c0541fc6fdb 41072 3 2023-08-26 06:35:05 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6664f713-0a40-40e6-90b3-fabbfbcab2fb 0xc0046dd537 0xc0046dd538}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6664f713-0a40-40e6-90b3-fabbfbcab2fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:35:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046dd5c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:12.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1833" for this suite. 08/26/23 06:35:12.162
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:12.17
Aug 26 06:35:12.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:35:12.171
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:12.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:12.189
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-55387a5f-6958-4510-8051-9d8738899591 08/26/23 06:35:12.192
STEP: Creating a pod to test consume configMaps 08/26/23 06:35:12.196
Aug 26 06:35:12.204: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa" in namespace "projected-9761" to be "Succeeded or Failed"
Aug 26 06:35:12.208: INFO: Pod "pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258854ms
Aug 26 06:35:14.213: INFO: Pod "pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009127435s
Aug 26 06:35:16.212: INFO: Pod "pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007729483s
STEP: Saw pod success 08/26/23 06:35:16.212
Aug 26 06:35:16.212: INFO: Pod "pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa" satisfied condition "Succeeded or Failed"
Aug 26 06:35:16.217: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa container agnhost-container: <nil>
STEP: delete the pod 08/26/23 06:35:16.23
Aug 26 06:35:16.247: INFO: Waiting for pod pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa to disappear
Aug 26 06:35:16.251: INFO: Pod pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:16.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9761" for this suite. 08/26/23 06:35:16.266
------------------------------
• [4.106 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:12.17
    Aug 26 06:35:12.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:35:12.171
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:12.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:12.189
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-55387a5f-6958-4510-8051-9d8738899591 08/26/23 06:35:12.192
    STEP: Creating a pod to test consume configMaps 08/26/23 06:35:12.196
    Aug 26 06:35:12.204: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa" in namespace "projected-9761" to be "Succeeded or Failed"
    Aug 26 06:35:12.208: INFO: Pod "pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258854ms
    Aug 26 06:35:14.213: INFO: Pod "pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009127435s
    Aug 26 06:35:16.212: INFO: Pod "pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007729483s
    STEP: Saw pod success 08/26/23 06:35:16.212
    Aug 26 06:35:16.212: INFO: Pod "pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa" satisfied condition "Succeeded or Failed"
    Aug 26 06:35:16.217: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 06:35:16.23
    Aug 26 06:35:16.247: INFO: Waiting for pod pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa to disappear
    Aug 26 06:35:16.251: INFO: Pod pod-projected-configmaps-bf7e76fa-deeb-412b-af01-4efa749823aa no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:16.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9761" for this suite. 08/26/23 06:35:16.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:16.277
Aug 26 06:35:16.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename init-container 08/26/23 06:35:16.278
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:16.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:16.307
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 08/26/23 06:35:16.309
Aug 26 06:35:16.309: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:21.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-223" for this suite. 08/26/23 06:35:21.329
------------------------------
• [SLOW TEST] [5.065 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:16.277
    Aug 26 06:35:16.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename init-container 08/26/23 06:35:16.278
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:16.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:16.307
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 08/26/23 06:35:16.309
    Aug 26 06:35:16.309: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:21.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-223" for this suite. 08/26/23 06:35:21.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:21.342
Aug 26 06:35:21.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 06:35:21.343
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:21.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:21.366
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-e29a33d3-d937-4d4e-8ee0-0374f1a97704 08/26/23 06:35:21.375
STEP: Creating the pod 08/26/23 06:35:21.38
Aug 26 06:35:21.389: INFO: Waiting up to 5m0s for pod "pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf" in namespace "configmap-2833" to be "running and ready"
Aug 26 06:35:21.393: INFO: Pod "pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.374005ms
Aug 26 06:35:21.393: INFO: The phase of Pod pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:35:23.398: INFO: Pod "pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009133874s
Aug 26 06:35:23.398: INFO: The phase of Pod pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf is Running (Ready = true)
Aug 26 06:35:23.398: INFO: Pod "pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-e29a33d3-d937-4d4e-8ee0-0374f1a97704 08/26/23 06:35:23.408
STEP: waiting to observe update in volume 08/26/23 06:35:23.416
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:25.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2833" for this suite. 08/26/23 06:35:25.44
------------------------------
• [4.106 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:21.342
    Aug 26 06:35:21.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 06:35:21.343
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:21.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:21.366
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-e29a33d3-d937-4d4e-8ee0-0374f1a97704 08/26/23 06:35:21.375
    STEP: Creating the pod 08/26/23 06:35:21.38
    Aug 26 06:35:21.389: INFO: Waiting up to 5m0s for pod "pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf" in namespace "configmap-2833" to be "running and ready"
    Aug 26 06:35:21.393: INFO: Pod "pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.374005ms
    Aug 26 06:35:21.393: INFO: The phase of Pod pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:35:23.398: INFO: Pod "pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009133874s
    Aug 26 06:35:23.398: INFO: The phase of Pod pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf is Running (Ready = true)
    Aug 26 06:35:23.398: INFO: Pod "pod-configmaps-82dbe1f5-a719-4ffe-b837-defd540619cf" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-e29a33d3-d937-4d4e-8ee0-0374f1a97704 08/26/23 06:35:23.408
    STEP: waiting to observe update in volume 08/26/23 06:35:23.416
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:25.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2833" for this suite. 08/26/23 06:35:25.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:25.449
Aug 26 06:35:25.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 06:35:25.45
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:25.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:25.475
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-526 08/26/23 06:35:25.478
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/26/23 06:35:25.497
STEP: creating service externalsvc in namespace services-526 08/26/23 06:35:25.497
STEP: creating replication controller externalsvc in namespace services-526 08/26/23 06:35:25.518
I0826 06:35:25.526008      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-526, replica count: 2
I0826 06:35:28.578592      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 08/26/23 06:35:28.583
Aug 26 06:35:28.602: INFO: Creating new exec pod
Aug 26 06:35:28.612: INFO: Waiting up to 5m0s for pod "execpod6hkts" in namespace "services-526" to be "running"
Aug 26 06:35:28.618: INFO: Pod "execpod6hkts": Phase="Pending", Reason="", readiness=false. Elapsed: 6.157339ms
Aug 26 06:35:30.625: INFO: Pod "execpod6hkts": Phase="Running", Reason="", readiness=true. Elapsed: 2.01264417s
Aug 26 06:35:30.625: INFO: Pod "execpod6hkts" satisfied condition "running"
Aug 26 06:35:30.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-526 exec execpod6hkts -- /bin/sh -x -c nslookup nodeport-service.services-526.svc.cluster.local'
Aug 26 06:35:30.847: INFO: stderr: "+ nslookup nodeport-service.services-526.svc.cluster.local\n"
Aug 26 06:35:30.847: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nnodeport-service.services-526.svc.cluster.local\tcanonical name = externalsvc.services-526.svc.cluster.local.\nName:\texternalsvc.services-526.svc.cluster.local\nAddress: 10.21.69.25\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-526, will wait for the garbage collector to delete the pods 08/26/23 06:35:30.847
Aug 26 06:35:30.910: INFO: Deleting ReplicationController externalsvc took: 8.245306ms
Aug 26 06:35:31.011: INFO: Terminating ReplicationController externalsvc pods took: 101.146864ms
Aug 26 06:35:33.236: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:33.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-526" for this suite. 08/26/23 06:35:33.261
------------------------------
• [SLOW TEST] [7.824 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:25.449
    Aug 26 06:35:25.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 06:35:25.45
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:25.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:25.475
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-526 08/26/23 06:35:25.478
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/26/23 06:35:25.497
    STEP: creating service externalsvc in namespace services-526 08/26/23 06:35:25.497
    STEP: creating replication controller externalsvc in namespace services-526 08/26/23 06:35:25.518
    I0826 06:35:25.526008      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-526, replica count: 2
    I0826 06:35:28.578592      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 08/26/23 06:35:28.583
    Aug 26 06:35:28.602: INFO: Creating new exec pod
    Aug 26 06:35:28.612: INFO: Waiting up to 5m0s for pod "execpod6hkts" in namespace "services-526" to be "running"
    Aug 26 06:35:28.618: INFO: Pod "execpod6hkts": Phase="Pending", Reason="", readiness=false. Elapsed: 6.157339ms
    Aug 26 06:35:30.625: INFO: Pod "execpod6hkts": Phase="Running", Reason="", readiness=true. Elapsed: 2.01264417s
    Aug 26 06:35:30.625: INFO: Pod "execpod6hkts" satisfied condition "running"
    Aug 26 06:35:30.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-526 exec execpod6hkts -- /bin/sh -x -c nslookup nodeport-service.services-526.svc.cluster.local'
    Aug 26 06:35:30.847: INFO: stderr: "+ nslookup nodeport-service.services-526.svc.cluster.local\n"
    Aug 26 06:35:30.847: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nnodeport-service.services-526.svc.cluster.local\tcanonical name = externalsvc.services-526.svc.cluster.local.\nName:\texternalsvc.services-526.svc.cluster.local\nAddress: 10.21.69.25\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-526, will wait for the garbage collector to delete the pods 08/26/23 06:35:30.847
    Aug 26 06:35:30.910: INFO: Deleting ReplicationController externalsvc took: 8.245306ms
    Aug 26 06:35:31.011: INFO: Terminating ReplicationController externalsvc pods took: 101.146864ms
    Aug 26 06:35:33.236: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:33.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-526" for this suite. 08/26/23 06:35:33.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:33.275
Aug 26 06:35:33.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename pods 08/26/23 06:35:33.276
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:33.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:33.304
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 08/26/23 06:35:33.309
STEP: setting up watch 08/26/23 06:35:33.31
STEP: submitting the pod to kubernetes 08/26/23 06:35:33.417
STEP: verifying the pod is in kubernetes 08/26/23 06:35:33.428
STEP: verifying pod creation was observed 08/26/23 06:35:33.436
Aug 26 06:35:33.436: INFO: Waiting up to 5m0s for pod "pod-submit-remove-b74df634-c2b1-4477-8952-bb63b2552be7" in namespace "pods-215" to be "running"
Aug 26 06:35:33.442: INFO: Pod "pod-submit-remove-b74df634-c2b1-4477-8952-bb63b2552be7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.400889ms
Aug 26 06:35:35.446: INFO: Pod "pod-submit-remove-b74df634-c2b1-4477-8952-bb63b2552be7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009830401s
Aug 26 06:35:35.446: INFO: Pod "pod-submit-remove-b74df634-c2b1-4477-8952-bb63b2552be7" satisfied condition "running"
STEP: deleting the pod gracefully 08/26/23 06:35:35.45
STEP: verifying pod deletion was observed 08/26/23 06:35:35.459
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:37.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-215" for this suite. 08/26/23 06:35:37.383
------------------------------
• [4.115 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:33.275
    Aug 26 06:35:33.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename pods 08/26/23 06:35:33.276
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:33.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:33.304
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 08/26/23 06:35:33.309
    STEP: setting up watch 08/26/23 06:35:33.31
    STEP: submitting the pod to kubernetes 08/26/23 06:35:33.417
    STEP: verifying the pod is in kubernetes 08/26/23 06:35:33.428
    STEP: verifying pod creation was observed 08/26/23 06:35:33.436
    Aug 26 06:35:33.436: INFO: Waiting up to 5m0s for pod "pod-submit-remove-b74df634-c2b1-4477-8952-bb63b2552be7" in namespace "pods-215" to be "running"
    Aug 26 06:35:33.442: INFO: Pod "pod-submit-remove-b74df634-c2b1-4477-8952-bb63b2552be7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.400889ms
    Aug 26 06:35:35.446: INFO: Pod "pod-submit-remove-b74df634-c2b1-4477-8952-bb63b2552be7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009830401s
    Aug 26 06:35:35.446: INFO: Pod "pod-submit-remove-b74df634-c2b1-4477-8952-bb63b2552be7" satisfied condition "running"
    STEP: deleting the pod gracefully 08/26/23 06:35:35.45
    STEP: verifying pod deletion was observed 08/26/23 06:35:35.459
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:37.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-215" for this suite. 08/26/23 06:35:37.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:37.391
Aug 26 06:35:37.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename gc 08/26/23 06:35:37.392
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:37.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:37.412
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 08/26/23 06:35:37.415
STEP: Wait for the Deployment to create new ReplicaSet 08/26/23 06:35:37.425
STEP: delete the deployment 08/26/23 06:35:37.936
STEP: wait for all rs to be garbage collected 08/26/23 06:35:37.946
STEP: expected 0 pods, got 2 pods 08/26/23 06:35:37.95
STEP: Gathering metrics 08/26/23 06:35:38.47
W0826 06:35:38.538391      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 26 06:35:38.538: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 26 06:35:38.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1023" for this suite. 08/26/23 06:35:38.55
------------------------------
• [1.174 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:37.391
    Aug 26 06:35:37.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename gc 08/26/23 06:35:37.392
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:37.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:37.412
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 08/26/23 06:35:37.415
    STEP: Wait for the Deployment to create new ReplicaSet 08/26/23 06:35:37.425
    STEP: delete the deployment 08/26/23 06:35:37.936
    STEP: wait for all rs to be garbage collected 08/26/23 06:35:37.946
    STEP: expected 0 pods, got 2 pods 08/26/23 06:35:37.95
    STEP: Gathering metrics 08/26/23 06:35:38.47
    W0826 06:35:38.538391      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 26 06:35:38.538: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:35:38.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1023" for this suite. 08/26/23 06:35:38.55
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:35:38.566
Aug 26 06:35:38.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename taint-multiple-pods 08/26/23 06:35:38.567
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:38.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:38.596
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Aug 26 06:35:38.599: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 26 06:36:38.663: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Aug 26 06:36:38.667: INFO: Starting informer...
STEP: Starting pods... 08/26/23 06:36:38.667
Aug 26 06:36:38.898: INFO: Pod1 is running on ip-10-0-1-101.us-west-2.compute.internal. Tainting Node
Aug 26 06:36:39.112: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9815" to be "running"
Aug 26 06:36:39.116: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468912ms
Aug 26 06:36:41.121: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009162468s
Aug 26 06:36:41.121: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Aug 26 06:36:41.121: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9815" to be "running"
Aug 26 06:36:41.126: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.484446ms
Aug 26 06:36:41.126: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Aug 26 06:36:41.126: INFO: Pod2 is running on ip-10-0-1-101.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 08/26/23 06:36:41.126
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/26/23 06:36:41.15
STEP: Waiting for Pod1 and Pod2 to be deleted 08/26/23 06:36:41.158
Aug 26 06:36:47.554: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 26 06:37:06.948: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/26/23 06:37:06.963
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:37:06.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-9815" for this suite. 08/26/23 06:37:06.985
------------------------------
• [SLOW TEST] [88.427 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:35:38.566
    Aug 26 06:35:38.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename taint-multiple-pods 08/26/23 06:35:38.567
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:35:38.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:35:38.596
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Aug 26 06:35:38.599: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 26 06:36:38.663: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Aug 26 06:36:38.667: INFO: Starting informer...
    STEP: Starting pods... 08/26/23 06:36:38.667
    Aug 26 06:36:38.898: INFO: Pod1 is running on ip-10-0-1-101.us-west-2.compute.internal. Tainting Node
    Aug 26 06:36:39.112: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9815" to be "running"
    Aug 26 06:36:39.116: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468912ms
    Aug 26 06:36:41.121: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009162468s
    Aug 26 06:36:41.121: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Aug 26 06:36:41.121: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9815" to be "running"
    Aug 26 06:36:41.126: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.484446ms
    Aug 26 06:36:41.126: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Aug 26 06:36:41.126: INFO: Pod2 is running on ip-10-0-1-101.us-west-2.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 08/26/23 06:36:41.126
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/26/23 06:36:41.15
    STEP: Waiting for Pod1 and Pod2 to be deleted 08/26/23 06:36:41.158
    Aug 26 06:36:47.554: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Aug 26 06:37:06.948: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/26/23 06:37:06.963
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:37:06.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-9815" for this suite. 08/26/23 06:37:06.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:37:06.994
Aug 26 06:37:06.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename subpath 08/26/23 06:37:06.995
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:07.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:07.015
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/26/23 06:37:07.018
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-jcvp 08/26/23 06:37:07.028
STEP: Creating a pod to test atomic-volume-subpath 08/26/23 06:37:07.028
Aug 26 06:37:07.036: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jcvp" in namespace "subpath-7321" to be "Succeeded or Failed"
Aug 26 06:37:07.039: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.117051ms
Aug 26 06:37:09.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 2.008012728s
Aug 26 06:37:11.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 4.00794962s
Aug 26 06:37:13.045: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 6.00918878s
Aug 26 06:37:15.049: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 8.012972743s
Aug 26 06:37:17.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 10.008166655s
Aug 26 06:37:19.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 12.007507554s
Aug 26 06:37:21.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 14.00776158s
Aug 26 06:37:23.045: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 16.008670002s
Aug 26 06:37:25.048: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 18.011498647s
Aug 26 06:37:27.053: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 20.017135811s
Aug 26 06:37:29.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=false. Elapsed: 22.008227394s
Aug 26 06:37:31.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00769629s
STEP: Saw pod success 08/26/23 06:37:31.044
Aug 26 06:37:31.044: INFO: Pod "pod-subpath-test-configmap-jcvp" satisfied condition "Succeeded or Failed"
Aug 26 06:37:31.048: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-subpath-test-configmap-jcvp container test-container-subpath-configmap-jcvp: <nil>
STEP: delete the pod 08/26/23 06:37:31.067
Aug 26 06:37:31.084: INFO: Waiting for pod pod-subpath-test-configmap-jcvp to disappear
Aug 26 06:37:31.087: INFO: Pod pod-subpath-test-configmap-jcvp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jcvp 08/26/23 06:37:31.087
Aug 26 06:37:31.087: INFO: Deleting pod "pod-subpath-test-configmap-jcvp" in namespace "subpath-7321"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 26 06:37:31.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7321" for this suite. 08/26/23 06:37:31.098
------------------------------
• [SLOW TEST] [24.110 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:37:06.994
    Aug 26 06:37:06.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename subpath 08/26/23 06:37:06.995
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:07.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:07.015
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/26/23 06:37:07.018
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-jcvp 08/26/23 06:37:07.028
    STEP: Creating a pod to test atomic-volume-subpath 08/26/23 06:37:07.028
    Aug 26 06:37:07.036: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jcvp" in namespace "subpath-7321" to be "Succeeded or Failed"
    Aug 26 06:37:07.039: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.117051ms
    Aug 26 06:37:09.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 2.008012728s
    Aug 26 06:37:11.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 4.00794962s
    Aug 26 06:37:13.045: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 6.00918878s
    Aug 26 06:37:15.049: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 8.012972743s
    Aug 26 06:37:17.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 10.008166655s
    Aug 26 06:37:19.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 12.007507554s
    Aug 26 06:37:21.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 14.00776158s
    Aug 26 06:37:23.045: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 16.008670002s
    Aug 26 06:37:25.048: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 18.011498647s
    Aug 26 06:37:27.053: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=true. Elapsed: 20.017135811s
    Aug 26 06:37:29.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Running", Reason="", readiness=false. Elapsed: 22.008227394s
    Aug 26 06:37:31.044: INFO: Pod "pod-subpath-test-configmap-jcvp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00769629s
    STEP: Saw pod success 08/26/23 06:37:31.044
    Aug 26 06:37:31.044: INFO: Pod "pod-subpath-test-configmap-jcvp" satisfied condition "Succeeded or Failed"
    Aug 26 06:37:31.048: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod pod-subpath-test-configmap-jcvp container test-container-subpath-configmap-jcvp: <nil>
    STEP: delete the pod 08/26/23 06:37:31.067
    Aug 26 06:37:31.084: INFO: Waiting for pod pod-subpath-test-configmap-jcvp to disappear
    Aug 26 06:37:31.087: INFO: Pod pod-subpath-test-configmap-jcvp no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-jcvp 08/26/23 06:37:31.087
    Aug 26 06:37:31.087: INFO: Deleting pod "pod-subpath-test-configmap-jcvp" in namespace "subpath-7321"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:37:31.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7321" for this suite. 08/26/23 06:37:31.098
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:37:31.105
Aug 26 06:37:31.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:37:31.108
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:31.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:31.125
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-543a9dd3-f33e-45a8-ae90-f38be175ea5e 08/26/23 06:37:31.138
STEP: Creating configMap with name cm-test-opt-upd-4f0b9499-2562-4364-bf1a-946f831fd874 08/26/23 06:37:31.143
STEP: Creating the pod 08/26/23 06:37:31.152
Aug 26 06:37:31.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9" in namespace "projected-2299" to be "running and ready"
Aug 26 06:37:31.166: INFO: Pod "pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.084834ms
Aug 26 06:37:31.166: INFO: The phase of Pod pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9 is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:37:33.170: INFO: Pod "pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007602063s
Aug 26 06:37:33.170: INFO: The phase of Pod pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9 is Running (Ready = true)
Aug 26 06:37:33.170: INFO: Pod "pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-543a9dd3-f33e-45a8-ae90-f38be175ea5e 08/26/23 06:37:33.205
STEP: Updating configmap cm-test-opt-upd-4f0b9499-2562-4364-bf1a-946f831fd874 08/26/23 06:37:33.21
STEP: Creating configMap with name cm-test-opt-create-deed9d39-c203-43f2-a3d7-484b9ec3e914 08/26/23 06:37:33.214
STEP: waiting to observe update in volume 08/26/23 06:37:33.218
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:37:35.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2299" for this suite. 08/26/23 06:37:35.279
------------------------------
• [4.183 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:37:31.105
    Aug 26 06:37:31.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:37:31.108
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:31.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:31.125
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-543a9dd3-f33e-45a8-ae90-f38be175ea5e 08/26/23 06:37:31.138
    STEP: Creating configMap with name cm-test-opt-upd-4f0b9499-2562-4364-bf1a-946f831fd874 08/26/23 06:37:31.143
    STEP: Creating the pod 08/26/23 06:37:31.152
    Aug 26 06:37:31.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9" in namespace "projected-2299" to be "running and ready"
    Aug 26 06:37:31.166: INFO: Pod "pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.084834ms
    Aug 26 06:37:31.166: INFO: The phase of Pod pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:37:33.170: INFO: Pod "pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007602063s
    Aug 26 06:37:33.170: INFO: The phase of Pod pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9 is Running (Ready = true)
    Aug 26 06:37:33.170: INFO: Pod "pod-projected-configmaps-439919f9-29f5-4296-816a-19c59fc082c9" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-543a9dd3-f33e-45a8-ae90-f38be175ea5e 08/26/23 06:37:33.205
    STEP: Updating configmap cm-test-opt-upd-4f0b9499-2562-4364-bf1a-946f831fd874 08/26/23 06:37:33.21
    STEP: Creating configMap with name cm-test-opt-create-deed9d39-c203-43f2-a3d7-484b9ec3e914 08/26/23 06:37:33.214
    STEP: waiting to observe update in volume 08/26/23 06:37:33.218
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:37:35.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2299" for this suite. 08/26/23 06:37:35.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:37:35.29
Aug 26 06:37:35.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 06:37:35.291
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:35.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:35.311
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 06:37:35.327
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:37:35.791
STEP: Deploying the webhook pod 08/26/23 06:37:35.801
STEP: Wait for the deployment to be ready 08/26/23 06:37:35.816
Aug 26 06:37:35.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 26 06:37:37.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 6, 37, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 37, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 37, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 37, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/26/23 06:37:39.846
STEP: Verifying the service has paired with the endpoint 08/26/23 06:37:39.858
Aug 26 06:37:40.858: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Aug 26 06:37:40.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8931-crds.webhook.example.com via the AdmissionRegistration API 08/26/23 06:37:41.376
STEP: Creating a custom resource while v1 is storage version 08/26/23 06:37:41.396
STEP: Patching Custom Resource Definition to set v2 as storage 08/26/23 06:37:43.477
STEP: Patching the custom resource while v2 is storage version 08/26/23 06:37:43.483
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:37:44.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6132" for this suite. 08/26/23 06:37:44.174
STEP: Destroying namespace "webhook-6132-markers" for this suite. 08/26/23 06:37:44.187
------------------------------
• [SLOW TEST] [8.910 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:37:35.29
    Aug 26 06:37:35.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 06:37:35.291
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:35.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:35.311
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 06:37:35.327
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:37:35.791
    STEP: Deploying the webhook pod 08/26/23 06:37:35.801
    STEP: Wait for the deployment to be ready 08/26/23 06:37:35.816
    Aug 26 06:37:35.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 26 06:37:37.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 26, 6, 37, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 37, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 26, 6, 37, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 26, 6, 37, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/26/23 06:37:39.846
    STEP: Verifying the service has paired with the endpoint 08/26/23 06:37:39.858
    Aug 26 06:37:40.858: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Aug 26 06:37:40.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8931-crds.webhook.example.com via the AdmissionRegistration API 08/26/23 06:37:41.376
    STEP: Creating a custom resource while v1 is storage version 08/26/23 06:37:41.396
    STEP: Patching Custom Resource Definition to set v2 as storage 08/26/23 06:37:43.477
    STEP: Patching the custom resource while v2 is storage version 08/26/23 06:37:43.483
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:37:44.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6132" for this suite. 08/26/23 06:37:44.174
    STEP: Destroying namespace "webhook-6132-markers" for this suite. 08/26/23 06:37:44.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:37:44.201
Aug 26 06:37:44.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 06:37:44.202
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:44.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:44.229
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 06:37:44.261
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:37:45.221
STEP: Deploying the webhook pod 08/26/23 06:37:45.227
STEP: Wait for the deployment to be ready 08/26/23 06:37:45.253
Aug 26 06:37:45.263: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 06:37:47.274
STEP: Verifying the service has paired with the endpoint 08/26/23 06:37:47.286
Aug 26 06:37:48.288: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/26/23 06:37:48.292
STEP: create a pod that should be updated by the webhook 08/26/23 06:37:48.311
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:37:48.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-432" for this suite. 08/26/23 06:37:48.432
STEP: Destroying namespace "webhook-432-markers" for this suite. 08/26/23 06:37:48.443
------------------------------
• [4.254 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:37:44.201
    Aug 26 06:37:44.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 06:37:44.202
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:44.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:44.229
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 06:37:44.261
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:37:45.221
    STEP: Deploying the webhook pod 08/26/23 06:37:45.227
    STEP: Wait for the deployment to be ready 08/26/23 06:37:45.253
    Aug 26 06:37:45.263: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 06:37:47.274
    STEP: Verifying the service has paired with the endpoint 08/26/23 06:37:47.286
    Aug 26 06:37:48.288: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/26/23 06:37:48.292
    STEP: create a pod that should be updated by the webhook 08/26/23 06:37:48.311
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:37:48.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-432" for this suite. 08/26/23 06:37:48.432
    STEP: Destroying namespace "webhook-432-markers" for this suite. 08/26/23 06:37:48.443
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:37:48.458
Aug 26 06:37:48.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename namespaces 08/26/23 06:37:48.459
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:48.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:48.501
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 08/26/23 06:37:48.506
STEP: patching the Namespace 08/26/23 06:37:48.521
STEP: get the Namespace and ensuring it has the label 08/26/23 06:37:48.533
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:37:48.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5114" for this suite. 08/26/23 06:37:48.55
STEP: Destroying namespace "nspatchtest-a4c66708-20f9-4878-a775-6ab5b25d1110-2786" for this suite. 08/26/23 06:37:48.561
------------------------------
• [0.115 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:37:48.458
    Aug 26 06:37:48.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename namespaces 08/26/23 06:37:48.459
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:48.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:48.501
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 08/26/23 06:37:48.506
    STEP: patching the Namespace 08/26/23 06:37:48.521
    STEP: get the Namespace and ensuring it has the label 08/26/23 06:37:48.533
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:37:48.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5114" for this suite. 08/26/23 06:37:48.55
    STEP: Destroying namespace "nspatchtest-a4c66708-20f9-4878-a775-6ab5b25d1110-2786" for this suite. 08/26/23 06:37:48.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:37:48.573
Aug 26 06:37:48.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename taint-single-pod 08/26/23 06:37:48.575
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:48.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:48.613
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Aug 26 06:37:48.616: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 26 06:38:48.666: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Aug 26 06:38:48.674: INFO: Starting informer...
STEP: Starting pod... 08/26/23 06:38:48.674
Aug 26 06:38:48.895: INFO: Pod is running on ip-10-0-1-5.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 08/26/23 06:38:48.895
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/26/23 06:38:48.912
STEP: Waiting short time to make sure Pod is queued for deletion 08/26/23 06:38:48.919
Aug 26 06:38:48.919: INFO: Pod wasn't evicted. Proceeding
Aug 26 06:38:48.919: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/26/23 06:38:48.944
STEP: Waiting some time to make sure that toleration time passed. 08/26/23 06:38:48.95
Aug 26 06:40:03.952: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:40:03.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-1895" for this suite. 08/26/23 06:40:03.965
------------------------------
• [SLOW TEST] [135.403 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:37:48.573
    Aug 26 06:37:48.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename taint-single-pod 08/26/23 06:37:48.575
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:37:48.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:37:48.613
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Aug 26 06:37:48.616: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 26 06:38:48.666: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Aug 26 06:38:48.674: INFO: Starting informer...
    STEP: Starting pod... 08/26/23 06:38:48.674
    Aug 26 06:38:48.895: INFO: Pod is running on ip-10-0-1-5.us-west-2.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 08/26/23 06:38:48.895
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/26/23 06:38:48.912
    STEP: Waiting short time to make sure Pod is queued for deletion 08/26/23 06:38:48.919
    Aug 26 06:38:48.919: INFO: Pod wasn't evicted. Proceeding
    Aug 26 06:38:48.919: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/26/23 06:38:48.944
    STEP: Waiting some time to make sure that toleration time passed. 08/26/23 06:38:48.95
    Aug 26 06:40:03.952: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:40:03.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-1895" for this suite. 08/26/23 06:40:03.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:40:03.981
Aug 26 06:40:03.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 06:40:03.983
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:04.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:04.007
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-8969 08/26/23 06:40:04.012
STEP: creating service affinity-clusterip-transition in namespace services-8969 08/26/23 06:40:04.012
STEP: creating replication controller affinity-clusterip-transition in namespace services-8969 08/26/23 06:40:04.027
I0826 06:40:04.041093      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8969, replica count: 3
I0826 06:40:07.091364      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 26 06:40:07.101: INFO: Creating new exec pod
Aug 26 06:40:07.110: INFO: Waiting up to 5m0s for pod "execpod-affinity8bzjd" in namespace "services-8969" to be "running"
Aug 26 06:40:07.113: INFO: Pod "execpod-affinity8bzjd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.646272ms
Aug 26 06:40:09.118: INFO: Pod "execpod-affinity8bzjd": Phase="Running", Reason="", readiness=true. Elapsed: 2.008418519s
Aug 26 06:40:09.118: INFO: Pod "execpod-affinity8bzjd" satisfied condition "running"
Aug 26 06:40:10.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8969 exec execpod-affinity8bzjd -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Aug 26 06:40:10.294: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 26 06:40:10.294: INFO: stdout: ""
Aug 26 06:40:10.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8969 exec execpod-affinity8bzjd -- /bin/sh -x -c nc -v -z -w 2 10.21.182.190 80'
Aug 26 06:40:10.453: INFO: stderr: "+ nc -v -z -w 2 10.21.182.190 80\nConnection to 10.21.182.190 80 port [tcp/http] succeeded!\n"
Aug 26 06:40:10.453: INFO: stdout: ""
Aug 26 06:40:10.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8969 exec execpod-affinity8bzjd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.182.190:80/ ; done'
Aug 26 06:40:10.729: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n"
Aug 26 06:40:10.729: INFO: stdout: "\naffinity-clusterip-transition-nhjgf\naffinity-clusterip-transition-pdrf9\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-nhjgf\naffinity-clusterip-transition-pdrf9\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-nhjgf\naffinity-clusterip-transition-pdrf9\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-nhjgf\naffinity-clusterip-transition-pdrf9\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-nhjgf\naffinity-clusterip-transition-pdrf9\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-nhjgf"
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-pdrf9
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-pdrf9
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-pdrf9
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-pdrf9
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-pdrf9
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
Aug 26 06:40:10.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8969 exec execpod-affinity8bzjd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.182.190:80/ ; done'
Aug 26 06:40:11.004: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n"
Aug 26 06:40:11.004: INFO: stdout: "\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw"
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
Aug 26 06:40:11.004: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8969, will wait for the garbage collector to delete the pods 08/26/23 06:40:11.021
Aug 26 06:40:11.085: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.921396ms
Aug 26 06:40:11.186: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.70113ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 06:40:13.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8969" for this suite. 08/26/23 06:40:13.118
------------------------------
• [SLOW TEST] [9.145 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:40:03.981
    Aug 26 06:40:03.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 06:40:03.983
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:04.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:04.007
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-8969 08/26/23 06:40:04.012
    STEP: creating service affinity-clusterip-transition in namespace services-8969 08/26/23 06:40:04.012
    STEP: creating replication controller affinity-clusterip-transition in namespace services-8969 08/26/23 06:40:04.027
    I0826 06:40:04.041093      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8969, replica count: 3
    I0826 06:40:07.091364      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 26 06:40:07.101: INFO: Creating new exec pod
    Aug 26 06:40:07.110: INFO: Waiting up to 5m0s for pod "execpod-affinity8bzjd" in namespace "services-8969" to be "running"
    Aug 26 06:40:07.113: INFO: Pod "execpod-affinity8bzjd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.646272ms
    Aug 26 06:40:09.118: INFO: Pod "execpod-affinity8bzjd": Phase="Running", Reason="", readiness=true. Elapsed: 2.008418519s
    Aug 26 06:40:09.118: INFO: Pod "execpod-affinity8bzjd" satisfied condition "running"
    Aug 26 06:40:10.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8969 exec execpod-affinity8bzjd -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Aug 26 06:40:10.294: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Aug 26 06:40:10.294: INFO: stdout: ""
    Aug 26 06:40:10.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8969 exec execpod-affinity8bzjd -- /bin/sh -x -c nc -v -z -w 2 10.21.182.190 80'
    Aug 26 06:40:10.453: INFO: stderr: "+ nc -v -z -w 2 10.21.182.190 80\nConnection to 10.21.182.190 80 port [tcp/http] succeeded!\n"
    Aug 26 06:40:10.453: INFO: stdout: ""
    Aug 26 06:40:10.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8969 exec execpod-affinity8bzjd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.182.190:80/ ; done'
    Aug 26 06:40:10.729: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n"
    Aug 26 06:40:10.729: INFO: stdout: "\naffinity-clusterip-transition-nhjgf\naffinity-clusterip-transition-pdrf9\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-nhjgf\naffinity-clusterip-transition-pdrf9\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-nhjgf\naffinity-clusterip-transition-pdrf9\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-nhjgf\naffinity-clusterip-transition-pdrf9\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-nhjgf\naffinity-clusterip-transition-pdrf9\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-nhjgf"
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-pdrf9
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-pdrf9
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-pdrf9
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-pdrf9
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-pdrf9
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:10.730: INFO: Received response from host: affinity-clusterip-transition-nhjgf
    Aug 26 06:40:10.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-8969 exec execpod-affinity8bzjd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.182.190:80/ ; done'
    Aug 26 06:40:11.004: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.182.190:80/\n"
    Aug 26 06:40:11.004: INFO: stdout: "\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw\naffinity-clusterip-transition-gcslw"
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Received response from host: affinity-clusterip-transition-gcslw
    Aug 26 06:40:11.004: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8969, will wait for the garbage collector to delete the pods 08/26/23 06:40:11.021
    Aug 26 06:40:11.085: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.921396ms
    Aug 26 06:40:11.186: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.70113ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:40:13.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8969" for this suite. 08/26/23 06:40:13.118
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:40:13.126
Aug 26 06:40:13.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 06:40:13.127
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:13.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:13.146
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 08/26/23 06:40:13.149
STEP: Counting existing ResourceQuota 08/26/23 06:40:18.152
STEP: Creating a ResourceQuota 08/26/23 06:40:23.157
STEP: Ensuring resource quota status is calculated 08/26/23 06:40:23.163
STEP: Creating a Secret 08/26/23 06:40:25.17
STEP: Ensuring resource quota status captures secret creation 08/26/23 06:40:25.188
STEP: Deleting a secret 08/26/23 06:40:27.193
STEP: Ensuring resource quota status released usage 08/26/23 06:40:27.201
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 06:40:29.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7393" for this suite. 08/26/23 06:40:29.218
------------------------------
• [SLOW TEST] [16.099 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:40:13.126
    Aug 26 06:40:13.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 06:40:13.127
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:13.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:13.146
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 08/26/23 06:40:13.149
    STEP: Counting existing ResourceQuota 08/26/23 06:40:18.152
    STEP: Creating a ResourceQuota 08/26/23 06:40:23.157
    STEP: Ensuring resource quota status is calculated 08/26/23 06:40:23.163
    STEP: Creating a Secret 08/26/23 06:40:25.17
    STEP: Ensuring resource quota status captures secret creation 08/26/23 06:40:25.188
    STEP: Deleting a secret 08/26/23 06:40:27.193
    STEP: Ensuring resource quota status released usage 08/26/23 06:40:27.201
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:40:29.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7393" for this suite. 08/26/23 06:40:29.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:40:29.231
Aug 26 06:40:29.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sysctl 08/26/23 06:40:29.232
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:29.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:29.249
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/26/23 06:40:29.252
STEP: Watching for error events or started pod 08/26/23 06:40:29.26
STEP: Waiting for pod completion 08/26/23 06:40:31.267
Aug 26 06:40:31.267: INFO: Waiting up to 3m0s for pod "sysctl-81a23d11-03ec-41bc-aa3e-7e5ba0a35dc2" in namespace "sysctl-832" to be "completed"
Aug 26 06:40:31.271: INFO: Pod "sysctl-81a23d11-03ec-41bc-aa3e-7e5ba0a35dc2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.96249ms
Aug 26 06:40:33.276: INFO: Pod "sysctl-81a23d11-03ec-41bc-aa3e-7e5ba0a35dc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008835654s
Aug 26 06:40:33.276: INFO: Pod "sysctl-81a23d11-03ec-41bc-aa3e-7e5ba0a35dc2" satisfied condition "completed"
STEP: Checking that the pod succeeded 08/26/23 06:40:33.28
STEP: Getting logs from the pod 08/26/23 06:40:33.28
STEP: Checking that the sysctl is actually updated 08/26/23 06:40:33.296
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:40:33.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-832" for this suite. 08/26/23 06:40:33.309
------------------------------
• [4.087 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:40:29.231
    Aug 26 06:40:29.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sysctl 08/26/23 06:40:29.232
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:29.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:29.249
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/26/23 06:40:29.252
    STEP: Watching for error events or started pod 08/26/23 06:40:29.26
    STEP: Waiting for pod completion 08/26/23 06:40:31.267
    Aug 26 06:40:31.267: INFO: Waiting up to 3m0s for pod "sysctl-81a23d11-03ec-41bc-aa3e-7e5ba0a35dc2" in namespace "sysctl-832" to be "completed"
    Aug 26 06:40:31.271: INFO: Pod "sysctl-81a23d11-03ec-41bc-aa3e-7e5ba0a35dc2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.96249ms
    Aug 26 06:40:33.276: INFO: Pod "sysctl-81a23d11-03ec-41bc-aa3e-7e5ba0a35dc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008835654s
    Aug 26 06:40:33.276: INFO: Pod "sysctl-81a23d11-03ec-41bc-aa3e-7e5ba0a35dc2" satisfied condition "completed"
    STEP: Checking that the pod succeeded 08/26/23 06:40:33.28
    STEP: Getting logs from the pod 08/26/23 06:40:33.28
    STEP: Checking that the sysctl is actually updated 08/26/23 06:40:33.296
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:40:33.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-832" for this suite. 08/26/23 06:40:33.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:40:33.319
Aug 26 06:40:33.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename svcaccounts 08/26/23 06:40:33.32
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:33.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:33.354
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Aug 26 06:40:33.363: INFO: Got root ca configmap in namespace "svcaccounts-9566"
Aug 26 06:40:33.370: INFO: Deleted root ca configmap in namespace "svcaccounts-9566"
STEP: waiting for a new root ca configmap created 08/26/23 06:40:33.87
Aug 26 06:40:33.874: INFO: Recreated root ca configmap in namespace "svcaccounts-9566"
Aug 26 06:40:33.881: INFO: Updated root ca configmap in namespace "svcaccounts-9566"
STEP: waiting for the root ca configmap reconciled 08/26/23 06:40:34.381
Aug 26 06:40:34.384: INFO: Reconciled root ca configmap in namespace "svcaccounts-9566"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 26 06:40:34.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9566" for this suite. 08/26/23 06:40:34.394
------------------------------
• [1.091 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:40:33.319
    Aug 26 06:40:33.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename svcaccounts 08/26/23 06:40:33.32
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:33.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:33.354
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Aug 26 06:40:33.363: INFO: Got root ca configmap in namespace "svcaccounts-9566"
    Aug 26 06:40:33.370: INFO: Deleted root ca configmap in namespace "svcaccounts-9566"
    STEP: waiting for a new root ca configmap created 08/26/23 06:40:33.87
    Aug 26 06:40:33.874: INFO: Recreated root ca configmap in namespace "svcaccounts-9566"
    Aug 26 06:40:33.881: INFO: Updated root ca configmap in namespace "svcaccounts-9566"
    STEP: waiting for the root ca configmap reconciled 08/26/23 06:40:34.381
    Aug 26 06:40:34.384: INFO: Reconciled root ca configmap in namespace "svcaccounts-9566"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:40:34.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9566" for this suite. 08/26/23 06:40:34.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:40:34.411
Aug 26 06:40:34.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-lifecycle-hook 08/26/23 06:40:34.412
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:34.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:34.431
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/26/23 06:40:34.443
Aug 26 06:40:34.453: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-925" to be "running and ready"
Aug 26 06:40:34.459: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.960533ms
Aug 26 06:40:34.459: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:40:36.465: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012055215s
Aug 26 06:40:36.465: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 26 06:40:36.465: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 08/26/23 06:40:36.469
Aug 26 06:40:36.474: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-925" to be "running and ready"
Aug 26 06:40:36.478: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468207ms
Aug 26 06:40:36.478: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:40:38.486: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012164622s
Aug 26 06:40:38.486: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Aug 26 06:40:38.486: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/26/23 06:40:38.493
Aug 26 06:40:38.505: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 26 06:40:38.510: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 26 06:40:40.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 26 06:40:40.515: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 26 06:40:42.512: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 26 06:40:42.517: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 08/26/23 06:40:42.517
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 26 06:40:42.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-925" for this suite. 08/26/23 06:40:42.544
------------------------------
• [SLOW TEST] [8.141 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:40:34.411
    Aug 26 06:40:34.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/26/23 06:40:34.412
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:34.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:34.431
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/26/23 06:40:34.443
    Aug 26 06:40:34.453: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-925" to be "running and ready"
    Aug 26 06:40:34.459: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.960533ms
    Aug 26 06:40:34.459: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:40:36.465: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012055215s
    Aug 26 06:40:36.465: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 26 06:40:36.465: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 08/26/23 06:40:36.469
    Aug 26 06:40:36.474: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-925" to be "running and ready"
    Aug 26 06:40:36.478: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468207ms
    Aug 26 06:40:36.478: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:40:38.486: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012164622s
    Aug 26 06:40:38.486: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Aug 26 06:40:38.486: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/26/23 06:40:38.493
    Aug 26 06:40:38.505: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 26 06:40:38.510: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 26 06:40:40.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 26 06:40:40.515: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 26 06:40:42.512: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 26 06:40:42.517: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 08/26/23 06:40:42.517
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:40:42.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-925" for this suite. 08/26/23 06:40:42.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:40:42.551
Aug 26 06:40:42.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename subpath 08/26/23 06:40:42.553
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:42.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:42.57
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/26/23 06:40:42.574
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-cq8s 08/26/23 06:40:42.583
STEP: Creating a pod to test atomic-volume-subpath 08/26/23 06:40:42.583
Aug 26 06:40:42.592: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cq8s" in namespace "subpath-4311" to be "Succeeded or Failed"
Aug 26 06:40:42.596: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.80156ms
Aug 26 06:40:44.600: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 2.008516731s
Aug 26 06:40:46.603: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 4.010850696s
Aug 26 06:40:48.604: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 6.012036528s
Aug 26 06:40:50.600: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 8.008168236s
Aug 26 06:40:52.600: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 10.008614694s
Aug 26 06:40:54.601: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 12.009424236s
Aug 26 06:40:56.602: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 14.010282657s
Aug 26 06:40:58.602: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 16.009964622s
Aug 26 06:41:00.600: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 18.007876005s
Aug 26 06:41:02.603: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 20.010673155s
Aug 26 06:41:04.601: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=false. Elapsed: 22.009368984s
Aug 26 06:41:06.601: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008953972s
STEP: Saw pod success 08/26/23 06:41:06.601
Aug 26 06:41:06.601: INFO: Pod "pod-subpath-test-projected-cq8s" satisfied condition "Succeeded or Failed"
Aug 26 06:41:06.606: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-subpath-test-projected-cq8s container test-container-subpath-projected-cq8s: <nil>
STEP: delete the pod 08/26/23 06:41:06.614
Aug 26 06:41:06.628: INFO: Waiting for pod pod-subpath-test-projected-cq8s to disappear
Aug 26 06:41:06.632: INFO: Pod pod-subpath-test-projected-cq8s no longer exists
STEP: Deleting pod pod-subpath-test-projected-cq8s 08/26/23 06:41:06.632
Aug 26 06:41:06.633: INFO: Deleting pod "pod-subpath-test-projected-cq8s" in namespace "subpath-4311"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 26 06:41:06.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4311" for this suite. 08/26/23 06:41:06.646
------------------------------
• [SLOW TEST] [24.102 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:40:42.551
    Aug 26 06:40:42.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename subpath 08/26/23 06:40:42.553
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:40:42.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:40:42.57
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/26/23 06:40:42.574
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-cq8s 08/26/23 06:40:42.583
    STEP: Creating a pod to test atomic-volume-subpath 08/26/23 06:40:42.583
    Aug 26 06:40:42.592: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cq8s" in namespace "subpath-4311" to be "Succeeded or Failed"
    Aug 26 06:40:42.596: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.80156ms
    Aug 26 06:40:44.600: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 2.008516731s
    Aug 26 06:40:46.603: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 4.010850696s
    Aug 26 06:40:48.604: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 6.012036528s
    Aug 26 06:40:50.600: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 8.008168236s
    Aug 26 06:40:52.600: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 10.008614694s
    Aug 26 06:40:54.601: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 12.009424236s
    Aug 26 06:40:56.602: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 14.010282657s
    Aug 26 06:40:58.602: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 16.009964622s
    Aug 26 06:41:00.600: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 18.007876005s
    Aug 26 06:41:02.603: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=true. Elapsed: 20.010673155s
    Aug 26 06:41:04.601: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Running", Reason="", readiness=false. Elapsed: 22.009368984s
    Aug 26 06:41:06.601: INFO: Pod "pod-subpath-test-projected-cq8s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008953972s
    STEP: Saw pod success 08/26/23 06:41:06.601
    Aug 26 06:41:06.601: INFO: Pod "pod-subpath-test-projected-cq8s" satisfied condition "Succeeded or Failed"
    Aug 26 06:41:06.606: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-subpath-test-projected-cq8s container test-container-subpath-projected-cq8s: <nil>
    STEP: delete the pod 08/26/23 06:41:06.614
    Aug 26 06:41:06.628: INFO: Waiting for pod pod-subpath-test-projected-cq8s to disappear
    Aug 26 06:41:06.632: INFO: Pod pod-subpath-test-projected-cq8s no longer exists
    STEP: Deleting pod pod-subpath-test-projected-cq8s 08/26/23 06:41:06.632
    Aug 26 06:41:06.633: INFO: Deleting pod "pod-subpath-test-projected-cq8s" in namespace "subpath-4311"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:41:06.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4311" for this suite. 08/26/23 06:41:06.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:41:06.654
Aug 26 06:41:06.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename security-context-test 08/26/23 06:41:06.656
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:41:06.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:41:06.677
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Aug 26 06:41:06.693: INFO: Waiting up to 5m0s for pod "busybox-user-65534-306a140b-948f-4bc9-a921-97142456f6d2" in namespace "security-context-test-7911" to be "Succeeded or Failed"
Aug 26 06:41:06.696: INFO: Pod "busybox-user-65534-306a140b-948f-4bc9-a921-97142456f6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.4602ms
Aug 26 06:41:08.702: INFO: Pod "busybox-user-65534-306a140b-948f-4bc9-a921-97142456f6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00895131s
Aug 26 06:41:10.701: INFO: Pod "busybox-user-65534-306a140b-948f-4bc9-a921-97142456f6d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008920486s
Aug 26 06:41:10.702: INFO: Pod "busybox-user-65534-306a140b-948f-4bc9-a921-97142456f6d2" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 26 06:41:10.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7911" for this suite. 08/26/23 06:41:10.716
------------------------------
• [4.068 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:41:06.654
    Aug 26 06:41:06.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename security-context-test 08/26/23 06:41:06.656
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:41:06.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:41:06.677
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Aug 26 06:41:06.693: INFO: Waiting up to 5m0s for pod "busybox-user-65534-306a140b-948f-4bc9-a921-97142456f6d2" in namespace "security-context-test-7911" to be "Succeeded or Failed"
    Aug 26 06:41:06.696: INFO: Pod "busybox-user-65534-306a140b-948f-4bc9-a921-97142456f6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.4602ms
    Aug 26 06:41:08.702: INFO: Pod "busybox-user-65534-306a140b-948f-4bc9-a921-97142456f6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00895131s
    Aug 26 06:41:10.701: INFO: Pod "busybox-user-65534-306a140b-948f-4bc9-a921-97142456f6d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008920486s
    Aug 26 06:41:10.702: INFO: Pod "busybox-user-65534-306a140b-948f-4bc9-a921-97142456f6d2" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:41:10.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7911" for this suite. 08/26/23 06:41:10.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:41:10.725
Aug 26 06:41:10.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename configmap 08/26/23 06:41:10.726
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:41:10.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:41:10.753
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-29153197-204b-4613-8313-ccfd71d86a5b 08/26/23 06:41:10.756
STEP: Creating a pod to test consume configMaps 08/26/23 06:41:10.762
Aug 26 06:41:10.773: INFO: Waiting up to 5m0s for pod "pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30" in namespace "configmap-579" to be "Succeeded or Failed"
Aug 26 06:41:10.778: INFO: Pod "pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413678ms
Aug 26 06:41:12.782: INFO: Pod "pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009058203s
Aug 26 06:41:14.784: INFO: Pod "pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010601275s
STEP: Saw pod success 08/26/23 06:41:14.784
Aug 26 06:41:14.784: INFO: Pod "pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30" satisfied condition "Succeeded or Failed"
Aug 26 06:41:14.788: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30 container agnhost-container: <nil>
STEP: delete the pod 08/26/23 06:41:14.795
Aug 26 06:41:14.809: INFO: Waiting for pod pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30 to disappear
Aug 26 06:41:14.814: INFO: Pod pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 26 06:41:14.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-579" for this suite. 08/26/23 06:41:14.821
------------------------------
• [4.103 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:41:10.725
    Aug 26 06:41:10.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename configmap 08/26/23 06:41:10.726
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:41:10.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:41:10.753
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-29153197-204b-4613-8313-ccfd71d86a5b 08/26/23 06:41:10.756
    STEP: Creating a pod to test consume configMaps 08/26/23 06:41:10.762
    Aug 26 06:41:10.773: INFO: Waiting up to 5m0s for pod "pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30" in namespace "configmap-579" to be "Succeeded or Failed"
    Aug 26 06:41:10.778: INFO: Pod "pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413678ms
    Aug 26 06:41:12.782: INFO: Pod "pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009058203s
    Aug 26 06:41:14.784: INFO: Pod "pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010601275s
    STEP: Saw pod success 08/26/23 06:41:14.784
    Aug 26 06:41:14.784: INFO: Pod "pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30" satisfied condition "Succeeded or Failed"
    Aug 26 06:41:14.788: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30 container agnhost-container: <nil>
    STEP: delete the pod 08/26/23 06:41:14.795
    Aug 26 06:41:14.809: INFO: Waiting for pod pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30 to disappear
    Aug 26 06:41:14.814: INFO: Pod pod-configmaps-18f8f1d7-b7ff-45b5-a92f-3bc5f781ab30 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:41:14.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-579" for this suite. 08/26/23 06:41:14.821
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:41:14.829
Aug 26 06:41:14.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename subpath 08/26/23 06:41:14.83
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:41:14.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:41:14.847
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/26/23 06:41:14.854
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-gljl 08/26/23 06:41:14.865
STEP: Creating a pod to test atomic-volume-subpath 08/26/23 06:41:14.865
Aug 26 06:41:14.873: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-gljl" in namespace "subpath-165" to be "Succeeded or Failed"
Aug 26 06:41:14.876: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.058129ms
Aug 26 06:41:16.881: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 2.007762311s
Aug 26 06:41:18.881: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 4.007542864s
Aug 26 06:41:20.882: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 6.009071607s
Aug 26 06:41:22.881: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 8.007585053s
Aug 26 06:41:24.884: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 10.010265371s
Aug 26 06:41:26.883: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 12.009278306s
Aug 26 06:41:28.882: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 14.008521892s
Aug 26 06:41:30.880: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 16.007074809s
Aug 26 06:41:32.884: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 18.010675752s
Aug 26 06:41:34.881: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 20.008056233s
Aug 26 06:41:36.881: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=false. Elapsed: 22.008220382s
Aug 26 06:41:38.905: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.031698614s
STEP: Saw pod success 08/26/23 06:41:38.905
Aug 26 06:41:38.905: INFO: Pod "pod-subpath-test-secret-gljl" satisfied condition "Succeeded or Failed"
Aug 26 06:41:38.909: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-subpath-test-secret-gljl container test-container-subpath-secret-gljl: <nil>
STEP: delete the pod 08/26/23 06:41:38.919
Aug 26 06:41:38.934: INFO: Waiting for pod pod-subpath-test-secret-gljl to disappear
Aug 26 06:41:38.939: INFO: Pod pod-subpath-test-secret-gljl no longer exists
STEP: Deleting pod pod-subpath-test-secret-gljl 08/26/23 06:41:38.939
Aug 26 06:41:38.939: INFO: Deleting pod "pod-subpath-test-secret-gljl" in namespace "subpath-165"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 26 06:41:38.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-165" for this suite. 08/26/23 06:41:38.956
------------------------------
• [SLOW TEST] [24.136 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:41:14.829
    Aug 26 06:41:14.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename subpath 08/26/23 06:41:14.83
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:41:14.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:41:14.847
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/26/23 06:41:14.854
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-gljl 08/26/23 06:41:14.865
    STEP: Creating a pod to test atomic-volume-subpath 08/26/23 06:41:14.865
    Aug 26 06:41:14.873: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-gljl" in namespace "subpath-165" to be "Succeeded or Failed"
    Aug 26 06:41:14.876: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.058129ms
    Aug 26 06:41:16.881: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 2.007762311s
    Aug 26 06:41:18.881: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 4.007542864s
    Aug 26 06:41:20.882: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 6.009071607s
    Aug 26 06:41:22.881: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 8.007585053s
    Aug 26 06:41:24.884: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 10.010265371s
    Aug 26 06:41:26.883: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 12.009278306s
    Aug 26 06:41:28.882: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 14.008521892s
    Aug 26 06:41:30.880: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 16.007074809s
    Aug 26 06:41:32.884: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 18.010675752s
    Aug 26 06:41:34.881: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=true. Elapsed: 20.008056233s
    Aug 26 06:41:36.881: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Running", Reason="", readiness=false. Elapsed: 22.008220382s
    Aug 26 06:41:38.905: INFO: Pod "pod-subpath-test-secret-gljl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.031698614s
    STEP: Saw pod success 08/26/23 06:41:38.905
    Aug 26 06:41:38.905: INFO: Pod "pod-subpath-test-secret-gljl" satisfied condition "Succeeded or Failed"
    Aug 26 06:41:38.909: INFO: Trying to get logs from node ip-10-0-1-31.us-west-2.compute.internal pod pod-subpath-test-secret-gljl container test-container-subpath-secret-gljl: <nil>
    STEP: delete the pod 08/26/23 06:41:38.919
    Aug 26 06:41:38.934: INFO: Waiting for pod pod-subpath-test-secret-gljl to disappear
    Aug 26 06:41:38.939: INFO: Pod pod-subpath-test-secret-gljl no longer exists
    STEP: Deleting pod pod-subpath-test-secret-gljl 08/26/23 06:41:38.939
    Aug 26 06:41:38.939: INFO: Deleting pod "pod-subpath-test-secret-gljl" in namespace "subpath-165"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:41:38.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-165" for this suite. 08/26/23 06:41:38.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:41:38.968
Aug 26 06:41:38.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-probe 08/26/23 06:41:38.97
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:41:38.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:41:38.997
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Aug 26 06:41:39.013: INFO: Waiting up to 5m0s for pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd" in namespace "container-probe-5779" to be "running and ready"
Aug 26 06:41:39.023: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.211712ms
Aug 26 06:41:39.023: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:41:41.028: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 2.014342569s
Aug 26 06:41:41.028: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
Aug 26 06:41:43.029: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 4.015782023s
Aug 26 06:41:43.029: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
Aug 26 06:41:45.027: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 6.013489624s
Aug 26 06:41:45.027: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
Aug 26 06:41:47.028: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 8.014605414s
Aug 26 06:41:47.028: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
Aug 26 06:41:49.032: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 10.018445799s
Aug 26 06:41:49.032: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
Aug 26 06:41:51.028: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 12.014351085s
Aug 26 06:41:51.028: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
Aug 26 06:41:53.026: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 14.013116714s
Aug 26 06:41:53.026: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
Aug 26 06:41:55.027: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 16.013679282s
Aug 26 06:41:55.027: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
Aug 26 06:41:57.030: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 18.016475834s
Aug 26 06:41:57.030: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
Aug 26 06:41:59.027: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 20.01351012s
Aug 26 06:41:59.027: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
Aug 26 06:42:01.027: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=true. Elapsed: 22.013482197s
Aug 26 06:42:01.027: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = true)
Aug 26 06:42:01.027: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd" satisfied condition "running and ready"
Aug 26 06:42:01.030: INFO: Container started at 2023-08-26 06:41:39 +0000 UTC, pod became ready at 2023-08-26 06:41:59 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 26 06:42:01.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5779" for this suite. 08/26/23 06:42:01.038
------------------------------
• [SLOW TEST] [22.085 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:41:38.968
    Aug 26 06:41:38.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-probe 08/26/23 06:41:38.97
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:41:38.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:41:38.997
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Aug 26 06:41:39.013: INFO: Waiting up to 5m0s for pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd" in namespace "container-probe-5779" to be "running and ready"
    Aug 26 06:41:39.023: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.211712ms
    Aug 26 06:41:39.023: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:41:41.028: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 2.014342569s
    Aug 26 06:41:41.028: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
    Aug 26 06:41:43.029: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 4.015782023s
    Aug 26 06:41:43.029: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
    Aug 26 06:41:45.027: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 6.013489624s
    Aug 26 06:41:45.027: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
    Aug 26 06:41:47.028: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 8.014605414s
    Aug 26 06:41:47.028: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
    Aug 26 06:41:49.032: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 10.018445799s
    Aug 26 06:41:49.032: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
    Aug 26 06:41:51.028: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 12.014351085s
    Aug 26 06:41:51.028: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
    Aug 26 06:41:53.026: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 14.013116714s
    Aug 26 06:41:53.026: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
    Aug 26 06:41:55.027: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 16.013679282s
    Aug 26 06:41:55.027: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
    Aug 26 06:41:57.030: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 18.016475834s
    Aug 26 06:41:57.030: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
    Aug 26 06:41:59.027: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=false. Elapsed: 20.01351012s
    Aug 26 06:41:59.027: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = false)
    Aug 26 06:42:01.027: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd": Phase="Running", Reason="", readiness=true. Elapsed: 22.013482197s
    Aug 26 06:42:01.027: INFO: The phase of Pod test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd is Running (Ready = true)
    Aug 26 06:42:01.027: INFO: Pod "test-webserver-e396f06b-02f6-4697-b41f-99953b5782dd" satisfied condition "running and ready"
    Aug 26 06:42:01.030: INFO: Container started at 2023-08-26 06:41:39 +0000 UTC, pod became ready at 2023-08-26 06:41:59 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:42:01.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5779" for this suite. 08/26/23 06:42:01.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:42:01.056
Aug 26 06:42:01.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename certificates 08/26/23 06:42:01.057
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:01.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:01.098
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 08/26/23 06:42:02.196
STEP: getting /apis/certificates.k8s.io 08/26/23 06:42:02.199
STEP: getting /apis/certificates.k8s.io/v1 08/26/23 06:42:02.2
STEP: creating 08/26/23 06:42:02.201
STEP: getting 08/26/23 06:42:02.219
STEP: listing 08/26/23 06:42:02.222
STEP: watching 08/26/23 06:42:02.227
Aug 26 06:42:02.227: INFO: starting watch
STEP: patching 08/26/23 06:42:02.228
STEP: updating 08/26/23 06:42:02.236
Aug 26 06:42:02.242: INFO: waiting for watch events with expected annotations
Aug 26 06:42:02.242: INFO: saw patched and updated annotations
STEP: getting /approval 08/26/23 06:42:02.242
STEP: patching /approval 08/26/23 06:42:02.246
STEP: updating /approval 08/26/23 06:42:02.253
STEP: getting /status 08/26/23 06:42:02.261
STEP: patching /status 08/26/23 06:42:02.266
STEP: updating /status 08/26/23 06:42:02.276
STEP: deleting 08/26/23 06:42:02.286
STEP: deleting a collection 08/26/23 06:42:02.341
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:42:02.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-7438" for this suite. 08/26/23 06:42:02.375
------------------------------
• [1.326 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:42:01.056
    Aug 26 06:42:01.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename certificates 08/26/23 06:42:01.057
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:01.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:01.098
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 08/26/23 06:42:02.196
    STEP: getting /apis/certificates.k8s.io 08/26/23 06:42:02.199
    STEP: getting /apis/certificates.k8s.io/v1 08/26/23 06:42:02.2
    STEP: creating 08/26/23 06:42:02.201
    STEP: getting 08/26/23 06:42:02.219
    STEP: listing 08/26/23 06:42:02.222
    STEP: watching 08/26/23 06:42:02.227
    Aug 26 06:42:02.227: INFO: starting watch
    STEP: patching 08/26/23 06:42:02.228
    STEP: updating 08/26/23 06:42:02.236
    Aug 26 06:42:02.242: INFO: waiting for watch events with expected annotations
    Aug 26 06:42:02.242: INFO: saw patched and updated annotations
    STEP: getting /approval 08/26/23 06:42:02.242
    STEP: patching /approval 08/26/23 06:42:02.246
    STEP: updating /approval 08/26/23 06:42:02.253
    STEP: getting /status 08/26/23 06:42:02.261
    STEP: patching /status 08/26/23 06:42:02.266
    STEP: updating /status 08/26/23 06:42:02.276
    STEP: deleting 08/26/23 06:42:02.286
    STEP: deleting a collection 08/26/23 06:42:02.341
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:42:02.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-7438" for this suite. 08/26/23 06:42:02.375
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:42:02.383
Aug 26 06:42:02.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename security-context-test 08/26/23 06:42:02.386
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:02.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:02.413
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Aug 26 06:42:02.430: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-67c8d874-3d8f-40f3-ad9e-33bb5394714b" in namespace "security-context-test-6473" to be "Succeeded or Failed"
Aug 26 06:42:02.434: INFO: Pod "busybox-readonly-false-67c8d874-3d8f-40f3-ad9e-33bb5394714b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.996358ms
Aug 26 06:42:04.438: INFO: Pod "busybox-readonly-false-67c8d874-3d8f-40f3-ad9e-33bb5394714b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008107295s
Aug 26 06:42:06.439: INFO: Pod "busybox-readonly-false-67c8d874-3d8f-40f3-ad9e-33bb5394714b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008743081s
Aug 26 06:42:06.439: INFO: Pod "busybox-readonly-false-67c8d874-3d8f-40f3-ad9e-33bb5394714b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 26 06:42:06.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6473" for this suite. 08/26/23 06:42:06.458
------------------------------
• [4.084 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:42:02.383
    Aug 26 06:42:02.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename security-context-test 08/26/23 06:42:02.386
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:02.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:02.413
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Aug 26 06:42:02.430: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-67c8d874-3d8f-40f3-ad9e-33bb5394714b" in namespace "security-context-test-6473" to be "Succeeded or Failed"
    Aug 26 06:42:02.434: INFO: Pod "busybox-readonly-false-67c8d874-3d8f-40f3-ad9e-33bb5394714b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.996358ms
    Aug 26 06:42:04.438: INFO: Pod "busybox-readonly-false-67c8d874-3d8f-40f3-ad9e-33bb5394714b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008107295s
    Aug 26 06:42:06.439: INFO: Pod "busybox-readonly-false-67c8d874-3d8f-40f3-ad9e-33bb5394714b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008743081s
    Aug 26 06:42:06.439: INFO: Pod "busybox-readonly-false-67c8d874-3d8f-40f3-ad9e-33bb5394714b" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:42:06.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6473" for this suite. 08/26/23 06:42:06.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:42:06.471
Aug 26 06:42:06.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename watch 08/26/23 06:42:06.474
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:06.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:06.493
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 08/26/23 06:42:06.498
STEP: creating a new configmap 08/26/23 06:42:06.5
STEP: modifying the configmap once 08/26/23 06:42:06.506
STEP: changing the label value of the configmap 08/26/23 06:42:06.519
STEP: Expecting to observe a delete notification for the watched object 08/26/23 06:42:06.536
Aug 26 06:42:06.537: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43643 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:42:06.537: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43644 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:42:06.537: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43645 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 08/26/23 06:42:06.537
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/26/23 06:42:06.555
STEP: changing the label value of the configmap back 08/26/23 06:42:16.556
STEP: modifying the configmap a third time 08/26/23 06:42:16.566
STEP: deleting the configmap 08/26/23 06:42:16.576
STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/26/23 06:42:16.586
Aug 26 06:42:16.586: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43694 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:42:16.586: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43695 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 26 06:42:16.586: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43696 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 26 06:42:16.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8679" for this suite. 08/26/23 06:42:16.596
------------------------------
• [SLOW TEST] [10.131 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:42:06.471
    Aug 26 06:42:06.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename watch 08/26/23 06:42:06.474
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:06.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:06.493
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 08/26/23 06:42:06.498
    STEP: creating a new configmap 08/26/23 06:42:06.5
    STEP: modifying the configmap once 08/26/23 06:42:06.506
    STEP: changing the label value of the configmap 08/26/23 06:42:06.519
    STEP: Expecting to observe a delete notification for the watched object 08/26/23 06:42:06.536
    Aug 26 06:42:06.537: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43643 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:42:06.537: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43644 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:42:06.537: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43645 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 08/26/23 06:42:06.537
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/26/23 06:42:06.555
    STEP: changing the label value of the configmap back 08/26/23 06:42:16.556
    STEP: modifying the configmap a third time 08/26/23 06:42:16.566
    STEP: deleting the configmap 08/26/23 06:42:16.576
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/26/23 06:42:16.586
    Aug 26 06:42:16.586: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43694 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:42:16.586: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43695 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 26 06:42:16.586: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8679  4025ec56-7bb2-4f06-beb7-0994d4035d9a 43696 0 2023-08-26 06:42:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-26 06:42:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:42:16.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8679" for this suite. 08/26/23 06:42:16.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:42:16.603
Aug 26 06:42:16.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 06:42:16.604
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:16.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:16.871
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 08/26/23 06:42:16.873
STEP: Creating a ResourceQuota 08/26/23 06:42:21.878
STEP: Ensuring resource quota status is calculated 08/26/23 06:42:21.886
STEP: Creating a Service 08/26/23 06:42:23.89
STEP: Creating a NodePort Service 08/26/23 06:42:23.908
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/26/23 06:42:23.935
STEP: Ensuring resource quota status captures service creation 08/26/23 06:42:23.961
STEP: Deleting Services 08/26/23 06:42:25.966
STEP: Ensuring resource quota status released usage 08/26/23 06:42:26.029
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 06:42:28.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5964" for this suite. 08/26/23 06:42:28.044
------------------------------
• [SLOW TEST] [11.449 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:42:16.603
    Aug 26 06:42:16.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 06:42:16.604
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:16.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:16.871
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 08/26/23 06:42:16.873
    STEP: Creating a ResourceQuota 08/26/23 06:42:21.878
    STEP: Ensuring resource quota status is calculated 08/26/23 06:42:21.886
    STEP: Creating a Service 08/26/23 06:42:23.89
    STEP: Creating a NodePort Service 08/26/23 06:42:23.908
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/26/23 06:42:23.935
    STEP: Ensuring resource quota status captures service creation 08/26/23 06:42:23.961
    STEP: Deleting Services 08/26/23 06:42:25.966
    STEP: Ensuring resource quota status released usage 08/26/23 06:42:26.029
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:42:28.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5964" for this suite. 08/26/23 06:42:28.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:42:28.055
Aug 26 06:42:28.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename init-container 08/26/23 06:42:28.056
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:28.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:28.09
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 08/26/23 06:42:28.094
Aug 26 06:42:28.094: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:42:31.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4766" for this suite. 08/26/23 06:42:31.179
------------------------------
• [3.139 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:42:28.055
    Aug 26 06:42:28.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename init-container 08/26/23 06:42:28.056
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:28.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:28.09
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 08/26/23 06:42:28.094
    Aug 26 06:42:28.094: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:42:31.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4766" for this suite. 08/26/23 06:42:31.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:42:31.197
Aug 26 06:42:31.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename cronjob 08/26/23 06:42:31.199
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:31.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:31.217
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 08/26/23 06:42:31.221
STEP: Ensuring more than one job is running at a time 08/26/23 06:42:31.228
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/26/23 06:44:01.234
STEP: Removing cronjob 08/26/23 06:44:01.24
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 26 06:44:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9465" for this suite. 08/26/23 06:44:01.253
------------------------------
• [SLOW TEST] [90.082 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:42:31.197
    Aug 26 06:42:31.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename cronjob 08/26/23 06:42:31.199
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:42:31.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:42:31.217
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 08/26/23 06:42:31.221
    STEP: Ensuring more than one job is running at a time 08/26/23 06:42:31.228
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/26/23 06:44:01.234
    STEP: Removing cronjob 08/26/23 06:44:01.24
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:44:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9465" for this suite. 08/26/23 06:44:01.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:44:01.282
Aug 26 06:44:01.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sched-preemption 08/26/23 06:44:01.283
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:44:01.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:44:01.313
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 26 06:44:01.344: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 26 06:45:01.413: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 08/26/23 06:45:01.417
Aug 26 06:45:01.450: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 26 06:45:01.462: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 26 06:45:01.508: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 26 06:45:01.516: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 26 06:45:01.546: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 26 06:45:01.572: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Aug 26 06:45:01.638: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Aug 26 06:45:01.648: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Aug 26 06:45:01.711: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Aug 26 06:45:01.731: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/26/23 06:45:01.731
Aug 26 06:45:01.732: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:01.739: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.250918ms
Aug 26 06:45:03.743: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.011201572s
Aug 26 06:45:03.743: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 26 06:45:03.743: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:03.746: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.292418ms
Aug 26 06:45:03.746: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:45:03.746: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:03.750: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.869897ms
Aug 26 06:45:03.750: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:45:03.750: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:03.754: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.533348ms
Aug 26 06:45:03.754: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:45:03.754: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:03.758: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.022003ms
Aug 26 06:45:03.758: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:45:03.758: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:03.761: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.263496ms
Aug 26 06:45:03.761: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:45:03.761: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:03.765: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.369442ms
Aug 26 06:45:03.765: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:45:03.765: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:03.768: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.039794ms
Aug 26 06:45:03.768: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:45:03.768: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:03.771: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.210109ms
Aug 26 06:45:03.771: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:45:03.771: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:03.774: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.176728ms
Aug 26 06:45:03.774: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/26/23 06:45:03.774
Aug 26 06:45:03.779: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-2051" to be "running"
Aug 26 06:45:03.785: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.936439ms
Aug 26 06:45:05.789: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009776886s
Aug 26 06:45:07.790: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010475629s
Aug 26 06:45:09.790: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010731112s
Aug 26 06:45:09.790: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:45:09.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-2051" for this suite. 08/26/23 06:45:09.885
------------------------------
• [SLOW TEST] [68.612 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:44:01.282
    Aug 26 06:44:01.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sched-preemption 08/26/23 06:44:01.283
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:44:01.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:44:01.313
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 26 06:44:01.344: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 26 06:45:01.413: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 08/26/23 06:45:01.417
    Aug 26 06:45:01.450: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 26 06:45:01.462: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 26 06:45:01.508: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 26 06:45:01.516: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 26 06:45:01.546: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 26 06:45:01.572: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Aug 26 06:45:01.638: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Aug 26 06:45:01.648: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Aug 26 06:45:01.711: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Aug 26 06:45:01.731: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/26/23 06:45:01.731
    Aug 26 06:45:01.732: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:01.739: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.250918ms
    Aug 26 06:45:03.743: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.011201572s
    Aug 26 06:45:03.743: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 26 06:45:03.743: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:03.746: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.292418ms
    Aug 26 06:45:03.746: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:45:03.746: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:03.750: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.869897ms
    Aug 26 06:45:03.750: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:45:03.750: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:03.754: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.533348ms
    Aug 26 06:45:03.754: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:45:03.754: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:03.758: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.022003ms
    Aug 26 06:45:03.758: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:45:03.758: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:03.761: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.263496ms
    Aug 26 06:45:03.761: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:45:03.761: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:03.765: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.369442ms
    Aug 26 06:45:03.765: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:45:03.765: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:03.768: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.039794ms
    Aug 26 06:45:03.768: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:45:03.768: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:03.771: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.210109ms
    Aug 26 06:45:03.771: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:45:03.771: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:03.774: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.176728ms
    Aug 26 06:45:03.774: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/26/23 06:45:03.774
    Aug 26 06:45:03.779: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-2051" to be "running"
    Aug 26 06:45:03.785: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.936439ms
    Aug 26 06:45:05.789: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009776886s
    Aug 26 06:45:07.790: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010475629s
    Aug 26 06:45:09.790: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010731112s
    Aug 26 06:45:09.790: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:45:09.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-2051" for this suite. 08/26/23 06:45:09.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:45:09.895
Aug 26 06:45:09.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename endpointslice 08/26/23 06:45:09.896
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:09.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:09.915
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Aug 26 06:45:09.926: INFO: Endpoints addresses: [10.0.1.206 10.0.1.230 10.0.1.54] , ports: [443]
Aug 26 06:45:09.926: INFO: EndpointSlices addresses: [10.0.1.206 10.0.1.230 10.0.1.54] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 26 06:45:09.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-109" for this suite. 08/26/23 06:45:09.932
------------------------------
• [0.042 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:45:09.895
    Aug 26 06:45:09.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename endpointslice 08/26/23 06:45:09.896
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:09.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:09.915
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Aug 26 06:45:09.926: INFO: Endpoints addresses: [10.0.1.206 10.0.1.230 10.0.1.54] , ports: [443]
    Aug 26 06:45:09.926: INFO: EndpointSlices addresses: [10.0.1.206 10.0.1.230 10.0.1.54] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:45:09.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-109" for this suite. 08/26/23 06:45:09.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:45:09.939
Aug 26 06:45:09.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:45:09.94
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:09.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:09.955
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 08/26/23 06:45:09.957
Aug 26 06:45:09.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: rename a version 08/26/23 06:45:14.627
STEP: check the new version name is served 08/26/23 06:45:14.65
STEP: check the old version name is removed 08/26/23 06:45:17.011
STEP: check the other version is not changed 08/26/23 06:45:17.806
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:45:21.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6533" for this suite. 08/26/23 06:45:21.435
------------------------------
• [SLOW TEST] [11.507 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:45:09.939
    Aug 26 06:45:09.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename crd-publish-openapi 08/26/23 06:45:09.94
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:09.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:09.955
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 08/26/23 06:45:09.957
    Aug 26 06:45:09.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: rename a version 08/26/23 06:45:14.627
    STEP: check the new version name is served 08/26/23 06:45:14.65
    STEP: check the old version name is removed 08/26/23 06:45:17.011
    STEP: check the other version is not changed 08/26/23 06:45:17.806
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:45:21.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6533" for this suite. 08/26/23 06:45:21.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:45:21.447
Aug 26 06:45:21.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 06:45:21.448
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:21.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:21.476
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 08/26/23 06:45:21.479
Aug 26 06:45:21.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 26 06:45:21.561: INFO: stderr: ""
Aug 26 06:45:21.561: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 08/26/23 06:45:21.561
Aug 26 06:45:21.561: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 26 06:45:21.561: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3194" to be "running and ready, or succeeded"
Aug 26 06:45:21.568: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375344ms
Aug 26 06:45:21.568: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-0-1-101.us-west-2.compute.internal' to be 'Running' but was 'Pending'
Aug 26 06:45:23.573: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012013276s
Aug 26 06:45:23.573: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 26 06:45:23.573: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 08/26/23 06:45:23.574
Aug 26 06:45:23.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator'
Aug 26 06:45:23.734: INFO: stderr: ""
Aug 26 06:45:23.734: INFO: stdout: "I0826 06:45:22.546844       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/7kp 466\nI0826 06:45:22.747245       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/v69 305\nI0826 06:45:22.947622       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/hz56 384\nI0826 06:45:23.146920       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/jsh 566\nI0826 06:45:23.347171       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/rh5 589\nI0826 06:45:23.547506       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/9g68 354\n"
STEP: limiting log lines 08/26/23 06:45:23.734
Aug 26 06:45:23.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator --tail=1'
Aug 26 06:45:23.861: INFO: stderr: ""
Aug 26 06:45:23.861: INFO: stdout: "I0826 06:45:23.747897       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/pz27 454\n"
Aug 26 06:45:23.861: INFO: got output "I0826 06:45:23.747897       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/pz27 454\n"
STEP: limiting log bytes 08/26/23 06:45:23.861
Aug 26 06:45:23.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator --limit-bytes=1'
Aug 26 06:45:23.974: INFO: stderr: ""
Aug 26 06:45:23.974: INFO: stdout: "I"
Aug 26 06:45:23.974: INFO: got output "I"
STEP: exposing timestamps 08/26/23 06:45:23.974
Aug 26 06:45:23.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 26 06:45:24.067: INFO: stderr: ""
Aug 26 06:45:24.068: INFO: stdout: "2023-08-26T06:45:23.947359841Z I0826 06:45:23.947158       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/9djl 420\n"
Aug 26 06:45:24.068: INFO: got output "2023-08-26T06:45:23.947359841Z I0826 06:45:23.947158       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/9djl 420\n"
STEP: restricting to a time range 08/26/23 06:45:24.068
Aug 26 06:45:26.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator --since=1s'
Aug 26 06:45:26.673: INFO: stderr: ""
Aug 26 06:45:26.673: INFO: stdout: "I0826 06:45:25.747831       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/8qxg 333\nI0826 06:45:25.947501       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/x5fp 516\nI0826 06:45:26.147698       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/qrdv 293\nI0826 06:45:26.347726       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/kc2 416\nI0826 06:45:26.546979       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/g9d9 345\n"
Aug 26 06:45:26.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator --since=24h'
Aug 26 06:45:26.765: INFO: stderr: ""
Aug 26 06:45:26.765: INFO: stdout: "I0826 06:45:22.546844       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/7kp 466\nI0826 06:45:22.747245       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/v69 305\nI0826 06:45:22.947622       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/hz56 384\nI0826 06:45:23.146920       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/jsh 566\nI0826 06:45:23.347171       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/rh5 589\nI0826 06:45:23.547506       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/9g68 354\nI0826 06:45:23.747897       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/pz27 454\nI0826 06:45:23.947158       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/9djl 420\nI0826 06:45:24.147142       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/bbl 285\nI0826 06:45:24.347666       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/rtq5 550\nI0826 06:45:24.546939       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/4l2 370\nI0826 06:45:24.747115       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/ksp 496\nI0826 06:45:24.947709       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/s9gh 503\nI0826 06:45:25.146977       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/bl4 320\nI0826 06:45:25.347181       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/2p9 400\nI0826 06:45:25.547527       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/lw2 361\nI0826 06:45:25.747831       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/8qxg 333\nI0826 06:45:25.947501       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/x5fp 516\nI0826 06:45:26.147698       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/qrdv 293\nI0826 06:45:26.347726       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/kc2 416\nI0826 06:45:26.546979       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/g9d9 345\nI0826 06:45:26.747339       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/k5hb 258\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Aug 26 06:45:26.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 delete pod logs-generator'
Aug 26 06:45:28.140: INFO: stderr: ""
Aug 26 06:45:28.140: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 06:45:28.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3194" for this suite. 08/26/23 06:45:28.148
------------------------------
• [SLOW TEST] [6.721 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:45:21.447
    Aug 26 06:45:21.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 06:45:21.448
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:21.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:21.476
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 08/26/23 06:45:21.479
    Aug 26 06:45:21.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Aug 26 06:45:21.561: INFO: stderr: ""
    Aug 26 06:45:21.561: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 08/26/23 06:45:21.561
    Aug 26 06:45:21.561: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Aug 26 06:45:21.561: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3194" to be "running and ready, or succeeded"
    Aug 26 06:45:21.568: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375344ms
    Aug 26 06:45:21.568: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-0-1-101.us-west-2.compute.internal' to be 'Running' but was 'Pending'
    Aug 26 06:45:23.573: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.012013276s
    Aug 26 06:45:23.573: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Aug 26 06:45:23.573: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 08/26/23 06:45:23.574
    Aug 26 06:45:23.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator'
    Aug 26 06:45:23.734: INFO: stderr: ""
    Aug 26 06:45:23.734: INFO: stdout: "I0826 06:45:22.546844       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/7kp 466\nI0826 06:45:22.747245       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/v69 305\nI0826 06:45:22.947622       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/hz56 384\nI0826 06:45:23.146920       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/jsh 566\nI0826 06:45:23.347171       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/rh5 589\nI0826 06:45:23.547506       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/9g68 354\n"
    STEP: limiting log lines 08/26/23 06:45:23.734
    Aug 26 06:45:23.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator --tail=1'
    Aug 26 06:45:23.861: INFO: stderr: ""
    Aug 26 06:45:23.861: INFO: stdout: "I0826 06:45:23.747897       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/pz27 454\n"
    Aug 26 06:45:23.861: INFO: got output "I0826 06:45:23.747897       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/pz27 454\n"
    STEP: limiting log bytes 08/26/23 06:45:23.861
    Aug 26 06:45:23.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator --limit-bytes=1'
    Aug 26 06:45:23.974: INFO: stderr: ""
    Aug 26 06:45:23.974: INFO: stdout: "I"
    Aug 26 06:45:23.974: INFO: got output "I"
    STEP: exposing timestamps 08/26/23 06:45:23.974
    Aug 26 06:45:23.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator --tail=1 --timestamps'
    Aug 26 06:45:24.067: INFO: stderr: ""
    Aug 26 06:45:24.068: INFO: stdout: "2023-08-26T06:45:23.947359841Z I0826 06:45:23.947158       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/9djl 420\n"
    Aug 26 06:45:24.068: INFO: got output "2023-08-26T06:45:23.947359841Z I0826 06:45:23.947158       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/9djl 420\n"
    STEP: restricting to a time range 08/26/23 06:45:24.068
    Aug 26 06:45:26.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator --since=1s'
    Aug 26 06:45:26.673: INFO: stderr: ""
    Aug 26 06:45:26.673: INFO: stdout: "I0826 06:45:25.747831       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/8qxg 333\nI0826 06:45:25.947501       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/x5fp 516\nI0826 06:45:26.147698       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/qrdv 293\nI0826 06:45:26.347726       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/kc2 416\nI0826 06:45:26.546979       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/g9d9 345\n"
    Aug 26 06:45:26.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 logs logs-generator logs-generator --since=24h'
    Aug 26 06:45:26.765: INFO: stderr: ""
    Aug 26 06:45:26.765: INFO: stdout: "I0826 06:45:22.546844       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/7kp 466\nI0826 06:45:22.747245       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/v69 305\nI0826 06:45:22.947622       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/hz56 384\nI0826 06:45:23.146920       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/jsh 566\nI0826 06:45:23.347171       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/rh5 589\nI0826 06:45:23.547506       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/9g68 354\nI0826 06:45:23.747897       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/pz27 454\nI0826 06:45:23.947158       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/9djl 420\nI0826 06:45:24.147142       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/bbl 285\nI0826 06:45:24.347666       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/rtq5 550\nI0826 06:45:24.546939       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/4l2 370\nI0826 06:45:24.747115       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/ksp 496\nI0826 06:45:24.947709       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/s9gh 503\nI0826 06:45:25.146977       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/bl4 320\nI0826 06:45:25.347181       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/2p9 400\nI0826 06:45:25.547527       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/lw2 361\nI0826 06:45:25.747831       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/8qxg 333\nI0826 06:45:25.947501       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/x5fp 516\nI0826 06:45:26.147698       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/qrdv 293\nI0826 06:45:26.347726       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/kc2 416\nI0826 06:45:26.546979       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/g9d9 345\nI0826 06:45:26.747339       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/k5hb 258\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Aug 26 06:45:26.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-3194 delete pod logs-generator'
    Aug 26 06:45:28.140: INFO: stderr: ""
    Aug 26 06:45:28.140: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:45:28.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3194" for this suite. 08/26/23 06:45:28.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:45:28.17
Aug 26 06:45:28.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename ephemeral-containers-test 08/26/23 06:45:28.176
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:28.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:28.205
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 08/26/23 06:45:28.21
Aug 26 06:45:28.232: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-384" to be "running and ready"
Aug 26 06:45:28.238: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.451643ms
Aug 26 06:45:28.238: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:45:30.245: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012627456s
Aug 26 06:45:30.245: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Aug 26 06:45:30.245: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 08/26/23 06:45:30.251
Aug 26 06:45:30.267: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-384" to be "container debugger running"
Aug 26 06:45:30.271: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.113518ms
Aug 26 06:45:32.279: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01222711s
Aug 26 06:45:34.277: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009739275s
Aug 26 06:45:34.277: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 08/26/23 06:45:34.277
Aug 26 06:45:34.277: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-384 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 26 06:45:34.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
Aug 26 06:45:34.278: INFO: ExecWithOptions: Clientset creation
Aug 26 06:45:34.278: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-384/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Aug 26 06:45:34.416: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:45:34.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-384" for this suite. 08/26/23 06:45:34.439
------------------------------
• [SLOW TEST] [6.277 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:45:28.17
    Aug 26 06:45:28.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename ephemeral-containers-test 08/26/23 06:45:28.176
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:28.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:28.205
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 08/26/23 06:45:28.21
    Aug 26 06:45:28.232: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-384" to be "running and ready"
    Aug 26 06:45:28.238: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.451643ms
    Aug 26 06:45:28.238: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:45:30.245: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012627456s
    Aug 26 06:45:30.245: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Aug 26 06:45:30.245: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 08/26/23 06:45:30.251
    Aug 26 06:45:30.267: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-384" to be "container debugger running"
    Aug 26 06:45:30.271: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.113518ms
    Aug 26 06:45:32.279: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01222711s
    Aug 26 06:45:34.277: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009739275s
    Aug 26 06:45:34.277: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 08/26/23 06:45:34.277
    Aug 26 06:45:34.277: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-384 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 26 06:45:34.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    Aug 26 06:45:34.278: INFO: ExecWithOptions: Clientset creation
    Aug 26 06:45:34.278: INFO: ExecWithOptions: execute(POST https://10.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-384/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Aug 26 06:45:34.416: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:45:34.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-384" for this suite. 08/26/23 06:45:34.439
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:45:34.448
Aug 26 06:45:34.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename gc 08/26/23 06:45:34.449
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:34.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:34.468
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 08/26/23 06:45:34.479
STEP: create the rc2 08/26/23 06:45:34.486
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/26/23 06:45:39.518
STEP: delete the rc simpletest-rc-to-be-deleted 08/26/23 06:45:40.41
STEP: wait for the rc to be deleted 08/26/23 06:45:40.421
Aug 26 06:45:45.459: INFO: 69 pods remaining
Aug 26 06:45:45.459: INFO: 69 pods has nil DeletionTimestamp
Aug 26 06:45:45.459: INFO: 
STEP: Gathering metrics 08/26/23 06:45:50.46
W0826 06:45:50.490973      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 26 06:45:50.491: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 26 06:45:50.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-26hjc" in namespace "gc-4757"
Aug 26 06:45:50.504: INFO: Deleting pod "simpletest-rc-to-be-deleted-28tcs" in namespace "gc-4757"
Aug 26 06:45:50.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tlpj" in namespace "gc-4757"
Aug 26 06:45:50.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vf2w" in namespace "gc-4757"
Aug 26 06:45:50.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wdmj" in namespace "gc-4757"
Aug 26 06:45:50.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z2p7" in namespace "gc-4757"
Aug 26 06:45:50.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-45bwl" in namespace "gc-4757"
Aug 26 06:45:50.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-4f4xb" in namespace "gc-4757"
Aug 26 06:45:50.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zmps" in namespace "gc-4757"
Aug 26 06:45:50.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-59z57" in namespace "gc-4757"
Aug 26 06:45:50.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rclq" in namespace "gc-4757"
Aug 26 06:45:50.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xf9v" in namespace "gc-4757"
Aug 26 06:45:50.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xrpd" in namespace "gc-4757"
Aug 26 06:45:50.755: INFO: Deleting pod "simpletest-rc-to-be-deleted-65dsz" in namespace "gc-4757"
Aug 26 06:45:50.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-76nbx" in namespace "gc-4757"
Aug 26 06:45:50.800: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bz68" in namespace "gc-4757"
Aug 26 06:45:50.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-7d4l7" in namespace "gc-4757"
Aug 26 06:45:50.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-7d5l2" in namespace "gc-4757"
Aug 26 06:45:50.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-7drkj" in namespace "gc-4757"
Aug 26 06:45:50.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dsdr" in namespace "gc-4757"
Aug 26 06:45:50.892: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v9l8" in namespace "gc-4757"
Aug 26 06:45:50.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-8brvm" in namespace "gc-4757"
Aug 26 06:45:50.934: INFO: Deleting pod "simpletest-rc-to-be-deleted-8glqg" in namespace "gc-4757"
Aug 26 06:45:50.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qrff" in namespace "gc-4757"
Aug 26 06:45:51.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vgd9" in namespace "gc-4757"
Aug 26 06:45:51.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-94595" in namespace "gc-4757"
Aug 26 06:45:51.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-94fwk" in namespace "gc-4757"
Aug 26 06:45:51.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-96d72" in namespace "gc-4757"
Aug 26 06:45:51.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-96rx2" in namespace "gc-4757"
Aug 26 06:45:51.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-978b8" in namespace "gc-4757"
Aug 26 06:45:51.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b7nn" in namespace "gc-4757"
Aug 26 06:45:51.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ltkx" in namespace "gc-4757"
Aug 26 06:45:51.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pfsx" in namespace "gc-4757"
Aug 26 06:45:51.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rb9p" in namespace "gc-4757"
Aug 26 06:45:51.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-9znzp" in namespace "gc-4757"
Aug 26 06:45:51.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2847" in namespace "gc-4757"
Aug 26 06:45:51.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9r87" in namespace "gc-4757"
Aug 26 06:45:51.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnl46" in namespace "gc-4757"
Aug 26 06:45:51.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-cj7ht" in namespace "gc-4757"
Aug 26 06:45:51.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl72b" in namespace "gc-4757"
Aug 26 06:45:51.733: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx4cz" in namespace "gc-4757"
Aug 26 06:45:51.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhfst" in namespace "gc-4757"
Aug 26 06:45:51.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-dn7bh" in namespace "gc-4757"
Aug 26 06:45:51.795: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpjff" in namespace "gc-4757"
Aug 26 06:45:51.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsq6r" in namespace "gc-4757"
Aug 26 06:45:51.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwq6q" in namespace "gc-4757"
Aug 26 06:45:51.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-f97wt" in namespace "gc-4757"
Aug 26 06:45:51.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftg9f" in namespace "gc-4757"
Aug 26 06:45:51.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx652" in namespace "gc-4757"
Aug 26 06:45:51.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsmlj" in namespace "gc-4757"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 26 06:45:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4757" for this suite. 08/26/23 06:45:51.954
------------------------------
• [SLOW TEST] [17.519 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:45:34.448
    Aug 26 06:45:34.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename gc 08/26/23 06:45:34.449
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:34.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:34.468
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 08/26/23 06:45:34.479
    STEP: create the rc2 08/26/23 06:45:34.486
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/26/23 06:45:39.518
    STEP: delete the rc simpletest-rc-to-be-deleted 08/26/23 06:45:40.41
    STEP: wait for the rc to be deleted 08/26/23 06:45:40.421
    Aug 26 06:45:45.459: INFO: 69 pods remaining
    Aug 26 06:45:45.459: INFO: 69 pods has nil DeletionTimestamp
    Aug 26 06:45:45.459: INFO: 
    STEP: Gathering metrics 08/26/23 06:45:50.46
    W0826 06:45:50.490973      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 26 06:45:50.491: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 26 06:45:50.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-26hjc" in namespace "gc-4757"
    Aug 26 06:45:50.504: INFO: Deleting pod "simpletest-rc-to-be-deleted-28tcs" in namespace "gc-4757"
    Aug 26 06:45:50.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tlpj" in namespace "gc-4757"
    Aug 26 06:45:50.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vf2w" in namespace "gc-4757"
    Aug 26 06:45:50.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wdmj" in namespace "gc-4757"
    Aug 26 06:45:50.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z2p7" in namespace "gc-4757"
    Aug 26 06:45:50.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-45bwl" in namespace "gc-4757"
    Aug 26 06:45:50.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-4f4xb" in namespace "gc-4757"
    Aug 26 06:45:50.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zmps" in namespace "gc-4757"
    Aug 26 06:45:50.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-59z57" in namespace "gc-4757"
    Aug 26 06:45:50.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rclq" in namespace "gc-4757"
    Aug 26 06:45:50.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xf9v" in namespace "gc-4757"
    Aug 26 06:45:50.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xrpd" in namespace "gc-4757"
    Aug 26 06:45:50.755: INFO: Deleting pod "simpletest-rc-to-be-deleted-65dsz" in namespace "gc-4757"
    Aug 26 06:45:50.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-76nbx" in namespace "gc-4757"
    Aug 26 06:45:50.800: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bz68" in namespace "gc-4757"
    Aug 26 06:45:50.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-7d4l7" in namespace "gc-4757"
    Aug 26 06:45:50.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-7d5l2" in namespace "gc-4757"
    Aug 26 06:45:50.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-7drkj" in namespace "gc-4757"
    Aug 26 06:45:50.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dsdr" in namespace "gc-4757"
    Aug 26 06:45:50.892: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v9l8" in namespace "gc-4757"
    Aug 26 06:45:50.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-8brvm" in namespace "gc-4757"
    Aug 26 06:45:50.934: INFO: Deleting pod "simpletest-rc-to-be-deleted-8glqg" in namespace "gc-4757"
    Aug 26 06:45:50.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qrff" in namespace "gc-4757"
    Aug 26 06:45:51.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vgd9" in namespace "gc-4757"
    Aug 26 06:45:51.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-94595" in namespace "gc-4757"
    Aug 26 06:45:51.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-94fwk" in namespace "gc-4757"
    Aug 26 06:45:51.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-96d72" in namespace "gc-4757"
    Aug 26 06:45:51.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-96rx2" in namespace "gc-4757"
    Aug 26 06:45:51.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-978b8" in namespace "gc-4757"
    Aug 26 06:45:51.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b7nn" in namespace "gc-4757"
    Aug 26 06:45:51.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ltkx" in namespace "gc-4757"
    Aug 26 06:45:51.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pfsx" in namespace "gc-4757"
    Aug 26 06:45:51.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rb9p" in namespace "gc-4757"
    Aug 26 06:45:51.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-9znzp" in namespace "gc-4757"
    Aug 26 06:45:51.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2847" in namespace "gc-4757"
    Aug 26 06:45:51.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9r87" in namespace "gc-4757"
    Aug 26 06:45:51.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnl46" in namespace "gc-4757"
    Aug 26 06:45:51.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-cj7ht" in namespace "gc-4757"
    Aug 26 06:45:51.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl72b" in namespace "gc-4757"
    Aug 26 06:45:51.733: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx4cz" in namespace "gc-4757"
    Aug 26 06:45:51.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhfst" in namespace "gc-4757"
    Aug 26 06:45:51.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-dn7bh" in namespace "gc-4757"
    Aug 26 06:45:51.795: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpjff" in namespace "gc-4757"
    Aug 26 06:45:51.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsq6r" in namespace "gc-4757"
    Aug 26 06:45:51.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwq6q" in namespace "gc-4757"
    Aug 26 06:45:51.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-f97wt" in namespace "gc-4757"
    Aug 26 06:45:51.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftg9f" in namespace "gc-4757"
    Aug 26 06:45:51.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx652" in namespace "gc-4757"
    Aug 26 06:45:51.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsmlj" in namespace "gc-4757"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:45:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4757" for this suite. 08/26/23 06:45:51.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:45:51.97
Aug 26 06:45:51.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename podtemplate 08/26/23 06:45:51.971
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:52.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:52.01
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 26 06:45:52.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6051" for this suite. 08/26/23 06:45:52.114
------------------------------
• [0.156 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:45:51.97
    Aug 26 06:45:51.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename podtemplate 08/26/23 06:45:51.971
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:52.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:52.01
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:45:52.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6051" for this suite. 08/26/23 06:45:52.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:45:52.127
Aug 26 06:45:52.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename secrets 08/26/23 06:45:52.129
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:52.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:52.166
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 26 06:45:52.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2171" for this suite. 08/26/23 06:45:52.274
------------------------------
• [0.170 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:45:52.127
    Aug 26 06:45:52.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename secrets 08/26/23 06:45:52.129
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:52.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:52.166
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:45:52.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2171" for this suite. 08/26/23 06:45:52.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:45:52.298
Aug 26 06:45:52.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename services 08/26/23 06:45:52.3
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:52.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:52.341
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-5859 08/26/23 06:45:52.344
STEP: creating service affinity-clusterip in namespace services-5859 08/26/23 06:45:52.345
STEP: creating replication controller affinity-clusterip in namespace services-5859 08/26/23 06:45:52.365
I0826 06:45:52.377409      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5859, replica count: 3
I0826 06:45:55.429133      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0826 06:45:58.429874      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 26 06:45:58.451: INFO: Creating new exec pod
Aug 26 06:45:58.472: INFO: Waiting up to 5m0s for pod "execpod-affinityhqz7f" in namespace "services-5859" to be "running"
Aug 26 06:45:58.483: INFO: Pod "execpod-affinityhqz7f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.279107ms
Aug 26 06:46:00.488: INFO: Pod "execpod-affinityhqz7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016203322s
Aug 26 06:46:00.488: INFO: Pod "execpod-affinityhqz7f" satisfied condition "running"
Aug 26 06:46:01.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-5859 exec execpod-affinityhqz7f -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Aug 26 06:46:04.174: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 26 06:46:04.174: INFO: stdout: ""
Aug 26 06:46:04.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-5859 exec execpod-affinityhqz7f -- /bin/sh -x -c nc -v -z -w 2 10.21.40.142 80'
Aug 26 06:46:04.332: INFO: stderr: "+ nc -v -z -w 2 10.21.40.142 80\nConnection to 10.21.40.142 80 port [tcp/http] succeeded!\n"
Aug 26 06:46:04.332: INFO: stdout: ""
Aug 26 06:46:04.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-5859 exec execpod-affinityhqz7f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.40.142:80/ ; done'
Aug 26 06:46:04.634: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n"
Aug 26 06:46:04.634: INFO: stdout: "\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd"
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
Aug 26 06:46:04.634: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5859, will wait for the garbage collector to delete the pods 08/26/23 06:46:04.654
Aug 26 06:46:04.726: INFO: Deleting ReplicationController affinity-clusterip took: 11.316743ms
Aug 26 06:46:04.826: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.641597ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 26 06:46:07.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5859" for this suite. 08/26/23 06:46:07.266
------------------------------
• [SLOW TEST] [14.987 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:45:52.298
    Aug 26 06:45:52.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename services 08/26/23 06:45:52.3
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:45:52.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:45:52.341
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-5859 08/26/23 06:45:52.344
    STEP: creating service affinity-clusterip in namespace services-5859 08/26/23 06:45:52.345
    STEP: creating replication controller affinity-clusterip in namespace services-5859 08/26/23 06:45:52.365
    I0826 06:45:52.377409      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5859, replica count: 3
    I0826 06:45:55.429133      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0826 06:45:58.429874      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 26 06:45:58.451: INFO: Creating new exec pod
    Aug 26 06:45:58.472: INFO: Waiting up to 5m0s for pod "execpod-affinityhqz7f" in namespace "services-5859" to be "running"
    Aug 26 06:45:58.483: INFO: Pod "execpod-affinityhqz7f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.279107ms
    Aug 26 06:46:00.488: INFO: Pod "execpod-affinityhqz7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016203322s
    Aug 26 06:46:00.488: INFO: Pod "execpod-affinityhqz7f" satisfied condition "running"
    Aug 26 06:46:01.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-5859 exec execpod-affinityhqz7f -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Aug 26 06:46:04.174: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Aug 26 06:46:04.174: INFO: stdout: ""
    Aug 26 06:46:04.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-5859 exec execpod-affinityhqz7f -- /bin/sh -x -c nc -v -z -w 2 10.21.40.142 80'
    Aug 26 06:46:04.332: INFO: stderr: "+ nc -v -z -w 2 10.21.40.142 80\nConnection to 10.21.40.142 80 port [tcp/http] succeeded!\n"
    Aug 26 06:46:04.332: INFO: stdout: ""
    Aug 26 06:46:04.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=services-5859 exec execpod-affinityhqz7f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.40.142:80/ ; done'
    Aug 26 06:46:04.634: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.40.142:80/\n"
    Aug 26 06:46:04.634: INFO: stdout: "\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd\naffinity-clusterip-vjjkd"
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Received response from host: affinity-clusterip-vjjkd
    Aug 26 06:46:04.634: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-5859, will wait for the garbage collector to delete the pods 08/26/23 06:46:04.654
    Aug 26 06:46:04.726: INFO: Deleting ReplicationController affinity-clusterip took: 11.316743ms
    Aug 26 06:46:04.826: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.641597ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:46:07.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5859" for this suite. 08/26/23 06:46:07.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:46:07.288
Aug 26 06:46:07.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename endpointslice 08/26/23 06:46:07.289
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:46:07.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:46:07.319
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 08/26/23 06:46:12.426
STEP: referencing matching pods with named port 08/26/23 06:46:17.436
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/26/23 06:46:22.449
STEP: recreating EndpointSlices after they've been deleted 08/26/23 06:46:27.458
Aug 26 06:46:27.487: INFO: EndpointSlice for Service endpointslice-3108/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 26 06:46:37.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3108" for this suite. 08/26/23 06:46:37.511
------------------------------
• [SLOW TEST] [30.231 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:46:07.288
    Aug 26 06:46:07.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename endpointslice 08/26/23 06:46:07.289
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:46:07.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:46:07.319
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 08/26/23 06:46:12.426
    STEP: referencing matching pods with named port 08/26/23 06:46:17.436
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/26/23 06:46:22.449
    STEP: recreating EndpointSlices after they've been deleted 08/26/23 06:46:27.458
    Aug 26 06:46:27.487: INFO: EndpointSlice for Service endpointslice-3108/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:46:37.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3108" for this suite. 08/26/23 06:46:37.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:46:37.521
Aug 26 06:46:37.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename projected 08/26/23 06:46:37.522
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:46:37.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:46:37.544
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 08/26/23 06:46:37.546
Aug 26 06:46:37.557: INFO: Waiting up to 5m0s for pod "labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad" in namespace "projected-4173" to be "running and ready"
Aug 26 06:46:37.564: INFO: Pod "labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.710543ms
Aug 26 06:46:37.564: INFO: The phase of Pod labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad is Pending, waiting for it to be Running (with Ready = true)
Aug 26 06:46:39.571: INFO: Pod "labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad": Phase="Running", Reason="", readiness=true. Elapsed: 2.013377075s
Aug 26 06:46:39.571: INFO: The phase of Pod labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad is Running (Ready = true)
Aug 26 06:46:39.571: INFO: Pod "labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad" satisfied condition "running and ready"
Aug 26 06:46:40.109: INFO: Successfully updated pod "labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 26 06:46:44.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4173" for this suite. 08/26/23 06:46:44.177
------------------------------
• [SLOW TEST] [6.677 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:46:37.521
    Aug 26 06:46:37.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename projected 08/26/23 06:46:37.522
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:46:37.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:46:37.544
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 08/26/23 06:46:37.546
    Aug 26 06:46:37.557: INFO: Waiting up to 5m0s for pod "labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad" in namespace "projected-4173" to be "running and ready"
    Aug 26 06:46:37.564: INFO: Pod "labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.710543ms
    Aug 26 06:46:37.564: INFO: The phase of Pod labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad is Pending, waiting for it to be Running (with Ready = true)
    Aug 26 06:46:39.571: INFO: Pod "labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad": Phase="Running", Reason="", readiness=true. Elapsed: 2.013377075s
    Aug 26 06:46:39.571: INFO: The phase of Pod labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad is Running (Ready = true)
    Aug 26 06:46:39.571: INFO: Pod "labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad" satisfied condition "running and ready"
    Aug 26 06:46:40.109: INFO: Successfully updated pod "labelsupdate49701c07-c122-4dd3-9623-00cdf3a99bad"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:46:44.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4173" for this suite. 08/26/23 06:46:44.177
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:46:44.198
Aug 26 06:46:44.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename webhook 08/26/23 06:46:44.2
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:46:44.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:46:44.229
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/26/23 06:46:44.247
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:46:44.72
STEP: Deploying the webhook pod 08/26/23 06:46:44.729
STEP: Wait for the deployment to be ready 08/26/23 06:46:44.744
Aug 26 06:46:44.753: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/26/23 06:46:46.766
STEP: Verifying the service has paired with the endpoint 08/26/23 06:46:46.779
Aug 26 06:46:47.780: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 08/26/23 06:46:47.784
STEP: Creating a configMap that does not comply to the validation webhook rules 08/26/23 06:46:47.82
STEP: Updating a validating webhook configuration's rules to not include the create operation 08/26/23 06:46:47.841
STEP: Creating a configMap that does not comply to the validation webhook rules 08/26/23 06:46:47.858
STEP: Patching a validating webhook configuration's rules to include the create operation 08/26/23 06:46:47.874
STEP: Creating a configMap that does not comply to the validation webhook rules 08/26/23 06:46:47.896
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:46:47.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3733" for this suite. 08/26/23 06:46:48.038
STEP: Destroying namespace "webhook-3733-markers" for this suite. 08/26/23 06:46:48.054
------------------------------
• [3.866 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:46:44.198
    Aug 26 06:46:44.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename webhook 08/26/23 06:46:44.2
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:46:44.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:46:44.229
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/26/23 06:46:44.247
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/26/23 06:46:44.72
    STEP: Deploying the webhook pod 08/26/23 06:46:44.729
    STEP: Wait for the deployment to be ready 08/26/23 06:46:44.744
    Aug 26 06:46:44.753: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/26/23 06:46:46.766
    STEP: Verifying the service has paired with the endpoint 08/26/23 06:46:46.779
    Aug 26 06:46:47.780: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 08/26/23 06:46:47.784
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/26/23 06:46:47.82
    STEP: Updating a validating webhook configuration's rules to not include the create operation 08/26/23 06:46:47.841
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/26/23 06:46:47.858
    STEP: Patching a validating webhook configuration's rules to include the create operation 08/26/23 06:46:47.874
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/26/23 06:46:47.896
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:46:47.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3733" for this suite. 08/26/23 06:46:48.038
    STEP: Destroying namespace "webhook-3733-markers" for this suite. 08/26/23 06:46:48.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:46:48.066
Aug 26 06:46:48.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename kubectl 08/26/23 06:46:48.068
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:46:48.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:46:48.097
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/26/23 06:46:48.102
Aug 26 06:46:48.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4306 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 26 06:46:48.341: INFO: stderr: ""
Aug 26 06:46:48.341: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 08/26/23 06:46:48.341
STEP: verifying the pod e2e-test-httpd-pod was created 08/26/23 06:46:53.393
Aug 26 06:46:53.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4306 get pod e2e-test-httpd-pod -o json'
Aug 26 06:46:53.486: INFO: stderr: ""
Aug 26 06:46:53.487: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"f4b68a4b003184726ff143c89f2484256b8404aa96ed640bd080ddc2b5c77c1e\",\n            \"cni.projectcalico.org/podIP\": \"10.20.199.100/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.20.199.100/32\"\n        },\n        \"creationTimestamp\": \"2023-08-26T06:46:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4306\",\n        \"resourceVersion\": \"47819\",\n        \"uid\": \"73352970-ac9e-4c38-90c7-7b3c2b1d9ff7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zl46c\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-1-5.us-west-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zl46c\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-26T06:46:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-26T06:46:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-26T06:46:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-26T06:46:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://0c8f145270ca667878dcd01ba7bca206902045314dabcab25864e8e5feade7bd\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-26T06:46:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.1.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.20.199.100\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.20.199.100\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-26T06:46:48Z\"\n    }\n}\n"
STEP: replace the image in the pod 08/26/23 06:46:53.487
Aug 26 06:46:53.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4306 replace -f -'
Aug 26 06:46:54.344: INFO: stderr: ""
Aug 26 06:46:54.344: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/26/23 06:46:54.344
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Aug 26 06:46:54.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4306 delete pods e2e-test-httpd-pod'
Aug 26 06:46:56.268: INFO: stderr: ""
Aug 26 06:46:56.268: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 26 06:46:56.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4306" for this suite. 08/26/23 06:46:56.276
------------------------------
• [SLOW TEST] [8.217 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:46:48.066
    Aug 26 06:46:48.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename kubectl 08/26/23 06:46:48.068
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:46:48.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:46:48.097
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/26/23 06:46:48.102
    Aug 26 06:46:48.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4306 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 26 06:46:48.341: INFO: stderr: ""
    Aug 26 06:46:48.341: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 08/26/23 06:46:48.341
    STEP: verifying the pod e2e-test-httpd-pod was created 08/26/23 06:46:53.393
    Aug 26 06:46:53.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4306 get pod e2e-test-httpd-pod -o json'
    Aug 26 06:46:53.486: INFO: stderr: ""
    Aug 26 06:46:53.487: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"f4b68a4b003184726ff143c89f2484256b8404aa96ed640bd080ddc2b5c77c1e\",\n            \"cni.projectcalico.org/podIP\": \"10.20.199.100/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.20.199.100/32\"\n        },\n        \"creationTimestamp\": \"2023-08-26T06:46:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4306\",\n        \"resourceVersion\": \"47819\",\n        \"uid\": \"73352970-ac9e-4c38-90c7-7b3c2b1d9ff7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zl46c\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-1-5.us-west-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zl46c\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-26T06:46:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-26T06:46:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-26T06:46:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-26T06:46:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://0c8f145270ca667878dcd01ba7bca206902045314dabcab25864e8e5feade7bd\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-26T06:46:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.1.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.20.199.100\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.20.199.100\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-26T06:46:48Z\"\n    }\n}\n"
    STEP: replace the image in the pod 08/26/23 06:46:53.487
    Aug 26 06:46:53.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4306 replace -f -'
    Aug 26 06:46:54.344: INFO: stderr: ""
    Aug 26 06:46:54.344: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/26/23 06:46:54.344
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Aug 26 06:46:54.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3637333530 --namespace=kubectl-4306 delete pods e2e-test-httpd-pod'
    Aug 26 06:46:56.268: INFO: stderr: ""
    Aug 26 06:46:56.268: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:46:56.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4306" for this suite. 08/26/23 06:46:56.276
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:46:56.284
Aug 26 06:46:56.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sched-pred 08/26/23 06:46:56.285
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:46:56.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:46:56.309
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 26 06:46:56.311: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 26 06:46:56.336: INFO: Waiting for terminating namespaces to be deleted...
Aug 26 06:46:56.341: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-101.us-west-2.compute.internal before test
Aug 26 06:46:56.362: INFO: calico-node-hgm7c from kube-system started at 2023-08-26 04:58:59 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.362: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 06:46:56.362: INFO: sonobuoy from sonobuoy started at 2023-08-26 05:16:19 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.362: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 26 06:46:56.362: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:46:56.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:46:56.362: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 06:46:56.362: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-126.us-west-2.compute.internal before test
Aug 26 06:46:56.383: INFO: calico-node-8272m from kube-system started at 2023-08-26 04:58:47 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.383: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 06:46:56.383: INFO: calico-typha-6dd9648c8f-th65n from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.383: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 06:46:56.383: INFO: sonobuoy-e2e-job-c6e00385bb4e4fb8 from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:46:56.383: INFO: 	Container e2e ready: true, restart count 0
Aug 26 06:46:56.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:46:56.383: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-ckdgn from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:46:56.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:46:56.383: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 06:46:56.383: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-23.us-west-2.compute.internal before test
Aug 26 06:46:56.411: INFO: calico-kube-controllers-76798f54cb-cdxwm from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.411: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 26 06:46:56.411: INFO: calico-node-trdnz from kube-system started at 2023-08-26 04:59:15 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.411: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 06:46:56.411: INFO: calico-typha-6dd9648c8f-x4b7r from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.411: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 06:46:56.411: INFO: calico-typha-autoscaler-54c8866496-tmj59 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.411: INFO: 	Container autoscaler ready: true, restart count 0
Aug 26 06:46:56.411: INFO: coredns-58ffcc48df-2q4rv from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.411: INFO: 	Container coredns ready: true, restart count 0
Aug 26 06:46:56.411: INFO: kube-dns-autoscaler-f68f756b6-78kdn from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.411: INFO: 	Container autoscaler ready: true, restart count 0
Aug 26 06:46:56.411: INFO: kube-state-metrics-99bbfb4cd-g7vn6 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.411: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 26 06:46:56.411: INFO: dashboard-metrics-scraper-6c57f89c7c-2vtv5 from kubernetes-dashboard started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.411: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 26 06:46:56.411: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-g65bg from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:46:56.411: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:46:56.411: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 06:46:56.411: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-31.us-west-2.compute.internal before test
Aug 26 06:46:56.441: INFO: calico-node-mxmcm from kube-system started at 2023-08-26 04:59:09 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.441: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 06:46:56.441: INFO: calico-typha-6dd9648c8f-k8wjx from kube-system started at 2023-08-26 06:38:49 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.441: INFO: 	Container calico-typha ready: true, restart count 0
Aug 26 06:46:56.441: INFO: metrics-server-b7db9955-wnbq8 from kube-system started at 2023-08-26 06:43:20 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.441: INFO: 	Container metrics-server ready: true, restart count 0
Aug 26 06:46:56.441: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-6gxwx from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:46:56.441: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:46:56.441: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 26 06:46:56.441: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-5.us-west-2.compute.internal before test
Aug 26 06:46:56.469: INFO: calico-node-jftpx from kube-system started at 2023-08-26 04:58:49 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.469: INFO: 	Container calico-node ready: true, restart count 0
Aug 26 06:46:56.469: INFO: coredns-58ffcc48df-h7drh from kube-system started at 2023-08-26 06:44:46 +0000 UTC (1 container statuses recorded)
Aug 26 06:46:56.469: INFO: 	Container coredns ready: true, restart count 0
Aug 26 06:46:56.469: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-tpq9m from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
Aug 26 06:46:56.469: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 26 06:46:56.469: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/26/23 06:46:56.469
Aug 26 06:46:56.480: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4578" to be "running"
Aug 26 06:46:56.487: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.731606ms
Aug 26 06:46:58.495: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014376144s
Aug 26 06:46:58.495: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/26/23 06:46:58.5
STEP: Trying to apply a random label on the found node. 08/26/23 06:46:58.524
STEP: verifying the node has the label kubernetes.io/e2e-dea8b18b-6238-4ac1-9e81-c439ac33696d 42 08/26/23 06:46:58.545
STEP: Trying to relaunch the pod, now with labels. 08/26/23 06:46:58.559
Aug 26 06:46:58.569: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4578" to be "not pending"
Aug 26 06:46:58.579: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 9.792419ms
Aug 26 06:47:00.585: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.015255974s
Aug 26 06:47:00.585: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-dea8b18b-6238-4ac1-9e81-c439ac33696d off the node ip-10-0-1-101.us-west-2.compute.internal 08/26/23 06:47:00.591
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dea8b18b-6238-4ac1-9e81-c439ac33696d 08/26/23 06:47:00.61
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:47:00.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4578" for this suite. 08/26/23 06:47:00.625
------------------------------
• [4.353 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:46:56.284
    Aug 26 06:46:56.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sched-pred 08/26/23 06:46:56.285
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:46:56.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:46:56.309
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 26 06:46:56.311: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 26 06:46:56.336: INFO: Waiting for terminating namespaces to be deleted...
    Aug 26 06:46:56.341: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-101.us-west-2.compute.internal before test
    Aug 26 06:46:56.362: INFO: calico-node-hgm7c from kube-system started at 2023-08-26 04:58:59 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.362: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 06:46:56.362: INFO: sonobuoy from sonobuoy started at 2023-08-26 05:16:19 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.362: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 26 06:46:56.362: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-pzw5c from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:46:56.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:46:56.362: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 06:46:56.362: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-126.us-west-2.compute.internal before test
    Aug 26 06:46:56.383: INFO: calico-node-8272m from kube-system started at 2023-08-26 04:58:47 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.383: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 06:46:56.383: INFO: calico-typha-6dd9648c8f-th65n from kube-system started at 2023-08-26 05:04:38 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.383: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 06:46:56.383: INFO: sonobuoy-e2e-job-c6e00385bb4e4fb8 from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:46:56.383: INFO: 	Container e2e ready: true, restart count 0
    Aug 26 06:46:56.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:46:56.383: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-ckdgn from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:46:56.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:46:56.383: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 06:46:56.383: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-23.us-west-2.compute.internal before test
    Aug 26 06:46:56.411: INFO: calico-kube-controllers-76798f54cb-cdxwm from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.411: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 26 06:46:56.411: INFO: calico-node-trdnz from kube-system started at 2023-08-26 04:59:15 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.411: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 06:46:56.411: INFO: calico-typha-6dd9648c8f-x4b7r from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.411: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 06:46:56.411: INFO: calico-typha-autoscaler-54c8866496-tmj59 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.411: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 26 06:46:56.411: INFO: coredns-58ffcc48df-2q4rv from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.411: INFO: 	Container coredns ready: true, restart count 0
    Aug 26 06:46:56.411: INFO: kube-dns-autoscaler-f68f756b6-78kdn from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.411: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 26 06:46:56.411: INFO: kube-state-metrics-99bbfb4cd-g7vn6 from kube-system started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.411: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 26 06:46:56.411: INFO: dashboard-metrics-scraper-6c57f89c7c-2vtv5 from kubernetes-dashboard started at 2023-08-26 04:59:35 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.411: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Aug 26 06:46:56.411: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-g65bg from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:46:56.411: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:46:56.411: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 06:46:56.411: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-31.us-west-2.compute.internal before test
    Aug 26 06:46:56.441: INFO: calico-node-mxmcm from kube-system started at 2023-08-26 04:59:09 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.441: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 06:46:56.441: INFO: calico-typha-6dd9648c8f-k8wjx from kube-system started at 2023-08-26 06:38:49 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.441: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 26 06:46:56.441: INFO: metrics-server-b7db9955-wnbq8 from kube-system started at 2023-08-26 06:43:20 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.441: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 26 06:46:56.441: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-6gxwx from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:46:56.441: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:46:56.441: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 26 06:46:56.441: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-1-5.us-west-2.compute.internal before test
    Aug 26 06:46:56.469: INFO: calico-node-jftpx from kube-system started at 2023-08-26 04:58:49 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.469: INFO: 	Container calico-node ready: true, restart count 0
    Aug 26 06:46:56.469: INFO: coredns-58ffcc48df-h7drh from kube-system started at 2023-08-26 06:44:46 +0000 UTC (1 container statuses recorded)
    Aug 26 06:46:56.469: INFO: 	Container coredns ready: true, restart count 0
    Aug 26 06:46:56.469: INFO: sonobuoy-systemd-logs-daemon-set-e1aba1e1cbc942b3-tpq9m from sonobuoy started at 2023-08-26 05:16:22 +0000 UTC (2 container statuses recorded)
    Aug 26 06:46:56.469: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 26 06:46:56.469: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/26/23 06:46:56.469
    Aug 26 06:46:56.480: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4578" to be "running"
    Aug 26 06:46:56.487: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.731606ms
    Aug 26 06:46:58.495: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014376144s
    Aug 26 06:46:58.495: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/26/23 06:46:58.5
    STEP: Trying to apply a random label on the found node. 08/26/23 06:46:58.524
    STEP: verifying the node has the label kubernetes.io/e2e-dea8b18b-6238-4ac1-9e81-c439ac33696d 42 08/26/23 06:46:58.545
    STEP: Trying to relaunch the pod, now with labels. 08/26/23 06:46:58.559
    Aug 26 06:46:58.569: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4578" to be "not pending"
    Aug 26 06:46:58.579: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 9.792419ms
    Aug 26 06:47:00.585: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.015255974s
    Aug 26 06:47:00.585: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-dea8b18b-6238-4ac1-9e81-c439ac33696d off the node ip-10-0-1-101.us-west-2.compute.internal 08/26/23 06:47:00.591
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-dea8b18b-6238-4ac1-9e81-c439ac33696d 08/26/23 06:47:00.61
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:47:00.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4578" for this suite. 08/26/23 06:47:00.625
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:47:00.637
Aug 26 06:47:00.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename sched-preemption 08/26/23 06:47:00.638
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:47:00.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:47:00.666
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 26 06:47:00.692: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 26 06:48:00.757: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 08/26/23 06:48:00.763
Aug 26 06:48:00.793: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 26 06:48:00.802: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 26 06:48:00.830: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 26 06:48:00.845: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 26 06:48:00.868: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 26 06:48:00.884: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Aug 26 06:48:00.905: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Aug 26 06:48:00.918: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Aug 26 06:48:00.943: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Aug 26 06:48:00.951: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/26/23 06:48:00.951
Aug 26 06:48:00.951: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1119" to be "running"
Aug 26 06:48:00.956: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.266179ms
Aug 26 06:48:02.961: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010277988s
Aug 26 06:48:02.961: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 26 06:48:02.961: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
Aug 26 06:48:02.965: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.102256ms
Aug 26 06:48:02.965: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:48:02.966: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
Aug 26 06:48:02.969: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.660974ms
Aug 26 06:48:02.969: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:48:02.969: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
Aug 26 06:48:02.973: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.240436ms
Aug 26 06:48:02.973: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:48:02.974: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
Aug 26 06:48:02.979: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.164096ms
Aug 26 06:48:02.979: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:48:02.979: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
Aug 26 06:48:02.983: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.781878ms
Aug 26 06:48:02.983: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:48:02.983: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
Aug 26 06:48:02.987: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.610012ms
Aug 26 06:48:02.987: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:48:02.987: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
Aug 26 06:48:02.991: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.731762ms
Aug 26 06:48:02.991: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:48:02.991: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
Aug 26 06:48:02.994: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.516155ms
Aug 26 06:48:02.995: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 26 06:48:02.995: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
Aug 26 06:48:02.998: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.820686ms
Aug 26 06:48:02.998: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 08/26/23 06:48:02.998
Aug 26 06:48:03.011: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Aug 26 06:48:03.015: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.893085ms
Aug 26 06:48:05.020: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008890455s
Aug 26 06:48:07.024: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012330139s
Aug 26 06:48:09.022: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010609379s
Aug 26 06:48:09.022: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:48:09.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1119" for this suite. 08/26/23 06:48:09.311
------------------------------
• [SLOW TEST] [68.682 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:47:00.637
    Aug 26 06:47:00.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename sched-preemption 08/26/23 06:47:00.638
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:47:00.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:47:00.666
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 26 06:47:00.692: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 26 06:48:00.757: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 08/26/23 06:48:00.763
    Aug 26 06:48:00.793: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 26 06:48:00.802: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 26 06:48:00.830: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 26 06:48:00.845: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 26 06:48:00.868: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 26 06:48:00.884: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Aug 26 06:48:00.905: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Aug 26 06:48:00.918: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Aug 26 06:48:00.943: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Aug 26 06:48:00.951: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/26/23 06:48:00.951
    Aug 26 06:48:00.951: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1119" to be "running"
    Aug 26 06:48:00.956: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.266179ms
    Aug 26 06:48:02.961: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010277988s
    Aug 26 06:48:02.961: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 26 06:48:02.961: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
    Aug 26 06:48:02.965: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.102256ms
    Aug 26 06:48:02.965: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:48:02.966: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
    Aug 26 06:48:02.969: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.660974ms
    Aug 26 06:48:02.969: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:48:02.969: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
    Aug 26 06:48:02.973: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.240436ms
    Aug 26 06:48:02.973: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:48:02.974: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
    Aug 26 06:48:02.979: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.164096ms
    Aug 26 06:48:02.979: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:48:02.979: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
    Aug 26 06:48:02.983: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.781878ms
    Aug 26 06:48:02.983: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:48:02.983: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
    Aug 26 06:48:02.987: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.610012ms
    Aug 26 06:48:02.987: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:48:02.987: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
    Aug 26 06:48:02.991: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.731762ms
    Aug 26 06:48:02.991: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:48:02.991: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
    Aug 26 06:48:02.994: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.516155ms
    Aug 26 06:48:02.995: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 26 06:48:02.995: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-1119" to be "running"
    Aug 26 06:48:02.998: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.820686ms
    Aug 26 06:48:02.998: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 08/26/23 06:48:02.998
    Aug 26 06:48:03.011: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Aug 26 06:48:03.015: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.893085ms
    Aug 26 06:48:05.020: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008890455s
    Aug 26 06:48:07.024: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012330139s
    Aug 26 06:48:09.022: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010609379s
    Aug 26 06:48:09.022: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:48:09.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1119" for this suite. 08/26/23 06:48:09.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:48:09.32
Aug 26 06:48:09.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename deployment 08/26/23 06:48:09.323
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:48:09.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:48:09.352
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Aug 26 06:48:09.366: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 26 06:48:14.379: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/26/23 06:48:14.379
Aug 26 06:48:14.385: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/26/23 06:48:14.414
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 26 06:48:14.437: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5420  2bdc1a48-14fc-4e35-a1a4-bcc8d8d6dc24 48298 1 2023-08-26 06:48:14 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-26 06:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00423bf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 26 06:48:14.454: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-5420  4098ef8f-e9bf-43d5-b731-f2e19562383c 48300 1 2023-08-26 06:48:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 2bdc1a48-14fc-4e35-a1a4-bcc8d8d6dc24 0xc00471c407 0xc00471c408}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bdc1a48-14fc-4e35-a1a4-bcc8d8d6dc24\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00471c498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 26 06:48:14.454: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 26 06:48:14.454: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5420  6163a764-4659-417d-8916-d0b02d3f8c23 48299 1 2023-08-26 06:48:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 2bdc1a48-14fc-4e35-a1a4-bcc8d8d6dc24 0xc00471c2d7 0xc00471c2d8}] [] [{e2e.test Update apps/v1 2023-08-26 06:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:48:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-26 06:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"2bdc1a48-14fc-4e35-a1a4-bcc8d8d6dc24\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00471c398 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 26 06:48:14.481: INFO: Pod "test-cleanup-controller-rxx6m" is available:
&Pod{ObjectMeta:{test-cleanup-controller-rxx6m test-cleanup-controller- deployment-5420  c0483868-1f8b-4676-afb6-d6ebbe3372de 48289 0 2023-08-26 06:48:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:468d10c6908eaef001fe38b938b3012611c6b759375efb5f282f6a1385cb70aa cni.projectcalico.org/podIP:10.20.50.202/32 cni.projectcalico.org/podIPs:10.20.50.202/32] [{apps/v1 ReplicaSet test-cleanup-controller 6163a764-4659-417d-8916-d0b02d3f8c23 0xc00471ca47 0xc00471ca48}] [] [{kube-controller-manager Update v1 2023-08-26 06:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6163a764-4659-417d-8916-d0b02d3f8c23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:48:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:48:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.50.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rt4w4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rt4w4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:48:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:48:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:48:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:48:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.101,PodIP:10.20.50.202,StartTime:2023-08-26 06:48:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:48:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a90e899336e8a86598ef262c251f427f5897e210185c8fc7b1018a3051e604f2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.50.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 26 06:48:14.482: INFO: Pod "test-cleanup-deployment-7698ff6f6b-p44xx" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-p44xx test-cleanup-deployment-7698ff6f6b- deployment-5420  d8b1b4a4-d75e-4abe-aebb-3912d39b4541 48304 0 2023-08-26 06:48:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 4098ef8f-e9bf-43d5-b731-f2e19562383c 0xc00471cc47 0xc00471cc48}] [] [{kube-controller-manager Update v1 2023-08-26 06:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4098ef8f-e9bf-43d5-b731-f2e19562383c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgq87,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgq87,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 26 06:48:14.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5420" for this suite. 08/26/23 06:48:14.502
------------------------------
• [SLOW TEST] [5.206 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:48:09.32
    Aug 26 06:48:09.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename deployment 08/26/23 06:48:09.323
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:48:09.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:48:09.352
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Aug 26 06:48:09.366: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Aug 26 06:48:14.379: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/26/23 06:48:14.379
    Aug 26 06:48:14.385: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/26/23 06:48:14.414
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 26 06:48:14.437: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5420  2bdc1a48-14fc-4e35-a1a4-bcc8d8d6dc24 48298 1 2023-08-26 06:48:14 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-26 06:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00423bf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 26 06:48:14.454: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-5420  4098ef8f-e9bf-43d5-b731-f2e19562383c 48300 1 2023-08-26 06:48:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 2bdc1a48-14fc-4e35-a1a4-bcc8d8d6dc24 0xc00471c407 0xc00471c408}] [] [{kube-controller-manager Update apps/v1 2023-08-26 06:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bdc1a48-14fc-4e35-a1a4-bcc8d8d6dc24\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00471c498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 06:48:14.454: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Aug 26 06:48:14.454: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5420  6163a764-4659-417d-8916-d0b02d3f8c23 48299 1 2023-08-26 06:48:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 2bdc1a48-14fc-4e35-a1a4-bcc8d8d6dc24 0xc00471c2d7 0xc00471c2d8}] [] [{e2e.test Update apps/v1 2023-08-26 06:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-26 06:48:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-26 06:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"2bdc1a48-14fc-4e35-a1a4-bcc8d8d6dc24\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00471c398 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 26 06:48:14.481: INFO: Pod "test-cleanup-controller-rxx6m" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-rxx6m test-cleanup-controller- deployment-5420  c0483868-1f8b-4676-afb6-d6ebbe3372de 48289 0 2023-08-26 06:48:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:468d10c6908eaef001fe38b938b3012611c6b759375efb5f282f6a1385cb70aa cni.projectcalico.org/podIP:10.20.50.202/32 cni.projectcalico.org/podIPs:10.20.50.202/32] [{apps/v1 ReplicaSet test-cleanup-controller 6163a764-4659-417d-8916-d0b02d3f8c23 0xc00471ca47 0xc00471ca48}] [] [{kube-controller-manager Update v1 2023-08-26 06:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6163a764-4659-417d-8916-d0b02d3f8c23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-26 06:48:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-26 06:48:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.50.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rt4w4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rt4w4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:48:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:48:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:48:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:48:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.101,PodIP:10.20.50.202,StartTime:2023-08-26 06:48:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-26 06:48:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a90e899336e8a86598ef262c251f427f5897e210185c8fc7b1018a3051e604f2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.50.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 26 06:48:14.482: INFO: Pod "test-cleanup-deployment-7698ff6f6b-p44xx" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-p44xx test-cleanup-deployment-7698ff6f6b- deployment-5420  d8b1b4a4-d75e-4abe-aebb-3912d39b4541 48304 0 2023-08-26 06:48:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 4098ef8f-e9bf-43d5-b731-f2e19562383c 0xc00471cc47 0xc00471cc48}] [] [{kube-controller-manager Update v1 2023-08-26 06:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4098ef8f-e9bf-43d5-b731-f2e19562383c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgq87,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgq87,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-101.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-26 06:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:48:14.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5420" for this suite. 08/26/23 06:48:14.502
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:48:14.526
Aug 26 06:48:14.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename emptydir-wrapper 08/26/23 06:48:14.527
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:48:14.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:48:14.572
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 08/26/23 06:48:14.577
STEP: Creating RC which spawns configmap-volume pods 08/26/23 06:48:14.976
Aug 26 06:48:15.001: INFO: Pod name wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af: Found 0 pods out of 5
Aug 26 06:48:20.028: INFO: Pod name wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/26/23 06:48:20.028
Aug 26 06:48:20.028: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:20.036: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Pending", Reason="", readiness=false. Elapsed: 7.311872ms
Aug 26 06:48:22.048: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019470201s
Aug 26 06:48:24.043: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01485648s
Aug 26 06:48:26.042: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013763842s
Aug 26 06:48:28.046: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017931772s
Aug 26 06:48:30.045: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Running", Reason="", readiness=true. Elapsed: 10.017047314s
Aug 26 06:48:30.045: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69" satisfied condition "running"
Aug 26 06:48:30.046: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-hfckx" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:30.052: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-hfckx": Phase="Running", Reason="", readiness=true. Elapsed: 6.631429ms
Aug 26 06:48:30.052: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-hfckx" satisfied condition "running"
Aug 26 06:48:30.052: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-k7j2p" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:30.057: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-k7j2p": Phase="Running", Reason="", readiness=true. Elapsed: 4.843896ms
Aug 26 06:48:30.057: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-k7j2p" satisfied condition "running"
Aug 26 06:48:30.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-kgwkz" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:30.062: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-kgwkz": Phase="Running", Reason="", readiness=true. Elapsed: 5.07456ms
Aug 26 06:48:30.062: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-kgwkz" satisfied condition "running"
Aug 26 06:48:30.062: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-tp7pr" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:30.070: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-tp7pr": Phase="Running", Reason="", readiness=true. Elapsed: 7.636735ms
Aug 26 06:48:30.070: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-tp7pr" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af in namespace emptydir-wrapper-9119, will wait for the garbage collector to delete the pods 08/26/23 06:48:30.07
Aug 26 06:48:30.149: INFO: Deleting ReplicationController wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af took: 14.772999ms
Aug 26 06:48:30.249: INFO: Terminating ReplicationController wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af pods took: 100.316052ms
STEP: Creating RC which spawns configmap-volume pods 08/26/23 06:48:34.966
Aug 26 06:48:35.030: INFO: Pod name wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08: Found 3 pods out of 5
Aug 26 06:48:40.046: INFO: Pod name wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/26/23 06:48:40.046
Aug 26 06:48:40.047: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:40.061: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Pending", Reason="", readiness=false. Elapsed: 14.582057ms
Aug 26 06:48:42.071: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024680237s
Aug 26 06:48:44.068: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021806161s
Aug 26 06:48:46.069: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022054276s
Aug 26 06:48:48.072: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025890871s
Aug 26 06:48:50.069: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Running", Reason="", readiness=true. Elapsed: 10.022829018s
Aug 26 06:48:50.069: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97" satisfied condition "running"
Aug 26 06:48:50.069: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-6s96h" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:50.075: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-6s96h": Phase="Running", Reason="", readiness=true. Elapsed: 5.368551ms
Aug 26 06:48:50.075: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-6s96h" satisfied condition "running"
Aug 26 06:48:50.075: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-tvvc2" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:50.080: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-tvvc2": Phase="Running", Reason="", readiness=true. Elapsed: 5.383703ms
Aug 26 06:48:50.080: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-tvvc2" satisfied condition "running"
Aug 26 06:48:50.080: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-vf2sk" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:50.086: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-vf2sk": Phase="Running", Reason="", readiness=true. Elapsed: 5.690282ms
Aug 26 06:48:50.086: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-vf2sk" satisfied condition "running"
Aug 26 06:48:50.086: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-xswd2" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:50.094: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-xswd2": Phase="Running", Reason="", readiness=true. Elapsed: 7.770034ms
Aug 26 06:48:50.094: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-xswd2" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08 in namespace emptydir-wrapper-9119, will wait for the garbage collector to delete the pods 08/26/23 06:48:50.094
Aug 26 06:48:50.166: INFO: Deleting ReplicationController wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08 took: 16.867114ms
Aug 26 06:48:50.267: INFO: Terminating ReplicationController wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08 pods took: 100.876831ms
STEP: Creating RC which spawns configmap-volume pods 08/26/23 06:48:54.277
Aug 26 06:48:54.298: INFO: Pod name wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954: Found 0 pods out of 5
Aug 26 06:48:59.314: INFO: Pod name wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/26/23 06:48:59.314
Aug 26 06:48:59.315: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:48:59.331: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Pending", Reason="", readiness=false. Elapsed: 16.655366ms
Aug 26 06:49:01.337: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02280989s
Aug 26 06:49:03.341: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026913762s
Aug 26 06:49:05.338: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023750353s
Aug 26 06:49:07.343: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02811073s
Aug 26 06:49:09.340: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Running", Reason="", readiness=true. Elapsed: 10.025810223s
Aug 26 06:49:09.340: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm" satisfied condition "running"
Aug 26 06:49:09.340: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-bg2hp" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:49:09.346: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-bg2hp": Phase="Running", Reason="", readiness=true. Elapsed: 5.83193ms
Aug 26 06:49:09.346: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-bg2hp" satisfied condition "running"
Aug 26 06:49:09.346: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-cwcxh" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:49:09.352: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-cwcxh": Phase="Running", Reason="", readiness=true. Elapsed: 5.8453ms
Aug 26 06:49:09.352: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-cwcxh" satisfied condition "running"
Aug 26 06:49:09.352: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-nl282" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:49:09.363: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-nl282": Phase="Running", Reason="", readiness=true. Elapsed: 11.110067ms
Aug 26 06:49:09.363: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-nl282" satisfied condition "running"
Aug 26 06:49:09.363: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-vfb8b" in namespace "emptydir-wrapper-9119" to be "running"
Aug 26 06:49:09.371: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-vfb8b": Phase="Running", Reason="", readiness=true. Elapsed: 7.427176ms
Aug 26 06:49:09.371: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-vfb8b" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954 in namespace emptydir-wrapper-9119, will wait for the garbage collector to delete the pods 08/26/23 06:49:09.371
Aug 26 06:49:09.445: INFO: Deleting ReplicationController wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954 took: 11.973691ms
Aug 26 06:49:09.646: INFO: Terminating ReplicationController wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954 pods took: 201.059912ms
STEP: Cleaning up the configMaps 08/26/23 06:49:12.847
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 26 06:49:13.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9119" for this suite. 08/26/23 06:49:13.361
------------------------------
• [SLOW TEST] [58.846 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:48:14.526
    Aug 26 06:48:14.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename emptydir-wrapper 08/26/23 06:48:14.527
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:48:14.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:48:14.572
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 08/26/23 06:48:14.577
    STEP: Creating RC which spawns configmap-volume pods 08/26/23 06:48:14.976
    Aug 26 06:48:15.001: INFO: Pod name wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af: Found 0 pods out of 5
    Aug 26 06:48:20.028: INFO: Pod name wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/26/23 06:48:20.028
    Aug 26 06:48:20.028: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:20.036: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Pending", Reason="", readiness=false. Elapsed: 7.311872ms
    Aug 26 06:48:22.048: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019470201s
    Aug 26 06:48:24.043: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01485648s
    Aug 26 06:48:26.042: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013763842s
    Aug 26 06:48:28.046: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017931772s
    Aug 26 06:48:30.045: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69": Phase="Running", Reason="", readiness=true. Elapsed: 10.017047314s
    Aug 26 06:48:30.045: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-8hz69" satisfied condition "running"
    Aug 26 06:48:30.046: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-hfckx" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:30.052: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-hfckx": Phase="Running", Reason="", readiness=true. Elapsed: 6.631429ms
    Aug 26 06:48:30.052: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-hfckx" satisfied condition "running"
    Aug 26 06:48:30.052: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-k7j2p" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:30.057: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-k7j2p": Phase="Running", Reason="", readiness=true. Elapsed: 4.843896ms
    Aug 26 06:48:30.057: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-k7j2p" satisfied condition "running"
    Aug 26 06:48:30.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-kgwkz" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:30.062: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-kgwkz": Phase="Running", Reason="", readiness=true. Elapsed: 5.07456ms
    Aug 26 06:48:30.062: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-kgwkz" satisfied condition "running"
    Aug 26 06:48:30.062: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-tp7pr" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:30.070: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-tp7pr": Phase="Running", Reason="", readiness=true. Elapsed: 7.636735ms
    Aug 26 06:48:30.070: INFO: Pod "wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af-tp7pr" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af in namespace emptydir-wrapper-9119, will wait for the garbage collector to delete the pods 08/26/23 06:48:30.07
    Aug 26 06:48:30.149: INFO: Deleting ReplicationController wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af took: 14.772999ms
    Aug 26 06:48:30.249: INFO: Terminating ReplicationController wrapped-volume-race-623524b2-e278-422a-9c08-d54c20e846af pods took: 100.316052ms
    STEP: Creating RC which spawns configmap-volume pods 08/26/23 06:48:34.966
    Aug 26 06:48:35.030: INFO: Pod name wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08: Found 3 pods out of 5
    Aug 26 06:48:40.046: INFO: Pod name wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/26/23 06:48:40.046
    Aug 26 06:48:40.047: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:40.061: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Pending", Reason="", readiness=false. Elapsed: 14.582057ms
    Aug 26 06:48:42.071: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024680237s
    Aug 26 06:48:44.068: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021806161s
    Aug 26 06:48:46.069: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022054276s
    Aug 26 06:48:48.072: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025890871s
    Aug 26 06:48:50.069: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97": Phase="Running", Reason="", readiness=true. Elapsed: 10.022829018s
    Aug 26 06:48:50.069: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-66p97" satisfied condition "running"
    Aug 26 06:48:50.069: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-6s96h" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:50.075: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-6s96h": Phase="Running", Reason="", readiness=true. Elapsed: 5.368551ms
    Aug 26 06:48:50.075: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-6s96h" satisfied condition "running"
    Aug 26 06:48:50.075: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-tvvc2" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:50.080: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-tvvc2": Phase="Running", Reason="", readiness=true. Elapsed: 5.383703ms
    Aug 26 06:48:50.080: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-tvvc2" satisfied condition "running"
    Aug 26 06:48:50.080: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-vf2sk" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:50.086: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-vf2sk": Phase="Running", Reason="", readiness=true. Elapsed: 5.690282ms
    Aug 26 06:48:50.086: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-vf2sk" satisfied condition "running"
    Aug 26 06:48:50.086: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-xswd2" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:50.094: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-xswd2": Phase="Running", Reason="", readiness=true. Elapsed: 7.770034ms
    Aug 26 06:48:50.094: INFO: Pod "wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08-xswd2" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08 in namespace emptydir-wrapper-9119, will wait for the garbage collector to delete the pods 08/26/23 06:48:50.094
    Aug 26 06:48:50.166: INFO: Deleting ReplicationController wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08 took: 16.867114ms
    Aug 26 06:48:50.267: INFO: Terminating ReplicationController wrapped-volume-race-b0e121a4-8e51-4112-8bb2-2d0bd05ede08 pods took: 100.876831ms
    STEP: Creating RC which spawns configmap-volume pods 08/26/23 06:48:54.277
    Aug 26 06:48:54.298: INFO: Pod name wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954: Found 0 pods out of 5
    Aug 26 06:48:59.314: INFO: Pod name wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/26/23 06:48:59.314
    Aug 26 06:48:59.315: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:48:59.331: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Pending", Reason="", readiness=false. Elapsed: 16.655366ms
    Aug 26 06:49:01.337: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02280989s
    Aug 26 06:49:03.341: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026913762s
    Aug 26 06:49:05.338: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023750353s
    Aug 26 06:49:07.343: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02811073s
    Aug 26 06:49:09.340: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm": Phase="Running", Reason="", readiness=true. Elapsed: 10.025810223s
    Aug 26 06:49:09.340: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-2gxkm" satisfied condition "running"
    Aug 26 06:49:09.340: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-bg2hp" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:49:09.346: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-bg2hp": Phase="Running", Reason="", readiness=true. Elapsed: 5.83193ms
    Aug 26 06:49:09.346: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-bg2hp" satisfied condition "running"
    Aug 26 06:49:09.346: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-cwcxh" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:49:09.352: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-cwcxh": Phase="Running", Reason="", readiness=true. Elapsed: 5.8453ms
    Aug 26 06:49:09.352: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-cwcxh" satisfied condition "running"
    Aug 26 06:49:09.352: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-nl282" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:49:09.363: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-nl282": Phase="Running", Reason="", readiness=true. Elapsed: 11.110067ms
    Aug 26 06:49:09.363: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-nl282" satisfied condition "running"
    Aug 26 06:49:09.363: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-vfb8b" in namespace "emptydir-wrapper-9119" to be "running"
    Aug 26 06:49:09.371: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-vfb8b": Phase="Running", Reason="", readiness=true. Elapsed: 7.427176ms
    Aug 26 06:49:09.371: INFO: Pod "wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954-vfb8b" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954 in namespace emptydir-wrapper-9119, will wait for the garbage collector to delete the pods 08/26/23 06:49:09.371
    Aug 26 06:49:09.445: INFO: Deleting ReplicationController wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954 took: 11.973691ms
    Aug 26 06:49:09.646: INFO: Terminating ReplicationController wrapped-volume-race-90602128-68b2-4f50-a3b7-8f7b4d982954 pods took: 201.059912ms
    STEP: Cleaning up the configMaps 08/26/23 06:49:12.847
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:49:13.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9119" for this suite. 08/26/23 06:49:13.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:49:13.373
Aug 26 06:49:13.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename container-runtime 08/26/23 06:49:13.376
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:49:13.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:49:13.402
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/26/23 06:49:13.416
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/26/23 06:49:30.534
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/26/23 06:49:30.542
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/26/23 06:49:30.551
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/26/23 06:49:30.551
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/26/23 06:49:30.583
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/26/23 06:49:33.617
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/26/23 06:49:35.647
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/26/23 06:49:35.658
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/26/23 06:49:35.658
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/26/23 06:49:35.689
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/26/23 06:49:36.71
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/26/23 06:49:39.731
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/26/23 06:49:39.74
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/26/23 06:49:39.74
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 26 06:49:39.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1292" for this suite. 08/26/23 06:49:39.792
------------------------------
• [SLOW TEST] [26.428 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:49:13.373
    Aug 26 06:49:13.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename container-runtime 08/26/23 06:49:13.376
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:49:13.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:49:13.402
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/26/23 06:49:13.416
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/26/23 06:49:30.534
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/26/23 06:49:30.542
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/26/23 06:49:30.551
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/26/23 06:49:30.551
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/26/23 06:49:30.583
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/26/23 06:49:33.617
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/26/23 06:49:35.647
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/26/23 06:49:35.658
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/26/23 06:49:35.658
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/26/23 06:49:35.689
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/26/23 06:49:36.71
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/26/23 06:49:39.731
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/26/23 06:49:39.74
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/26/23 06:49:39.74
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:49:39.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1292" for this suite. 08/26/23 06:49:39.792
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:49:39.802
Aug 26 06:49:39.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename proxy 08/26/23 06:49:39.809
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:49:39.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:49:39.835
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Aug 26 06:49:39.838: INFO: Creating pod...
Aug 26 06:49:39.858: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8050" to be "running"
Aug 26 06:49:39.865: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.70465ms
Aug 26 06:49:41.871: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013485842s
Aug 26 06:49:41.871: INFO: Pod "agnhost" satisfied condition "running"
Aug 26 06:49:41.871: INFO: Creating service...
Aug 26 06:49:41.887: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/DELETE
Aug 26 06:49:41.922: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 26 06:49:41.922: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/GET
Aug 26 06:49:41.937: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 26 06:49:41.937: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/HEAD
Aug 26 06:49:41.943: INFO: http.Client request:HEAD | StatusCode:200
Aug 26 06:49:41.943: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 26 06:49:41.949: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 26 06:49:41.949: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/PATCH
Aug 26 06:49:41.954: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 26 06:49:41.954: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/POST
Aug 26 06:49:41.961: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 26 06:49:41.961: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/PUT
Aug 26 06:49:41.966: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 26 06:49:41.966: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/DELETE
Aug 26 06:49:41.975: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 26 06:49:41.975: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/GET
Aug 26 06:49:41.982: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 26 06:49:41.983: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/HEAD
Aug 26 06:49:41.991: INFO: http.Client request:HEAD | StatusCode:200
Aug 26 06:49:41.992: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/OPTIONS
Aug 26 06:49:42.000: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 26 06:49:42.000: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/PATCH
Aug 26 06:49:42.009: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 26 06:49:42.009: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/POST
Aug 26 06:49:42.019: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 26 06:49:42.019: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/PUT
Aug 26 06:49:42.030: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 26 06:49:42.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8050" for this suite. 08/26/23 06:49:42.04
------------------------------
• [2.247 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:49:39.802
    Aug 26 06:49:39.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename proxy 08/26/23 06:49:39.809
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:49:39.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:49:39.835
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Aug 26 06:49:39.838: INFO: Creating pod...
    Aug 26 06:49:39.858: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8050" to be "running"
    Aug 26 06:49:39.865: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.70465ms
    Aug 26 06:49:41.871: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013485842s
    Aug 26 06:49:41.871: INFO: Pod "agnhost" satisfied condition "running"
    Aug 26 06:49:41.871: INFO: Creating service...
    Aug 26 06:49:41.887: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/DELETE
    Aug 26 06:49:41.922: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 26 06:49:41.922: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/GET
    Aug 26 06:49:41.937: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 26 06:49:41.937: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/HEAD
    Aug 26 06:49:41.943: INFO: http.Client request:HEAD | StatusCode:200
    Aug 26 06:49:41.943: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/OPTIONS
    Aug 26 06:49:41.949: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 26 06:49:41.949: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/PATCH
    Aug 26 06:49:41.954: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 26 06:49:41.954: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/POST
    Aug 26 06:49:41.961: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 26 06:49:41.961: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/pods/agnhost/proxy/some/path/with/PUT
    Aug 26 06:49:41.966: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 26 06:49:41.966: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/DELETE
    Aug 26 06:49:41.975: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 26 06:49:41.975: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/GET
    Aug 26 06:49:41.982: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 26 06:49:41.983: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/HEAD
    Aug 26 06:49:41.991: INFO: http.Client request:HEAD | StatusCode:200
    Aug 26 06:49:41.992: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/OPTIONS
    Aug 26 06:49:42.000: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 26 06:49:42.000: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/PATCH
    Aug 26 06:49:42.009: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 26 06:49:42.009: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/POST
    Aug 26 06:49:42.019: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 26 06:49:42.019: INFO: Starting http.Client for https://10.21.0.1:443/api/v1/namespaces/proxy-8050/services/test-service/proxy/some/path/with/PUT
    Aug 26 06:49:42.030: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:49:42.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8050" for this suite. 08/26/23 06:49:42.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:49:42.09
Aug 26 06:49:42.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename var-expansion 08/26/23 06:49:42.092
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:49:42.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:49:42.116
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 08/26/23 06:49:42.119
Aug 26 06:49:42.132: INFO: Waiting up to 5m0s for pod "var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b" in namespace "var-expansion-929" to be "Succeeded or Failed"
Aug 26 06:49:42.144: INFO: Pod "var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.790687ms
Aug 26 06:49:44.159: INFO: Pod "var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027533198s
Aug 26 06:49:46.155: INFO: Pod "var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023188723s
STEP: Saw pod success 08/26/23 06:49:46.155
Aug 26 06:49:46.155: INFO: Pod "var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b" satisfied condition "Succeeded or Failed"
Aug 26 06:49:46.160: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b container dapi-container: <nil>
STEP: delete the pod 08/26/23 06:49:46.187
Aug 26 06:49:46.212: INFO: Waiting for pod var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b to disappear
Aug 26 06:49:46.324: INFO: Pod var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 26 06:49:46.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-929" for this suite. 08/26/23 06:49:46.339
------------------------------
• [4.259 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:49:42.09
    Aug 26 06:49:42.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename var-expansion 08/26/23 06:49:42.092
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:49:42.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:49:42.116
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 08/26/23 06:49:42.119
    Aug 26 06:49:42.132: INFO: Waiting up to 5m0s for pod "var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b" in namespace "var-expansion-929" to be "Succeeded or Failed"
    Aug 26 06:49:42.144: INFO: Pod "var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.790687ms
    Aug 26 06:49:44.159: INFO: Pod "var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027533198s
    Aug 26 06:49:46.155: INFO: Pod "var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023188723s
    STEP: Saw pod success 08/26/23 06:49:46.155
    Aug 26 06:49:46.155: INFO: Pod "var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b" satisfied condition "Succeeded or Failed"
    Aug 26 06:49:46.160: INFO: Trying to get logs from node ip-10-0-1-101.us-west-2.compute.internal pod var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b container dapi-container: <nil>
    STEP: delete the pod 08/26/23 06:49:46.187
    Aug 26 06:49:46.212: INFO: Waiting for pod var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b to disappear
    Aug 26 06:49:46.324: INFO: Pod var-expansion-625f6d22-22e0-4f10-85a7-3a7cd17ab83b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:49:46.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-929" for this suite. 08/26/23 06:49:46.339
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:49:46.35
Aug 26 06:49:46.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename daemonsets 08/26/23 06:49:46.351
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:49:46.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:49:46.391
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 08/26/23 06:49:46.475
STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 06:49:46.482
Aug 26 06:49:46.505: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:49:46.506: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:49:46.506: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:49:46.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:49:46.513: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:49:47.524: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:49:47.524: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:49:47.524: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:49:47.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:49:47.532: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
Aug 26 06:49:48.523: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:49:48.523: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:49:48.523: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 26 06:49:48.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 26 06:49:48.530: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Getting /status 08/26/23 06:49:48.534
Aug 26 06:49:48.539: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 08/26/23 06:49:48.539
Aug 26 06:49:48.551: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 08/26/23 06:49:48.551
Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: ADDED
Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.554: INFO: Found daemon set daemon-set in namespace daemonsets-5938 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 26 06:49:48.554: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 08/26/23 06:49:48.554
STEP: watching for the daemon set status to be patched 08/26/23 06:49:48.565
Aug 26 06:49:48.568: INFO: Observed &DaemonSet event: ADDED
Aug 26 06:49:48.568: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.568: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.568: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.568: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.569: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.569: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.569: INFO: Observed daemon set daemon-set in namespace daemonsets-5938 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 26 06:49:48.569: INFO: Observed &DaemonSet event: MODIFIED
Aug 26 06:49:48.569: INFO: Found daemon set daemon-set in namespace daemonsets-5938 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 26 06:49:48.569: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/26/23 06:49:48.576
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5938, will wait for the garbage collector to delete the pods 08/26/23 06:49:48.576
Aug 26 06:49:48.645: INFO: Deleting DaemonSet.extensions daemon-set took: 10.968372ms
Aug 26 06:49:48.746: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.210322ms
Aug 26 06:49:51.151: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 26 06:49:51.151: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 26 06:49:51.159: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50016"},"items":null}

Aug 26 06:49:51.166: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50016"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 26 06:49:51.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5938" for this suite. 08/26/23 06:49:51.219
------------------------------
• [4.882 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:49:46.35
    Aug 26 06:49:46.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename daemonsets 08/26/23 06:49:46.351
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:49:46.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:49:46.391
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 08/26/23 06:49:46.475
    STEP: Check that daemon pods launch on every node of the cluster. 08/26/23 06:49:46.482
    Aug 26 06:49:46.505: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:49:46.506: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:49:46.506: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:49:46.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:49:46.513: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:49:47.524: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:49:47.524: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:49:47.524: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:49:47.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:49:47.532: INFO: Node ip-10-0-1-101.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Aug 26 06:49:48.523: INFO: DaemonSet pods can't tolerate node ip-10-0-1-206.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:49:48.523: INFO: DaemonSet pods can't tolerate node ip-10-0-1-230.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:49:48.523: INFO: DaemonSet pods can't tolerate node ip-10-0-1-54.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 26 06:49:48.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 26 06:49:48.530: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Getting /status 08/26/23 06:49:48.534
    Aug 26 06:49:48.539: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 08/26/23 06:49:48.539
    Aug 26 06:49:48.551: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 08/26/23 06:49:48.551
    Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: ADDED
    Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.554: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.554: INFO: Found daemon set daemon-set in namespace daemonsets-5938 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 26 06:49:48.554: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 08/26/23 06:49:48.554
    STEP: watching for the daemon set status to be patched 08/26/23 06:49:48.565
    Aug 26 06:49:48.568: INFO: Observed &DaemonSet event: ADDED
    Aug 26 06:49:48.568: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.568: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.568: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.568: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.569: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.569: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.569: INFO: Observed daemon set daemon-set in namespace daemonsets-5938 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 26 06:49:48.569: INFO: Observed &DaemonSet event: MODIFIED
    Aug 26 06:49:48.569: INFO: Found daemon set daemon-set in namespace daemonsets-5938 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Aug 26 06:49:48.569: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/26/23 06:49:48.576
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5938, will wait for the garbage collector to delete the pods 08/26/23 06:49:48.576
    Aug 26 06:49:48.645: INFO: Deleting DaemonSet.extensions daemon-set took: 10.968372ms
    Aug 26 06:49:48.746: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.210322ms
    Aug 26 06:49:51.151: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 26 06:49:51.151: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 26 06:49:51.159: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50016"},"items":null}

    Aug 26 06:49:51.166: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50016"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:49:51.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5938" for this suite. 08/26/23 06:49:51.219
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/26/23 06:49:51.232
Aug 26 06:49:51.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
STEP: Building a namespace api object, basename resourcequota 08/26/23 06:49:51.234
STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:49:51.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:49:51.259
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 08/26/23 06:49:51.261
STEP: Creating a ResourceQuota 08/26/23 06:49:56.267
STEP: Ensuring resource quota status is calculated 08/26/23 06:49:56.279
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 26 06:49:58.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8123" for this suite. 08/26/23 06:49:58.294
------------------------------
• [SLOW TEST] [7.071 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/26/23 06:49:51.232
    Aug 26 06:49:51.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3637333530
    STEP: Building a namespace api object, basename resourcequota 08/26/23 06:49:51.234
    STEP: Waiting for a default service account to be provisioned in namespace 08/26/23 06:49:51.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/26/23 06:49:51.259
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 08/26/23 06:49:51.261
    STEP: Creating a ResourceQuota 08/26/23 06:49:56.267
    STEP: Ensuring resource quota status is calculated 08/26/23 06:49:56.279
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 26 06:49:58.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8123" for this suite. 08/26/23 06:49:58.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Aug 26 06:49:58.309: INFO: Running AfterSuite actions on node 1
Aug 26 06:49:58.309: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Aug 26 06:49:58.309: INFO: Running AfterSuite actions on node 1
    Aug 26 06:49:58.309: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.136 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5597.604 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h33m18.093484975s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m


I0830 14:26:36.859271      21 e2e.go:129] Starting e2e run "32f79c61-eaf9-42e7-98c1-4ec0a998fff3" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1661869596 - Will randomize all specs
Will run 356 of 6965 specs

Aug 30 14:26:38.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
E0830 14:26:38.741288      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Aug 30 14:26:38.742: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 30 14:26:38.827: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 30 14:26:38.941: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 30 14:26:38.941: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Aug 30 14:26:38.941: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 30 14:26:38.973: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Aug 30 14:26:38.973: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Aug 30 14:26:38.973: INFO: e2e test version: v1.24.0
Aug 30 14:26:38.981: INFO: kube-apiserver version: v1.24.0+9546431
Aug 30 14:26:38.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 14:26:39.000: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:26:39.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename discovery
Aug 30 14:26:39.171: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
W0830 14:26:39.171013      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:26:39.652: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 30 14:26:39.660: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 30 14:26:39.660: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 30 14:26:39.660: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 30 14:26:39.660: INFO: Checking APIGroup: apps
Aug 30 14:26:39.671: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 30 14:26:39.671: INFO: Versions found [{apps/v1 v1}]
Aug 30 14:26:39.671: INFO: apps/v1 matches apps/v1
Aug 30 14:26:39.671: INFO: Checking APIGroup: events.k8s.io
Aug 30 14:26:39.680: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 30 14:26:39.680: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Aug 30 14:26:39.680: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 30 14:26:39.680: INFO: Checking APIGroup: authentication.k8s.io
Aug 30 14:26:39.689: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 30 14:26:39.689: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 30 14:26:39.689: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 30 14:26:39.689: INFO: Checking APIGroup: authorization.k8s.io
Aug 30 14:26:39.724: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 30 14:26:39.724: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 30 14:26:39.724: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 30 14:26:39.724: INFO: Checking APIGroup: autoscaling
Aug 30 14:26:39.733: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 30 14:26:39.733: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Aug 30 14:26:39.733: INFO: autoscaling/v2 matches autoscaling/v2
Aug 30 14:26:39.733: INFO: Checking APIGroup: batch
Aug 30 14:26:39.741: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 30 14:26:39.742: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Aug 30 14:26:39.742: INFO: batch/v1 matches batch/v1
Aug 30 14:26:39.742: INFO: Checking APIGroup: certificates.k8s.io
Aug 30 14:26:39.751: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 30 14:26:39.751: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 30 14:26:39.751: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 30 14:26:39.751: INFO: Checking APIGroup: networking.k8s.io
Aug 30 14:26:39.761: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 30 14:26:39.761: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 30 14:26:39.761: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 30 14:26:39.761: INFO: Checking APIGroup: policy
Aug 30 14:26:39.768: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 30 14:26:39.768: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Aug 30 14:26:39.769: INFO: policy/v1 matches policy/v1
Aug 30 14:26:39.769: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 30 14:26:39.775: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 30 14:26:39.775: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 30 14:26:39.775: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 30 14:26:39.775: INFO: Checking APIGroup: storage.k8s.io
Aug 30 14:26:39.781: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 30 14:26:39.781: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 30 14:26:39.781: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 30 14:26:39.781: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 30 14:26:39.791: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 30 14:26:39.791: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 30 14:26:39.791: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 30 14:26:39.791: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 30 14:26:39.798: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 30 14:26:39.798: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 30 14:26:39.798: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 30 14:26:39.798: INFO: Checking APIGroup: scheduling.k8s.io
Aug 30 14:26:39.805: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 30 14:26:39.805: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 30 14:26:39.805: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 30 14:26:39.805: INFO: Checking APIGroup: coordination.k8s.io
Aug 30 14:26:39.812: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 30 14:26:39.812: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 30 14:26:39.812: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 30 14:26:39.812: INFO: Checking APIGroup: node.k8s.io
Aug 30 14:26:39.820: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 30 14:26:39.820: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Aug 30 14:26:39.820: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 30 14:26:39.820: INFO: Checking APIGroup: discovery.k8s.io
Aug 30 14:26:39.829: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 30 14:26:39.829: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Aug 30 14:26:39.834: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 30 14:26:39.834: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 30 14:26:39.843: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Aug 30 14:26:39.843: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Aug 30 14:26:39.843: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Aug 30 14:26:39.843: INFO: Checking APIGroup: apps.openshift.io
Aug 30 14:26:39.850: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Aug 30 14:26:39.850: INFO: Versions found [{apps.openshift.io/v1 v1}]
Aug 30 14:26:39.850: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Aug 30 14:26:39.850: INFO: Checking APIGroup: authorization.openshift.io
Aug 30 14:26:39.860: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Aug 30 14:26:39.860: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Aug 30 14:26:39.860: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Aug 30 14:26:39.860: INFO: Checking APIGroup: build.openshift.io
Aug 30 14:26:39.867: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Aug 30 14:26:39.867: INFO: Versions found [{build.openshift.io/v1 v1}]
Aug 30 14:26:39.867: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Aug 30 14:26:39.867: INFO: Checking APIGroup: image.openshift.io
Aug 30 14:26:39.875: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Aug 30 14:26:39.875: INFO: Versions found [{image.openshift.io/v1 v1}]
Aug 30 14:26:39.875: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Aug 30 14:26:39.875: INFO: Checking APIGroup: oauth.openshift.io
Aug 30 14:26:39.886: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Aug 30 14:26:39.886: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Aug 30 14:26:39.886: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Aug 30 14:26:39.886: INFO: Checking APIGroup: project.openshift.io
Aug 30 14:26:39.896: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Aug 30 14:26:39.896: INFO: Versions found [{project.openshift.io/v1 v1}]
Aug 30 14:26:39.896: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Aug 30 14:26:39.896: INFO: Checking APIGroup: quota.openshift.io
Aug 30 14:26:39.906: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Aug 30 14:26:39.906: INFO: Versions found [{quota.openshift.io/v1 v1}]
Aug 30 14:26:39.906: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Aug 30 14:26:39.906: INFO: Checking APIGroup: route.openshift.io
Aug 30 14:26:39.916: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Aug 30 14:26:39.916: INFO: Versions found [{route.openshift.io/v1 v1}]
Aug 30 14:26:39.916: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Aug 30 14:26:39.916: INFO: Checking APIGroup: security.openshift.io
Aug 30 14:26:39.923: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Aug 30 14:26:39.923: INFO: Versions found [{security.openshift.io/v1 v1}]
Aug 30 14:26:39.923: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Aug 30 14:26:39.923: INFO: Checking APIGroup: template.openshift.io
Aug 30 14:26:39.932: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Aug 30 14:26:39.932: INFO: Versions found [{template.openshift.io/v1 v1}]
Aug 30 14:26:39.932: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Aug 30 14:26:39.932: INFO: Checking APIGroup: user.openshift.io
Aug 30 14:26:39.945: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Aug 30 14:26:39.945: INFO: Versions found [{user.openshift.io/v1 v1}]
Aug 30 14:26:39.945: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Aug 30 14:26:39.945: INFO: Checking APIGroup: packages.operators.coreos.com
Aug 30 14:26:39.954: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Aug 30 14:26:39.954: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Aug 30 14:26:39.954: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Aug 30 14:26:39.954: INFO: Checking APIGroup: config.openshift.io
Aug 30 14:26:39.968: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Aug 30 14:26:39.968: INFO: Versions found [{config.openshift.io/v1 v1}]
Aug 30 14:26:39.968: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Aug 30 14:26:39.968: INFO: Checking APIGroup: operator.openshift.io
Aug 30 14:26:39.978: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Aug 30 14:26:39.978: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Aug 30 14:26:39.978: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Aug 30 14:26:39.978: INFO: Checking APIGroup: apiserver.openshift.io
Aug 30 14:26:39.986: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Aug 30 14:26:39.986: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Aug 30 14:26:39.986: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Aug 30 14:26:39.987: INFO: Checking APIGroup: cloudcredential.openshift.io
Aug 30 14:26:39.995: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Aug 30 14:26:39.995: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Aug 30 14:26:39.995: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Aug 30 14:26:39.995: INFO: Checking APIGroup: console.openshift.io
Aug 30 14:26:40.004: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Aug 30 14:26:40.004: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Aug 30 14:26:40.004: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Aug 30 14:26:40.004: INFO: Checking APIGroup: crd.projectcalico.org
Aug 30 14:26:40.020: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 30 14:26:40.020: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 30 14:26:40.020: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 30 14:26:40.020: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Aug 30 14:26:40.029: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Aug 30 14:26:40.029: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Aug 30 14:26:40.029: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Aug 30 14:26:40.029: INFO: Checking APIGroup: ingress.operator.openshift.io
Aug 30 14:26:40.053: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Aug 30 14:26:40.053: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Aug 30 14:26:40.053: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Aug 30 14:26:40.053: INFO: Checking APIGroup: k8s.cni.cncf.io
Aug 30 14:26:40.066: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Aug 30 14:26:40.066: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Aug 30 14:26:40.066: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Aug 30 14:26:40.066: INFO: Checking APIGroup: machineconfiguration.openshift.io
Aug 30 14:26:40.081: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Aug 30 14:26:40.081: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Aug 30 14:26:40.081: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Aug 30 14:26:40.081: INFO: Checking APIGroup: monitoring.coreos.com
Aug 30 14:26:40.090: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Aug 30 14:26:40.090: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Aug 30 14:26:40.090: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Aug 30 14:26:40.090: INFO: Checking APIGroup: network.operator.openshift.io
Aug 30 14:26:40.099: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Aug 30 14:26:40.099: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Aug 30 14:26:40.099: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Aug 30 14:26:40.099: INFO: Checking APIGroup: operator.tigera.io
Aug 30 14:26:40.110: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Aug 30 14:26:40.110: INFO: Versions found [{operator.tigera.io/v1 v1}]
Aug 30 14:26:40.110: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Aug 30 14:26:40.110: INFO: Checking APIGroup: operators.coreos.com
Aug 30 14:26:40.125: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Aug 30 14:26:40.125: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Aug 30 14:26:40.125: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Aug 30 14:26:40.125: INFO: Checking APIGroup: performance.openshift.io
Aug 30 14:26:40.143: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Aug 30 14:26:40.143: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Aug 30 14:26:40.143: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Aug 30 14:26:40.143: INFO: Checking APIGroup: samples.operator.openshift.io
Aug 30 14:26:40.150: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Aug 30 14:26:40.150: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Aug 30 14:26:40.150: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Aug 30 14:26:40.150: INFO: Checking APIGroup: security.internal.openshift.io
Aug 30 14:26:40.160: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Aug 30 14:26:40.160: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Aug 30 14:26:40.160: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Aug 30 14:26:40.160: INFO: Checking APIGroup: snapshot.storage.k8s.io
Aug 30 14:26:40.171: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Aug 30 14:26:40.171: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Aug 30 14:26:40.171: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Aug 30 14:26:40.171: INFO: Checking APIGroup: tuned.openshift.io
Aug 30 14:26:40.181: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Aug 30 14:26:40.181: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Aug 30 14:26:40.181: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Aug 30 14:26:40.181: INFO: Checking APIGroup: controlplane.operator.openshift.io
Aug 30 14:26:40.194: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Aug 30 14:26:40.194: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Aug 30 14:26:40.194: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Aug 30 14:26:40.194: INFO: Checking APIGroup: ibm.com
Aug 30 14:26:40.204: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Aug 30 14:26:40.204: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Aug 30 14:26:40.204: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Aug 30 14:26:40.204: INFO: Checking APIGroup: migration.k8s.io
Aug 30 14:26:40.212: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Aug 30 14:26:40.212: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Aug 30 14:26:40.212: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Aug 30 14:26:40.212: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Aug 30 14:26:40.221: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Aug 30 14:26:40.221: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Aug 30 14:26:40.221: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Aug 30 14:26:40.221: INFO: Checking APIGroup: helm.openshift.io
Aug 30 14:26:40.230: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Aug 30 14:26:40.230: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Aug 30 14:26:40.230: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Aug 30 14:26:40.230: INFO: Checking APIGroup: metrics.k8s.io
Aug 30 14:26:40.250: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 30 14:26:40.250: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 30 14:26:40.250: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Aug 30 14:26:40.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4379" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":1,"skipped":17,"failed":0}
SSSSSSSSSSE0830 14:26:40.331197      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:26:40.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
W0830 14:26:41.288550      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:26:41.360: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 14:26:43.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 14:26:45.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 14:26:47.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 14:26:49.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 14:26:51.475: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 14:26:53.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 26, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 14:26:56.501: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:26:57.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9016" for this suite.
STEP: Destroying namespace "webhook-9016-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:17.305 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":2,"skipped":48,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:26:57.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-d9480a69-78e7-44de-a2ef-4df69071e7d7
STEP: Creating a pod to test consume configMaps
W0830 14:26:57.868973      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:26:57.870: INFO: Waiting up to 5m0s for pod "pod-configmaps-414eaaab-573e-40f5-be33-4118b4af1b7d" in namespace "configmap-8771" to be "Succeeded or Failed"
Aug 30 14:26:57.888: INFO: Pod "pod-configmaps-414eaaab-573e-40f5-be33-4118b4af1b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.685948ms
Aug 30 14:26:59.916: INFO: Pod "pod-configmaps-414eaaab-573e-40f5-be33-4118b4af1b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045854735s
Aug 30 14:27:01.937: INFO: Pod "pod-configmaps-414eaaab-573e-40f5-be33-4118b4af1b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066449481s
Aug 30 14:27:03.957: INFO: Pod "pod-configmaps-414eaaab-573e-40f5-be33-4118b4af1b7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.086855805s
STEP: Saw pod success
Aug 30 14:27:03.957: INFO: Pod "pod-configmaps-414eaaab-573e-40f5-be33-4118b4af1b7d" satisfied condition "Succeeded or Failed"
Aug 30 14:27:03.972: INFO: Trying to get logs from node 10.63.224.189 pod pod-configmaps-414eaaab-573e-40f5-be33-4118b4af1b7d container agnhost-container: <nil>
STEP: delete the pod
Aug 30 14:27:04.113: INFO: Waiting for pod pod-configmaps-414eaaab-573e-40f5-be33-4118b4af1b7d to disappear
Aug 30 14:27:04.130: INFO: Pod pod-configmaps-414eaaab-573e-40f5-be33-4118b4af1b7d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 14:27:04.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8771" for this suite.

• [SLOW TEST:6.605 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":3,"skipped":61,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:27:04.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9639.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9639.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9639.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9639.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9639.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9639.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9639.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9639.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9639.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9639.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9639.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9639.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 119.7.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.7.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.7.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.7.119_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9639.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9639.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9639.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9639.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9639.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9639.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9639.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9639.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9639.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9639.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9639.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9639.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 119.7.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.7.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.7.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.7.119_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
W0830 14:27:04.658211      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 14:27:22.761: INFO: Unable to read wheezy_udp@dns-test-service.dns-9639.svc.cluster.local from pod dns-9639/dns-test-615891e3-5567-4903-8c5f-26d6cf401db5: the server could not find the requested resource (get pods dns-test-615891e3-5567-4903-8c5f-26d6cf401db5)
Aug 30 14:27:22.800: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9639.svc.cluster.local from pod dns-9639/dns-test-615891e3-5567-4903-8c5f-26d6cf401db5: the server could not find the requested resource (get pods dns-test-615891e3-5567-4903-8c5f-26d6cf401db5)
Aug 30 14:27:22.997: INFO: Unable to read jessie_udp@dns-test-service.dns-9639.svc.cluster.local from pod dns-9639/dns-test-615891e3-5567-4903-8c5f-26d6cf401db5: the server could not find the requested resource (get pods dns-test-615891e3-5567-4903-8c5f-26d6cf401db5)
Aug 30 14:27:23.029: INFO: Unable to read jessie_tcp@dns-test-service.dns-9639.svc.cluster.local from pod dns-9639/dns-test-615891e3-5567-4903-8c5f-26d6cf401db5: the server could not find the requested resource (get pods dns-test-615891e3-5567-4903-8c5f-26d6cf401db5)
Aug 30 14:27:23.189: INFO: Lookups using dns-9639/dns-test-615891e3-5567-4903-8c5f-26d6cf401db5 failed for: [wheezy_udp@dns-test-service.dns-9639.svc.cluster.local wheezy_tcp@dns-test-service.dns-9639.svc.cluster.local jessie_udp@dns-test-service.dns-9639.svc.cluster.local jessie_tcp@dns-test-service.dns-9639.svc.cluster.local]

Aug 30 14:27:28.711: INFO: DNS probes using dns-9639/dns-test-615891e3-5567-4903-8c5f-26d6cf401db5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 30 14:27:28.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9639" for this suite.

• [SLOW TEST:24.741 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":4,"skipped":87,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:27:28.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-28787d78-cd04-40ac-a527-35a2247d18f8
STEP: Creating a pod to test consume secrets
W0830 14:27:29.231862      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:27:29.232: INFO: Waiting up to 5m0s for pod "pod-secrets-17bd6b07-9250-4362-b527-966143ce6428" in namespace "secrets-72" to be "Succeeded or Failed"
Aug 30 14:27:29.250: INFO: Pod "pod-secrets-17bd6b07-9250-4362-b527-966143ce6428": Phase="Pending", Reason="", readiness=false. Elapsed: 18.319889ms
Aug 30 14:27:31.270: INFO: Pod "pod-secrets-17bd6b07-9250-4362-b527-966143ce6428": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037957166s
Aug 30 14:27:33.289: INFO: Pod "pod-secrets-17bd6b07-9250-4362-b527-966143ce6428": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057167948s
Aug 30 14:27:35.308: INFO: Pod "pod-secrets-17bd6b07-9250-4362-b527-966143ce6428": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075760431s
STEP: Saw pod success
Aug 30 14:27:35.308: INFO: Pod "pod-secrets-17bd6b07-9250-4362-b527-966143ce6428" satisfied condition "Succeeded or Failed"
Aug 30 14:27:35.328: INFO: Trying to get logs from node 10.63.224.189 pod pod-secrets-17bd6b07-9250-4362-b527-966143ce6428 container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 14:27:35.681: INFO: Waiting for pod pod-secrets-17bd6b07-9250-4362-b527-966143ce6428 to disappear
Aug 30 14:27:35.694: INFO: Pod pod-secrets-17bd6b07-9250-4362-b527-966143ce6428 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 30 14:27:35.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-72" for this suite.

• [SLOW TEST:6.782 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":5,"skipped":89,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:27:35.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
W0830 14:27:35.989344      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:27:36.013: INFO: The status of Pod pod-hostip-7a8d4db6-3bf9-421a-b6bd-ef08d4f0b851 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:27:38.050: INFO: The status of Pod pod-hostip-7a8d4db6-3bf9-421a-b6bd-ef08d4f0b851 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:27:40.035: INFO: The status of Pod pod-hostip-7a8d4db6-3bf9-421a-b6bd-ef08d4f0b851 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:27:42.033: INFO: The status of Pod pod-hostip-7a8d4db6-3bf9-421a-b6bd-ef08d4f0b851 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:27:44.052: INFO: The status of Pod pod-hostip-7a8d4db6-3bf9-421a-b6bd-ef08d4f0b851 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:27:46.038: INFO: The status of Pod pod-hostip-7a8d4db6-3bf9-421a-b6bd-ef08d4f0b851 is Running (Ready = true)
Aug 30 14:27:46.095: INFO: Pod pod-hostip-7a8d4db6-3bf9-421a-b6bd-ef08d4f0b851 has hostIP: 10.63.224.189
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 30 14:27:46.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2806" for this suite.

• [SLOW TEST:10.393 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":6,"skipped":145,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:27:46.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
W0830 14:27:47.324059      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:27:47.361: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 14:27:49.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 27, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 27, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 27, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 27, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 14:27:52.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:27:53.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6659" for this suite.
STEP: Destroying namespace "webhook-6659-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.241 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":7,"skipped":156,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:27:53.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 14:27:53.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1281" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":8,"skipped":166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:27:53.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 30 14:27:54.177: INFO: Waiting up to 5m0s for pod "downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499" in namespace "downward-api-7293" to be "Succeeded or Failed"
W0830 14:27:54.177232      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:27:54.192: INFO: Pod "downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499": Phase="Pending", Reason="", readiness=false. Elapsed: 14.616933ms
Aug 30 14:27:56.220: INFO: Pod "downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042811274s
Aug 30 14:27:58.245: INFO: Pod "downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068114562s
Aug 30 14:28:00.265: INFO: Pod "downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499": Phase="Pending", Reason="", readiness=false. Elapsed: 6.088035084s
Aug 30 14:28:02.287: INFO: Pod "downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499": Phase="Pending", Reason="", readiness=false. Elapsed: 8.109427777s
Aug 30 14:28:04.311: INFO: Pod "downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.133485405s
STEP: Saw pod success
Aug 30 14:28:04.311: INFO: Pod "downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499" satisfied condition "Succeeded or Failed"
Aug 30 14:28:04.337: INFO: Trying to get logs from node 10.63.224.187 pod downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499 container dapi-container: <nil>
STEP: delete the pod
Aug 30 14:28:04.499: INFO: Waiting for pod downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499 to disappear
Aug 30 14:28:04.517: INFO: Pod downward-api-fcb02f13-d941-4df9-838c-fbed0fa4c499 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 30 14:28:04.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7293" for this suite.

• [SLOW TEST:10.592 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":9,"skipped":192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:28:04.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Aug 30 14:28:04.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7430 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.36 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 30 14:28:04.959: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"logs-generator\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"logs-generator\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"logs-generator\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"logs-generator\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 14:28:04.959: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Aug 30 14:28:04.959: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 30 14:28:04.959: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7430" to be "running and ready, or succeeded"
Aug 30 14:28:04.977: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.630952ms
Aug 30 14:28:07.005: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04648178s
Aug 30 14:28:09.061: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.10169304s
Aug 30 14:28:09.061: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 30 14:28:09.061: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 30 14:28:09.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7430 logs logs-generator logs-generator'
Aug 30 14:28:09.273: INFO: stderr: ""
Aug 30 14:28:09.273: INFO: stdout: "I0830 14:28:06.768133       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/zv2 232\nI0830 14:28:06.968406       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/ct5t 478\nI0830 14:28:07.168323       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/nbvw 490\nI0830 14:28:07.369232       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/2fjg 442\nI0830 14:28:07.568950       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/wnp 562\nI0830 14:28:07.768592       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/gvx 371\nI0830 14:28:07.968251       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/fgj 243\nI0830 14:28:08.168814       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/wtm2 250\nI0830 14:28:08.368247       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/rj55 279\nI0830 14:28:08.568806       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/8bz6 497\nI0830 14:28:08.768200       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/zrk 518\nI0830 14:28:08.968630       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/8xx 380\nI0830 14:28:09.168222       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/xrqq 512\n"
STEP: limiting log lines
Aug 30 14:28:09.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7430 logs logs-generator logs-generator --tail=1'
Aug 30 14:28:09.492: INFO: stderr: ""
Aug 30 14:28:09.492: INFO: stdout: "I0830 14:28:09.368665       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/ckkp 342\n"
Aug 30 14:28:09.492: INFO: got output "I0830 14:28:09.368665       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/ckkp 342\n"
STEP: limiting log bytes
Aug 30 14:28:09.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7430 logs logs-generator logs-generator --limit-bytes=1'
Aug 30 14:28:09.678: INFO: stderr: ""
Aug 30 14:28:09.678: INFO: stdout: "I"
Aug 30 14:28:09.678: INFO: got output "I"
STEP: exposing timestamps
Aug 30 14:28:09.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7430 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 30 14:28:09.854: INFO: stderr: ""
Aug 30 14:28:09.854: INFO: stdout: "2022-08-30T09:28:09.769081090-05:00 I0830 14:28:09.768923       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/m2m 556\n"
Aug 30 14:28:09.854: INFO: got output "2022-08-30T09:28:09.769081090-05:00 I0830 14:28:09.768923       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/m2m 556\n"
STEP: restricting to a time range
Aug 30 14:28:12.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7430 logs logs-generator logs-generator --since=1s'
Aug 30 14:28:12.560: INFO: stderr: ""
Aug 30 14:28:12.561: INFO: stdout: "I0830 14:28:11.568436       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/b8p 421\nI0830 14:28:11.768308       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/6j5b 254\nI0830 14:28:11.969595       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/89z 280\nI0830 14:28:12.169318       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/42s6 530\nI0830 14:28:12.368784       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/xxnh 426\n"
Aug 30 14:28:12.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7430 logs logs-generator logs-generator --since=24h'
Aug 30 14:28:12.790: INFO: stderr: ""
Aug 30 14:28:12.790: INFO: stdout: "I0830 14:28:06.768133       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/zv2 232\nI0830 14:28:06.968406       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/ct5t 478\nI0830 14:28:07.168323       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/nbvw 490\nI0830 14:28:07.369232       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/2fjg 442\nI0830 14:28:07.568950       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/wnp 562\nI0830 14:28:07.768592       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/gvx 371\nI0830 14:28:07.968251       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/fgj 243\nI0830 14:28:08.168814       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/wtm2 250\nI0830 14:28:08.368247       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/rj55 279\nI0830 14:28:08.568806       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/8bz6 497\nI0830 14:28:08.768200       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/zrk 518\nI0830 14:28:08.968630       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/8xx 380\nI0830 14:28:09.168222       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/xrqq 512\nI0830 14:28:09.368665       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/ckkp 342\nI0830 14:28:09.568090       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/z2d 426\nI0830 14:28:09.768923       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/m2m 556\nI0830 14:28:09.968660       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/h2v 370\nI0830 14:28:10.169120       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/kxws 535\nI0830 14:28:10.369351       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/jw87 235\nI0830 14:28:10.568750       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/22sd 585\nI0830 14:28:10.768604       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/x6d 360\nI0830 14:28:10.969251       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/8cc 374\nI0830 14:28:11.168325       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/kk6z 400\nI0830 14:28:11.368951       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/g4k4 582\nI0830 14:28:11.568436       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/b8p 421\nI0830 14:28:11.768308       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/6j5b 254\nI0830 14:28:11.969595       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/89z 280\nI0830 14:28:12.169318       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/42s6 530\nI0830 14:28:12.368784       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/xxnh 426\nI0830 14:28:12.568449       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/trc 472\nI0830 14:28:12.768936       1 logs_generator.go:76] 30 POST /api/v1/namespaces/default/pods/llz8 252\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Aug 30 14:28:12.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7430 delete pod logs-generator'
Aug 30 14:28:15.264: INFO: stderr: ""
Aug 30 14:28:15.264: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 14:28:15.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7430" for this suite.

• [SLOW TEST:10.770 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":10,"skipped":218,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:28:15.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 14:28:15.536470      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:28:15.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc009945-6347-41c3-a304-1f5fed5a8d7d" in namespace "projected-5839" to be "Succeeded or Failed"
Aug 30 14:28:15.557: INFO: Pod "downwardapi-volume-fc009945-6347-41c3-a304-1f5fed5a8d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.974319ms
Aug 30 14:28:17.580: INFO: Pod "downwardapi-volume-fc009945-6347-41c3-a304-1f5fed5a8d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044201846s
Aug 30 14:28:19.601: INFO: Pod "downwardapi-volume-fc009945-6347-41c3-a304-1f5fed5a8d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065168018s
Aug 30 14:28:21.618: INFO: Pod "downwardapi-volume-fc009945-6347-41c3-a304-1f5fed5a8d7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082003401s
STEP: Saw pod success
Aug 30 14:28:21.619: INFO: Pod "downwardapi-volume-fc009945-6347-41c3-a304-1f5fed5a8d7d" satisfied condition "Succeeded or Failed"
Aug 30 14:28:21.637: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-fc009945-6347-41c3-a304-1f5fed5a8d7d container client-container: <nil>
STEP: delete the pod
Aug 30 14:28:21.812: INFO: Waiting for pod downwardapi-volume-fc009945-6347-41c3-a304-1f5fed5a8d7d to disappear
Aug 30 14:28:21.832: INFO: Pod downwardapi-volume-fc009945-6347-41c3-a304-1f5fed5a8d7d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 14:28:21.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5839" for this suite.

• [SLOW TEST:6.566 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":11,"skipped":231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:28:21.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-3679
STEP: creating replication controller nodeport-test in namespace services-3679
W0830 14:28:22.173221      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nodeport-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nodeport-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nodeport-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nodeport-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 14:28:22.173416      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3679, replica count: 2
I0830 14:28:25.225230      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 14:28:28.226124      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 14:28:31.228511      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 14:28:34.229429      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 14:28:34.229: INFO: Creating new exec pod
W0830 14:28:34.284611      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:28:37.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-3679 exec execpodjqkvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 30 14:28:37.798: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 30 14:28:37.799: INFO: stdout: "nodeport-test-lb92t"
Aug 30 14:28:37.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-3679 exec execpodjqkvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.60.33 80'
Aug 30 14:28:38.433: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.60.33 80\nConnection to 172.21.60.33 80 port [tcp/http] succeeded!\n"
Aug 30 14:28:38.433: INFO: stdout: ""
Aug 30 14:28:39.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-3679 exec execpodjqkvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.60.33 80'
Aug 30 14:28:39.822: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.60.33 80\nConnection to 172.21.60.33 80 port [tcp/http] succeeded!\n"
Aug 30 14:28:39.822: INFO: stdout: ""
Aug 30 14:28:40.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-3679 exec execpodjqkvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.60.33 80'
Aug 30 14:28:40.832: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.60.33 80\nConnection to 172.21.60.33 80 port [tcp/http] succeeded!\n"
Aug 30 14:28:40.832: INFO: stdout: ""
Aug 30 14:28:41.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-3679 exec execpodjqkvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.60.33 80'
Aug 30 14:28:41.925: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.60.33 80\nConnection to 172.21.60.33 80 port [tcp/http] succeeded!\n"
Aug 30 14:28:41.925: INFO: stdout: ""
Aug 30 14:28:42.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-3679 exec execpodjqkvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.60.33 80'
Aug 30 14:28:42.870: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.60.33 80\nConnection to 172.21.60.33 80 port [tcp/http] succeeded!\n"
Aug 30 14:28:42.870: INFO: stdout: ""
Aug 30 14:28:43.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-3679 exec execpodjqkvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.60.33 80'
Aug 30 14:28:43.827: INFO: stderr: "+ echo+  hostNamenc -v\n -t -w 2 172.21.60.33 80\nConnection to 172.21.60.33 80 port [tcp/http] succeeded!\n"
Aug 30 14:28:43.827: INFO: stdout: "nodeport-test-kgsvt"
Aug 30 14:28:43.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-3679 exec execpodjqkvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.189 32419'
Aug 30 14:28:44.280: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.63.224.189 32419\nConnection to 10.63.224.189 32419 port [tcp/*] succeeded!\n"
Aug 30 14:28:44.280: INFO: stdout: ""
Aug 30 14:28:45.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-3679 exec execpodjqkvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.189 32419'
Aug 30 14:28:45.683: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.63.224.189 32419\nConnection to 10.63.224.189 32419 port [tcp/*] succeeded!\n"
Aug 30 14:28:45.683: INFO: stdout: "nodeport-test-kgsvt"
Aug 30 14:28:45.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-3679 exec execpodjqkvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.187 32419'
Aug 30 14:28:46.079: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.63.224.187 32419\nConnection to 10.63.224.187 32419 port [tcp/*] succeeded!\n"
Aug 30 14:28:46.079: INFO: stdout: "nodeport-test-lb92t"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 14:28:46.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3679" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:24.229 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":12,"skipped":255,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:28:46.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Aug 30 14:28:46.430: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Aug 30 14:28:46.559: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 30 14:28:46.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-42" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":13,"skipped":291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:28:46.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-a71441d3-ca5d-4d1e-bf10-d243c9af42a5
STEP: Creating a pod to test consume configMaps
W0830 14:28:47.014101      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:28:47.014: INFO: Waiting up to 5m0s for pod "pod-configmaps-fc4ab844-0463-485b-bd0b-65e60a1e0600" in namespace "configmap-3787" to be "Succeeded or Failed"
Aug 30 14:28:47.028: INFO: Pod "pod-configmaps-fc4ab844-0463-485b-bd0b-65e60a1e0600": Phase="Pending", Reason="", readiness=false. Elapsed: 13.9512ms
Aug 30 14:28:49.046: INFO: Pod "pod-configmaps-fc4ab844-0463-485b-bd0b-65e60a1e0600": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031530022s
Aug 30 14:28:51.065: INFO: Pod "pod-configmaps-fc4ab844-0463-485b-bd0b-65e60a1e0600": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050405276s
Aug 30 14:28:53.093: INFO: Pod "pod-configmaps-fc4ab844-0463-485b-bd0b-65e60a1e0600": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078547098s
STEP: Saw pod success
Aug 30 14:28:53.093: INFO: Pod "pod-configmaps-fc4ab844-0463-485b-bd0b-65e60a1e0600" satisfied condition "Succeeded or Failed"
Aug 30 14:28:53.114: INFO: Trying to get logs from node 10.63.224.189 pod pod-configmaps-fc4ab844-0463-485b-bd0b-65e60a1e0600 container agnhost-container: <nil>
STEP: delete the pod
Aug 30 14:28:53.201: INFO: Waiting for pod pod-configmaps-fc4ab844-0463-485b-bd0b-65e60a1e0600 to disappear
Aug 30 14:28:53.232: INFO: Pod pod-configmaps-fc4ab844-0463-485b-bd0b-65e60a1e0600 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 14:28:53.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3787" for this suite.

• [SLOW TEST:6.575 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":14,"skipped":328,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:28:53.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-5a2b9056-77fd-484f-8fb3-4d8a73aa0b5d in namespace container-probe-2022
W0830 14:28:53.576787      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:28:57.621: INFO: Started pod liveness-5a2b9056-77fd-484f-8fb3-4d8a73aa0b5d in namespace container-probe-2022
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 14:28:57.638: INFO: Initial restart count of pod liveness-5a2b9056-77fd-484f-8fb3-4d8a73aa0b5d is 0
Aug 30 14:29:15.853: INFO: Restart count of pod container-probe-2022/liveness-5a2b9056-77fd-484f-8fb3-4d8a73aa0b5d is now 1 (18.214398405s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 30 14:29:15.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2022" for this suite.

• [SLOW TEST:22.641 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":15,"skipped":329,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:29:15.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-288b2b69-8a9a-4798-936a-a0eb6e45429c
STEP: Creating a pod to test consume secrets
W0830 14:29:16.180508      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:29:16.180: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-39dc38c6-6ed8-4161-8ce4-b1ac021c22da" in namespace "projected-5181" to be "Succeeded or Failed"
Aug 30 14:29:16.200: INFO: Pod "pod-projected-secrets-39dc38c6-6ed8-4161-8ce4-b1ac021c22da": Phase="Pending", Reason="", readiness=false. Elapsed: 19.326582ms
Aug 30 14:29:18.220: INFO: Pod "pod-projected-secrets-39dc38c6-6ed8-4161-8ce4-b1ac021c22da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03952653s
Aug 30 14:29:20.239: INFO: Pod "pod-projected-secrets-39dc38c6-6ed8-4161-8ce4-b1ac021c22da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058567698s
Aug 30 14:29:22.256: INFO: Pod "pod-projected-secrets-39dc38c6-6ed8-4161-8ce4-b1ac021c22da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075936548s
STEP: Saw pod success
Aug 30 14:29:22.256: INFO: Pod "pod-projected-secrets-39dc38c6-6ed8-4161-8ce4-b1ac021c22da" satisfied condition "Succeeded or Failed"
Aug 30 14:29:22.271: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-secrets-39dc38c6-6ed8-4161-8ce4-b1ac021c22da container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 30 14:29:22.379: INFO: Waiting for pod pod-projected-secrets-39dc38c6-6ed8-4161-8ce4-b1ac021c22da to disappear
Aug 30 14:29:22.394: INFO: Pod pod-projected-secrets-39dc38c6-6ed8-4161-8ce4-b1ac021c22da no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 30 14:29:22.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5181" for this suite.

• [SLOW TEST:6.483 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":16,"skipped":333,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:29:22.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
W0830 14:29:22.596456      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 30 14:29:33.200: INFO: Successfully updated pod "adopt-release-4fbq5"
STEP: Checking that the Job readopts the Pod
Aug 30 14:29:33.200: INFO: Waiting up to 15m0s for pod "adopt-release-4fbq5" in namespace "job-4425" to be "adopted"
Aug 30 14:29:33.220: INFO: Pod "adopt-release-4fbq5": Phase="Running", Reason="", readiness=true. Elapsed: 20.035421ms
Aug 30 14:29:35.238: INFO: Pod "adopt-release-4fbq5": Phase="Running", Reason="", readiness=true. Elapsed: 2.038392783s
Aug 30 14:29:35.238: INFO: Pod "adopt-release-4fbq5" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 30 14:29:35.822: INFO: Successfully updated pod "adopt-release-4fbq5"
STEP: Checking that the Job releases the Pod
Aug 30 14:29:35.823: INFO: Waiting up to 15m0s for pod "adopt-release-4fbq5" in namespace "job-4425" to be "released"
Aug 30 14:29:35.843: INFO: Pod "adopt-release-4fbq5": Phase="Running", Reason="", readiness=true. Elapsed: 20.468909ms
Aug 30 14:29:37.890: INFO: Pod "adopt-release-4fbq5": Phase="Running", Reason="", readiness=true. Elapsed: 2.067474962s
Aug 30 14:29:37.890: INFO: Pod "adopt-release-4fbq5" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 30 14:29:37.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4425" for this suite.

• [SLOW TEST:15.528 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":17,"skipped":347,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:29:37.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
W0830 14:29:39.973782      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:29:39.996: INFO: Pod name wrapped-volume-race-674c6ee1-4cac-4b6d-9bf9-77bf54162637: Found 0 pods out of 5
Aug 30 14:29:45.045: INFO: Pod name wrapped-volume-race-674c6ee1-4cac-4b6d-9bf9-77bf54162637: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-674c6ee1-4cac-4b6d-9bf9-77bf54162637 in namespace emptydir-wrapper-1677, will wait for the garbage collector to delete the pods
Aug 30 14:30:01.348: INFO: Deleting ReplicationController wrapped-volume-race-674c6ee1-4cac-4b6d-9bf9-77bf54162637 took: 37.080919ms
Aug 30 14:30:01.549: INFO: Terminating ReplicationController wrapped-volume-race-674c6ee1-4cac-4b6d-9bf9-77bf54162637 pods took: 200.509172ms
STEP: Creating RC which spawns configmap-volume pods
W0830 14:30:06.034152      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:30:06.054: INFO: Pod name wrapped-volume-race-fbc5ba51-0aad-47d8-a13d-cf2a5cbbb473: Found 0 pods out of 5
Aug 30 14:30:11.118: INFO: Pod name wrapped-volume-race-fbc5ba51-0aad-47d8-a13d-cf2a5cbbb473: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fbc5ba51-0aad-47d8-a13d-cf2a5cbbb473 in namespace emptydir-wrapper-1677, will wait for the garbage collector to delete the pods
Aug 30 14:30:11.357: INFO: Deleting ReplicationController wrapped-volume-race-fbc5ba51-0aad-47d8-a13d-cf2a5cbbb473 took: 32.600339ms
Aug 30 14:30:11.458: INFO: Terminating ReplicationController wrapped-volume-race-fbc5ba51-0aad-47d8-a13d-cf2a5cbbb473 pods took: 100.484872ms
STEP: Creating RC which spawns configmap-volume pods
W0830 14:30:15.747732      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:30:15.763: INFO: Pod name wrapped-volume-race-7ac6bf7b-f63b-482d-ad99-40d137e1325a: Found 0 pods out of 5
Aug 30 14:30:20.817: INFO: Pod name wrapped-volume-race-7ac6bf7b-f63b-482d-ad99-40d137e1325a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7ac6bf7b-f63b-482d-ad99-40d137e1325a in namespace emptydir-wrapper-1677, will wait for the garbage collector to delete the pods
Aug 30 14:30:25.132: INFO: Deleting ReplicationController wrapped-volume-race-7ac6bf7b-f63b-482d-ad99-40d137e1325a took: 93.896864ms
Aug 30 14:30:25.333: INFO: Terminating ReplicationController wrapped-volume-race-7ac6bf7b-f63b-482d-ad99-40d137e1325a pods took: 200.961575ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Aug 30 14:30:31.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1677" for this suite.

• [SLOW TEST:53.479 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":18,"skipped":349,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:30:31.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
W0830 14:30:31.671735      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 14:30:35.882: INFO: DNS probes using dns-7889/dns-test-cd9a3031-e1f3-467d-aafa-72565bb38c06 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 30 14:30:35.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7889" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":19,"skipped":373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:30:36.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5207 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5207;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5207 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5207;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5207.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5207.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5207.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5207.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5207.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5207.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5207.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5207.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5207.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5207.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5207.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5207.svc;check="$$(dig +notcp +noall +answer +search 80.242.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.242.80_udp@PTR;check="$$(dig +tcp +noall +answer +search 80.242.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.242.80_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5207 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5207;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5207 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5207;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5207.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5207.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5207.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5207.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5207.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5207.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5207.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5207.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5207.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5207.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5207.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5207.svc;check="$$(dig +notcp +noall +answer +search 80.242.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.242.80_udp@PTR;check="$$(dig +tcp +noall +answer +search 80.242.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.242.80_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
W0830 14:30:36.285340      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 14:30:40.453: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:40.478: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:40.560: INFO: Unable to read wheezy_udp@dns-test-service.dns-5207 from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:40.601: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5207 from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:40.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-5207.svc from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:40.665: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5207.svc from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:40.702: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5207.svc from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:40.723: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5207.svc from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:40.905: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:40.948: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:40.978: INFO: Unable to read jessie_udp@dns-test-service.dns-5207 from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:41.082: INFO: Unable to read jessie_tcp@dns-test-service.dns-5207 from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:41.194: INFO: Unable to read jessie_udp@dns-test-service.dns-5207.svc from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:41.269: INFO: Unable to read jessie_tcp@dns-test-service.dns-5207.svc from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:41.399: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5207.svc from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:41.500: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5207.svc from pod dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c: the server could not find the requested resource (get pods dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c)
Aug 30 14:30:41.719: INFO: Lookups using dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5207 wheezy_tcp@dns-test-service.dns-5207 wheezy_udp@dns-test-service.dns-5207.svc wheezy_tcp@dns-test-service.dns-5207.svc wheezy_udp@_http._tcp.dns-test-service.dns-5207.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5207.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5207 jessie_tcp@dns-test-service.dns-5207 jessie_udp@dns-test-service.dns-5207.svc jessie_tcp@dns-test-service.dns-5207.svc jessie_udp@_http._tcp.dns-test-service.dns-5207.svc jessie_tcp@_http._tcp.dns-test-service.dns-5207.svc]

Aug 30 14:30:47.445: INFO: DNS probes using dns-5207/dns-test-6dd6bcdb-5c7c-4ea7-9617-264e56fe196c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 30 14:30:47.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5207" for this suite.

• [SLOW TEST:11.716 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":20,"skipped":404,"failed":0}
SSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:30:47.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
W0830 14:30:47.853689      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:30:47.853: INFO: created test-podtemplate-1
W0830 14:30:47.878833      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:30:47.880: INFO: created test-podtemplate-2
W0830 14:30:47.910166      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:30:47.910: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Aug 30 14:30:47.932: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Aug 30 14:30:48.003: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Aug 30 14:30:48.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4881" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":21,"skipped":411,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:30:48.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 30 14:31:05.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8239" for this suite.

• [SLOW TEST:17.486 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":22,"skipped":420,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:31:05.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
W0830 14:31:05.760621      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 30 14:31:12.008: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 30 14:31:12.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0830 14:31:12.008338      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-3025" for this suite.

• [SLOW TEST:6.516 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":23,"skipped":437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:31:12.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
W0830 14:31:14.453213      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 14:31:14.511979      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 14:31:14.575607      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for all pods to be running
Aug 30 14:31:14.591: INFO: running pods: 0 < 3
Aug 30 14:31:16.613: INFO: running pods: 0 < 3
Aug 30 14:31:18.621: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 30 14:31:20.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-850" for this suite.

• [SLOW TEST:8.611 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":24,"skipped":498,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:31:20.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 14:31:21.002230      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:31:21.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd53552e-3769-4a87-a607-a677a7707424" in namespace "downward-api-2590" to be "Succeeded or Failed"
Aug 30 14:31:21.048: INFO: Pod "downwardapi-volume-dd53552e-3769-4a87-a607-a677a7707424": Phase="Pending", Reason="", readiness=false. Elapsed: 44.609164ms
Aug 30 14:31:23.087: INFO: Pod "downwardapi-volume-dd53552e-3769-4a87-a607-a677a7707424": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083291383s
Aug 30 14:31:25.107: INFO: Pod "downwardapi-volume-dd53552e-3769-4a87-a607-a677a7707424": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103705285s
Aug 30 14:31:27.126: INFO: Pod "downwardapi-volume-dd53552e-3769-4a87-a607-a677a7707424": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122425744s
Aug 30 14:31:29.144: INFO: Pod "downwardapi-volume-dd53552e-3769-4a87-a607-a677a7707424": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.140970732s
STEP: Saw pod success
Aug 30 14:31:29.145: INFO: Pod "downwardapi-volume-dd53552e-3769-4a87-a607-a677a7707424" satisfied condition "Succeeded or Failed"
Aug 30 14:31:29.161: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-dd53552e-3769-4a87-a607-a677a7707424 container client-container: <nil>
STEP: delete the pod
Aug 30 14:31:29.709: INFO: Waiting for pod downwardapi-volume-dd53552e-3769-4a87-a607-a677a7707424 to disappear
Aug 30 14:31:29.743: INFO: Pod downwardapi-volume-dd53552e-3769-4a87-a607-a677a7707424 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 14:31:29.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2590" for this suite.

• [SLOW TEST:9.112 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":25,"skipped":505,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:31:29.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6808.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6808.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6808.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6808.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
W0830 14:31:30.031403      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 14:31:34.215: INFO: DNS probes using dns-6808/dns-test-6e63a818-0029-4fc8-ad04-f3462a6a6bd0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 30 14:31:34.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6808" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":26,"skipped":528,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:31:34.327: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-0c8331d9-1d25-4492-ad22-e3553918db97
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 30 14:31:34.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5398" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":27,"skipped":555,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:31:34.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
W0830 14:31:34.935221      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods launch on every node of the cluster.
Aug 30 14:31:35.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:35.017: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:36.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:36.097: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:37.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:37.071: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:38.068: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:38.068: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:39.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:39.078: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:40.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:40.072: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:41.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:41.075: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:42.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:42.115: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:43.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:43.116: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:44.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:44.077: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:45.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:45.073: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:46.062: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:46.062: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:47.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:47.087: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:48.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 14:31:48.071: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:31:49.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 14:31:49.065: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Aug 30 14:31:49.110: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Aug 30 14:31:49.163: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Aug 30 14:31:49.175: INFO: Observed &DaemonSet event: ADDED
Aug 30 14:31:49.176: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.177: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.178: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.179: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.181: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.182: INFO: Found daemon set daemon-set in namespace daemonsets-9874 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 30 14:31:49.182: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Aug 30 14:31:49.223: INFO: Observed &DaemonSet event: ADDED
Aug 30 14:31:49.223: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.224: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.224: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.225: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.225: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.225: INFO: Observed daemon set daemon-set in namespace daemonsets-9874 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 30 14:31:49.226: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 14:31:49.226: INFO: Found daemon set daemon-set in namespace daemonsets-9874 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 30 14:31:49.226: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9874, will wait for the garbage collector to delete the pods
Aug 30 14:31:49.375: INFO: Deleting DaemonSet.extensions daemon-set took: 59.545971ms
Aug 30 14:31:49.475: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.211666ms
Aug 30 14:31:52.696: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:31:52.697: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 14:31:52.713: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"72578"},"items":null}

Aug 30 14:31:52.731: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"72578"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 30 14:31:52.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9874" for this suite.

• [SLOW TEST:18.307 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":28,"skipped":562,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:31:52.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Aug 30 14:31:53.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3668 cluster-info'
Aug 30 14:31:53.138: INFO: stderr: ""
Aug 30 14:31:53.138: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 14:31:53.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3668" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":29,"skipped":573,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:31:53.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 30 14:31:53.430: INFO: Waiting up to 5m0s for pod "pod-b8f84c60-51f4-4569-8af7-d09fbdf9b399" in namespace "emptydir-539" to be "Succeeded or Failed"
W0830 14:31:53.430713      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:31:53.446: INFO: Pod "pod-b8f84c60-51f4-4569-8af7-d09fbdf9b399": Phase="Pending", Reason="", readiness=false. Elapsed: 15.033797ms
Aug 30 14:31:55.466: INFO: Pod "pod-b8f84c60-51f4-4569-8af7-d09fbdf9b399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035772134s
Aug 30 14:31:57.494: INFO: Pod "pod-b8f84c60-51f4-4569-8af7-d09fbdf9b399": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063579547s
Aug 30 14:31:59.516: INFO: Pod "pod-b8f84c60-51f4-4569-8af7-d09fbdf9b399": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.085578949s
STEP: Saw pod success
Aug 30 14:31:59.516: INFO: Pod "pod-b8f84c60-51f4-4569-8af7-d09fbdf9b399" satisfied condition "Succeeded or Failed"
Aug 30 14:31:59.533: INFO: Trying to get logs from node 10.63.224.189 pod pod-b8f84c60-51f4-4569-8af7-d09fbdf9b399 container test-container: <nil>
STEP: delete the pod
Aug 30 14:31:59.646: INFO: Waiting for pod pod-b8f84c60-51f4-4569-8af7-d09fbdf9b399 to disappear
Aug 30 14:31:59.663: INFO: Pod pod-b8f84c60-51f4-4569-8af7-d09fbdf9b399 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 14:31:59.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-539" for this suite.

• [SLOW TEST:6.528 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":30,"skipped":600,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:31:59.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-44e329d0-84a3-4c4b-9f6e-1f3cb85a8f3d
STEP: Creating a pod to test consume secrets
Aug 30 14:31:59.984: INFO: Waiting up to 5m0s for pod "pod-secrets-f3df1a47-e6b5-498b-b8e9-5d86922ae0e4" in namespace "secrets-7543" to be "Succeeded or Failed"
W0830 14:31:59.984088      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:32:00.016: INFO: Pod "pod-secrets-f3df1a47-e6b5-498b-b8e9-5d86922ae0e4": Phase="Pending", Reason="", readiness=false. Elapsed: 32.451039ms
Aug 30 14:32:02.036: INFO: Pod "pod-secrets-f3df1a47-e6b5-498b-b8e9-5d86922ae0e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051795108s
Aug 30 14:32:04.062: INFO: Pod "pod-secrets-f3df1a47-e6b5-498b-b8e9-5d86922ae0e4": Phase="Running", Reason="", readiness=false. Elapsed: 4.07794823s
Aug 30 14:32:06.080: INFO: Pod "pod-secrets-f3df1a47-e6b5-498b-b8e9-5d86922ae0e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.095960595s
STEP: Saw pod success
Aug 30 14:32:06.080: INFO: Pod "pod-secrets-f3df1a47-e6b5-498b-b8e9-5d86922ae0e4" satisfied condition "Succeeded or Failed"
Aug 30 14:32:06.096: INFO: Trying to get logs from node 10.63.224.189 pod pod-secrets-f3df1a47-e6b5-498b-b8e9-5d86922ae0e4 container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 14:32:06.218: INFO: Waiting for pod pod-secrets-f3df1a47-e6b5-498b-b8e9-5d86922ae0e4 to disappear
Aug 30 14:32:06.234: INFO: Pod pod-secrets-f3df1a47-e6b5-498b-b8e9-5d86922ae0e4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 30 14:32:06.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7543" for this suite.

• [SLOW TEST:6.540 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":31,"skipped":610,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:32:06.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
W0830 14:32:06.591031      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:32:06.630: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:32:08.651: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:32:10.656: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:32:12.676: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:32:14.654: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:32:16.651: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:32:18.650: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:32:20.652: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
W0830 14:32:20.756272      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-with-prestop-exec-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-prestop-exec-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-prestop-exec-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-prestop-exec-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:32:20.772: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:32:22.786: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:32:24.794: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Aug 30 14:32:24.844: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 30 14:32:24.862: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 30 14:32:26.863: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 30 14:32:26.916: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 30 14:32:28.863: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 30 14:32:28.891: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 30 14:32:29.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8147" for this suite.

• [SLOW TEST:22.796 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":32,"skipped":614,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:32:29.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
W0830 14:32:36.392790      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 30 14:32:40.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-428" for this suite.

• [SLOW TEST:11.465 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":33,"skipped":614,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:32:40.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6272.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6272.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6272.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6272.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
W0830 14:32:40.860099      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 14:32:44.968: INFO: Unable to read wheezy_udp@dns-test-service-3.dns-6272.svc.cluster.local from pod dns-6272/dns-test-8a0a0846-3d96-43b5-9938-d0805b8807ce: Get "https://172.21.0.1:443/api/v1/namespaces/dns-6272/pods/dns-test-8a0a0846-3d96-43b5-9938-d0805b8807ce/proxy/results/wheezy_udp@dns-test-service-3.dns-6272.svc.cluster.local": stream error: stream ID 2047; INTERNAL_ERROR; received from peer
Aug 30 14:32:44.998: INFO: Lookups using dns-6272/dns-test-8a0a0846-3d96-43b5-9938-d0805b8807ce failed for: [wheezy_udp@dns-test-service-3.dns-6272.svc.cluster.local]

Aug 30 14:32:50.080: INFO: DNS probes using dns-test-8a0a0846-3d96-43b5-9938-d0805b8807ce succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6272.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6272.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6272.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6272.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
W0830 14:32:50.266859      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 14:32:54.393: INFO: DNS probes using dns-test-99a9d12f-9fd7-472d-b5b5-05f68a82ee0c succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6272.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6272.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6272.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6272.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
W0830 14:32:54.589031      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 14:32:58.754: INFO: DNS probes using dns-test-5a4c08f5-fb4a-4970-b9ef-343702c7a759 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 30 14:32:58.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6272" for this suite.

• [SLOW TEST:18.353 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":34,"skipped":625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:32:58.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 14:32:59.668417      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 14:32:59.724: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 14:33:01.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 32, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 32, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 32, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 32, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 14:33:04.924: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 30 14:33:05.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:33:05.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-786" for this suite.
STEP: Destroying namespace "webhook-786-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.497 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":35,"skipped":647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:33:05.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
W0830 14:33:05.657680      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:33:05.658: INFO: Waiting up to 5m0s for pod "var-expansion-7c3d3939-5f8f-4fee-a272-0a5a748b0dd8" in namespace "var-expansion-2937" to be "Succeeded or Failed"
Aug 30 14:33:05.675: INFO: Pod "var-expansion-7c3d3939-5f8f-4fee-a272-0a5a748b0dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.993655ms
Aug 30 14:33:07.696: INFO: Pod "var-expansion-7c3d3939-5f8f-4fee-a272-0a5a748b0dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037854957s
Aug 30 14:33:09.712: INFO: Pod "var-expansion-7c3d3939-5f8f-4fee-a272-0a5a748b0dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054486738s
Aug 30 14:33:11.731: INFO: Pod "var-expansion-7c3d3939-5f8f-4fee-a272-0a5a748b0dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073062428s
STEP: Saw pod success
Aug 30 14:33:11.731: INFO: Pod "var-expansion-7c3d3939-5f8f-4fee-a272-0a5a748b0dd8" satisfied condition "Succeeded or Failed"
Aug 30 14:33:11.775: INFO: Trying to get logs from node 10.63.224.189 pod var-expansion-7c3d3939-5f8f-4fee-a272-0a5a748b0dd8 container dapi-container: <nil>
STEP: delete the pod
Aug 30 14:33:11.911: INFO: Waiting for pod var-expansion-7c3d3939-5f8f-4fee-a272-0a5a748b0dd8 to disappear
Aug 30 14:33:11.926: INFO: Pod var-expansion-7c3d3939-5f8f-4fee-a272-0a5a748b0dd8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 30 14:33:11.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2937" for this suite.

• [SLOW TEST:6.600 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":36,"skipped":700,"failed":0}
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:33:12.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:33:12.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7122
W0830 14:33:12.197353      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 14:33:12.197662      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7122, replica count: 1
I0830 14:33:13.248826      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 14:33:14.250527      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 14:33:15.250683      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 14:33:15.408: INFO: Created: latency-svc-xqmqp
Aug 30 14:33:15.436: INFO: Got endpoints: latency-svc-xqmqp [85.214735ms]
Aug 30 14:33:15.489: INFO: Created: latency-svc-tskc2
Aug 30 14:33:15.502: INFO: Created: latency-svc-2xqcm
Aug 30 14:33:15.502: INFO: Got endpoints: latency-svc-tskc2 [65.28601ms]
Aug 30 14:33:15.523: INFO: Created: latency-svc-f75qn
Aug 30 14:33:15.525: INFO: Got endpoints: latency-svc-2xqcm [88.014593ms]
Aug 30 14:33:15.556: INFO: Created: latency-svc-c8wlr
Aug 30 14:33:15.571: INFO: Created: latency-svc-f4fbt
Aug 30 14:33:15.571: INFO: Got endpoints: latency-svc-c8wlr [133.527517ms]
Aug 30 14:33:15.571: INFO: Got endpoints: latency-svc-f75qn [134.020136ms]
Aug 30 14:33:15.582: INFO: Got endpoints: latency-svc-f4fbt [144.597103ms]
Aug 30 14:33:15.582: INFO: Created: latency-svc-lvnr9
Aug 30 14:33:15.595: INFO: Created: latency-svc-66f6g
Aug 30 14:33:15.604: INFO: Got endpoints: latency-svc-lvnr9 [166.387526ms]
Aug 30 14:33:15.628: INFO: Created: latency-svc-98rzl
Aug 30 14:33:15.631: INFO: Got endpoints: latency-svc-66f6g [194.567512ms]
Aug 30 14:33:15.653: INFO: Got endpoints: latency-svc-98rzl [216.070862ms]
Aug 30 14:33:15.671: INFO: Created: latency-svc-tr2dk
Aug 30 14:33:15.671: INFO: Created: latency-svc-49v8f
Aug 30 14:33:15.677: INFO: Created: latency-svc-h2sdt
Aug 30 14:33:15.704: INFO: Created: latency-svc-m29ss
Aug 30 14:33:15.705: INFO: Got endpoints: latency-svc-tr2dk [268.394359ms]
Aug 30 14:33:15.705: INFO: Got endpoints: latency-svc-49v8f [267.935483ms]
Aug 30 14:33:15.716: INFO: Got endpoints: latency-svc-h2sdt [279.208005ms]
Aug 30 14:33:15.718: INFO: Created: latency-svc-8szcg
Aug 30 14:33:15.731: INFO: Created: latency-svc-4tsqh
Aug 30 14:33:15.750: INFO: Got endpoints: latency-svc-m29ss [312.113716ms]
Aug 30 14:33:15.750: INFO: Created: latency-svc-2d6fh
Aug 30 14:33:15.779: INFO: Got endpoints: latency-svc-8szcg [342.032856ms]
Aug 30 14:33:15.779: INFO: Got endpoints: latency-svc-4tsqh [342.244463ms]
Aug 30 14:33:15.799: INFO: Got endpoints: latency-svc-2d6fh [362.641761ms]
Aug 30 14:33:15.800: INFO: Created: latency-svc-ljjb2
Aug 30 14:33:15.800: INFO: Created: latency-svc-mb66q
Aug 30 14:33:15.804: INFO: Got endpoints: latency-svc-ljjb2 [301.782927ms]
Aug 30 14:33:15.816: INFO: Got endpoints: latency-svc-mb66q [290.282186ms]
Aug 30 14:33:15.817: INFO: Created: latency-svc-t9xzf
Aug 30 14:33:15.838: INFO: Created: latency-svc-bjrcv
Aug 30 14:33:15.843: INFO: Got endpoints: latency-svc-t9xzf [272.068609ms]
Aug 30 14:33:15.854: INFO: Created: latency-svc-48fml
Aug 30 14:33:15.865: INFO: Got endpoints: latency-svc-bjrcv [293.744809ms]
Aug 30 14:33:15.885: INFO: Got endpoints: latency-svc-48fml [302.824782ms]
Aug 30 14:33:15.886: INFO: Created: latency-svc-q2t42
Aug 30 14:33:15.889: INFO: Created: latency-svc-b7ngf
Aug 30 14:33:15.903: INFO: Got endpoints: latency-svc-q2t42 [298.515607ms]
Aug 30 14:33:15.909: INFO: Created: latency-svc-p2mzd
Aug 30 14:33:15.922: INFO: Got endpoints: latency-svc-b7ngf [290.539728ms]
Aug 30 14:33:15.936: INFO: Got endpoints: latency-svc-p2mzd [282.611985ms]
Aug 30 14:33:15.937: INFO: Created: latency-svc-fg87r
Aug 30 14:33:15.945: INFO: Created: latency-svc-7wwlv
Aug 30 14:33:15.950: INFO: Got endpoints: latency-svc-fg87r [245.277292ms]
Aug 30 14:33:15.969: INFO: Got endpoints: latency-svc-7wwlv [263.675876ms]
Aug 30 14:33:16.256: INFO: Created: latency-svc-zvvcm
Aug 30 14:33:16.256: INFO: Created: latency-svc-kr2mm
Aug 30 14:33:16.267: INFO: Created: latency-svc-872jf
Aug 30 14:33:16.267: INFO: Created: latency-svc-vqvgq
Aug 30 14:33:16.267: INFO: Created: latency-svc-q2575
Aug 30 14:33:16.267: INFO: Created: latency-svc-72v2t
Aug 30 14:33:16.269: INFO: Created: latency-svc-dx4jh
Aug 30 14:33:16.270: INFO: Created: latency-svc-mnbfh
Aug 30 14:33:16.270: INFO: Created: latency-svc-4z85r
Aug 30 14:33:16.271: INFO: Created: latency-svc-q2ww5
Aug 30 14:33:16.271: INFO: Created: latency-svc-zrz42
Aug 30 14:33:16.271: INFO: Created: latency-svc-wwf5c
Aug 30 14:33:16.272: INFO: Created: latency-svc-qmf56
Aug 30 14:33:16.273: INFO: Created: latency-svc-6drms
Aug 30 14:33:16.281: INFO: Created: latency-svc-8kcnc
Aug 30 14:33:16.284: INFO: Got endpoints: latency-svc-kr2mm [347.917753ms]
Aug 30 14:33:16.297: INFO: Got endpoints: latency-svc-6drms [480.882465ms]
Aug 30 14:33:16.297: INFO: Got endpoints: latency-svc-zvvcm [375.110871ms]
Aug 30 14:33:16.299: INFO: Got endpoints: latency-svc-872jf [396.168594ms]
Aug 30 14:33:16.316: INFO: Got endpoints: latency-svc-qmf56 [430.809763ms]
Aug 30 14:33:16.316: INFO: Got endpoints: latency-svc-q2575 [365.951157ms]
Aug 30 14:33:16.321: INFO: Got endpoints: latency-svc-vqvgq [521.562673ms]
Aug 30 14:33:16.337: INFO: Got endpoints: latency-svc-72v2t [471.887123ms]
Aug 30 14:33:16.340: INFO: Got endpoints: latency-svc-dx4jh [497.421877ms]
Aug 30 14:33:16.341: INFO: Got endpoints: latency-svc-q2ww5 [624.703279ms]
Aug 30 14:33:16.341: INFO: Got endpoints: latency-svc-8kcnc [372.06801ms]
Aug 30 14:33:16.354: INFO: Created: latency-svc-4s69f
Aug 30 14:33:16.362: INFO: Got endpoints: latency-svc-4z85r [582.016058ms]
Aug 30 14:33:16.377: INFO: Created: latency-svc-kgljk
Aug 30 14:33:16.382: INFO: Got endpoints: latency-svc-4s69f [98.186899ms]
Aug 30 14:33:16.383: INFO: Got endpoints: latency-svc-zrz42 [578.967036ms]
Aug 30 14:33:16.384: INFO: Got endpoints: latency-svc-mnbfh [632.384223ms]
Aug 30 14:33:16.421: INFO: Created: latency-svc-h6fzs
Aug 30 14:33:16.423: INFO: Got endpoints: latency-svc-kgljk [125.252783ms]
Aug 30 14:33:16.426: INFO: Got endpoints: latency-svc-wwf5c [646.959524ms]
Aug 30 14:33:16.427: INFO: Created: latency-svc-hb7z4
Aug 30 14:33:16.437: INFO: Got endpoints: latency-svc-h6fzs [139.873399ms]
Aug 30 14:33:16.447: INFO: Got endpoints: latency-svc-hb7z4 [148.152489ms]
Aug 30 14:33:16.453: INFO: Created: latency-svc-jpn59
Aug 30 14:33:16.473: INFO: Created: latency-svc-m8fkg
Aug 30 14:33:16.475: INFO: Got endpoints: latency-svc-jpn59 [159.345596ms]
Aug 30 14:33:16.502: INFO: Got endpoints: latency-svc-m8fkg [180.553052ms]
Aug 30 14:33:16.799: INFO: Created: latency-svc-jzr5v
Aug 30 14:33:16.799: INFO: Created: latency-svc-pcn5q
Aug 30 14:33:16.802: INFO: Created: latency-svc-h8s7d
Aug 30 14:33:16.811: INFO: Created: latency-svc-vc4hc
Aug 30 14:33:16.817: INFO: Created: latency-svc-rs6hf
Aug 30 14:33:16.818: INFO: Created: latency-svc-cplpd
Aug 30 14:33:16.818: INFO: Created: latency-svc-l9lpc
Aug 30 14:33:16.819: INFO: Created: latency-svc-dsv75
Aug 30 14:33:16.819: INFO: Created: latency-svc-jbjxf
Aug 30 14:33:16.819: INFO: Created: latency-svc-c6mgh
Aug 30 14:33:16.819: INFO: Created: latency-svc-ptnsf
Aug 30 14:33:16.820: INFO: Created: latency-svc-qdzdv
Aug 30 14:33:16.820: INFO: Created: latency-svc-qxqbc
Aug 30 14:33:16.822: INFO: Created: latency-svc-wh5ns
Aug 30 14:33:16.830: INFO: Created: latency-svc-xzrvc
Aug 30 14:33:16.847: INFO: Got endpoints: latency-svc-pcn5q [530.737615ms]
Aug 30 14:33:16.862: INFO: Got endpoints: latency-svc-jzr5v [386.829196ms]
Aug 30 14:33:16.866: INFO: Got endpoints: latency-svc-vc4hc [432.086011ms]
Aug 30 14:33:16.866: INFO: Got endpoints: latency-svc-dsv75 [529.086229ms]
Aug 30 14:33:16.866: INFO: Got endpoints: latency-svc-h8s7d [525.210721ms]
Aug 30 14:33:16.880: INFO: Got endpoints: latency-svc-l9lpc [436.477786ms]
Aug 30 14:33:16.897: INFO: Got endpoints: latency-svc-qdzdv [556.5518ms]
Aug 30 14:33:16.898: INFO: Created: latency-svc-ht678
Aug 30 14:33:16.898: INFO: Got endpoints: latency-svc-jbjxf [536.606396ms]
Aug 30 14:33:16.899: INFO: Got endpoints: latency-svc-wh5ns [516.883863ms]
Aug 30 14:33:16.912: INFO: Got endpoints: latency-svc-xzrvc [409.794338ms]
Aug 30 14:33:16.912: INFO: Got endpoints: latency-svc-cplpd [570.569985ms]
Aug 30 14:33:16.917: INFO: Created: latency-svc-8cswr
Aug 30 14:33:16.942: INFO: Got endpoints: latency-svc-rs6hf [557.557744ms]
Aug 30 14:33:16.943: INFO: Got endpoints: latency-svc-ptnsf [559.418084ms]
Aug 30 14:33:16.945: INFO: Got endpoints: latency-svc-qxqbc [498.313719ms]
Aug 30 14:33:16.946: INFO: Got endpoints: latency-svc-c6mgh [512.229602ms]
Aug 30 14:33:16.947: INFO: Got endpoints: latency-svc-ht678 [99.532075ms]
Aug 30 14:33:16.948: INFO: Created: latency-svc-qt789
Aug 30 14:33:16.963: INFO: Got endpoints: latency-svc-8cswr [97.758141ms]
Aug 30 14:33:16.970: INFO: Got endpoints: latency-svc-qt789 [103.981654ms]
Aug 30 14:33:16.971: INFO: Created: latency-svc-5rglc
Aug 30 14:33:16.999: INFO: Got endpoints: latency-svc-5rglc [137.137494ms]
Aug 30 14:33:17.026: INFO: Created: latency-svc-2c8l9
Aug 30 14:33:17.026: INFO: Created: latency-svc-7wpwr
Aug 30 14:33:17.039: INFO: Created: latency-svc-n7qgl
Aug 30 14:33:17.043: INFO: Got endpoints: latency-svc-7wpwr [79.006832ms]
Aug 30 14:33:17.060: INFO: Got endpoints: latency-svc-2c8l9 [193.662335ms]
Aug 30 14:33:17.065: INFO: Created: latency-svc-2rb4r
Aug 30 14:33:17.088: INFO: Got endpoints: latency-svc-n7qgl [190.999808ms]
Aug 30 14:33:17.092: INFO: Created: latency-svc-t2m46
Aug 30 14:33:17.096: INFO: Got endpoints: latency-svc-2rb4r [197.275016ms]
Aug 30 14:33:17.105: INFO: Got endpoints: latency-svc-t2m46 [206.965741ms]
Aug 30 14:33:17.368: INFO: Created: latency-svc-kh9lp
Aug 30 14:33:17.371: INFO: Created: latency-svc-2xthg
Aug 30 14:33:17.371: INFO: Created: latency-svc-lbkm6
Aug 30 14:33:17.391: INFO: Created: latency-svc-26rqh
Aug 30 14:33:17.371: INFO: Created: latency-svc-fbnrx
Aug 30 14:33:17.379: INFO: Created: latency-svc-cnhsg
Aug 30 14:33:17.380: INFO: Created: latency-svc-qfpz4
Aug 30 14:33:17.380: INFO: Created: latency-svc-vm49k
Aug 30 14:33:17.380: INFO: Created: latency-svc-nhvfh
Aug 30 14:33:17.380: INFO: Created: latency-svc-p8bmn
Aug 30 14:33:17.381: INFO: Created: latency-svc-p8ldd
Aug 30 14:33:17.381: INFO: Created: latency-svc-qq25l
Aug 30 14:33:17.382: INFO: Created: latency-svc-zw5wf
Aug 30 14:33:17.382: INFO: Created: latency-svc-wn82l
Aug 30 14:33:17.382: INFO: Created: latency-svc-7qnm7
Aug 30 14:33:17.396: INFO: Got endpoints: latency-svc-lbkm6 [290.583782ms]
Aug 30 14:33:17.427: INFO: Got endpoints: latency-svc-kh9lp [366.697397ms]
Aug 30 14:33:17.429: INFO: Got endpoints: latency-svc-2xthg [516.515732ms]
Aug 30 14:33:17.430: INFO: Got endpoints: latency-svc-26rqh [486.813962ms]
Aug 30 14:33:17.431: INFO: Got endpoints: latency-svc-fbnrx [519.129533ms]
Aug 30 14:33:17.441: INFO: Got endpoints: latency-svc-qfpz4 [353.117871ms]
Aug 30 14:33:17.443: INFO: Created: latency-svc-v947h
Aug 30 14:33:17.461: INFO: Got endpoints: latency-svc-cnhsg [518.945711ms]
Aug 30 14:33:17.470: INFO: Created: latency-svc-5rdvg
Aug 30 14:33:17.486: INFO: Got endpoints: latency-svc-p8ldd [539.465997ms]
Aug 30 14:33:17.486: INFO: Got endpoints: latency-svc-qq25l [540.288971ms]
Aug 30 14:33:17.490: INFO: Created: latency-svc-h9lrt
Aug 30 14:33:17.490: INFO: Got endpoints: latency-svc-zw5wf [609.920129ms]
Aug 30 14:33:17.490: INFO: Got endpoints: latency-svc-p8bmn [394.079948ms]
Aug 30 14:33:17.506: INFO: Got endpoints: latency-svc-nhvfh [535.251751ms]
Aug 30 14:33:17.519: INFO: Created: latency-svc-b67pn
Aug 30 14:33:17.520: INFO: Got endpoints: latency-svc-wn82l [520.7033ms]
Aug 30 14:33:17.521: INFO: Got endpoints: latency-svc-vm49k [573.733965ms]
Aug 30 14:33:17.521: INFO: Got endpoints: latency-svc-7qnm7 [477.146441ms]
Aug 30 14:33:17.521: INFO: Got endpoints: latency-svc-v947h [124.854326ms]
Aug 30 14:33:17.538: INFO: Created: latency-svc-w6797
Aug 30 14:33:17.541: INFO: Got endpoints: latency-svc-5rdvg [114.037565ms]
Aug 30 14:33:17.558: INFO: Got endpoints: latency-svc-b67pn [128.09479ms]
Aug 30 14:33:17.564: INFO: Got endpoints: latency-svc-h9lrt [135.215814ms]
Aug 30 14:33:17.566: INFO: Got endpoints: latency-svc-w6797 [135.027664ms]
Aug 30 14:33:17.568: INFO: Created: latency-svc-5rjrv
Aug 30 14:33:17.590: INFO: Got endpoints: latency-svc-5rjrv [148.225689ms]
Aug 30 14:33:17.598: INFO: Created: latency-svc-7p6tm
Aug 30 14:33:17.610: INFO: Created: latency-svc-2jp7s
Aug 30 14:33:17.620: INFO: Got endpoints: latency-svc-7p6tm [159.025298ms]
Aug 30 14:33:17.633: INFO: Created: latency-svc-nmw25
Aug 30 14:33:17.652: INFO: Got endpoints: latency-svc-2jp7s [165.889407ms]
Aug 30 14:33:17.657: INFO: Got endpoints: latency-svc-nmw25 [170.737256ms]
Aug 30 14:33:17.675: INFO: Created: latency-svc-5kpr8
Aug 30 14:33:17.711: INFO: Created: latency-svc-wncjl
Aug 30 14:33:17.723: INFO: Created: latency-svc-l58sz
Aug 30 14:33:17.736: INFO: Got endpoints: latency-svc-5kpr8 [245.916557ms]
Aug 30 14:33:17.736: INFO: Got endpoints: latency-svc-wncjl [245.499513ms]
Aug 30 14:33:17.743: INFO: Got endpoints: latency-svc-l58sz [236.507861ms]
Aug 30 14:33:17.761: INFO: Created: latency-svc-9rfzh
Aug 30 14:33:17.773: INFO: Created: latency-svc-xhdzh
Aug 30 14:33:17.783: INFO: Got endpoints: latency-svc-9rfzh [262.309214ms]
Aug 30 14:33:17.798: INFO: Created: latency-svc-w6thz
Aug 30 14:33:17.803: INFO: Got endpoints: latency-svc-xhdzh [282.388087ms]
Aug 30 14:33:17.812: INFO: Created: latency-svc-nf46c
Aug 30 14:33:17.815: INFO: Got endpoints: latency-svc-w6thz [294.487748ms]
Aug 30 14:33:17.831: INFO: Created: latency-svc-vfpzz
Aug 30 14:33:17.831: INFO: Got endpoints: latency-svc-nf46c [310.303453ms]
Aug 30 14:33:17.849: INFO: Got endpoints: latency-svc-vfpzz [308.211553ms]
Aug 30 14:33:18.116: INFO: Created: latency-svc-6mfwk
Aug 30 14:33:18.117: INFO: Created: latency-svc-2hfsm
Aug 30 14:33:18.138: INFO: Created: latency-svc-fg8km
Aug 30 14:33:18.138: INFO: Created: latency-svc-sdpbn
Aug 30 14:33:18.139: INFO: Created: latency-svc-pz6f7
Aug 30 14:33:18.140: INFO: Created: latency-svc-67g7s
Aug 30 14:33:18.141: INFO: Created: latency-svc-dp9c6
Aug 30 14:33:18.141: INFO: Created: latency-svc-26qbj
Aug 30 14:33:18.143: INFO: Created: latency-svc-s6t29
Aug 30 14:33:18.143: INFO: Created: latency-svc-8s5w6
Aug 30 14:33:18.144: INFO: Created: latency-svc-ncl8k
Aug 30 14:33:18.144: INFO: Created: latency-svc-zb9bl
Aug 30 14:33:18.144: INFO: Created: latency-svc-vkl6g
Aug 30 14:33:18.146: INFO: Created: latency-svc-kl67q
Aug 30 14:33:18.148: INFO: Got endpoints: latency-svc-6mfwk [332.35173ms]
Aug 30 14:33:18.148: INFO: Created: latency-svc-9hxs8
Aug 30 14:33:18.148: INFO: Got endpoints: latency-svc-2hfsm [344.859657ms]
Aug 30 14:33:18.149: INFO: Got endpoints: latency-svc-sdpbn [528.904106ms]
Aug 30 14:33:18.160: INFO: Got endpoints: latency-svc-9hxs8 [310.813235ms]
Aug 30 14:33:18.161: INFO: Got endpoints: latency-svc-fg8km [328.963482ms]
Aug 30 14:33:18.176: INFO: Got endpoints: latency-svc-dp9c6 [585.455047ms]
Aug 30 14:33:18.191: INFO: Created: latency-svc-rdz9r
Aug 30 14:33:18.192: INFO: Got endpoints: latency-svc-26qbj [627.930964ms]
Aug 30 14:33:18.192: INFO: Got endpoints: latency-svc-kl67q [535.638341ms]
Aug 30 14:33:18.193: INFO: Got endpoints: latency-svc-67g7s [541.315349ms]
Aug 30 14:33:18.193: INFO: Got endpoints: latency-svc-vkl6g [627.148529ms]
Aug 30 14:33:18.217: INFO: Got endpoints: latency-svc-zb9bl [480.705883ms]
Aug 30 14:33:18.221: INFO: Created: latency-svc-xf9cn
Aug 30 14:33:18.229: INFO: Got endpoints: latency-svc-ncl8k [671.201374ms]
Aug 30 14:33:18.230: INFO: Got endpoints: latency-svc-8s5w6 [446.94823ms]
Aug 30 14:33:18.230: INFO: Got endpoints: latency-svc-pz6f7 [487.168383ms]
Aug 30 14:33:18.230: INFO: Got endpoints: latency-svc-s6t29 [493.994771ms]
Aug 30 14:33:18.247: INFO: Got endpoints: latency-svc-rdz9r [99.700026ms]
Aug 30 14:33:18.247: INFO: Created: latency-svc-lzh6n
Aug 30 14:33:18.257: INFO: Got endpoints: latency-svc-xf9cn [109.033583ms]
Aug 30 14:33:18.262: INFO: Created: latency-svc-dfbxk
Aug 30 14:33:18.269: INFO: Got endpoints: latency-svc-lzh6n [120.161149ms]
Aug 30 14:33:18.289: INFO: Got endpoints: latency-svc-dfbxk [128.118861ms]
Aug 30 14:33:18.291: INFO: Created: latency-svc-ghzdt
Aug 30 14:33:18.313: INFO: Created: latency-svc-zzdwb
Aug 30 14:33:18.328: INFO: Got endpoints: latency-svc-ghzdt [167.878629ms]
Aug 30 14:33:18.344: INFO: Got endpoints: latency-svc-zzdwb [167.847414ms]
Aug 30 14:33:18.348: INFO: Created: latency-svc-bkjl8
Aug 30 14:33:18.370: INFO: Created: latency-svc-nbl8z
Aug 30 14:33:18.381: INFO: Got endpoints: latency-svc-bkjl8 [188.124756ms]
Aug 30 14:33:18.386: INFO: Created: latency-svc-m6pvz
Aug 30 14:33:18.397: INFO: Got endpoints: latency-svc-nbl8z [203.616607ms]
Aug 30 14:33:18.403: INFO: Got endpoints: latency-svc-m6pvz [211.358156ms]
Aug 30 14:33:18.409: INFO: Created: latency-svc-hstn9
Aug 30 14:33:18.427: INFO: Created: latency-svc-smtzm
Aug 30 14:33:18.435: INFO: Got endpoints: latency-svc-hstn9 [241.576613ms]
Aug 30 14:33:18.446: INFO: Created: latency-svc-kr6jq
Aug 30 14:33:18.446: INFO: Got endpoints: latency-svc-smtzm [228.898408ms]
Aug 30 14:33:18.469: INFO: Got endpoints: latency-svc-kr6jq [239.708446ms]
Aug 30 14:33:18.469: INFO: Created: latency-svc-dwtjx
Aug 30 14:33:18.480: INFO: Created: latency-svc-bdrmw
Aug 30 14:33:18.491: INFO: Got endpoints: latency-svc-dwtjx [260.686523ms]
Aug 30 14:33:18.506: INFO: Got endpoints: latency-svc-bdrmw [276.274713ms]
Aug 30 14:33:18.508: INFO: Created: latency-svc-k4dtk
Aug 30 14:33:18.530: INFO: Created: latency-svc-82dx5
Aug 30 14:33:18.535: INFO: Got endpoints: latency-svc-k4dtk [305.15011ms]
Aug 30 14:33:18.544: INFO: Got endpoints: latency-svc-82dx5 [295.684464ms]
Aug 30 14:33:18.557: INFO: Created: latency-svc-svpqk
Aug 30 14:33:18.576: INFO: Created: latency-svc-fb59g
Aug 30 14:33:18.581: INFO: Got endpoints: latency-svc-svpqk [323.46347ms]
Aug 30 14:33:18.594: INFO: Created: latency-svc-hsg4h
Aug 30 14:33:18.600: INFO: Got endpoints: latency-svc-fb59g [330.236461ms]
Aug 30 14:33:18.625: INFO: Created: latency-svc-j9jn8
Aug 30 14:33:18.627: INFO: Got endpoints: latency-svc-hsg4h [338.373132ms]
Aug 30 14:33:18.632: INFO: Created: latency-svc-hfm4h
Aug 30 14:33:18.645: INFO: Got endpoints: latency-svc-j9jn8 [316.571496ms]
Aug 30 14:33:18.659: INFO: Got endpoints: latency-svc-hfm4h [313.631111ms]
Aug 30 14:33:18.660: INFO: Created: latency-svc-qn997
Aug 30 14:33:18.668: INFO: Created: latency-svc-prqpg
Aug 30 14:33:18.675: INFO: Got endpoints: latency-svc-qn997 [293.935422ms]
Aug 30 14:33:18.691: INFO: Created: latency-svc-n4wvv
Aug 30 14:33:18.691: INFO: Got endpoints: latency-svc-prqpg [293.855103ms]
Aug 30 14:33:18.708: INFO: Got endpoints: latency-svc-n4wvv [303.229265ms]
Aug 30 14:33:18.709: INFO: Created: latency-svc-9f57r
Aug 30 14:33:18.726: INFO: Created: latency-svc-w49wb
Aug 30 14:33:18.732: INFO: Got endpoints: latency-svc-9f57r [297.177591ms]
Aug 30 14:33:18.751: INFO: Created: latency-svc-tl4j7
Aug 30 14:33:18.755: INFO: Got endpoints: latency-svc-w49wb [308.128964ms]
Aug 30 14:33:18.767: INFO: Created: latency-svc-gmlmz
Aug 30 14:33:18.782: INFO: Got endpoints: latency-svc-tl4j7 [313.019263ms]
Aug 30 14:33:18.785: INFO: Created: latency-svc-rn8zh
Aug 30 14:33:18.788: INFO: Got endpoints: latency-svc-gmlmz [297.101064ms]
Aug 30 14:33:18.812: INFO: Got endpoints: latency-svc-rn8zh [305.640785ms]
Aug 30 14:33:18.831: INFO: Created: latency-svc-vd8kg
Aug 30 14:33:18.866: INFO: Got endpoints: latency-svc-vd8kg [330.711135ms]
Aug 30 14:33:18.866: INFO: Created: latency-svc-zv975
Aug 30 14:33:18.876: INFO: Created: latency-svc-q6stx
Aug 30 14:33:18.878: INFO: Got endpoints: latency-svc-zv975 [333.595061ms]
Aug 30 14:33:18.899: INFO: Got endpoints: latency-svc-q6stx [318.885617ms]
Aug 30 14:33:18.907: INFO: Created: latency-svc-pkhj6
Aug 30 14:33:18.929: INFO: Created: latency-svc-9pg4z
Aug 30 14:33:18.935: INFO: Got endpoints: latency-svc-pkhj6 [335.076625ms]
Aug 30 14:33:18.944: INFO: Created: latency-svc-q68pq
Aug 30 14:33:18.947: INFO: Got endpoints: latency-svc-9pg4z [319.940541ms]
Aug 30 14:33:18.964: INFO: Created: latency-svc-plcmg
Aug 30 14:33:18.973: INFO: Got endpoints: latency-svc-q68pq [328.611092ms]
Aug 30 14:33:18.985: INFO: Created: latency-svc-55j8v
Aug 30 14:33:18.991: INFO: Got endpoints: latency-svc-plcmg [331.206882ms]
Aug 30 14:33:19.006: INFO: Got endpoints: latency-svc-55j8v [330.716117ms]
Aug 30 14:33:19.015: INFO: Created: latency-svc-b6dq6
Aug 30 14:33:19.024: INFO: Created: latency-svc-n89nr
Aug 30 14:33:19.035: INFO: Got endpoints: latency-svc-b6dq6 [343.929619ms]
Aug 30 14:33:19.047: INFO: Created: latency-svc-lv4vz
Aug 30 14:33:19.063: INFO: Got endpoints: latency-svc-n89nr [355.055382ms]
Aug 30 14:33:19.066: INFO: Created: latency-svc-s588j
Aug 30 14:33:19.078: INFO: Got endpoints: latency-svc-lv4vz [345.269118ms]
Aug 30 14:33:19.083: INFO: Got endpoints: latency-svc-s588j [327.46282ms]
Aug 30 14:33:19.090: INFO: Created: latency-svc-fj2bn
Aug 30 14:33:19.108: INFO: Created: latency-svc-v84sl
Aug 30 14:33:19.112: INFO: Got endpoints: latency-svc-fj2bn [323.964903ms]
Aug 30 14:33:19.134: INFO: Created: latency-svc-gh7p2
Aug 30 14:33:19.134: INFO: Got endpoints: latency-svc-v84sl [352.074464ms]
Aug 30 14:33:19.152: INFO: Got endpoints: latency-svc-gh7p2 [330.201131ms]
Aug 30 14:33:19.161: INFO: Created: latency-svc-8w9gx
Aug 30 14:33:19.172: INFO: Created: latency-svc-trs8p
Aug 30 14:33:19.183: INFO: Got endpoints: latency-svc-8w9gx [316.483408ms]
Aug 30 14:33:19.194: INFO: Got endpoints: latency-svc-trs8p [316.57013ms]
Aug 30 14:33:19.194: INFO: Created: latency-svc-hkbj5
Aug 30 14:33:19.211: INFO: Got endpoints: latency-svc-hkbj5 [311.118188ms]
Aug 30 14:33:19.217: INFO: Created: latency-svc-cjbhh
Aug 30 14:33:19.239: INFO: Created: latency-svc-tk47x
Aug 30 14:33:19.243: INFO: Got endpoints: latency-svc-cjbhh [307.9543ms]
Aug 30 14:33:19.260: INFO: Got endpoints: latency-svc-tk47x [312.514901ms]
Aug 30 14:33:19.272: INFO: Created: latency-svc-k4mzr
Aug 30 14:33:19.284: INFO: Created: latency-svc-tj4gq
Aug 30 14:33:19.298: INFO: Got endpoints: latency-svc-k4mzr [324.272089ms]
Aug 30 14:33:19.325: INFO: Created: latency-svc-qp84x
Aug 30 14:33:19.325: INFO: Got endpoints: latency-svc-tj4gq [334.185312ms]
Aug 30 14:33:19.336: INFO: Created: latency-svc-jpjr2
Aug 30 14:33:19.346: INFO: Got endpoints: latency-svc-qp84x [340.455196ms]
Aug 30 14:33:19.365: INFO: Created: latency-svc-6f2q4
Aug 30 14:33:19.368: INFO: Got endpoints: latency-svc-jpjr2 [332.706841ms]
Aug 30 14:33:19.401: INFO: Got endpoints: latency-svc-6f2q4 [338.305049ms]
Aug 30 14:33:19.689: INFO: Created: latency-svc-8bf75
Aug 30 14:33:19.689: INFO: Created: latency-svc-p7wfn
Aug 30 14:33:19.689: INFO: Created: latency-svc-4kjh6
Aug 30 14:33:19.689: INFO: Created: latency-svc-f28cq
Aug 30 14:33:19.689: INFO: Created: latency-svc-lfg66
Aug 30 14:33:19.689: INFO: Created: latency-svc-44vch
Aug 30 14:33:19.690: INFO: Created: latency-svc-bgkqb
Aug 30 14:33:19.690: INFO: Created: latency-svc-jm6jq
Aug 30 14:33:19.690: INFO: Created: latency-svc-lf9t4
Aug 30 14:33:19.690: INFO: Created: latency-svc-wd8xn
Aug 30 14:33:19.690: INFO: Created: latency-svc-wbdzz
Aug 30 14:33:19.690: INFO: Created: latency-svc-bjdq5
Aug 30 14:33:19.690: INFO: Created: latency-svc-6fpnd
Aug 30 14:33:19.690: INFO: Created: latency-svc-fdm6h
Aug 30 14:33:19.690: INFO: Created: latency-svc-jfp8t
Aug 30 14:33:19.703: INFO: Got endpoints: latency-svc-8bf75 [459.618225ms]
Aug 30 14:33:19.706: INFO: Got endpoints: latency-svc-4kjh6 [446.554144ms]
Aug 30 14:33:19.707: INFO: Got endpoints: latency-svc-f28cq [495.909485ms]
Aug 30 14:33:19.714: INFO: Got endpoints: latency-svc-44vch [415.904829ms]
Aug 30 14:33:19.715: INFO: Got endpoints: latency-svc-p7wfn [313.047945ms]
Aug 30 14:33:19.729: INFO: Got endpoints: latency-svc-wd8xn [546.003768ms]
Aug 30 14:33:19.729: INFO: Got endpoints: latency-svc-jm6jq [577.142953ms]
Aug 30 14:33:19.771: INFO: Got endpoints: latency-svc-lf9t4 [635.756635ms]
Aug 30 14:33:19.776: INFO: Created: latency-svc-nmcxb
Aug 30 14:33:19.778: INFO: Got endpoints: latency-svc-lfg66 [430.915448ms]
Aug 30 14:33:19.778: INFO: Got endpoints: latency-svc-fdm6h [452.309655ms]
Aug 30 14:33:19.778: INFO: Got endpoints: latency-svc-bjdq5 [410.133611ms]
Aug 30 14:33:19.778: INFO: Got endpoints: latency-svc-bgkqb [695.491912ms]
Aug 30 14:33:19.821: INFO: Got endpoints: latency-svc-jfp8t [708.86971ms]
Aug 30 14:33:19.822: INFO: Created: latency-svc-hgwpl
Aug 30 14:33:19.826: INFO: Got endpoints: latency-svc-wbdzz [631.674433ms]
Aug 30 14:33:19.826: INFO: Got endpoints: latency-svc-6fpnd [747.603776ms]
Aug 30 14:33:19.826: INFO: Got endpoints: latency-svc-nmcxb [123.304094ms]
Aug 30 14:33:19.839: INFO: Created: latency-svc-trpx6
Aug 30 14:33:19.849: INFO: Created: latency-svc-pd756
Aug 30 14:33:19.867: INFO: Created: latency-svc-xtfbh
Aug 30 14:33:19.868: INFO: Got endpoints: latency-svc-trpx6 [152.986493ms]
Aug 30 14:33:19.868: INFO: Got endpoints: latency-svc-hgwpl [161.162719ms]
Aug 30 14:33:19.877: INFO: Got endpoints: latency-svc-pd756 [162.453785ms]
Aug 30 14:33:19.887: INFO: Got endpoints: latency-svc-xtfbh [180.56082ms]
Aug 30 14:33:19.901: INFO: Created: latency-svc-54rr5
Aug 30 14:33:19.923: INFO: Created: latency-svc-zvqp2
Aug 30 14:33:19.929: INFO: Got endpoints: latency-svc-54rr5 [199.486847ms]
Aug 30 14:33:19.946: INFO: Created: latency-svc-lmvls
Aug 30 14:33:19.950: INFO: Got endpoints: latency-svc-zvqp2 [221.109946ms]
Aug 30 14:33:19.958: INFO: Created: latency-svc-qhnx8
Aug 30 14:33:19.966: INFO: Got endpoints: latency-svc-lmvls [194.158176ms]
Aug 30 14:33:19.977: INFO: Created: latency-svc-2kkp2
Aug 30 14:33:19.982: INFO: Got endpoints: latency-svc-qhnx8 [204.196265ms]
Aug 30 14:33:19.999: INFO: Created: latency-svc-sfgts
Aug 30 14:33:20.002: INFO: Got endpoints: latency-svc-2kkp2 [223.802449ms]
Aug 30 14:33:20.027: INFO: Got endpoints: latency-svc-sfgts [248.809966ms]
Aug 30 14:33:20.027: INFO: Created: latency-svc-ffwvv
Aug 30 14:33:20.039: INFO: Got endpoints: latency-svc-ffwvv [260.644945ms]
Aug 30 14:33:20.046: INFO: Created: latency-svc-dsz9f
Aug 30 14:33:20.059: INFO: Got endpoints: latency-svc-dsz9f [237.664906ms]
Aug 30 14:33:20.071: INFO: Created: latency-svc-fjvwn
Aug 30 14:33:20.100: INFO: Got endpoints: latency-svc-fjvwn [273.647899ms]
Aug 30 14:33:20.100: INFO: Latencies: [65.28601ms 79.006832ms 88.014593ms 97.758141ms 98.186899ms 99.532075ms 99.700026ms 103.981654ms 109.033583ms 114.037565ms 120.161149ms 123.304094ms 124.854326ms 125.252783ms 128.09479ms 128.118861ms 133.527517ms 134.020136ms 135.027664ms 135.215814ms 137.137494ms 139.873399ms 144.597103ms 148.152489ms 148.225689ms 152.986493ms 159.025298ms 159.345596ms 161.162719ms 162.453785ms 165.889407ms 166.387526ms 167.847414ms 167.878629ms 170.737256ms 180.553052ms 180.56082ms 188.124756ms 190.999808ms 193.662335ms 194.158176ms 194.567512ms 197.275016ms 199.486847ms 203.616607ms 204.196265ms 206.965741ms 211.358156ms 216.070862ms 221.109946ms 223.802449ms 228.898408ms 236.507861ms 237.664906ms 239.708446ms 241.576613ms 245.277292ms 245.499513ms 245.916557ms 248.809966ms 260.644945ms 260.686523ms 262.309214ms 263.675876ms 267.935483ms 268.394359ms 272.068609ms 273.647899ms 276.274713ms 279.208005ms 282.388087ms 282.611985ms 290.282186ms 290.539728ms 290.583782ms 293.744809ms 293.855103ms 293.935422ms 294.487748ms 295.684464ms 297.101064ms 297.177591ms 298.515607ms 301.782927ms 302.824782ms 303.229265ms 305.15011ms 305.640785ms 307.9543ms 308.128964ms 308.211553ms 310.303453ms 310.813235ms 311.118188ms 312.113716ms 312.514901ms 313.019263ms 313.047945ms 313.631111ms 316.483408ms 316.57013ms 316.571496ms 318.885617ms 319.940541ms 323.46347ms 323.964903ms 324.272089ms 327.46282ms 328.611092ms 328.963482ms 330.201131ms 330.236461ms 330.711135ms 330.716117ms 331.206882ms 332.35173ms 332.706841ms 333.595061ms 334.185312ms 335.076625ms 338.305049ms 338.373132ms 340.455196ms 342.032856ms 342.244463ms 343.929619ms 344.859657ms 345.269118ms 347.917753ms 352.074464ms 353.117871ms 355.055382ms 362.641761ms 365.951157ms 366.697397ms 372.06801ms 375.110871ms 386.829196ms 394.079948ms 396.168594ms 409.794338ms 410.133611ms 415.904829ms 430.809763ms 430.915448ms 432.086011ms 436.477786ms 446.554144ms 446.94823ms 452.309655ms 459.618225ms 471.887123ms 477.146441ms 480.705883ms 480.882465ms 486.813962ms 487.168383ms 493.994771ms 495.909485ms 497.421877ms 498.313719ms 512.229602ms 516.515732ms 516.883863ms 518.945711ms 519.129533ms 520.7033ms 521.562673ms 525.210721ms 528.904106ms 529.086229ms 530.737615ms 535.251751ms 535.638341ms 536.606396ms 539.465997ms 540.288971ms 541.315349ms 546.003768ms 556.5518ms 557.557744ms 559.418084ms 570.569985ms 573.733965ms 577.142953ms 578.967036ms 582.016058ms 585.455047ms 609.920129ms 624.703279ms 627.148529ms 627.930964ms 631.674433ms 632.384223ms 635.756635ms 646.959524ms 671.201374ms 695.491912ms 708.86971ms 747.603776ms]
Aug 30 14:33:20.100: INFO: 50 %ile: 316.57013ms
Aug 30 14:33:20.100: INFO: 90 %ile: 557.557744ms
Aug 30 14:33:20.100: INFO: 99 %ile: 708.86971ms
Aug 30 14:33:20.100: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Aug 30 14:33:20.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7122" for this suite.

• [SLOW TEST:8.158 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":37,"skipped":704,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:33:20.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6686
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6686
STEP: creating replication controller externalsvc in namespace services-6686
W0830 14:33:20.513107      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "externalsvc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalsvc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalsvc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalsvc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 14:33:20.513274      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6686, replica count: 2
I0830 14:33:23.580584      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 30 14:33:23.700: INFO: Creating new exec pod
W0830 14:33:23.754739      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:33:27.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-6686 exec execpodrb7pp -- /bin/sh -x -c nslookup clusterip-service.services-6686.svc.cluster.local'
Aug 30 14:33:28.295: INFO: stderr: "+ nslookup clusterip-service.services-6686.svc.cluster.local\n"
Aug 30 14:33:28.295: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-6686.svc.cluster.local\tcanonical name = externalsvc.services-6686.svc.cluster.local.\nName:\texternalsvc.services-6686.svc.cluster.local\nAddress: 172.21.223.48\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6686, will wait for the garbage collector to delete the pods
Aug 30 14:33:28.400: INFO: Deleting ReplicationController externalsvc took: 31.830346ms
Aug 30 14:33:28.501: INFO: Terminating ReplicationController externalsvc pods took: 100.515482ms
Aug 30 14:33:31.591: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 14:33:31.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6686" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:11.545 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":38,"skipped":812,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:33:31.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 14:33:32.043622      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:33:32.043: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce75f4bd-a60b-444d-8488-aa13eaa95cfd" in namespace "projected-2017" to be "Succeeded or Failed"
Aug 30 14:33:32.080: INFO: Pod "downwardapi-volume-ce75f4bd-a60b-444d-8488-aa13eaa95cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.59662ms
Aug 30 14:33:34.101: INFO: Pod "downwardapi-volume-ce75f4bd-a60b-444d-8488-aa13eaa95cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058075574s
Aug 30 14:33:36.125: INFO: Pod "downwardapi-volume-ce75f4bd-a60b-444d-8488-aa13eaa95cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081799393s
Aug 30 14:33:38.149: INFO: Pod "downwardapi-volume-ce75f4bd-a60b-444d-8488-aa13eaa95cfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.105548995s
STEP: Saw pod success
Aug 30 14:33:38.149: INFO: Pod "downwardapi-volume-ce75f4bd-a60b-444d-8488-aa13eaa95cfd" satisfied condition "Succeeded or Failed"
Aug 30 14:33:38.165: INFO: Trying to get logs from node 10.63.224.187 pod downwardapi-volume-ce75f4bd-a60b-444d-8488-aa13eaa95cfd container client-container: <nil>
STEP: delete the pod
Aug 30 14:33:38.330: INFO: Waiting for pod downwardapi-volume-ce75f4bd-a60b-444d-8488-aa13eaa95cfd to disappear
Aug 30 14:33:38.345: INFO: Pod downwardapi-volume-ce75f4bd-a60b-444d-8488-aa13eaa95cfd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 14:33:38.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2017" for this suite.

• [SLOW TEST:6.685 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":39,"skipped":821,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:33:38.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 30 14:33:38.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4022" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":40,"skipped":839,"failed":0}
SSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:33:38.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
W0830 14:33:39.012031      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: listing
STEP: watching
Aug 30 14:33:39.051: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 30 14:33:39.086: INFO: starting watch
STEP: patching
STEP: updating
W0830 14:33:39.120217      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 14:33:39.179960      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:33:39.180: INFO: waiting for watch events with expected annotations
Aug 30 14:33:39.180: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
W0830 14:33:39.308530      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 30 14:33:39.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5390" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":41,"skipped":845,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:33:39.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 14:33:40.293597      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 14:33:40.334: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 14:33:42.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 33, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 33, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 33, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 33, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 14:33:45.504: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:33:46.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-76" for this suite.
STEP: Destroying namespace "webhook-76-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.789 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":42,"skipped":847,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:33:46.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6854
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
W0830 14:33:46.503851      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:33:46.521: INFO: Found 0 stateful pods, waiting for 3
Aug 30 14:33:56.541: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 14:33:56.541: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 14:33:56.542: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 14:33:56.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6854 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 14:33:57.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 14:33:57.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 14:33:57.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
W0830 14:34:07.170473      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:34:07.170: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 30 14:34:17.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6854 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 14:34:17.728: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 14:34:17.728: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 14:34:17.728: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 14:34:27.869: INFO: Waiting for StatefulSet statefulset-6854/ss2 to complete update
Aug 30 14:34:27.869: INFO: Waiting for Pod statefulset-6854/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Aug 30 14:34:27.869: INFO: Waiting for Pod statefulset-6854/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Aug 30 14:34:37.904: INFO: Waiting for StatefulSet statefulset-6854/ss2 to complete update
Aug 30 14:34:37.904: INFO: Waiting for Pod statefulset-6854/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Aug 30 14:34:47.939: INFO: Waiting for StatefulSet statefulset-6854/ss2 to complete update
Aug 30 14:34:47.939: INFO: Waiting for Pod statefulset-6854/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Aug 30 14:34:57.907: INFO: Waiting for StatefulSet statefulset-6854/ss2 to complete update
STEP: Rolling back to a previous revision
Aug 30 14:35:07.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6854 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 14:35:08.300: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 14:35:08.300: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 14:35:08.300: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 14:35:18.437: INFO: Updating stateful set ss2
W0830 14:35:18.437079      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Rolling back update in reverse ordinal order
Aug 30 14:35:28.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6854 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 14:35:29.047: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 14:35:29.047: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 14:35:29.047: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 14:35:39.174: INFO: Waiting for StatefulSet statefulset-6854/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 30 14:35:49.213: INFO: Deleting all statefulset in ns statefulset-6854
Aug 30 14:35:49.229: INFO: Scaling statefulset ss2 to 0
W0830 14:35:49.280358      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:35:59.331: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 14:35:59.347: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 30 14:35:59.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6854" for this suite.

• [SLOW TEST:133.237 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":43,"skipped":852,"failed":0}
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:35:59.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
W0830 14:35:59.925702      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: updating the pod
Aug 30 14:38:00.622: INFO: Successfully updated pod "var-expansion-7afbc424-257f-4a4f-962b-dd99750eb78f"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Aug 30 14:38:02.664: INFO: Deleting pod "var-expansion-7afbc424-257f-4a4f-962b-dd99750eb78f" in namespace "var-expansion-7299"
Aug 30 14:38:02.700: INFO: Wait up to 5m0s for pod "var-expansion-7afbc424-257f-4a4f-962b-dd99750eb78f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 30 14:38:34.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7299" for this suite.

• [SLOW TEST:155.286 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":44,"skipped":852,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:38:34.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
W0830 14:38:37.014624      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Aug 30 14:38:37.027: INFO: pods: 0 < 3
Aug 30 14:38:39.047: INFO: running pods: 0 < 3
Aug 30 14:38:41.046: INFO: running pods: 2 < 3
Aug 30 14:38:43.045: INFO: running pods: 2 < 3
Aug 30 14:38:45.060: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Aug 30 14:38:51.443: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 30 14:38:53.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9721" for this suite.

• [SLOW TEST:18.935 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":45,"skipped":867,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:38:53.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 14:38:53.959755      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:38:53.960: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aefb6f64-b8b6-4dbf-a1a2-60918bb0aee9" in namespace "downward-api-5238" to be "Succeeded or Failed"
Aug 30 14:38:53.978: INFO: Pod "downwardapi-volume-aefb6f64-b8b6-4dbf-a1a2-60918bb0aee9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.464137ms
Aug 30 14:38:55.997: INFO: Pod "downwardapi-volume-aefb6f64-b8b6-4dbf-a1a2-60918bb0aee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037483324s
Aug 30 14:38:58.022: INFO: Pod "downwardapi-volume-aefb6f64-b8b6-4dbf-a1a2-60918bb0aee9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062322745s
Aug 30 14:39:00.043: INFO: Pod "downwardapi-volume-aefb6f64-b8b6-4dbf-a1a2-60918bb0aee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.083038457s
STEP: Saw pod success
Aug 30 14:39:00.043: INFO: Pod "downwardapi-volume-aefb6f64-b8b6-4dbf-a1a2-60918bb0aee9" satisfied condition "Succeeded or Failed"
Aug 30 14:39:00.067: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-aefb6f64-b8b6-4dbf-a1a2-60918bb0aee9 container client-container: <nil>
STEP: delete the pod
Aug 30 14:39:00.199: INFO: Waiting for pod downwardapi-volume-aefb6f64-b8b6-4dbf-a1a2-60918bb0aee9 to disappear
Aug 30 14:39:00.218: INFO: Pod downwardapi-volume-aefb6f64-b8b6-4dbf-a1a2-60918bb0aee9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 14:39:00.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5238" for this suite.

• [SLOW TEST:6.545 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":46,"skipped":884,"failed":0}
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:39:00.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
W0830 14:39:00.459735      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 30 14:39:05.622: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 30 14:39:05.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2570" for this suite.

• [SLOW TEST:5.476 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":47,"skipped":887,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:39:05.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
W0830 14:39:05.961443      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "busybox-main-container", "busybox-sub-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "busybox-main-container", "busybox-sub-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "busybox-main-container", "busybox-sub-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "busybox-main-container", "busybox-sub-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Reading file content from the nginx-container
Aug 30 14:39:09.999: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3670 PodName:pod-sharedvolume-055889cb-cd70-434d-9aa0-4af6445322b3 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 14:39:09.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 14:39:10.001: INFO: ExecWithOptions: Clientset creation
Aug 30 14:39:10.001: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-3670/pods/pod-sharedvolume-055889cb-cd70-434d-9aa0-4af6445322b3/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 30 14:39:10.314: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 14:39:10.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3670" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":48,"skipped":896,"failed":0}
SSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:39:10.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
W0830 14:39:10.573938      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Aug 30 14:39:10.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4232" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":49,"skipped":900,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:39:10.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-2790-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 30 14:39:10.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2790" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":50,"skipped":914,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:39:11.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 30 14:39:11.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 14:39:21.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:39:57.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4444" for this suite.

• [SLOW TEST:46.703 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":51,"skipped":928,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:39:57.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
W0830 14:39:58.409975      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:39:58.435: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 30 14:40:00.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 39, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 39, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 39, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 39, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 14:40:03.586: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:40:03.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4899" for this suite.
STEP: Destroying namespace "webhook-4899-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.137 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":52,"skipped":928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:40:03.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Aug 30 14:40:04.062: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 30 14:40:04.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 create -f -'
Aug 30 14:40:05.960: INFO: stderr: ""
Aug 30 14:40:05.961: INFO: stdout: "service/agnhost-replica created\n"
Aug 30 14:40:05.961: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 30 14:40:05.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 create -f -'
Aug 30 14:40:07.561: INFO: stderr: ""
Aug 30 14:40:07.561: INFO: stdout: "service/agnhost-primary created\n"
Aug 30 14:40:07.561: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 30 14:40:07.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 create -f -'
Aug 30 14:40:07.995: INFO: stderr: ""
Aug 30 14:40:07.995: INFO: stdout: "service/frontend created\n"
Aug 30 14:40:07.995: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 30 14:40:07.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 create -f -'
Aug 30 14:40:08.445: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"guestbook-frontend\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"guestbook-frontend\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"guestbook-frontend\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"guestbook-frontend\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 14:40:08.445: INFO: stdout: "deployment.apps/frontend created\n"
Aug 30 14:40:08.446: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 30 14:40:08.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 create -f -'
Aug 30 14:40:08.888: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 14:40:08.888: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 30 14:40:08.888: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 30 14:40:08.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 create -f -'
Aug 30 14:40:09.449: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"replica\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"replica\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"replica\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"replica\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 14:40:09.449: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Aug 30 14:40:09.449: INFO: Waiting for all frontend pods to be Running.
Aug 30 14:40:14.501: INFO: Waiting for frontend to serve content.
Aug 30 14:40:14.543: INFO: Trying to add a new entry to the guestbook.
Aug 30 14:40:14.582: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 30 14:40:14.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 delete --grace-period=0 --force -f -'
Aug 30 14:40:14.819: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 14:40:14.819: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Aug 30 14:40:14.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 delete --grace-period=0 --force -f -'
Aug 30 14:40:14.997: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 14:40:14.997: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 30 14:40:14.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 delete --grace-period=0 --force -f -'
Aug 30 14:40:15.274: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 14:40:15.274: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 30 14:40:15.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 delete --grace-period=0 --force -f -'
Aug 30 14:40:15.421: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 14:40:15.422: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 30 14:40:15.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 delete --grace-period=0 --force -f -'
Aug 30 14:40:15.533: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 14:40:15.533: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 30 14:40:15.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1344 delete --grace-period=0 --force -f -'
Aug 30 14:40:15.689: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 14:40:15.689: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 14:40:15.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1344" for this suite.

• [SLOW TEST:11.880 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":53,"skipped":957,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:40:15.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Aug 30 14:40:16.017: INFO: Found Service test-service-kwczw in namespace services-3817 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 30 14:40:16.017: INFO: Service test-service-kwczw created
STEP: Getting /status
Aug 30 14:40:16.043: INFO: Service test-service-kwczw has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Aug 30 14:40:16.084: INFO: observed Service test-service-kwczw in namespace services-3817 with annotations: map[] & LoadBalancer: {[]}
Aug 30 14:40:16.084: INFO: Found Service test-service-kwczw in namespace services-3817 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 30 14:40:16.084: INFO: Service test-service-kwczw has service status patched
STEP: updating the ServiceStatus
Aug 30 14:40:16.125: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Aug 30 14:40:16.133: INFO: Observed Service test-service-kwczw in namespace services-3817 with annotations: map[] & Conditions: {[]}
Aug 30 14:40:16.133: INFO: Observed event: &Service{ObjectMeta:{test-service-kwczw  services-3817  c2bbcb98-8b2d-412e-8c7a-87912089068e 79716 0 2022-08-30 14:40:15 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-08-30 14:40:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-08-30 14:40:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.88.71,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.88.71],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 30 14:40:16.135: INFO: Found Service test-service-kwczw in namespace services-3817 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 30 14:40:16.135: INFO: Service test-service-kwczw has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Aug 30 14:40:16.184: INFO: observed Service test-service-kwczw in namespace services-3817 with labels: map[test-service-static:true]
Aug 30 14:40:16.184: INFO: observed Service test-service-kwczw in namespace services-3817 with labels: map[test-service-static:true]
Aug 30 14:40:16.184: INFO: observed Service test-service-kwczw in namespace services-3817 with labels: map[test-service-static:true]
Aug 30 14:40:16.184: INFO: Found Service test-service-kwczw in namespace services-3817 with labels: map[test-service:patched test-service-static:true]
Aug 30 14:40:16.184: INFO: Service test-service-kwczw patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Aug 30 14:40:16.259: INFO: Observed event: ADDED
Aug 30 14:40:16.259: INFO: Observed event: MODIFIED
Aug 30 14:40:16.259: INFO: Observed event: MODIFIED
Aug 30 14:40:16.259: INFO: Observed event: MODIFIED
Aug 30 14:40:16.259: INFO: Found Service test-service-kwczw in namespace services-3817 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 30 14:40:16.259: INFO: Service test-service-kwczw deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 14:40:16.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3817" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":54,"skipped":966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:40:16.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0830 14:40:16.491047      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "srv" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "srv" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "srv" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "srv" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:40:16.505: INFO: The status of Pod server-envvars-5031dfd0-3c83-4b55-af46-90a435b7fc36 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:40:18.521: INFO: The status of Pod server-envvars-5031dfd0-3c83-4b55-af46-90a435b7fc36 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:40:20.523: INFO: The status of Pod server-envvars-5031dfd0-3c83-4b55-af46-90a435b7fc36 is Running (Ready = true)
Aug 30 14:40:20.627: INFO: Waiting up to 5m0s for pod "client-envvars-36c838ba-b79e-4fb4-a2a6-c764d9976a73" in namespace "pods-6687" to be "Succeeded or Failed"
W0830 14:40:20.627420      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "env3cont" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env3cont" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env3cont" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env3cont" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:40:20.638: INFO: Pod "client-envvars-36c838ba-b79e-4fb4-a2a6-c764d9976a73": Phase="Pending", Reason="", readiness=false. Elapsed: 10.323611ms
Aug 30 14:40:22.661: INFO: Pod "client-envvars-36c838ba-b79e-4fb4-a2a6-c764d9976a73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033181994s
Aug 30 14:40:24.685: INFO: Pod "client-envvars-36c838ba-b79e-4fb4-a2a6-c764d9976a73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057851441s
Aug 30 14:40:26.702: INFO: Pod "client-envvars-36c838ba-b79e-4fb4-a2a6-c764d9976a73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074834957s
STEP: Saw pod success
Aug 30 14:40:26.702: INFO: Pod "client-envvars-36c838ba-b79e-4fb4-a2a6-c764d9976a73" satisfied condition "Succeeded or Failed"
Aug 30 14:40:26.713: INFO: Trying to get logs from node 10.63.224.187 pod client-envvars-36c838ba-b79e-4fb4-a2a6-c764d9976a73 container env3cont: <nil>
STEP: delete the pod
Aug 30 14:40:27.069: INFO: Waiting for pod client-envvars-36c838ba-b79e-4fb4-a2a6-c764d9976a73 to disappear
Aug 30 14:40:27.100: INFO: Pod client-envvars-36c838ba-b79e-4fb4-a2a6-c764d9976a73 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 30 14:40:27.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6687" for this suite.

• [SLOW TEST:10.827 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":55,"skipped":1001,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:40:27.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9891
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-9891
W0830 14:40:27.306677      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:40:27.319: INFO: Found 0 stateful pods, waiting for 1
Aug 30 14:40:37.343: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 30 14:40:37.467: INFO: Deleting all statefulset in ns statefulset-9891
Aug 30 14:40:37.480: INFO: Scaling statefulset ss to 0
W0830 14:40:37.527484      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:40:47.559: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 14:40:47.571: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 30 14:40:47.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9891" for this suite.

• [SLOW TEST:20.517 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":56,"skipped":1020,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:40:47.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Aug 30 14:40:47.888: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Aug 30 14:40:49.946: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Aug 30 14:40:52.000: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Aug 30 14:40:54.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-2006" for this suite.

• [SLOW TEST:6.403 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":57,"skipped":1035,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:40:54.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
W0830 14:40:54.188257      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Aug 30 14:41:04.307: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 30 14:41:04.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0830 14:41:04.307546      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-5090" for this suite.

• [SLOW TEST:10.284 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":58,"skipped":1037,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:41:04.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 14:41:04.539702      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:41:04.540: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87eb46e5-0404-4ec4-8bb1-70b5b856eaf0" in namespace "downward-api-851" to be "Succeeded or Failed"
Aug 30 14:41:04.567: INFO: Pod "downwardapi-volume-87eb46e5-0404-4ec4-8bb1-70b5b856eaf0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.532388ms
Aug 30 14:41:06.593: INFO: Pod "downwardapi-volume-87eb46e5-0404-4ec4-8bb1-70b5b856eaf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053134904s
Aug 30 14:41:08.619: INFO: Pod "downwardapi-volume-87eb46e5-0404-4ec4-8bb1-70b5b856eaf0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078859153s
Aug 30 14:41:10.637: INFO: Pod "downwardapi-volume-87eb46e5-0404-4ec4-8bb1-70b5b856eaf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.097256079s
STEP: Saw pod success
Aug 30 14:41:10.637: INFO: Pod "downwardapi-volume-87eb46e5-0404-4ec4-8bb1-70b5b856eaf0" satisfied condition "Succeeded or Failed"
Aug 30 14:41:10.649: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-87eb46e5-0404-4ec4-8bb1-70b5b856eaf0 container client-container: <nil>
STEP: delete the pod
Aug 30 14:41:10.747: INFO: Waiting for pod downwardapi-volume-87eb46e5-0404-4ec4-8bb1-70b5b856eaf0 to disappear
Aug 30 14:41:10.756: INFO: Pod downwardapi-volume-87eb46e5-0404-4ec4-8bb1-70b5b856eaf0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 14:41:10.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-851" for this suite.

• [SLOW TEST:6.462 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":59,"skipped":1040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:41:10.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8122
STEP: creating service affinity-nodeport in namespace services-8122
STEP: creating replication controller affinity-nodeport in namespace services-8122
W0830 14:41:11.128859      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-nodeport" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 14:41:11.128989      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8122, replica count: 3
I0830 14:41:14.180104      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 14:41:14.233: INFO: Creating new exec pod
W0830 14:41:14.290169      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:41:19.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8122 exec execpod-affinitygsw59 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Aug 30 14:41:19.808: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 30 14:41:19.808: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 14:41:19.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8122 exec execpod-affinitygsw59 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.84.2 80'
Aug 30 14:41:20.245: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.84.2 80\nConnection to 172.21.84.2 80 port [tcp/http] succeeded!\n"
Aug 30 14:41:20.245: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 14:41:20.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8122 exec execpod-affinitygsw59 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.187 30904'
Aug 30 14:41:20.713: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.63.224.187 30904\nConnection to 10.63.224.187 30904 port [tcp/*] succeeded!\n"
Aug 30 14:41:20.713: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 14:41:20.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8122 exec execpod-affinitygsw59 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.158 30904'
Aug 30 14:41:21.185: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.63.224.158 30904\nConnection to 10.63.224.158 30904 port [tcp/*] succeeded!\n"
Aug 30 14:41:21.185: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 14:41:21.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8122 exec execpod-affinitygsw59 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.63.224.158:30904/ ; done'
Aug 30 14:41:21.854: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:30904/\n"
Aug 30 14:41:21.855: INFO: stdout: "\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4\naffinity-nodeport-8nvh4"
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Received response from host: affinity-nodeport-8nvh4
Aug 30 14:41:21.855: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8122, will wait for the garbage collector to delete the pods
Aug 30 14:41:21.972: INFO: Deleting ReplicationController affinity-nodeport took: 18.269919ms
Aug 30 14:41:22.073: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.205931ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 14:41:25.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8122" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:14.605 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":60,"skipped":1077,"failed":0}
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:41:25.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 30 14:41:25.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4708" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":61,"skipped":1078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:41:25.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
W0830 14:41:25.813433      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:41:25.814: INFO: Waiting up to 5m0s for pod "downward-api-f630b4b9-f5b5-45cd-8b39-462585fffbff" in namespace "downward-api-4188" to be "Succeeded or Failed"
Aug 30 14:41:25.829: INFO: Pod "downward-api-f630b4b9-f5b5-45cd-8b39-462585fffbff": Phase="Pending", Reason="", readiness=false. Elapsed: 15.192354ms
Aug 30 14:41:27.849: INFO: Pod "downward-api-f630b4b9-f5b5-45cd-8b39-462585fffbff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035377748s
Aug 30 14:41:29.876: INFO: Pod "downward-api-f630b4b9-f5b5-45cd-8b39-462585fffbff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062528922s
Aug 30 14:41:31.900: INFO: Pod "downward-api-f630b4b9-f5b5-45cd-8b39-462585fffbff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08591972s
STEP: Saw pod success
Aug 30 14:41:31.900: INFO: Pod "downward-api-f630b4b9-f5b5-45cd-8b39-462585fffbff" satisfied condition "Succeeded or Failed"
Aug 30 14:41:31.936: INFO: Trying to get logs from node 10.63.224.189 pod downward-api-f630b4b9-f5b5-45cd-8b39-462585fffbff container dapi-container: <nil>
STEP: delete the pod
Aug 30 14:41:32.073: INFO: Waiting for pod downward-api-f630b4b9-f5b5-45cd-8b39-462585fffbff to disappear
Aug 30 14:41:32.086: INFO: Pod downward-api-f630b4b9-f5b5-45cd-8b39-462585fffbff no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 30 14:41:32.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4188" for this suite.

• [SLOW TEST:6.507 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":62,"skipped":1123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:41:32.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:41:32.304: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-aeea5c0a-0d0b-417a-ba72-cd3b391b0828" in namespace "security-context-test-3955" to be "Succeeded or Failed"
W0830 14:41:32.303807      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-privileged-false-aeea5c0a-0d0b-417a-ba72-cd3b391b0828" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-privileged-false-aeea5c0a-0d0b-417a-ba72-cd3b391b0828" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-privileged-false-aeea5c0a-0d0b-417a-ba72-cd3b391b0828" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-privileged-false-aeea5c0a-0d0b-417a-ba72-cd3b391b0828" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:41:32.316: INFO: Pod "busybox-privileged-false-aeea5c0a-0d0b-417a-ba72-cd3b391b0828": Phase="Pending", Reason="", readiness=false. Elapsed: 11.746104ms
Aug 30 14:41:34.332: INFO: Pod "busybox-privileged-false-aeea5c0a-0d0b-417a-ba72-cd3b391b0828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027647391s
Aug 30 14:41:36.348: INFO: Pod "busybox-privileged-false-aeea5c0a-0d0b-417a-ba72-cd3b391b0828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044133028s
Aug 30 14:41:36.348: INFO: Pod "busybox-privileged-false-aeea5c0a-0d0b-417a-ba72-cd3b391b0828" satisfied condition "Succeeded or Failed"
Aug 30 14:41:36.377: INFO: Got logs for pod "busybox-privileged-false-aeea5c0a-0d0b-417a-ba72-cd3b391b0828": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 30 14:41:36.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3955" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":63,"skipped":1178,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:41:36.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 14:41:36.599811      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:41:36.600: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ec0a689-be2c-4e6a-84d6-6429a955d544" in namespace "downward-api-7642" to be "Succeeded or Failed"
Aug 30 14:41:36.615: INFO: Pod "downwardapi-volume-4ec0a689-be2c-4e6a-84d6-6429a955d544": Phase="Pending", Reason="", readiness=false. Elapsed: 14.93025ms
Aug 30 14:41:38.634: INFO: Pod "downwardapi-volume-4ec0a689-be2c-4e6a-84d6-6429a955d544": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03436278s
Aug 30 14:41:40.655: INFO: Pod "downwardapi-volume-4ec0a689-be2c-4e6a-84d6-6429a955d544": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054670914s
Aug 30 14:41:42.670: INFO: Pod "downwardapi-volume-4ec0a689-be2c-4e6a-84d6-6429a955d544": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069570075s
STEP: Saw pod success
Aug 30 14:41:42.670: INFO: Pod "downwardapi-volume-4ec0a689-be2c-4e6a-84d6-6429a955d544" satisfied condition "Succeeded or Failed"
Aug 30 14:41:42.683: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-4ec0a689-be2c-4e6a-84d6-6429a955d544 container client-container: <nil>
STEP: delete the pod
Aug 30 14:41:42.740: INFO: Waiting for pod downwardapi-volume-4ec0a689-be2c-4e6a-84d6-6429a955d544 to disappear
Aug 30 14:41:42.750: INFO: Pod downwardapi-volume-4ec0a689-be2c-4e6a-84d6-6429a955d544 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 14:41:42.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7642" for this suite.

• [SLOW TEST:6.365 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":64,"skipped":1184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:41:42.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
W0830 14:41:42.967745      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:41:42.981: INFO: The status of Pod annotationupdate42c4ecd6-96d2-43b2-bf8c-2d7b08cf26f8 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:41:44.997: INFO: The status of Pod annotationupdate42c4ecd6-96d2-43b2-bf8c-2d7b08cf26f8 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:41:46.997: INFO: The status of Pod annotationupdate42c4ecd6-96d2-43b2-bf8c-2d7b08cf26f8 is Running (Ready = true)
Aug 30 14:41:47.605: INFO: Successfully updated pod "annotationupdate42c4ecd6-96d2-43b2-bf8c-2d7b08cf26f8"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 14:41:49.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8194" for this suite.

• [SLOW TEST:6.917 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":65,"skipped":1208,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:41:49.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:41:49.931: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32" in namespace "security-context-test-9408" to be "Succeeded or Failed"
W0830 14:41:49.931048      21 warnings.go:70] would violate PodSecurity "restricted:latest": unrestricted capabilities (container "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:41:49.948: INFO: Pod "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32": Phase="Pending", Reason="", readiness=false. Elapsed: 17.488921ms
Aug 30 14:41:51.965: INFO: Pod "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034130998s
Aug 30 14:41:53.983: INFO: Pod "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051760128s
Aug 30 14:41:56.001: INFO: Pod "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070521679s
Aug 30 14:41:58.018: INFO: Pod "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086994421s
Aug 30 14:42:00.035: INFO: Pod "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32": Phase="Pending", Reason="", readiness=false. Elapsed: 10.10382005s
Aug 30 14:42:02.058: INFO: Pod "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.126782575s
Aug 30 14:42:02.058: INFO: Pod "alpine-nnp-false-bdbf71fc-c78c-4aed-80ec-95be95559b32" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 30 14:42:02.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9408" for this suite.

• [SLOW TEST:12.427 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":66,"skipped":1262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:42:02.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 14:42:02.860277      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 14:42:02.941: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 14:42:04.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 42, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 42, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 14:42:08.054: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
W0830 14:42:08.230777      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webhook-disallow" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webhook-disallow" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webhook-disallow" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webhook-disallow" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: create a pod that causes the webhook to hang
W0830 14:42:18.304819      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "wait-forever" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "wait-forever" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "wait-forever" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "wait-forever" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:42:18.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6751" for this suite.
STEP: Destroying namespace "webhook-6751-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:16.887 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":67,"skipped":1286,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:42:19.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 30 14:42:19.511: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6565  cb4de098-c119-4a20-a9aa-e270f7612b01 81869 0 2022-08-30 14:42:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-30 14:42:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 14:42:19.511: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6565  cb4de098-c119-4a20-a9aa-e270f7612b01 81875 0 2022-08-30 14:42:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-30 14:42:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 14:42:19.512: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6565  cb4de098-c119-4a20-a9aa-e270f7612b01 81880 0 2022-08-30 14:42:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-30 14:42:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 30 14:42:29.708: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6565  cb4de098-c119-4a20-a9aa-e270f7612b01 82011 0 2022-08-30 14:42:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-30 14:42:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 14:42:29.709: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6565  cb4de098-c119-4a20-a9aa-e270f7612b01 82012 0 2022-08-30 14:42:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-30 14:42:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 14:42:29.709: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6565  cb4de098-c119-4a20-a9aa-e270f7612b01 82013 0 2022-08-30 14:42:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-30 14:42:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 30 14:42:29.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6565" for this suite.

• [SLOW TEST:10.719 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":68,"skipped":1287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:42:29.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
W0830 14:42:29.934395      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:42:29.946: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 30 14:42:34.966: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
W0830 14:42:35.006413      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:42:35.007: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
W0830 14:42:35.026329      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "test-rs", "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "test-rs", "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "test-rs", "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "test-rs", "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:42:35.044: INFO: observed ReplicaSet test-rs in namespace replicaset-2029 with ReadyReplicas 1, AvailableReplicas 1
Aug 30 14:42:35.313: INFO: observed ReplicaSet test-rs in namespace replicaset-2029 with ReadyReplicas 1, AvailableReplicas 1
Aug 30 14:42:35.473: INFO: observed ReplicaSet test-rs in namespace replicaset-2029 with ReadyReplicas 1, AvailableReplicas 1
Aug 30 14:42:35.545: INFO: observed ReplicaSet test-rs in namespace replicaset-2029 with ReadyReplicas 1, AvailableReplicas 1
Aug 30 14:42:37.156: INFO: observed ReplicaSet test-rs in namespace replicaset-2029 with ReadyReplicas 2, AvailableReplicas 2
Aug 30 14:42:37.571: INFO: observed Replicaset test-rs in namespace replicaset-2029 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 30 14:42:37.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2029" for this suite.

• [SLOW TEST:7.890 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":69,"skipped":1329,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:42:37.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
W0830 14:42:38.074573      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:42:38.137: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:42:40.155: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:42:42.152: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
W0830 14:42:42.210586      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-with-prestop-http-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-prestop-http-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-prestop-http-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-prestop-http-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:42:42.222: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:42:44.240: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:42:46.238: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Aug 30 14:42:46.268: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 14:42:46.280: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 14:42:48.281: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 14:42:48.299: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 14:42:50.281: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 14:42:50.297: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 30 14:42:50.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6671" for this suite.

• [SLOW TEST:12.749 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":70,"skipped":1345,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:42:50.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
W0830 14:42:50.562593      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 30 14:42:54.664: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 30 14:42:54.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2871" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":71,"skipped":1350,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:42:54.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
W0830 14:42:55.367436      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-crd-conversion-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-crd-conversion-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-crd-conversion-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-crd-conversion-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:42:55.391: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Aug 30 14:42:57.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 42, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 42, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 42, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 42, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-677b6dd845\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 14:43:00.549: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:43:00.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:43:04.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1008" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:9.800 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":72,"skipped":1356,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:43:04.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:43:04.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:43:05.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6977" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":73,"skipped":1375,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:43:05.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:43:05.735: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
W0830 14:43:05.757150      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:43:05.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:43:05.904: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:43:06.952: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:43:06.952: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:43:07.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 14:43:07.937: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:43:08.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 14:43:08.959: INFO: Node 10.63.224.189 is running 0 daemon pod, expected 1
Aug 30 14:43:09.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 14:43:09.937: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
W0830 14:43:10.013550      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods images are updated.
Aug 30 14:43:10.044: INFO: Wrong image for pod: daemon-set-5brpx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:10.044: INFO: Wrong image for pod: daemon-set-5tmcc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:10.044: INFO: Wrong image for pod: daemon-set-r6lc8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:11.096: INFO: Wrong image for pod: daemon-set-5brpx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:11.096: INFO: Wrong image for pod: daemon-set-5tmcc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:12.083: INFO: Wrong image for pod: daemon-set-5brpx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:12.083: INFO: Wrong image for pod: daemon-set-5tmcc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:13.088: INFO: Wrong image for pod: daemon-set-5brpx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:13.088: INFO: Wrong image for pod: daemon-set-5tmcc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:13.088: INFO: Pod daemon-set-rmbwt is not available
Aug 30 14:43:14.090: INFO: Wrong image for pod: daemon-set-5brpx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:14.090: INFO: Wrong image for pod: daemon-set-5tmcc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:14.090: INFO: Pod daemon-set-rmbwt is not available
Aug 30 14:43:15.084: INFO: Wrong image for pod: daemon-set-5brpx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:16.078: INFO: Wrong image for pod: daemon-set-5brpx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:17.079: INFO: Wrong image for pod: daemon-set-5brpx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:17.079: INFO: Pod daemon-set-5x6sv is not available
Aug 30 14:43:18.079: INFO: Wrong image for pod: daemon-set-5brpx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:18.080: INFO: Pod daemon-set-5x6sv is not available
Aug 30 14:43:19.078: INFO: Wrong image for pod: daemon-set-5brpx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 30 14:43:19.078: INFO: Pod daemon-set-5x6sv is not available
Aug 30 14:43:21.084: INFO: Pod daemon-set-qzs4s is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 30 14:43:21.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 14:43:21.147: INFO: Node 10.63.224.187 is running 0 daemon pod, expected 1
Aug 30 14:43:22.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 14:43:22.193: INFO: Node 10.63.224.187 is running 0 daemon pod, expected 1
Aug 30 14:43:23.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 14:43:23.203: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2796, will wait for the garbage collector to delete the pods
Aug 30 14:43:23.375: INFO: Deleting DaemonSet.extensions daemon-set took: 50.940699ms
Aug 30 14:43:23.476: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.98343ms
Aug 30 14:43:26.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:43:26.292: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 14:43:26.303: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"83039"},"items":null}

Aug 30 14:43:26.315: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"83039"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 30 14:43:26.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2796" for this suite.

• [SLOW TEST:20.900 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":74,"skipped":1387,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:43:26.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
W0830 14:43:26.598580      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:43:26.599: INFO: Waiting up to 5m0s for pod "var-expansion-be881c6a-0198-4363-994d-74cedfe017cc" in namespace "var-expansion-3924" to be "Succeeded or Failed"
Aug 30 14:43:26.613: INFO: Pod "var-expansion-be881c6a-0198-4363-994d-74cedfe017cc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.70292ms
Aug 30 14:43:28.632: INFO: Pod "var-expansion-be881c6a-0198-4363-994d-74cedfe017cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033220194s
Aug 30 14:43:30.655: INFO: Pod "var-expansion-be881c6a-0198-4363-994d-74cedfe017cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056488508s
Aug 30 14:43:32.671: INFO: Pod "var-expansion-be881c6a-0198-4363-994d-74cedfe017cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.072277603s
STEP: Saw pod success
Aug 30 14:43:32.671: INFO: Pod "var-expansion-be881c6a-0198-4363-994d-74cedfe017cc" satisfied condition "Succeeded or Failed"
Aug 30 14:43:32.682: INFO: Trying to get logs from node 10.63.224.189 pod var-expansion-be881c6a-0198-4363-994d-74cedfe017cc container dapi-container: <nil>
STEP: delete the pod
Aug 30 14:43:32.781: INFO: Waiting for pod var-expansion-be881c6a-0198-4363-994d-74cedfe017cc to disappear
Aug 30 14:43:32.794: INFO: Pod var-expansion-be881c6a-0198-4363-994d-74cedfe017cc no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 30 14:43:32.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3924" for this suite.

• [SLOW TEST:6.457 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":75,"skipped":1394,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:43:32.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
W0830 14:43:33.099733      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 30 14:43:49.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1489" for this suite.

• [SLOW TEST:16.280 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":76,"skipped":1405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:43:49.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:43:49.279: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0830 14:43:49.297020      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:43:49.307: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 30 14:43:54.324: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 30 14:43:54.324: INFO: Creating deployment "test-rolling-update-deployment"
Aug 30 14:43:54.358: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
W0830 14:43:54.358497      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:43:54.392: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 30 14:43:56.419: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 30 14:43:56.430: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 43, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 43, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 43, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 43, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-5579c56cf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 14:43:58.447: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 14:43:58.478: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9386  731ee891-4194-4141-a3a9-6000d375de19 83610 1 2022-08-30 14:43:54 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-08-30 14:43:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 14:43:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a29018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-30 14:43:54 +0000 UTC,LastTransitionTime:2022-08-30 14:43:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-5579c56cf8" has successfully progressed.,LastUpdateTime:2022-08-30 14:43:56 +0000 UTC,LastTransitionTime:2022-08-30 14:43:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 30 14:43:58.490: INFO: New ReplicaSet "test-rolling-update-deployment-5579c56cf8" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-5579c56cf8  deployment-9386  bce737d3-3ae2-4ade-8f08-12dfca63ac51 83599 1 2022-08-30 14:43:54 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 731ee891-4194-4141-a3a9-6000d375de19 0xc000cdc057 0xc000cdc058}] []  [{kube-controller-manager Update apps/v1 2022-08-30 14:43:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"731ee891-4194-4141-a3a9-6000d375de19\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 14:43:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 5579c56cf8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000cdc108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 14:43:58.490: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 30 14:43:58.490: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9386  813666b5-c652-4634-b7dd-d6550c8d7e28 83608 2 2022-08-30 14:43:49 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 731ee891-4194-4141-a3a9-6000d375de19 0xc002a29d97 0xc002a29d98}] []  [{e2e.test Update apps/v1 2022-08-30 14:43:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 14:43:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"731ee891-4194-4141-a3a9-6000d375de19\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-30 14:43:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002a29f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 14:43:58.501: INFO: Pod "test-rolling-update-deployment-5579c56cf8-vkb59" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-5579c56cf8-vkb59 test-rolling-update-deployment-5579c56cf8- deployment-9386  96960132-d207-4c48-adfa-7c366136e56a 83598 0 2022-08-30 14:43:54 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[cni.projectcalico.org/containerID:4890dc44a93718c9addf2089d48d8817366bfd0a4e4a7a51c35876be9a6365cf cni.projectcalico.org/podIP:172.30.233.212/32 cni.projectcalico.org/podIPs:172.30.233.212/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.212"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.212"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-5579c56cf8 bce737d3-3ae2-4ade-8f08-12dfca63ac51 0xc000cdc697 0xc000cdc698}] []  [{kube-controller-manager Update v1 2022-08-30 14:43:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bce737d3-3ae2-4ade-8f08-12dfca63ac51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 14:43:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 14:43:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 14:43:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.233.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cxhzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cxhzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dfg6n,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 14:43:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 14:43:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 14:43:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 14:43:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.189,PodIP:172.30.233.212,StartTime:2022-08-30 14:43:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 14:43:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403,ContainerID:cri-o://51599c014d67b96175fb0c2283e539de71f82927bc9c498c95b3058364bd04f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.233.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 30 14:43:58.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9386" for this suite.

• [SLOW TEST:9.364 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":77,"skipped":1461,"failed":0}
SS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:43:58.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 14:43:58.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1904" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":78,"skipped":1463,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:43:59.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-7602b67e-7c2e-4623-b9f3-456041e7d30c
STEP: Creating a pod to test consume configMaps
W0830 14:43:59.238380      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:43:59.238: INFO: Waiting up to 5m0s for pod "pod-configmaps-05f06695-afc7-4f45-baeb-258e42a14f17" in namespace "configmap-1604" to be "Succeeded or Failed"
Aug 30 14:43:59.251: INFO: Pod "pod-configmaps-05f06695-afc7-4f45-baeb-258e42a14f17": Phase="Pending", Reason="", readiness=false. Elapsed: 12.044386ms
Aug 30 14:44:01.270: INFO: Pod "pod-configmaps-05f06695-afc7-4f45-baeb-258e42a14f17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030729998s
Aug 30 14:44:03.311: INFO: Pod "pod-configmaps-05f06695-afc7-4f45-baeb-258e42a14f17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071371478s
Aug 30 14:44:05.346: INFO: Pod "pod-configmaps-05f06695-afc7-4f45-baeb-258e42a14f17": Phase="Pending", Reason="", readiness=false. Elapsed: 6.106786823s
Aug 30 14:44:07.363: INFO: Pod "pod-configmaps-05f06695-afc7-4f45-baeb-258e42a14f17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.12361894s
STEP: Saw pod success
Aug 30 14:44:07.363: INFO: Pod "pod-configmaps-05f06695-afc7-4f45-baeb-258e42a14f17" satisfied condition "Succeeded or Failed"
Aug 30 14:44:07.375: INFO: Trying to get logs from node 10.63.224.189 pod pod-configmaps-05f06695-afc7-4f45-baeb-258e42a14f17 container agnhost-container: <nil>
STEP: delete the pod
Aug 30 14:44:07.461: INFO: Waiting for pod pod-configmaps-05f06695-afc7-4f45-baeb-258e42a14f17 to disappear
Aug 30 14:44:07.487: INFO: Pod pod-configmaps-05f06695-afc7-4f45-baeb-258e42a14f17 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 14:44:07.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1604" for this suite.

• [SLOW TEST:8.571 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":79,"skipped":1484,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:44:07.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7667
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-7667
W0830 14:44:07.773668      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7667
Aug 30 14:44:07.784: INFO: Found 0 stateful pods, waiting for 1
Aug 30 14:44:17.809: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 30 14:44:17.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-7667 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 14:44:18.279: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 14:44:18.279: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 14:44:18.279: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 14:44:18.299: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 30 14:44:28.329: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 14:44:28.329: INFO: Waiting for statefulset status.replicas updated to 0
W0830 14:44:28.374661      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:44:28.386: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 30 14:44:28.386: INFO: ss-0  10.63.224.189  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:07 +0000 UTC  }]
Aug 30 14:44:28.386: INFO: 
Aug 30 14:44:28.386: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 30 14:44:29.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987804072s
Aug 30 14:44:30.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973230747s
Aug 30 14:44:31.437: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.955890436s
Aug 30 14:44:32.465: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.936854002s
Aug 30 14:44:33.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.909539894s
Aug 30 14:44:34.497: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.894488571s
Aug 30 14:44:35.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.877113123s
Aug 30 14:44:36.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.851295945s
Aug 30 14:44:37.581: INFO: Verifying statefulset ss doesn't scale past 3 for another 809.885228ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7667
Aug 30 14:44:38.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-7667 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 14:44:39.076: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 14:44:39.076: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 14:44:39.076: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 14:44:39.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-7667 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 14:44:39.557: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 30 14:44:39.557: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 14:44:39.557: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 14:44:39.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-7667 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 14:44:39.951: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 30 14:44:39.951: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 14:44:39.951: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 14:44:39.970: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 14:44:39.970: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 14:44:39.970: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 30 14:44:39.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-7667 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 14:44:40.457: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 14:44:40.457: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 14:44:40.457: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 14:44:40.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-7667 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 14:44:40.886: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 14:44:40.886: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 14:44:40.886: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 14:44:40.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-7667 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 14:44:41.228: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 14:44:41.228: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 14:44:41.228: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 14:44:41.228: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 14:44:41.241: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 30 14:44:51.281: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 14:44:51.281: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 14:44:51.281: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
W0830 14:44:51.343611      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:44:51.360: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 30 14:44:51.360: INFO: ss-0  10.63.224.189  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:07 +0000 UTC  }]
Aug 30 14:44:51.360: INFO: ss-1  10.63.224.158  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:28 +0000 UTC  }]
Aug 30 14:44:51.360: INFO: ss-2  10.63.224.187  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:28 +0000 UTC  }]
Aug 30 14:44:51.361: INFO: 
Aug 30 14:44:51.361: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 30 14:44:52.380: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 30 14:44:52.380: INFO: ss-0  10.63.224.189  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:07 +0000 UTC  }]
Aug 30 14:44:52.380: INFO: ss-1  10.63.224.158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:28 +0000 UTC  }]
Aug 30 14:44:52.381: INFO: ss-2  10.63.224.187  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:28 +0000 UTC  }]
Aug 30 14:44:52.381: INFO: 
Aug 30 14:44:52.381: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 30 14:44:53.393: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Aug 30 14:44:53.393: INFO: ss-1  10.63.224.158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 14:44:28 +0000 UTC  }]
Aug 30 14:44:53.393: INFO: 
Aug 30 14:44:53.393: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 30 14:44:54.407: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.948590884s
Aug 30 14:44:55.425: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.935378429s
Aug 30 14:44:56.439: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.917725153s
Aug 30 14:44:57.455: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.904220107s
Aug 30 14:44:58.469: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.887863458s
Aug 30 14:44:59.501: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.874087264s
Aug 30 14:45:00.522: INFO: Verifying statefulset ss doesn't scale past 0 for another 841.469291ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7667
Aug 30 14:45:01.539: INFO: Scaling statefulset ss to 0
W0830 14:45:01.607479      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:45:01.618: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 30 14:45:01.633: INFO: Deleting all statefulset in ns statefulset-7667
Aug 30 14:45:01.649: INFO: Scaling statefulset ss to 0
W0830 14:45:01.698351      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:45:01.710: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 14:45:01.729: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 30 14:45:01.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7667" for this suite.

• [SLOW TEST:54.291 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":80,"skipped":1500,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:45:01.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Aug 30 14:45:01.977: INFO: Creating e2e-svc-a-bz7mz
Aug 30 14:45:02.082: INFO: Creating e2e-svc-b-zfqsq
Aug 30 14:45:02.193: INFO: Creating e2e-svc-c-mh2nb
STEP: deleting service collection
Aug 30 14:45:02.573: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 14:45:02.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7271" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":81,"skipped":1509,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:45:02.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:45:02.764: INFO: Creating pod...
W0830 14:45:02.846181      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:45:06.890: INFO: Creating service...
Aug 30 14:45:06.982: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/pods/agnhost/proxy?method=DELETE
Aug 30 14:45:07.025: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 30 14:45:07.025: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/pods/agnhost/proxy?method=OPTIONS
Aug 30 14:45:07.093: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 30 14:45:07.093: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/pods/agnhost/proxy?method=PATCH
Aug 30 14:45:07.135: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 30 14:45:07.141: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/pods/agnhost/proxy?method=POST
Aug 30 14:45:07.184: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 30 14:45:07.184: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/pods/agnhost/proxy?method=PUT
Aug 30 14:45:07.238: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 30 14:45:07.238: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 30 14:45:07.326: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 30 14:45:07.326: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 30 14:45:07.433: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 30 14:45:07.433: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 30 14:45:07.547: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 30 14:45:07.548: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/services/e2e-proxy-test-service/proxy?method=POST
Aug 30 14:45:07.632: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 30 14:45:07.632: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/services/e2e-proxy-test-service/proxy?method=PUT
Aug 30 14:45:07.664: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 30 14:45:07.665: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/pods/agnhost/proxy?method=GET
Aug 30 14:45:07.677: INFO: http.Client request:GET StatusCode:301
Aug 30 14:45:07.677: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/services/e2e-proxy-test-service/proxy?method=GET
Aug 30 14:45:07.691: INFO: http.Client request:GET StatusCode:301
Aug 30 14:45:07.691: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/pods/agnhost/proxy?method=HEAD
Aug 30 14:45:07.704: INFO: http.Client request:HEAD StatusCode:301
Aug 30 14:45:07.704: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9260/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 30 14:45:07.719: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Aug 30 14:45:07.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9260" for this suite.

• [SLOW TEST:5.172 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":82,"skipped":1511,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:45:07.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 14:45:08.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6999" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":83,"skipped":1513,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:45:08.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce
W0830 14:45:08.514420      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:45:08.527: INFO: Pod name my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce: Found 0 pods out of 1
Aug 30 14:45:13.563: INFO: Pod name my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce: Found 1 pods out of 1
Aug 30 14:45:13.563: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce" are running
Aug 30 14:45:13.578: INFO: Pod "my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce-7trp6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-30 14:45:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-30 14:45:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-30 14:45:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-30 14:45:08 +0000 UTC Reason: Message:}])
Aug 30 14:45:13.579: INFO: Trying to dial the pod
Aug 30 14:45:18.677: INFO: Controller my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce: Got expected result from replica 1 [my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce-7trp6]: "my-hostname-basic-f8b2d21e-4d44-4a34-bd74-d003b0f746ce-7trp6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 30 14:45:18.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8032" for this suite.

• [SLOW TEST:10.363 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":84,"skipped":1523,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:45:18.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
W0830 14:45:18.915277      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "terminate-cmd-rpa" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpa" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpa" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpa" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
W0830 14:45:39.403216      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "terminate-cmd-rpof" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpof" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpof" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpof" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
W0830 14:45:45.641067      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "terminate-cmd-rpn" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpn" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpn" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpn" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 30 14:45:50.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3712" for this suite.

• [SLOW TEST:32.156 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":85,"skipped":1534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:45:50.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 30 14:45:51.053: INFO: Waiting up to 5m0s for pod "pod-66a42776-907d-4850-a96a-98166018a977" in namespace "emptydir-4074" to be "Succeeded or Failed"
W0830 14:45:51.052926      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:45:51.067: INFO: Pod "pod-66a42776-907d-4850-a96a-98166018a977": Phase="Pending", Reason="", readiness=false. Elapsed: 13.9539ms
Aug 30 14:45:53.082: INFO: Pod "pod-66a42776-907d-4850-a96a-98166018a977": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029321671s
Aug 30 14:45:55.100: INFO: Pod "pod-66a42776-907d-4850-a96a-98166018a977": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046939157s
Aug 30 14:45:57.119: INFO: Pod "pod-66a42776-907d-4850-a96a-98166018a977": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065552529s
STEP: Saw pod success
Aug 30 14:45:57.119: INFO: Pod "pod-66a42776-907d-4850-a96a-98166018a977" satisfied condition "Succeeded or Failed"
Aug 30 14:45:57.133: INFO: Trying to get logs from node 10.63.224.189 pod pod-66a42776-907d-4850-a96a-98166018a977 container test-container: <nil>
STEP: delete the pod
Aug 30 14:45:57.255: INFO: Waiting for pod pod-66a42776-907d-4850-a96a-98166018a977 to disappear
Aug 30 14:45:57.268: INFO: Pod pod-66a42776-907d-4850-a96a-98166018a977 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 14:45:57.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4074" for this suite.

• [SLOW TEST:6.439 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":86,"skipped":1557,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:45:57.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Aug 30 14:45:57.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4846" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":87,"skipped":1568,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:45:57.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
W0830 14:45:58.023410      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:45:58.046: INFO: The status of Pod annotationupdate476e093c-7285-4110-94ab-7e81a3f72055 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:46:00.067: INFO: The status of Pod annotationupdate476e093c-7285-4110-94ab-7e81a3f72055 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:46:02.086: INFO: The status of Pod annotationupdate476e093c-7285-4110-94ab-7e81a3f72055 is Running (Ready = true)
Aug 30 14:46:02.693: INFO: Successfully updated pod "annotationupdate476e093c-7285-4110-94ab-7e81a3f72055"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 14:46:04.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5983" for this suite.

• [SLOW TEST:6.991 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":88,"skipped":1571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:46:04.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
W0830 14:46:05.128058      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:46:05.189: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:46:05.189: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:46:06.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:46:06.226: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:46:07.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:46:07.229: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:46:08.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 14:46:08.222: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 30 14:46:08.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 14:46:08.356: INFO: Node 10.63.224.187 is running 0 daemon pod, expected 1
Aug 30 14:46:09.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 14:46:09.390: INFO: Node 10.63.224.187 is running 0 daemon pod, expected 1
Aug 30 14:46:10.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 14:46:10.393: INFO: Node 10.63.224.187 is running 0 daemon pod, expected 1
Aug 30 14:46:11.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 14:46:11.395: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4570, will wait for the garbage collector to delete the pods
Aug 30 14:46:11.503: INFO: Deleting DaemonSet.extensions daemon-set took: 21.695746ms
Aug 30 14:46:11.704: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.588566ms
Aug 30 14:46:15.130: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:46:15.130: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 14:46:15.142: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"85574"},"items":null}

Aug 30 14:46:15.151: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"85574"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 30 14:46:15.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4570" for this suite.

• [SLOW TEST:10.406 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":89,"skipped":1609,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:46:15.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
W0830 14:46:15.465304      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:46:15.490: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:46:17.507: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:46:19.509: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
W0830 14:46:19.587477      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-with-poststart-exec-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-poststart-exec-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-poststart-exec-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-poststart-exec-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:46:19.612: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:46:21.652: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:46:23.630: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 30 14:46:23.768: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 30 14:46:23.788: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 30 14:46:25.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 30 14:46:25.811: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 30 14:46:27.789: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 30 14:46:27.807: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 30 14:46:27.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4216" for this suite.

• [SLOW TEST:12.594 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":90,"skipped":1611,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:46:27.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:46:28.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Creating first CR 
Aug 30 14:46:30.755: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-30T14:46:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-30T14:46:30Z]] name:name1 resourceVersion:85803 uid:350f31c1-910e-4903-b922-b6e8f7345738] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 30 14:46:40.789: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-30T14:46:40Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-30T14:46:40Z]] name:name2 resourceVersion:85880 uid:11f94d59-768d-4386-a34a-2acb8e089d92] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 30 14:46:50.841: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-30T14:46:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-30T14:46:50Z]] name:name1 resourceVersion:85938 uid:350f31c1-910e-4903-b922-b6e8f7345738] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 30 14:47:00.876: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-30T14:46:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-30T14:47:00Z]] name:name2 resourceVersion:85987 uid:11f94d59-768d-4386-a34a-2acb8e089d92] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 30 14:47:10.928: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-30T14:46:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-30T14:46:50Z]] name:name1 resourceVersion:86046 uid:350f31c1-910e-4903-b922-b6e8f7345738] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 30 14:47:20.973: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-30T14:46:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-30T14:47:00Z]] name:name2 resourceVersion:86094 uid:11f94d59-768d-4386-a34a-2acb8e089d92] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:47:31.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8625" for this suite.

• [SLOW TEST:63.749 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":91,"skipped":1614,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:47:31.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
W0830 14:47:31.770678      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:47:31.792: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 30 14:47:36.812: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 30 14:47:36.812: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
W0830 14:47:37.382277      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 14:47:37.427: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2141  79dd07c0-88a7-4ac7-9e71-4999c99b5455 86236 1 2022-08-30 14:47:37 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-08-30 14:47:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027635b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 30 14:47:37.439: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Aug 30 14:47:37.439: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 30 14:47:37.440: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2141  cb796540-7a69-4198-aed3-82c798c3f407 86240 1 2022-08-30 14:47:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 79dd07c0-88a7-4ac7-9e71-4999c99b5455 0xc003bc0737 0xc003bc0738}] []  [{e2e.test Update apps/v1 2022-08-30 14:47:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 14:47:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-08-30 14:47:37 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"79dd07c0-88a7-4ac7-9e71-4999c99b5455\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003bc0808 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 14:47:37.470: INFO: Pod "test-cleanup-controller-slg7m" is available:
&Pod{ObjectMeta:{test-cleanup-controller-slg7m test-cleanup-controller- deployment-2141  1e3744b4-4771-4f6e-9091-b845184d96ed 86220 0 2022-08-30 14:47:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:4607b360c19641a9251487000e2383dc34f3119ae51390e9633ccd6b4c6f63ca cni.projectcalico.org/podIP:172.30.233.224/32 cni.projectcalico.org/podIPs:172.30.233.224/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.224"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.224"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller cb796540-7a69-4198-aed3-82c798c3f407 0xc0027638d7 0xc0027638d8}] []  [{kube-controller-manager Update v1 2022-08-30 14:47:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb796540-7a69-4198-aed3-82c798c3f407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 14:47:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 14:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 14:47:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.233.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvrmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvrmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 14:47:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 14:47:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 14:47:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 14:47:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.189,PodIP:172.30.233.224,StartTime:2022-08-30 14:47:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 14:47:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://f99f78f114b920d60c0a1b36d07d8142a496c2720087619b8701feffd14b87a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.233.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 30 14:47:37.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2141" for this suite.

• [SLOW TEST:5.933 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":92,"skipped":1629,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:47:37.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-6537
STEP: creating service affinity-nodeport-transition in namespace services-6537
STEP: creating replication controller affinity-nodeport-transition in namespace services-6537
W0830 14:47:37.781167      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-nodeport-transition" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport-transition" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport-transition" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport-transition" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 14:47:37.781262      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6537, replica count: 3
I0830 14:47:40.832433      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 14:47:40.938: INFO: Creating new exec pod
W0830 14:47:41.000314      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:47:46.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-6537 exec execpod-affinityf68st -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Aug 30 14:47:46.510: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 30 14:47:46.510: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 14:47:46.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-6537 exec execpod-affinityf68st -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.97.102 80'
Aug 30 14:47:46.930: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.97.102 80\nConnection to 172.21.97.102 80 port [tcp/http] succeeded!\n"
Aug 30 14:47:46.930: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 14:47:46.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-6537 exec execpod-affinityf68st -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.187 31179'
Aug 30 14:47:47.384: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.63.224.187 31179\nConnection to 10.63.224.187 31179 port [tcp/*] succeeded!\n"
Aug 30 14:47:47.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 14:47:47.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-6537 exec execpod-affinityf68st -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.189 31179'
Aug 30 14:47:47.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.63.224.189 31179\nConnection to 10.63.224.189 31179 port [tcp/*] succeeded!\n"
Aug 30 14:47:47.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 14:47:47.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-6537 exec execpod-affinityf68st -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.63.224.158:31179/ ; done'
Aug 30 14:47:48.518: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n"
Aug 30 14:47:48.518: INFO: stdout: "\naffinity-nodeport-transition-jhjjg\naffinity-nodeport-transition-sxtfc\naffinity-nodeport-transition-jhjjg\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-jhjjg\naffinity-nodeport-transition-jhjjg\naffinity-nodeport-transition-sxtfc\naffinity-nodeport-transition-jhjjg\naffinity-nodeport-transition-sxtfc\naffinity-nodeport-transition-jhjjg\naffinity-nodeport-transition-jhjjg\naffinity-nodeport-transition-jhjjg\naffinity-nodeport-transition-jhjjg\naffinity-nodeport-transition-sxtfc\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-jhjjg"
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-jhjjg
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-sxtfc
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-jhjjg
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-jhjjg
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-jhjjg
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-sxtfc
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-jhjjg
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-sxtfc
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-jhjjg
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-jhjjg
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-jhjjg
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-jhjjg
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-sxtfc
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:48.519: INFO: Received response from host: affinity-nodeport-transition-jhjjg
Aug 30 14:47:48.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-6537 exec execpod-affinityf68st -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.63.224.158:31179/ ; done'
Aug 30 14:47:49.155: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31179/\n"
Aug 30 14:47:49.155: INFO: stdout: "\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r\naffinity-nodeport-transition-wdw9r"
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Received response from host: affinity-nodeport-transition-wdw9r
Aug 30 14:47:49.155: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6537, will wait for the garbage collector to delete the pods
Aug 30 14:47:49.279: INFO: Deleting ReplicationController affinity-nodeport-transition took: 24.070581ms
Aug 30 14:47:49.380: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.074834ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 14:47:52.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6537" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:14.907 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":93,"skipped":1632,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:47:52.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 14:47:53.044678      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 14:47:53.077: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 30 14:47:55.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 14, 47, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 47, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 14, 47, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 14, 47, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 14:47:58.224: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:48:10.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5364" for this suite.
STEP: Destroying namespace "webhook-5364-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:18.518 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":94,"skipped":1639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:48:10.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
W0830 14:48:11.127213      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:48:11.127: INFO: Waiting up to 5m0s for pod "var-expansion-9ba6337c-4894-4465-9081-d3267aa276aa" in namespace "var-expansion-3694" to be "Succeeded or Failed"
Aug 30 14:48:11.152: INFO: Pod "var-expansion-9ba6337c-4894-4465-9081-d3267aa276aa": Phase="Pending", Reason="", readiness=false. Elapsed: 24.671821ms
Aug 30 14:48:13.165: INFO: Pod "var-expansion-9ba6337c-4894-4465-9081-d3267aa276aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038120077s
Aug 30 14:48:15.186: INFO: Pod "var-expansion-9ba6337c-4894-4465-9081-d3267aa276aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058360649s
Aug 30 14:48:17.201: INFO: Pod "var-expansion-9ba6337c-4894-4465-9081-d3267aa276aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07337461s
STEP: Saw pod success
Aug 30 14:48:17.201: INFO: Pod "var-expansion-9ba6337c-4894-4465-9081-d3267aa276aa" satisfied condition "Succeeded or Failed"
Aug 30 14:48:17.211: INFO: Trying to get logs from node 10.63.224.189 pod var-expansion-9ba6337c-4894-4465-9081-d3267aa276aa container dapi-container: <nil>
STEP: delete the pod
Aug 30 14:48:17.316: INFO: Waiting for pod var-expansion-9ba6337c-4894-4465-9081-d3267aa276aa to disappear
Aug 30 14:48:17.329: INFO: Pod var-expansion-9ba6337c-4894-4465-9081-d3267aa276aa no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 30 14:48:17.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3694" for this suite.

• [SLOW TEST:6.414 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":95,"skipped":1687,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:48:17.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 30 14:48:20.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9418" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":96,"skipped":1690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:48:20.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 30 14:48:20.818: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 14:49:21.189: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:49:21.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:49:21.487: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Aug 30 14:49:21.515: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Aug 30 14:49:21.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5952" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 30 14:49:21.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1660" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:61.427 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":97,"skipped":1714,"failed":0}
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:49:22.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0830 14:49:22.252394      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-host-aliasesfdc0b221-b830-48da-941a-5fe2b828f199" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-host-aliasesfdc0b221-b830-48da-941a-5fe2b828f199" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-host-aliasesfdc0b221-b830-48da-941a-5fe2b828f199" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-host-aliasesfdc0b221-b830-48da-941a-5fe2b828f199" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:49:22.269: INFO: The status of Pod busybox-host-aliasesfdc0b221-b830-48da-941a-5fe2b828f199 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:49:24.289: INFO: The status of Pod busybox-host-aliasesfdc0b221-b830-48da-941a-5fe2b828f199 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:49:26.289: INFO: The status of Pod busybox-host-aliasesfdc0b221-b830-48da-941a-5fe2b828f199 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 30 14:49:26.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3480" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":98,"skipped":1721,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:49:26.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8010
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
W0830 14:49:26.540915      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:49:26.554: INFO: Found 0 stateful pods, waiting for 1
Aug 30 14:49:36.585: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
W0830 14:49:36.665345      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "test-ss", "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "test-ss", "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "test-ss", "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "test-ss", "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:49:36.690: INFO: Found 1 stateful pods, waiting for 2
Aug 30 14:49:46.725: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 14:49:46.725: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 30 14:49:46.804: INFO: Deleting all statefulset in ns statefulset-8010
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 30 14:49:46.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8010" for this suite.

• [SLOW TEST:20.515 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":99,"skipped":1738,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:49:46.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-0d086fe6-430e-47c5-a986-4b6c901c8894
STEP: Creating a pod to test consume secrets
W0830 14:49:47.156502      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:49:47.157: INFO: Waiting up to 5m0s for pod "pod-secrets-debf021c-7f02-4dec-b14d-2e352b9a4f5f" in namespace "secrets-677" to be "Succeeded or Failed"
Aug 30 14:49:47.167: INFO: Pod "pod-secrets-debf021c-7f02-4dec-b14d-2e352b9a4f5f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.204115ms
Aug 30 14:49:49.197: INFO: Pod "pod-secrets-debf021c-7f02-4dec-b14d-2e352b9a4f5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039952497s
Aug 30 14:49:51.228: INFO: Pod "pod-secrets-debf021c-7f02-4dec-b14d-2e352b9a4f5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070847692s
Aug 30 14:49:53.271: INFO: Pod "pod-secrets-debf021c-7f02-4dec-b14d-2e352b9a4f5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.113979343s
STEP: Saw pod success
Aug 30 14:49:53.271: INFO: Pod "pod-secrets-debf021c-7f02-4dec-b14d-2e352b9a4f5f" satisfied condition "Succeeded or Failed"
Aug 30 14:49:53.284: INFO: Trying to get logs from node 10.63.224.158 pod pod-secrets-debf021c-7f02-4dec-b14d-2e352b9a4f5f container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 14:49:53.514: INFO: Waiting for pod pod-secrets-debf021c-7f02-4dec-b14d-2e352b9a4f5f to disappear
Aug 30 14:49:53.528: INFO: Pod pod-secrets-debf021c-7f02-4dec-b14d-2e352b9a4f5f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 30 14:49:53.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-677" for this suite.

• [SLOW TEST:6.661 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":100,"skipped":1753,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:49:53.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 30 14:49:53.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-6676 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Aug 30 14:49:53.950: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 14:49:53.950: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Aug 30 14:49:53.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-6676 delete pods e2e-test-httpd-pod'
Aug 30 14:49:59.119: INFO: stderr: ""
Aug 30 14:49:59.119: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 14:49:59.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6676" for this suite.

• [SLOW TEST:5.588 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1537
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":101,"skipped":1768,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:49:59.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Aug 30 14:49:59.314: INFO: namespace kubectl-4213
Aug 30 14:49:59.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-4213 create -f -'
Aug 30 14:49:59.784: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"agnhost-primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"agnhost-primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"agnhost-primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"agnhost-primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 14:49:59.784: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 30 14:50:00.797: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 14:50:00.798: INFO: Found 0 / 1
Aug 30 14:50:01.799: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 14:50:01.799: INFO: Found 0 / 1
Aug 30 14:50:02.801: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 14:50:02.801: INFO: Found 1 / 1
Aug 30 14:50:02.801: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 30 14:50:02.818: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 14:50:02.819: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 30 14:50:02.819: INFO: wait on agnhost-primary startup in kubectl-4213 
Aug 30 14:50:02.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-4213 logs agnhost-primary-ddzjj agnhost-primary'
Aug 30 14:50:02.964: INFO: stderr: ""
Aug 30 14:50:02.964: INFO: stdout: "Paused\n"
STEP: exposing RC
Aug 30 14:50:02.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-4213 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 30 14:50:03.129: INFO: stderr: ""
Aug 30 14:50:03.129: INFO: stdout: "service/rm2 exposed\n"
Aug 30 14:50:03.146: INFO: Service rm2 in namespace kubectl-4213 found.
STEP: exposing service
Aug 30 14:50:05.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-4213 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 30 14:50:05.415: INFO: stderr: ""
Aug 30 14:50:05.415: INFO: stdout: "service/rm3 exposed\n"
Aug 30 14:50:05.446: INFO: Service rm3 in namespace kubectl-4213 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 14:50:07.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4213" for this suite.

• [SLOW TEST:8.362 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":102,"skipped":1778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:50:07.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-718/secret-test-f13b5b40-3830-4599-b7ec-a222a03d3aa2
STEP: Creating a pod to test consume secrets
Aug 30 14:50:07.775: INFO: Waiting up to 5m0s for pod "pod-configmaps-83f48538-a382-41cf-bf28-c6933974b829" in namespace "secrets-718" to be "Succeeded or Failed"
W0830 14:50:07.774662      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:50:07.792: INFO: Pod "pod-configmaps-83f48538-a382-41cf-bf28-c6933974b829": Phase="Pending", Reason="", readiness=false. Elapsed: 17.443645ms
Aug 30 14:50:09.813: INFO: Pod "pod-configmaps-83f48538-a382-41cf-bf28-c6933974b829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038159299s
Aug 30 14:50:11.835: INFO: Pod "pod-configmaps-83f48538-a382-41cf-bf28-c6933974b829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059970477s
STEP: Saw pod success
Aug 30 14:50:11.835: INFO: Pod "pod-configmaps-83f48538-a382-41cf-bf28-c6933974b829" satisfied condition "Succeeded or Failed"
Aug 30 14:50:11.854: INFO: Trying to get logs from node 10.63.224.187 pod pod-configmaps-83f48538-a382-41cf-bf28-c6933974b829 container env-test: <nil>
STEP: delete the pod
Aug 30 14:50:12.112: INFO: Waiting for pod pod-configmaps-83f48538-a382-41cf-bf28-c6933974b829 to disappear
Aug 30 14:50:12.130: INFO: Pod pod-configmaps-83f48538-a382-41cf-bf28-c6933974b829 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 30 14:50:12.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-718" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":103,"skipped":1814,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:50:12.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Aug 30 14:50:21.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7763" for this suite.
STEP: Destroying namespace "nsdeletetest-9332" for this suite.
Aug 30 14:50:21.983: INFO: Namespace nsdeletetest-9332 was already deleted
STEP: Destroying namespace "nsdeletetest-694" for this suite.

• [SLOW TEST:9.742 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":104,"skipped":1833,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:50:22.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-10211ea7-b4b6-446e-a176-f2a13d1ba069
STEP: Creating a pod to test consume configMaps
W0830 14:50:22.323855      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:50:22.324: INFO: Waiting up to 5m0s for pod "pod-configmaps-fe7a74f1-af58-496b-9d9a-bf2e9b71d37f" in namespace "configmap-7016" to be "Succeeded or Failed"
Aug 30 14:50:22.345: INFO: Pod "pod-configmaps-fe7a74f1-af58-496b-9d9a-bf2e9b71d37f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.219456ms
Aug 30 14:50:24.402: INFO: Pod "pod-configmaps-fe7a74f1-af58-496b-9d9a-bf2e9b71d37f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07813277s
Aug 30 14:50:26.420: INFO: Pod "pod-configmaps-fe7a74f1-af58-496b-9d9a-bf2e9b71d37f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095522822s
Aug 30 14:50:28.436: INFO: Pod "pod-configmaps-fe7a74f1-af58-496b-9d9a-bf2e9b71d37f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.112103904s
STEP: Saw pod success
Aug 30 14:50:28.436: INFO: Pod "pod-configmaps-fe7a74f1-af58-496b-9d9a-bf2e9b71d37f" satisfied condition "Succeeded or Failed"
Aug 30 14:50:28.450: INFO: Trying to get logs from node 10.63.224.189 pod pod-configmaps-fe7a74f1-af58-496b-9d9a-bf2e9b71d37f container configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 14:50:28.546: INFO: Waiting for pod pod-configmaps-fe7a74f1-af58-496b-9d9a-bf2e9b71d37f to disappear
Aug 30 14:50:28.559: INFO: Pod pod-configmaps-fe7a74f1-af58-496b-9d9a-bf2e9b71d37f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 14:50:28.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7016" for this suite.

• [SLOW TEST:6.589 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":105,"skipped":1843,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:50:28.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0830 14:50:28.837137      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 30 14:50:30.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3697" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":106,"skipped":1866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:50:30.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 30 14:50:31.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9362" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":107,"skipped":1888,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:50:31.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
W0830 14:50:38.563676      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-rs" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-rs" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-rs" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-rs" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 30 14:50:42.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4543" for this suite.

• [SLOW TEST:11.399 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":108,"skipped":1889,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:50:42.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:50:42.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Aug 30 14:50:52.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-9398 --namespace=crd-publish-openapi-9398 create -f -'
Aug 30 14:50:54.440: INFO: stderr: ""
Aug 30 14:50:54.440: INFO: stdout: "e2e-test-crd-publish-openapi-1881-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 30 14:50:54.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-9398 --namespace=crd-publish-openapi-9398 delete e2e-test-crd-publish-openapi-1881-crds test-cr'
Aug 30 14:50:54.607: INFO: stderr: ""
Aug 30 14:50:54.607: INFO: stdout: "e2e-test-crd-publish-openapi-1881-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 30 14:50:54.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-9398 --namespace=crd-publish-openapi-9398 apply -f -'
Aug 30 14:50:56.665: INFO: stderr: ""
Aug 30 14:50:56.665: INFO: stdout: "e2e-test-crd-publish-openapi-1881-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 30 14:50:56.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-9398 --namespace=crd-publish-openapi-9398 delete e2e-test-crd-publish-openapi-1881-crds test-cr'
Aug 30 14:50:56.824: INFO: stderr: ""
Aug 30 14:50:56.824: INFO: stdout: "e2e-test-crd-publish-openapi-1881-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 30 14:50:56.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-9398 explain e2e-test-crd-publish-openapi-1881-crds'
Aug 30 14:50:58.138: INFO: stderr: ""
Aug 30 14:50:58.138: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1881-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:51:07.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9398" for this suite.

• [SLOW TEST:25.239 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":109,"skipped":1890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:51:07.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 14:51:08.127285      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:51:08.127: INFO: Waiting up to 5m0s for pod "downwardapi-volume-305ed29c-58b6-4432-9dee-90684929cf6c" in namespace "projected-3141" to be "Succeeded or Failed"
Aug 30 14:51:08.139: INFO: Pod "downwardapi-volume-305ed29c-58b6-4432-9dee-90684929cf6c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.715185ms
Aug 30 14:51:10.151: INFO: Pod "downwardapi-volume-305ed29c-58b6-4432-9dee-90684929cf6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024499708s
Aug 30 14:51:12.168: INFO: Pod "downwardapi-volume-305ed29c-58b6-4432-9dee-90684929cf6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040681342s
Aug 30 14:51:14.196: INFO: Pod "downwardapi-volume-305ed29c-58b6-4432-9dee-90684929cf6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068782084s
STEP: Saw pod success
Aug 30 14:51:14.196: INFO: Pod "downwardapi-volume-305ed29c-58b6-4432-9dee-90684929cf6c" satisfied condition "Succeeded or Failed"
Aug 30 14:51:14.211: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-305ed29c-58b6-4432-9dee-90684929cf6c container client-container: <nil>
STEP: delete the pod
Aug 30 14:51:14.316: INFO: Waiting for pod downwardapi-volume-305ed29c-58b6-4432-9dee-90684929cf6c to disappear
Aug 30 14:51:14.331: INFO: Pod downwardapi-volume-305ed29c-58b6-4432-9dee-90684929cf6c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 14:51:14.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3141" for this suite.

• [SLOW TEST:6.458 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":110,"skipped":1930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:51:14.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-6xc9
STEP: Creating a pod to test atomic-volume-subpath
W0830 14:51:14.624154      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container-subpath-projected-6xc9" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-projected-6xc9" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-projected-6xc9" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-projected-6xc9" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:51:14.624: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6xc9" in namespace "subpath-9153" to be "Succeeded or Failed"
Aug 30 14:51:14.644: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Pending", Reason="", readiness=false. Elapsed: 19.996207ms
Aug 30 14:51:16.668: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043717739s
Aug 30 14:51:18.685: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=true. Elapsed: 4.060443166s
Aug 30 14:51:20.702: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=true. Elapsed: 6.07721714s
Aug 30 14:51:22.720: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=true. Elapsed: 8.09595417s
Aug 30 14:51:24.741: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=true. Elapsed: 10.116209268s
Aug 30 14:51:26.766: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=true. Elapsed: 12.141508052s
Aug 30 14:51:28.785: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=true. Elapsed: 14.160787075s
Aug 30 14:51:30.808: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=true. Elapsed: 16.183270258s
Aug 30 14:51:32.829: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=true. Elapsed: 18.20490641s
Aug 30 14:51:34.843: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=true. Elapsed: 20.21896302s
Aug 30 14:51:36.865: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=false. Elapsed: 22.240715556s
Aug 30 14:51:38.903: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Running", Reason="", readiness=false. Elapsed: 24.278896072s
Aug 30 14:51:40.927: INFO: Pod "pod-subpath-test-projected-6xc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.302318625s
STEP: Saw pod success
Aug 30 14:51:40.927: INFO: Pod "pod-subpath-test-projected-6xc9" satisfied condition "Succeeded or Failed"
Aug 30 14:51:40.940: INFO: Trying to get logs from node 10.63.224.189 pod pod-subpath-test-projected-6xc9 container test-container-subpath-projected-6xc9: <nil>
STEP: delete the pod
Aug 30 14:51:41.032: INFO: Waiting for pod pod-subpath-test-projected-6xc9 to disappear
Aug 30 14:51:41.044: INFO: Pod pod-subpath-test-projected-6xc9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-6xc9
Aug 30 14:51:41.044: INFO: Deleting pod "pod-subpath-test-projected-6xc9" in namespace "subpath-9153"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 30 14:51:41.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9153" for this suite.

• [SLOW TEST:26.710 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":111,"skipped":1953,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:51:41.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 30 14:51:41.282: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92af6b3a-8a1d-476b-8c09-4606570604bd" in namespace "downward-api-4509" to be "Succeeded or Failed"
W0830 14:51:41.281856      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:51:41.299: INFO: Pod "downwardapi-volume-92af6b3a-8a1d-476b-8c09-4606570604bd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.081814ms
Aug 30 14:51:43.318: INFO: Pod "downwardapi-volume-92af6b3a-8a1d-476b-8c09-4606570604bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035276263s
Aug 30 14:51:45.339: INFO: Pod "downwardapi-volume-92af6b3a-8a1d-476b-8c09-4606570604bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057158411s
Aug 30 14:51:47.395: INFO: Pod "downwardapi-volume-92af6b3a-8a1d-476b-8c09-4606570604bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.112862376s
STEP: Saw pod success
Aug 30 14:51:47.395: INFO: Pod "downwardapi-volume-92af6b3a-8a1d-476b-8c09-4606570604bd" satisfied condition "Succeeded or Failed"
Aug 30 14:51:47.412: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-92af6b3a-8a1d-476b-8c09-4606570604bd container client-container: <nil>
STEP: delete the pod
Aug 30 14:51:47.544: INFO: Waiting for pod downwardapi-volume-92af6b3a-8a1d-476b-8c09-4606570604bd to disappear
Aug 30 14:51:47.561: INFO: Pod downwardapi-volume-92af6b3a-8a1d-476b-8c09-4606570604bd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 14:51:47.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4509" for this suite.

• [SLOW TEST:6.521 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":112,"skipped":1973,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:51:47.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Aug 30 14:51:47.837: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Aug 30 14:51:47.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2258" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":113,"skipped":1978,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:51:48.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Aug 30 14:51:48.121: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 14:52:48.443: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:52:48.466: INFO: Starting informer...
STEP: Starting pod...
W0830 14:52:48.560046      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:52:48.775: INFO: Pod is running on 10.63.224.189. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 30 14:52:48.856: INFO: Pod wasn't evicted. Proceeding
Aug 30 14:52:48.856: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 30 14:54:03.946: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Aug 30 14:54:03.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7277" for this suite.

• [SLOW TEST:135.970 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":114,"skipped":1988,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:54:03.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 30 14:54:10.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5025" for this suite.

• [SLOW TEST:6.392 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":115,"skipped":1994,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:54:10.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
W0830 14:54:10.569585      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:54:10.583: INFO: Waiting up to 5m0s for pod "pod-a20dbbd5-2712-47df-8b88-9c48a5285149" in namespace "emptydir-3760" to be "Succeeded or Failed"
Aug 30 14:54:10.597: INFO: Pod "pod-a20dbbd5-2712-47df-8b88-9c48a5285149": Phase="Pending", Reason="", readiness=false. Elapsed: 14.245761ms
Aug 30 14:54:12.613: INFO: Pod "pod-a20dbbd5-2712-47df-8b88-9c48a5285149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02972614s
Aug 30 14:54:14.636: INFO: Pod "pod-a20dbbd5-2712-47df-8b88-9c48a5285149": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052534555s
Aug 30 14:54:16.653: INFO: Pod "pod-a20dbbd5-2712-47df-8b88-9c48a5285149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069515563s
STEP: Saw pod success
Aug 30 14:54:16.653: INFO: Pod "pod-a20dbbd5-2712-47df-8b88-9c48a5285149" satisfied condition "Succeeded or Failed"
Aug 30 14:54:16.663: INFO: Trying to get logs from node 10.63.224.189 pod pod-a20dbbd5-2712-47df-8b88-9c48a5285149 container test-container: <nil>
STEP: delete the pod
Aug 30 14:54:16.786: INFO: Waiting for pod pod-a20dbbd5-2712-47df-8b88-9c48a5285149 to disappear
Aug 30 14:54:16.824: INFO: Pod pod-a20dbbd5-2712-47df-8b88-9c48a5285149 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 14:54:16.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3760" for this suite.

• [SLOW TEST:6.510 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":116,"skipped":2007,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:54:16.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3603.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3603.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3603.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3603.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
W0830 14:54:17.153450      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 14:54:21.320: INFO: DNS probes using dns-3603/dns-test-46d17c72-872d-46da-a475-29d6501b2a80 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 30 14:54:21.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3603" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":117,"skipped":2027,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:54:21.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 30 14:54:28.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1545" for this suite.

• [SLOW TEST:7.223 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":118,"skipped":2048,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:54:28.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:54:28.800: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-34e22b99-bf9f-4006-9b36-fb8c5da4ed48
STEP: Creating secret with name s-test-opt-upd-8f9a461f-6be4-40ae-9271-63982a5f5785
STEP: Creating the pod
W0830 14:54:28.968525      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:54:28.990: INFO: The status of Pod pod-secrets-088d585c-1261-4c18-97c3-67fd7dbc5556 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:54:31.009: INFO: The status of Pod pod-secrets-088d585c-1261-4c18-97c3-67fd7dbc5556 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:54:33.006: INFO: The status of Pod pod-secrets-088d585c-1261-4c18-97c3-67fd7dbc5556 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-34e22b99-bf9f-4006-9b36-fb8c5da4ed48
STEP: Updating secret s-test-opt-upd-8f9a461f-6be4-40ae-9271-63982a5f5785
STEP: Creating secret with name s-test-opt-create-4e4222d9-57c9-4805-96bf-ac3840b0e751
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 30 14:55:48.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4849" for this suite.

• [SLOW TEST:80.276 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":119,"skipped":2065,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:55:48.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:55:49.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Aug 30 14:55:58.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-3189 --namespace=crd-publish-openapi-3189 create -f -'
Aug 30 14:55:59.779: INFO: stderr: ""
Aug 30 14:55:59.779: INFO: stdout: "e2e-test-crd-publish-openapi-2815-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 30 14:55:59.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-3189 --namespace=crd-publish-openapi-3189 delete e2e-test-crd-publish-openapi-2815-crds test-cr'
Aug 30 14:55:59.978: INFO: stderr: ""
Aug 30 14:55:59.978: INFO: stdout: "e2e-test-crd-publish-openapi-2815-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 30 14:55:59.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-3189 --namespace=crd-publish-openapi-3189 apply -f -'
Aug 30 14:56:01.831: INFO: stderr: ""
Aug 30 14:56:01.831: INFO: stdout: "e2e-test-crd-publish-openapi-2815-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 30 14:56:01.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-3189 --namespace=crd-publish-openapi-3189 delete e2e-test-crd-publish-openapi-2815-crds test-cr'
Aug 30 14:56:02.043: INFO: stderr: ""
Aug 30 14:56:02.043: INFO: stdout: "e2e-test-crd-publish-openapi-2815-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 30 14:56:02.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-3189 explain e2e-test-crd-publish-openapi-2815-crds'
Aug 30 14:56:02.567: INFO: stderr: ""
Aug 30 14:56:02.567: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2815-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:56:12.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3189" for this suite.

• [SLOW TEST:23.957 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":120,"skipped":2065,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:56:12.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
W0830 14:56:13.106694      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 30 14:56:19.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4225" for this suite.

• [SLOW TEST:6.406 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":121,"skipped":2134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:56:19.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 14:56:19.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-9608 create -f -'
Aug 30 14:56:20.200: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"agnhost-primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"agnhost-primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"agnhost-primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"agnhost-primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 14:56:20.200: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 30 14:56:20.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-9608 create -f -'
Aug 30 14:56:21.861: INFO: stderr: ""
Aug 30 14:56:21.861: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 30 14:56:22.898: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 14:56:22.898: INFO: Found 1 / 1
Aug 30 14:56:22.899: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 30 14:56:22.914: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 14:56:22.914: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 30 14:56:22.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-9608 describe pod agnhost-primary-swq6j'
Aug 30 14:56:23.047: INFO: stderr: ""
Aug 30 14:56:23.047: INFO: stdout: "Name:         agnhost-primary-swq6j\nNamespace:    kubectl-9608\nPriority:     0\nNode:         10.63.224.189/10.63.224.189\nStart Time:   Tue, 30 Aug 2022 14:56:20 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 0c6b5d7f8875b205faa2da2438ab47adcacbc4ea9bc34ecb2693450ae075ca74\n              cni.projectcalico.org/podIP: 172.30.233.232/32\n              cni.projectcalico.org/podIPs: 172.30.233.232/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.233.232\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.233.232\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.30.233.232\nIPs:\n  IP:           172.30.233.232\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://0c7c1eab662eb98d44804823af3114936436d923f60b044b2ec5cafb9f2fe437\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.36\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 30 Aug 2022 14:56:22 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z5bjc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-z5bjc:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-9608/agnhost-primary-swq6j to 10.63.224.189 by kube-scheduler-66b794b989-b5tfz\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.233.232/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.36\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Aug 30 14:56:23.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-9608 describe rc agnhost-primary'
Aug 30 14:56:23.267: INFO: stderr: ""
Aug 30 14:56:23.267: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9608\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.36\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-swq6j\n"
Aug 30 14:56:23.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-9608 describe service agnhost-primary'
Aug 30 14:56:23.454: INFO: stderr: ""
Aug 30 14:56:23.454: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9608\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.216.14\nIPs:               172.21.216.14\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.233.232:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 30 14:56:23.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-9608 describe node 10.63.224.158'
Aug 30 14:56:24.240: INFO: stderr: ""
Aug 30 14:56:24.240: INFO: stdout: "Name:               10.63.224.158\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=au-syd\n                    failure-domain.beta.kubernetes.io/zone=syd04\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=130.198.120.151\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.63.224.158\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=au-syd\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cc6vecns0e8mkrruk1rg-kubee2epvgt-default-00000283\n                    ibm-cloud.kubernetes.io/worker-pool-id=cc6vecns0e8mkrruk1rg-1f70c31\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.11.1_1520_openshift\n                    ibm-cloud.kubernetes.io/zone=syd04\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.63.224.158\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2723070\n                    publicVLAN=2723068\n                    topology.kubernetes.io/region=au-syd\n                    topology.kubernetes.io/zone=syd04\nAnnotations:        projectcalico.org/IPv4Address: 10.63.224.158/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.78.0\nCreationTimestamp:  Tue, 30 Aug 2022 12:07:36 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.63.224.158\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 30 Aug 2022 14:56:17 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 30 Aug 2022 12:09:04 +0000   Tue, 30 Aug 2022 12:09:04 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 30 Aug 2022 14:53:35 +0000   Tue, 30 Aug 2022 12:07:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 30 Aug 2022 14:53:35 +0000   Tue, 30 Aug 2022 12:07:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 30 Aug 2022 14:53:35 +0000   Tue, 30 Aug 2022 12:07:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 30 Aug 2022 14:53:35 +0000   Tue, 30 Aug 2022 12:09:18 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.63.224.158\n  ExternalIP:  130.198.120.151\n  Hostname:    10.63.224.158\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102671288Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16386596Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93973048857\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13597220Ki\n  pods:               110\nSystem Info:\n  Machine ID:                                       d4e01dc89b8b4f4c9fe3b73e636cc419\n  System UUID:                                      8265f055-4293-def0-b715-57b69434c8d9\n  Boot ID:                                          621b9fde-de1f-4a12-94fe-30a0a7d79d08\n  Kernel Version:                                   4.18.0-372.19.1.el8_6.x86_64\n  OS Image:                                         Red Hat Enterprise Linux 8.6 (Ootpa)\n  Operating System:                                 linux\n  Architecture:                                     amd64\n  Container Runtime Version:                        cri-o://1.24.2-4.rhaos4.11.gitd6283df.el8\n  Kubelet Version:                                  v1.24.0+4f0dd4d\n  Kube-Proxy Version:                               v1.24.0+4f0dd4d\nPodCIDR:                                            172.30.1.0/24\nPodCIDRs:                                           172.30.1.0/24\nProviderID:                                         ibm://fee034388aa6435883a1f720010ab3a2///cc6vecns0e8mkrruk1rg/kube-cc6vecns0e8mkrruk1rg-kubee2epvgt-default-00000283\nNon-terminated Pods:                                (54 in total)\n  Namespace                                         Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                         ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                                     calico-kube-controllers-867bb5b44d-wqzpr                   10m (0%)      0 (0%)      25Mi (0%)        3Gi (23%)      167m\n  calico-system                                     calico-node-rfpmd                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         167m\n  ibm-system                                        ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-q4rqr      5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         159m\n  kube-system                                       ibm-file-plugin-ccc878f48-frwb7                            50m (1%)      200m (5%)   100Mi (0%)       200Mi (1%)     178m\n  kube-system                                       ibm-keepalived-watcher-c2b7b                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         168m\n  kube-system                                       ibm-master-proxy-static-10.63.224.158                      26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      167m\n  kube-system                                       ibm-storage-watcher-6fcfdb7ccc-5s2xq                       50m (1%)      200m (5%)   100Mi (0%)       200Mi (1%)     178m\n  kube-system                                       ibmcloud-block-storage-driver-kx4tx                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     168m\n  kube-system                                       ibmcloud-block-storage-plugin-7495f48b76-lht72             50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     178m\n  kube-system                                       vpn-59795d4f7c-rf6fw                                       5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         157m\n  openshift-cluster-node-tuning-operator            cluster-node-tuning-operator-6c4d4b46dd-dsdts              10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h7m\n  openshift-cluster-node-tuning-operator            tuned-mpvrs                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         163m\n  openshift-cluster-samples-operator                cluster-samples-operator-79cb65b9b5-jbclm                  20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h7m\n  openshift-cluster-storage-operator                cluster-storage-operator-97dcf6b44-krf6f                   10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h7m\n  openshift-cluster-storage-operator                csi-snapshot-controller-7d8bf4bb58-6lkk4                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3m35s\n  openshift-cluster-storage-operator                csi-snapshot-controller-operator-7b97dc6dfd-xjp62          10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         3h7m\n  openshift-cluster-storage-operator                csi-snapshot-webhook-8f4fd6cc6-48czg                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3m35s\n  openshift-console-operator                        console-operator-55bcdb7b48-tgxd4                          10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h7m\n  openshift-console                                 console-5dd7474d84-js5g9                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         162m\n  openshift-dns-operator                            dns-operator-5549dbd7c9-c4h6t                              20m (0%)      0 (0%)      69Mi (0%)        0 (0%)         3h7m\n  openshift-dns                                     dns-default-v6jl9                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         163m\n  openshift-dns                                     node-resolver-ppzz9                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         163m\n  openshift-image-registry                          cluster-image-registry-operator-cb6448756-qh2wd            10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h7m\n  openshift-image-registry                          image-registry-7fb4f45578-scb5q                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         163m\n  openshift-image-registry                          node-ca-4pbx7                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         163m\n  openshift-ingress-canary                          ingress-canary-vwqf6                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         163m\n  openshift-ingress-operator                        ingress-operator-57457886bd-v8vkg                          20m (0%)      0 (0%)      96Mi (0%)        0 (0%)         3h7m\n  openshift-ingress                                 router-default-bdd65b74f-jwc99                             100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         3m35s\n  openshift-insights                                insights-operator-6d9b46b7c5-6q6pl                         10m (0%)      0 (0%)      30Mi (0%)        0 (0%)         3h7m\n  openshift-kube-proxy                              openshift-kube-proxy-ms7hb                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         168m\n  openshift-kube-storage-version-migrator-operator  kube-storage-version-migrator-operator-fb46b8c8d-n5q2j     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h7m\n  openshift-marketplace                             marketplace-operator-84ff65f9c9-mcwn4                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h7m\n  openshift-monitoring                              cluster-monitoring-operator-677fb4cf4b-wfrk9               11m (0%)      0 (0%)      95Mi (0%)        0 (0%)         3h7m\n  openshift-monitoring                              kube-state-metrics-b6455c4dc-pknn6                         4m (0%)       0 (0%)      110Mi (0%)       0 (0%)         162m\n  openshift-monitoring                              node-exporter-ct2nk                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         162m\n  openshift-monitoring                              prometheus-adapter-67874b74f7-2kkcq                        1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         161m\n  openshift-monitoring                              prometheus-operator-admission-webhook-f5f88b968-wkmnq      5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         3m35s\n  openshift-monitoring                              thanos-querier-75bdcf8599-tdj9h                            15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         3m35s\n  openshift-multus                                  multus-additional-cni-plugins-fwxxj                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         168m\n  openshift-multus                                  multus-admission-controller-gvnw7                          20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         167m\n  openshift-multus                                  multus-n2s54                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         168m\n  openshift-multus                                  network-metrics-daemon-ncgdn                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         168m\n  openshift-network-diagnostics                     network-check-source-5cb989cf6f-8pbqc                      10m (0%)      0 (0%)      40Mi (0%)        0 (0%)         168m\n  openshift-network-diagnostics                     network-check-target-tqzgf                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         168m\n  openshift-operator-lifecycle-manager              catalog-operator-5d9dd4bb98-hwstf                          10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         3h7m\n  openshift-operator-lifecycle-manager              olm-operator-757497677b-z5lbc                              10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         3h7m\n  openshift-operator-lifecycle-manager              package-server-manager-784548687f-9vg2b                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h7m\n  openshift-operator-lifecycle-manager              packageserver-b5cbb5ff5-2vxb9                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         163m\n  openshift-roks-metrics                            metrics-67dbdb4ffd-gj9lp                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h7m\n  openshift-roks-metrics                            push-gateway-58dc4cbdf8-kl5x7                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h7m\n  openshift-service-ca-operator                     service-ca-operator-5df8fbb45b-fz8bw                       10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         3h7m\n  sonobuoy                                          sonobuoy-e2e-job-a8c1c5c47c2346af                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         30m\n  sonobuoy                                          sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-fngwd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         30m\n  tigera-operator                                   tigera-operator-7b76886f74-5rs97                           100m (2%)     0 (0%)      40Mi (0%)        0 (0%)         179m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1371m (35%)      1300m (33%)\n  memory             3704339Ki (27%)  4669728Ki (34%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:\n  Type     Reason                                            Age                  From                   Message\n  ----     ------                                            ----                 ----                   -------\n  Warning  listen tcp4 :31179: bind: address already in use  8m46s                kube-proxy             can't open port \"nodePort for services-6537/affinity-nodeport-transition\" (:31179/tcp4), skipping it\n  Warning  listen tcp4 :30904: bind: address already in use  15m                  kube-proxy             can't open port \"nodePort for services-8122/affinity-nodeport\" (:30904/tcp4), skipping it\n  Warning  listen tcp4 :32419: bind: address already in use  28m                  kube-proxy             can't open port \"nodePort for services-3679/nodeport-test:http\" (:32419/tcp4), skipping it\n  Warning  listen tcp4 :30244: bind: address already in use  67m                  kube-proxy             can't open port \"nodePort for default/ha-apps-across-workers-59c9b1ab72be4c90995be318fa9155ad-svc\" (:30244/tcp4), skipping it\n  Warning  listen tcp4 :32343: bind: address already in use  69m                  kube-proxy             can't open port \"nodePort for default/load-tester-b6516d1628584e649c6177f8edfed970-svc\" (:32343/tcp4), skipping it\n  Warning  listen tcp4 :30626: bind: address already in use  164m                 kube-proxy             can't open port \"nodePort for openshift-ingress/router-default:http\" (:30626/tcp4), skipping it\n  Warning  listen tcp4 :31648: bind: address already in use  70m                  kube-proxy             can't open port \"nodePort for default/rolling-deploy-ff3ed71da7d748eba6aa921a9d37efc1-svc\" (:31648/tcp4), skipping it\n  Warning  listen tcp4 :32610: bind: address already in use  70m                  kube-proxy             can't open port \"nodePort for default/scale-d8e972da51bb4bf7baf176ce797470d5-svc\" (:32610/tcp4), skipping it\n  Normal   Starting                                          168m                 kube-proxy             \n  Warning  listen tcp4 :31698: bind: address already in use  164m                 kube-proxy             can't open port \"nodePort for openshift-ingress/router-default:https\" (:31698/tcp4), skipping it\n  Warning  listen tcp4 :31561: bind: address already in use  78m                  kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-9:port-1\" (:31561/tcp4), skipping it\n  Warning  listen tcp4 :32324: bind: address already in use  140m                 kube-proxy             can't open port \"nodePort for long-validation-test-ns-01/validate\" (:32324/tcp4), skipping it\n  Warning  listen tcp4 :32360: bind: address already in use  139m                 kube-proxy             can't open port \"nodePort for long-validation-test-ns-02/validate\" (:32360/tcp4), skipping it\n  Warning  listen tcp4 :32294: bind: address already in use  138m                 kube-proxy             can't open port \"nodePort for long-validation-test-ns-03/validate\" (:32294/tcp4), skipping it\n  Warning  listen tcp4 :30367: bind: address already in use  138m                 kube-proxy             can't open port \"nodePort for long-validation-test-ns-04/validate\" (:30367/tcp4), skipping it\n  Warning  listen tcp4 :31262: bind: address already in use  138m                 kube-proxy             can't open port \"nodePort for long-validation-test-ns-05/validate\" (:31262/tcp4), skipping it\n  Warning  listen tcp4 :32644: bind: address already in use  134m                 kube-proxy             can't open port \"nodePort for network-test-ns/network-test\" (:32644/tcp4), skipping it\n  Warning  listen tcp4 :32322: bind: address already in use  126m                 kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-1:port-1\" (:32322/tcp4), skipping it\n  Warning  listen tcp4 :31942: bind: address already in use  123m                 kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-12:port-1\" (:31942/tcp4), skipping it\n  Warning  listen tcp4 :30108: bind: address already in use  122m                 kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-2:port-1\" (:30108/tcp4), skipping it\n  Warning  listen tcp4 :30612: bind: address already in use  120m                 kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-2:port-1\" (:30612/tcp4), skipping it\n  Warning  listen tcp4 :31903: bind: address already in use  82m                  kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-7:port-1\" (:31903/tcp4), skipping it\n  Warning  listen udp4 :31092: bind: address already in use  113m                 kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-5:port-1\" (:31092/udp4), skipping it\n  Warning  listen tcp4 :32158: bind: address already in use  110m                 kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-7:port-1\" (:32158/tcp4), skipping it\n  Warning  listen tcp4 :30250: bind: address already in use  108m                 kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-8:port-1\" (:30250/tcp4), skipping it\n  Warning  listen tcp4 :32677: bind: address already in use  107m                 kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-9:port-1\" (:32677/tcp4), skipping it\n  Warning  listen tcp4 :30298: bind: address already in use  100m                 kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-1:port-1\" (:30298/tcp4), skipping it\n  Warning  listen tcp4 :31431: bind: address already in use  98m                  kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-12:port-1\" (:31431/tcp4), skipping it\n  Warning  listen tcp4 :30268: bind: address already in use  96m                  kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-2:port-1\" (:30268/tcp4), skipping it\n  Warning  listen tcp4 :31461: bind: address already in use  94m                  kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-2:port-1\" (:31461/tcp4), skipping it\n  Warning  listen tcp4 :31749: bind: address already in use  85m                  kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-3:port-1\" (:31749/tcp4), skipping it\n  Warning  listen udp4 :31000: bind: address already in use  84m                  kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-5:port-1\" (:31000/udp4), skipping it\n  Warning  listen tcp4 :30288: bind: address already in use  113m                 kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-original/pvg-test-basic-lb-cruiser-3:port-1\" (:30288/tcp4), skipping it\n  Warning  listen tcp4 :32363: bind: address already in use  79m                  kube-proxy             can't open port \"nodePort for pvg-armada-lb-cruiser-ipvs/pvg-test-basic-lb-cruiser-8:port-1\" (:32363/tcp4), skipping it\n  Normal   NodeAllocatableEnforced                           168m                 kubelet                Updated Node Allocatable limit across pods\n  Normal   Starting                                          168m                 kubelet                Starting kubelet.\n  Normal   NodeHasSufficientPID                              168m (x7 over 168m)  kubelet                Node 10.63.224.158 status is now: NodeHasSufficientPID\n  Normal   NodeHasSufficientMemory                           168m (x8 over 168m)  kubelet                Node 10.63.224.158 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure                             168m (x8 over 168m)  kubelet                Node 10.63.224.158 status is now: NodeHasNoDiskPressure\n  Normal   Synced                                            168m                 cloud-node-controller  Node synced successfully\n  Normal   RegisteredNode                                    168m                 node-controller        Node 10.63.224.158 event: Registered Node 10.63.224.158 in Controller\n  Normal   RegisteredNode                                    163m                 node-controller        Node 10.63.224.158 event: Registered Node 10.63.224.158 in Controller\n"
Aug 30 14:56:24.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-9608 describe namespace kubectl-9608'
Aug 30 14:56:24.397: INFO: stderr: ""
Aug 30 14:56:24.397: INFO: stdout: "Name:         kubectl-9608\nLabels:       e2e-framework=kubectl\n              e2e-run=32f79c61-eaf9-42e7-98c1-4ec0a998fff3\n              kubernetes.io/metadata.name=kubectl-9608\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  openshift.io/sa.scc.mcs: s0:c46,c20\n              openshift.io/sa.scc.supplemental-groups: 1002110000/10000\n              openshift.io/sa.scc.uid-range: 1002110000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 14:56:24.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9608" for this suite.

• [SLOW TEST:5.112 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1110
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":122,"skipped":2166,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:56:24.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
W0830 14:56:24.696096      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods launch on every node of the cluster.
Aug 30 14:56:24.736: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:56:24.736: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:56:25.821: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 14:56:25.821: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:56:26.803: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 14:56:26.803: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 14:56:27.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 14:56:27.787: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Aug 30 14:56:27.922: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"92708"},"items":null}

Aug 30 14:56:27.943: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"92708"},"items":[{"metadata":{"name":"daemon-set-66l4j","generateName":"daemon-set-","namespace":"daemonsets-777","uid":"9712d5af-e320-43fa-af7f-89a14cb179fb","resourceVersion":"92705","creationTimestamp":"2022-08-30T14:56:24Z","deletionTimestamp":"2022-08-30T14:56:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3d1f566c62df670782cc0a211ef1a33b155f575af88ec5de1a6efb58c532b52a","cni.projectcalico.org/podIP":"172.30.233.233/32","cni.projectcalico.org/podIPs":"172.30.233.233/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.233.233\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.233.233\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"861cd72a-9160-4abd-ae6e-db3bd294db90","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"861cd72a-9160-4abd-ae6e-db3bd294db90\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.233.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fllz7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fllz7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.63.224.189","securityContext":{"seLinuxOptions":{"level":"s0:c46,c25"}},"imagePullSecrets":[{"name":"default-dockercfg-j5vgl"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.63.224.189"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:24Z"}],"hostIP":"10.63.224.189","podIP":"172.30.233.233","podIPs":[{"ip":"172.30.233.233"}],"startTime":"2022-08-30T14:56:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-30T14:56:26Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://b4f03fd9bb9b8ee63d98dcce603bc2d1fa202435404d64c621a8b068435f7390","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-p2d7v","generateName":"daemon-set-","namespace":"daemonsets-777","uid":"13d6f552-266c-405a-bc5e-fdfe549d8090","resourceVersion":"92707","creationTimestamp":"2022-08-30T14:56:24Z","deletionTimestamp":"2022-08-30T14:56:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0bbac7b5c52c1af8f5e878d1b37d5ae42cefacba7350b49f9e7a00529aca5a5d","cni.projectcalico.org/podIP":"172.30.38.255/32","cni.projectcalico.org/podIPs":"172.30.38.255/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.38.255\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.38.255\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"861cd72a-9160-4abd-ae6e-db3bd294db90","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"861cd72a-9160-4abd-ae6e-db3bd294db90\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qwh86","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qwh86","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.63.224.187","securityContext":{"seLinuxOptions":{"level":"s0:c46,c25"}},"imagePullSecrets":[{"name":"default-dockercfg-j5vgl"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.63.224.187"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:24Z"}],"hostIP":"10.63.224.187","podIP":"172.30.38.255","podIPs":[{"ip":"172.30.38.255"}],"startTime":"2022-08-30T14:56:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-30T14:56:26Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://d09987224c178722cc24d022786a29f3df059151f82ac7b1261ce998484e4c2a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-s6b7z","generateName":"daemon-set-","namespace":"daemonsets-777","uid":"84290003-0f57-4a76-8776-378da803f51a","resourceVersion":"92706","creationTimestamp":"2022-08-30T14:56:24Z","deletionTimestamp":"2022-08-30T14:56:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"086133f02ef22edef189262ddc0825e5ae1d089d580cdeeec9296bc55e3818eb","cni.projectcalico.org/podIP":"172.30.78.61/32","cni.projectcalico.org/podIPs":"172.30.78.61/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.78.61\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.78.61\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"861cd72a-9160-4abd-ae6e-db3bd294db90","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"861cd72a-9160-4abd-ae6e-db3bd294db90\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-30T14:56:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.78.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7gb7w","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7gb7w","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.63.224.158","securityContext":{"seLinuxOptions":{"level":"s0:c46,c25"}},"imagePullSecrets":[{"name":"default-dockercfg-j5vgl"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.63.224.158"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:24Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-30T14:56:24Z"}],"hostIP":"10.63.224.158","podIP":"172.30.78.61","podIPs":[{"ip":"172.30.78.61"}],"startTime":"2022-08-30T14:56:24Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-30T14:56:26Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://6842ab00203686e74bd9c51751c9caa31c7ae1ae92ff56fbcf23c09294b545ba","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 30 14:56:28.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-777" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":123,"skipped":2171,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:56:28.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3428
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-3428
W0830 14:56:28.276292      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:56:28.292: INFO: Found 0 stateful pods, waiting for 1
Aug 30 14:56:38.314: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
W0830 14:56:38.395126      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Getting /status
Aug 30 14:56:38.412: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Aug 30 14:56:38.469: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Aug 30 14:56:38.477: INFO: Observed &StatefulSet event: ADDED
Aug 30 14:56:38.477: INFO: Found Statefulset ss in namespace statefulset-3428 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 30 14:56:38.477: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Aug 30 14:56:38.477: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 30 14:56:38.508: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Aug 30 14:56:38.517: INFO: Observed &StatefulSet event: ADDED
Aug 30 14:56:38.517: INFO: Observed Statefulset ss in namespace statefulset-3428 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 30 14:56:38.517: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 30 14:56:38.517: INFO: Deleting all statefulset in ns statefulset-3428
Aug 30 14:56:38.540: INFO: Scaling statefulset ss to 0
W0830 14:56:38.590446      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:56:48.627: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 14:56:48.674: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 30 14:56:48.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3428" for this suite.

• [SLOW TEST:20.702 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":124,"skipped":2174,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:56:48.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
W0830 14:56:49.020808      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:56:49.037: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:56:51.059: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
W0830 14:56:51.125675      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-with-poststart-http-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-poststart-http-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-poststart-http-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-poststart-http-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:56:51.146: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:56:53.164: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 30 14:56:53.300: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 14:56:53.328: INFO: Pod pod-with-poststart-http-hook still exists
Aug 30 14:56:55.330: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 14:56:55.347: INFO: Pod pod-with-poststart-http-hook still exists
Aug 30 14:56:57.330: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 14:56:57.350: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 30 14:56:57.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2682" for this suite.

• [SLOW TEST:8.605 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":125,"skipped":2178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:56:57.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-3038
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 30 14:56:57.502: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
W0830 14:56:57.626939      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 14:56:57.675452      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 14:56:57.736521      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:56:57.758: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:56:59.783: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:57:01.786: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 14:57:03.809: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 14:57:05.780: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 14:57:07.788: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 14:57:09.784: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 14:57:11.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 14:57:13.778: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 14:57:15.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 14:57:17.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 14:57:19.786: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 30 14:57:19.832: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 30 14:57:19.865: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
W0830 14:57:19.927641      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 14:57:19.969681      21 warnings.go:70] would violate PodSecurity "restricted:latest": host namespaces (hostNetwork=true)
Aug 30 14:57:22.110: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 30 14:57:22.110: INFO: Going to poll 172.30.78.36 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 30 14:57:22.131: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.78.36:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3038 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 14:57:22.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 14:57:22.133: INFO: ExecWithOptions: Clientset creation
Aug 30 14:57:22.133: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3038/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.78.36%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 14:57:22.911: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 30 14:57:22.912: INFO: Going to poll 172.30.38.231 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 30 14:57:22.928: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.38.231:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3038 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 14:57:22.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 14:57:22.930: INFO: ExecWithOptions: Clientset creation
Aug 30 14:57:22.930: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3038/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.38.231%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 14:57:23.471: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 30 14:57:23.471: INFO: Going to poll 172.30.233.255 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 30 14:57:23.492: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.233.255:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3038 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 14:57:23.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 14:57:23.496: INFO: ExecWithOptions: Clientset creation
Aug 30 14:57:23.496: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3038/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.233.255%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 14:57:24.056: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 30 14:57:24.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3038" for this suite.

• [SLOW TEST:26.705 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":126,"skipped":2223,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:57:24.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 14:57:24.283268      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:57:24.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a908edcc-37cf-4db2-acd4-5c3c7055d439" in namespace "downward-api-2192" to be "Succeeded or Failed"
Aug 30 14:57:24.313: INFO: Pod "downwardapi-volume-a908edcc-37cf-4db2-acd4-5c3c7055d439": Phase="Pending", Reason="", readiness=false. Elapsed: 26.269831ms
Aug 30 14:57:26.330: INFO: Pod "downwardapi-volume-a908edcc-37cf-4db2-acd4-5c3c7055d439": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043097282s
Aug 30 14:57:28.352: INFO: Pod "downwardapi-volume-a908edcc-37cf-4db2-acd4-5c3c7055d439": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064599472s
Aug 30 14:57:30.379: INFO: Pod "downwardapi-volume-a908edcc-37cf-4db2-acd4-5c3c7055d439": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091743s
STEP: Saw pod success
Aug 30 14:57:30.379: INFO: Pod "downwardapi-volume-a908edcc-37cf-4db2-acd4-5c3c7055d439" satisfied condition "Succeeded or Failed"
Aug 30 14:57:30.417: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-a908edcc-37cf-4db2-acd4-5c3c7055d439 container client-container: <nil>
STEP: delete the pod
Aug 30 14:57:30.509: INFO: Waiting for pod downwardapi-volume-a908edcc-37cf-4db2-acd4-5c3c7055d439 to disappear
Aug 30 14:57:30.524: INFO: Pod downwardapi-volume-a908edcc-37cf-4db2-acd4-5c3c7055d439 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 14:57:30.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2192" for this suite.

• [SLOW TEST:6.474 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":127,"skipped":2225,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:57:30.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Aug 30 14:57:30.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 14:58:22.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1368" for this suite.

• [SLOW TEST:51.637 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":128,"skipped":2235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:58:22.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-d733a0fd-b803-4452-aef6-208a7cd15c7b
STEP: Creating a pod to test consume secrets
W0830 14:58:22.447278      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:58:22.447: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e4ae5e26-3ff1-4318-828d-269389443c92" in namespace "projected-28" to be "Succeeded or Failed"
Aug 30 14:58:22.500: INFO: Pod "pod-projected-secrets-e4ae5e26-3ff1-4318-828d-269389443c92": Phase="Pending", Reason="", readiness=false. Elapsed: 53.189919ms
Aug 30 14:58:24.528: INFO: Pod "pod-projected-secrets-e4ae5e26-3ff1-4318-828d-269389443c92": Phase="Running", Reason="", readiness=true. Elapsed: 2.081208629s
Aug 30 14:58:26.547: INFO: Pod "pod-projected-secrets-e4ae5e26-3ff1-4318-828d-269389443c92": Phase="Running", Reason="", readiness=false. Elapsed: 4.099516854s
Aug 30 14:58:28.566: INFO: Pod "pod-projected-secrets-e4ae5e26-3ff1-4318-828d-269389443c92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.118699828s
STEP: Saw pod success
Aug 30 14:58:28.566: INFO: Pod "pod-projected-secrets-e4ae5e26-3ff1-4318-828d-269389443c92" satisfied condition "Succeeded or Failed"
Aug 30 14:58:28.583: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-secrets-e4ae5e26-3ff1-4318-828d-269389443c92 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 30 14:58:28.754: INFO: Waiting for pod pod-projected-secrets-e4ae5e26-3ff1-4318-828d-269389443c92 to disappear
Aug 30 14:58:28.784: INFO: Pod pod-projected-secrets-e4ae5e26-3ff1-4318-828d-269389443c92 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 30 14:58:28.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-28" for this suite.

• [SLOW TEST:6.620 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":129,"skipped":2259,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:58:28.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
W0830 14:58:29.035351      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:58:29.076: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:58:31.109: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:58:33.098: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
W0830 14:58:33.142409      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 30 14:58:34.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7896" for this suite.

• [SLOW TEST:5.404 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":130,"skipped":2290,"failed":0}
SSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:58:34.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
W0830 14:58:34.504414      21 warnings.go:70] would violate PodSecurity "restricted:latest": hostPort (container "agnhost" uses hostPort 54323)
Aug 30 14:58:34.550: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:58:36.567: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:58:38.573: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.63.224.158 on the node which pod1 resides and expect scheduled
W0830 14:58:38.622273      21 warnings.go:70] would violate PodSecurity "restricted:latest": hostPort (container "agnhost" uses hostPort 54323)
Aug 30 14:58:38.639: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:58:40.662: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:58:42.670: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.63.224.158 but use UDP protocol on the node which pod2 resides
W0830 14:58:42.714726      21 warnings.go:70] would violate PodSecurity "restricted:latest": hostPort (container "agnhost" uses hostPort 54323)
Aug 30 14:58:42.732: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:58:44.760: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:58:46.753: INFO: The status of Pod pod3 is Running (Ready = true)
W0830 14:58:46.808050      21 warnings.go:70] would violate PodSecurity "restricted:latest": host namespaces (hostNetwork=true)
Aug 30 14:58:46.826: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 30 14:58:48.853: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Aug 30 14:58:48.870: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.63.224.158 http://127.0.0.1:54323/hostname] Namespace:hostport-7936 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 14:58:48.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 14:58:48.872: INFO: ExecWithOptions: Clientset creation
Aug 30 14:58:48.872: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-7936/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.63.224.158+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.63.224.158, port: 54323
Aug 30 14:58:49.198: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.63.224.158:54323/hostname] Namespace:hostport-7936 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 14:58:49.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 14:58:49.199: INFO: ExecWithOptions: Clientset creation
Aug 30 14:58:49.199: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-7936/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.63.224.158%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.63.224.158, port: 54323 UDP
Aug 30 14:58:49.532: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.63.224.158 54323] Namespace:hostport-7936 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 14:58:49.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 14:58:49.534: INFO: ExecWithOptions: Clientset creation
Aug 30 14:58:49.535: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-7936/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.63.224.158+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Aug 30 14:58:54.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-7936" for this suite.

• [SLOW TEST:20.594 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":131,"skipped":2295,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 14:58:54.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 30 14:58:55.016: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 14:59:55.350: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
W0830 14:59:55.501174      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod0-0-sched-preemption-low-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod0-0-sched-preemption-low-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod0-0-sched-preemption-low-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod0-0-sched-preemption-low-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:59:55.501: INFO: Created pod: pod0-0-sched-preemption-low-priority
W0830 14:59:55.547885      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod0-1-sched-preemption-medium-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod0-1-sched-preemption-medium-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod0-1-sched-preemption-medium-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod0-1-sched-preemption-medium-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:59:55.548: INFO: Created pod: pod0-1-sched-preemption-medium-priority
W0830 14:59:55.627461      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod1-0-sched-preemption-medium-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod1-0-sched-preemption-medium-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod1-0-sched-preemption-medium-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod1-0-sched-preemption-medium-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:59:55.627: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 30 14:59:55.680: INFO: Created pod: pod1-1-sched-preemption-medium-priority
W0830 14:59:55.678262      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod1-1-sched-preemption-medium-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod1-1-sched-preemption-medium-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod1-1-sched-preemption-medium-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod1-1-sched-preemption-medium-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:59:55.795: INFO: Created pod: pod2-0-sched-preemption-medium-priority
W0830 14:59:55.795338      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod2-0-sched-preemption-medium-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod2-0-sched-preemption-medium-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod2-0-sched-preemption-medium-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod2-0-sched-preemption-medium-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 14:59:55.840: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
W0830 14:59:55.840682      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod2-1-sched-preemption-medium-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod2-1-sched-preemption-medium-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod2-1-sched-preemption-medium-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod2-1-sched-preemption-medium-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 30 15:00:16.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5388" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:81.774 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":132,"skipped":2296,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:00:16.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Aug 30 15:00:16.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:01:04.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3019" for this suite.

• [SLOW TEST:47.637 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":133,"skipped":2301,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:01:04.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Aug 30 15:01:04.383: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Aug 30 15:01:04.416: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 30 15:01:04.416: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
W0830 15:01:04.483294      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring Pod has resource requirements applied from LimitRange
Aug 30 15:01:04.498: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 30 15:01:04.498: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
W0830 15:01:04.561347      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Aug 30 15:01:04.577: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 30 15:01:04.577: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
W0830 15:01:06.743780      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Aug 30 15:01:11.825: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
W0830 15:01:11.889287      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Aug 30 15:01:11.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8031" for this suite.

• [SLOW TEST:7.697 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":134,"skipped":2316,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:01:11.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4414.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4414.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4414.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4414.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4414.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4414.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4414.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4414.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
W0830 15:01:12.175541      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 30 15:01:16.296: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local from pod dns-4414/dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009: the server could not find the requested resource (get pods dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009)
Aug 30 15:01:16.321: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local from pod dns-4414/dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009: the server could not find the requested resource (get pods dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009)
Aug 30 15:01:16.345: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4414.svc.cluster.local from pod dns-4414/dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009: the server could not find the requested resource (get pods dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009)
Aug 30 15:01:16.371: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4414.svc.cluster.local from pod dns-4414/dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009: the server could not find the requested resource (get pods dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009)
Aug 30 15:01:16.395: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local from pod dns-4414/dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009: the server could not find the requested resource (get pods dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009)
Aug 30 15:01:16.424: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local from pod dns-4414/dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009: the server could not find the requested resource (get pods dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009)
Aug 30 15:01:16.475: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4414.svc.cluster.local from pod dns-4414/dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009: the server could not find the requested resource (get pods dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009)
Aug 30 15:01:16.475: INFO: Lookups using dns-4414/dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4414.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4414.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4414.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4414.svc.cluster.local]

Aug 30 15:01:21.660: INFO: DNS probes using dns-4414/dns-test-1a384ad6-d895-471c-afb9-2c1fb5afa009 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 30 15:01:21.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4414" for this suite.

• [SLOW TEST:9.872 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":135,"skipped":2342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:01:21.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
W0830 15:01:21.934756      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 30 15:06:22.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4309" for this suite.

• [SLOW TEST:300.292 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":136,"skipped":2400,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:06:22.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
W0830 15:06:22.334285      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:06:22.334: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1466  2f6c22e9-f44a-4f80-9047-096448b05092 97614 0 2022-08-30 15:06:22 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2022-08-30 15:06:22 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6n96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6n96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 15:06:22.351: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:06:24.375: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:06:26.392: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Aug 30 15:06:26.392: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1466 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:06:26.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:06:26.394: INFO: ExecWithOptions: Clientset creation
Aug 30 15:06:26.395: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-1466/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Aug 30 15:06:26.714: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1466 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:06:26.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:06:26.716: INFO: ExecWithOptions: Clientset creation
Aug 30 15:06:26.716: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-1466/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 15:06:27.007: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 30 15:06:27.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1466" for this suite.

• [SLOW TEST:5.022 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":137,"skipped":2412,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:06:27.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 15:06:27.743237      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 15:06:27.802: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 15:06:29.850: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 15, 6, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 6, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 15, 6, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 6, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 15:06:32.931: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:06:32.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:06:36.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8195" for this suite.
STEP: Destroying namespace "webhook-8195-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:9.771 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":138,"skipped":2414,"failed":0}
SSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:06:36.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 30 15:06:37.229: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 30 15:06:37.286: INFO: starting watch
STEP: patching
STEP: updating
Aug 30 15:06:37.412: INFO: waiting for watch events with expected annotations
Aug 30 15:06:37.413: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 30 15:06:37.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8608" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":139,"skipped":2417,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:06:37.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6378
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6378
W0830 15:06:37.843127      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6378
Aug 30 15:06:37.867: INFO: Found 0 stateful pods, waiting for 1
Aug 30 15:06:47.887: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 30 15:06:47.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6378 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 15:06:48.365: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 15:06:48.365: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 15:06:48.365: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 15:06:48.383: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 30 15:06:58.424: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 15:06:58.424: INFO: Waiting for statefulset status.replicas updated to 0
W0830 15:06:58.466038      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:06:58.477: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997666s
Aug 30 15:06:59.495: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988186868s
Aug 30 15:07:00.527: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.970248114s
Aug 30 15:07:01.577: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.938229552s
Aug 30 15:07:02.619: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.887019451s
Aug 30 15:07:03.643: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.845628599s
Aug 30 15:07:04.662: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.822065442s
Aug 30 15:07:05.679: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.802937926s
Aug 30 15:07:06.717: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.786373792s
Aug 30 15:07:07.736: INFO: Verifying statefulset ss doesn't scale past 1 for another 748.375208ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6378
Aug 30 15:07:08.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6378 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 15:07:09.326: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 15:07:09.326: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 15:07:09.326: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 15:07:09.347: INFO: Found 1 stateful pods, waiting for 3
Aug 30 15:07:19.368: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 15:07:19.368: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 15:07:19.368: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 30 15:07:19.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6378 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 15:07:19.812: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 15:07:19.813: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 15:07:19.813: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 15:07:19.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6378 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 15:07:20.199: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 15:07:20.199: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 15:07:20.199: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 15:07:20.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6378 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 15:07:20.613: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 15:07:20.613: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 15:07:20.613: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 15:07:20.614: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 15:07:20.630: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 30 15:07:30.665: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 15:07:30.665: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 15:07:30.665: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
W0830 15:07:30.716858      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:07:30.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999997137s
Aug 30 15:07:31.753: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982120779s
Aug 30 15:07:32.773: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.962843907s
Aug 30 15:07:33.800: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.942744364s
Aug 30 15:07:34.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.91600501s
Aug 30 15:07:35.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.895580914s
Aug 30 15:07:36.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.876620359s
Aug 30 15:07:37.886: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.857512304s
Aug 30 15:07:38.906: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.829678516s
Aug 30 15:07:39.928: INFO: Verifying statefulset ss doesn't scale past 3 for another 810.207781ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6378
Aug 30 15:07:40.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6378 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 15:07:41.329: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 15:07:41.329: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 15:07:41.329: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 15:07:41.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6378 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 15:07:41.666: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 15:07:41.666: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 15:07:41.667: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 15:07:41.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=statefulset-6378 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 15:07:42.092: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 15:07:42.092: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 15:07:42.092: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 15:07:42.092: INFO: Scaling statefulset ss to 0
W0830 15:07:42.130202      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 30 15:07:52.171: INFO: Deleting all statefulset in ns statefulset-6378
Aug 30 15:07:52.198: INFO: Scaling statefulset ss to 0
W0830 15:07:52.245565      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:07:52.282: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 15:07:52.297: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 30 15:07:52.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6378" for this suite.

• [SLOW TEST:74.810 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":140,"skipped":2431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:07:52.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
W0830 15:07:52.605666      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:07:52.606: INFO: Waiting up to 5m0s for pod "downward-api-8c73a142-8b02-4010-bd80-ba003c501a30" in namespace "downward-api-7670" to be "Succeeded or Failed"
Aug 30 15:07:52.632: INFO: Pod "downward-api-8c73a142-8b02-4010-bd80-ba003c501a30": Phase="Pending", Reason="", readiness=false. Elapsed: 26.083498ms
Aug 30 15:07:54.651: INFO: Pod "downward-api-8c73a142-8b02-4010-bd80-ba003c501a30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04531479s
Aug 30 15:07:56.669: INFO: Pod "downward-api-8c73a142-8b02-4010-bd80-ba003c501a30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063565366s
Aug 30 15:07:58.689: INFO: Pod "downward-api-8c73a142-8b02-4010-bd80-ba003c501a30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.083322314s
STEP: Saw pod success
Aug 30 15:07:58.689: INFO: Pod "downward-api-8c73a142-8b02-4010-bd80-ba003c501a30" satisfied condition "Succeeded or Failed"
Aug 30 15:07:58.707: INFO: Trying to get logs from node 10.63.224.189 pod downward-api-8c73a142-8b02-4010-bd80-ba003c501a30 container dapi-container: <nil>
STEP: delete the pod
Aug 30 15:07:58.851: INFO: Waiting for pod downward-api-8c73a142-8b02-4010-bd80-ba003c501a30 to disappear
Aug 30 15:07:58.870: INFO: Pod downward-api-8c73a142-8b02-4010-bd80-ba003c501a30 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 30 15:07:58.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7670" for this suite.

• [SLOW TEST:6.494 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":141,"skipped":2466,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:07:58.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-6edd867c-f370-4c40-b98f-2457d0cd8f07
STEP: Creating a pod to test consume configMaps
Aug 30 15:07:59.151: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39c87b7c-864b-44e1-a3a1-fe7cf9b48a01" in namespace "projected-6401" to be "Succeeded or Failed"
W0830 15:07:59.151484      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:07:59.165: INFO: Pod "pod-projected-configmaps-39c87b7c-864b-44e1-a3a1-fe7cf9b48a01": Phase="Pending", Reason="", readiness=false. Elapsed: 13.897276ms
Aug 30 15:08:01.184: INFO: Pod "pod-projected-configmaps-39c87b7c-864b-44e1-a3a1-fe7cf9b48a01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032948959s
Aug 30 15:08:03.204: INFO: Pod "pod-projected-configmaps-39c87b7c-864b-44e1-a3a1-fe7cf9b48a01": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053278337s
Aug 30 15:08:05.228: INFO: Pod "pod-projected-configmaps-39c87b7c-864b-44e1-a3a1-fe7cf9b48a01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07707636s
STEP: Saw pod success
Aug 30 15:08:05.228: INFO: Pod "pod-projected-configmaps-39c87b7c-864b-44e1-a3a1-fe7cf9b48a01" satisfied condition "Succeeded or Failed"
Aug 30 15:08:05.248: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-configmaps-39c87b7c-864b-44e1-a3a1-fe7cf9b48a01 container agnhost-container: <nil>
STEP: delete the pod
Aug 30 15:08:05.347: INFO: Waiting for pod pod-projected-configmaps-39c87b7c-864b-44e1-a3a1-fe7cf9b48a01 to disappear
Aug 30 15:08:05.362: INFO: Pod pod-projected-configmaps-39c87b7c-864b-44e1-a3a1-fe7cf9b48a01 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 30 15:08:05.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6401" for this suite.

• [SLOW TEST:6.486 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":142,"skipped":2467,"failed":0}
SS
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:08:05.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0830 15:08:05.651634      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 30 15:08:09.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9059" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":143,"skipped":2469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:08:09.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Aug 30 15:08:09.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-6229 create -f -'
Aug 30 15:08:12.497: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"httpd\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"httpd\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"httpd\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"httpd\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 15:08:12.498: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Aug 30 15:08:12.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-6229 diff -f -'
Aug 30 15:08:15.297: INFO: rc: 1
Aug 30 15:08:15.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-6229 delete -f -'
Aug 30 15:08:15.437: INFO: stderr: ""
Aug 30 15:08:15.437: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 15:08:15.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6229" for this suite.

• [SLOW TEST:5.735 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:896
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":144,"skipped":2491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:08:15.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:08:15.664: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-55e4eb2b-7216-4df0-ab6a-31478de0302d
STEP: Creating configMap with name cm-test-opt-upd-edbb4ef1-4f07-424c-858b-0d6d3487dbb3
STEP: Creating the pod
W0830 15:08:15.799246      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:08:15.841: INFO: The status of Pod pod-projected-configmaps-5efecefa-ceb3-4e5e-a7fe-cae97c22f135 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:08:17.856: INFO: The status of Pod pod-projected-configmaps-5efecefa-ceb3-4e5e-a7fe-cae97c22f135 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:08:19.868: INFO: The status of Pod pod-projected-configmaps-5efecefa-ceb3-4e5e-a7fe-cae97c22f135 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-55e4eb2b-7216-4df0-ab6a-31478de0302d
STEP: Updating configmap cm-test-opt-upd-edbb4ef1-4f07-424c-858b-0d6d3487dbb3
STEP: Creating configMap with name cm-test-opt-create-1e090b57-45b4-4b70-bba0-d5815bd79fbb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 30 15:08:24.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2404" for this suite.

• [SLOW TEST:8.845 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":145,"skipped":2519,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:08:24.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-475cbbe1-b092-458e-a001-57bd14f94675
STEP: Creating a pod to test consume configMaps
W0830 15:08:24.633967      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:08:24.634: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f2e775f-5463-466a-ac4a-6a676b1b426b" in namespace "projected-2851" to be "Succeeded or Failed"
Aug 30 15:08:24.666: INFO: Pod "pod-projected-configmaps-0f2e775f-5463-466a-ac4a-6a676b1b426b": Phase="Pending", Reason="", readiness=false. Elapsed: 31.613321ms
Aug 30 15:08:26.685: INFO: Pod "pod-projected-configmaps-0f2e775f-5463-466a-ac4a-6a676b1b426b": Phase="Running", Reason="", readiness=true. Elapsed: 2.050162723s
Aug 30 15:08:28.707: INFO: Pod "pod-projected-configmaps-0f2e775f-5463-466a-ac4a-6a676b1b426b": Phase="Running", Reason="", readiness=false. Elapsed: 4.072668932s
Aug 30 15:08:30.726: INFO: Pod "pod-projected-configmaps-0f2e775f-5463-466a-ac4a-6a676b1b426b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092143391s
STEP: Saw pod success
Aug 30 15:08:30.727: INFO: Pod "pod-projected-configmaps-0f2e775f-5463-466a-ac4a-6a676b1b426b" satisfied condition "Succeeded or Failed"
Aug 30 15:08:30.742: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-configmaps-0f2e775f-5463-466a-ac4a-6a676b1b426b container agnhost-container: <nil>
STEP: delete the pod
Aug 30 15:08:30.835: INFO: Waiting for pod pod-projected-configmaps-0f2e775f-5463-466a-ac4a-6a676b1b426b to disappear
Aug 30 15:08:30.850: INFO: Pod pod-projected-configmaps-0f2e775f-5463-466a-ac4a-6a676b1b426b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 30 15:08:30.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2851" for this suite.

• [SLOW TEST:6.575 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":146,"skipped":2522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:08:30.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:08:31.097: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-e3ae5526-0606-495f-9f07-e6c2d98f5ff0
STEP: Creating the pod
W0830 15:08:31.212407      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:08:31.247: INFO: The status of Pod pod-configmaps-9d582dcb-c55a-4808-942d-a2e9868358aa is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:08:33.265: INFO: The status of Pod pod-configmaps-9d582dcb-c55a-4808-942d-a2e9868358aa is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:08:35.262: INFO: The status of Pod pod-configmaps-9d582dcb-c55a-4808-942d-a2e9868358aa is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-e3ae5526-0606-495f-9f07-e6c2d98f5ff0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 15:10:01.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1608" for this suite.

• [SLOW TEST:90.696 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":147,"skipped":2659,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:10:01.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 30 15:10:01.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-8651 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 30 15:10:02.032: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 15:10:02.032: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Aug 30 15:10:02.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-8651 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Aug 30 15:10:02.518: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 15:10:02.518: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 30 15:10:02.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-8651 delete pods e2e-test-httpd-pod'
Aug 30 15:10:06.370: INFO: stderr: ""
Aug 30 15:10:06.370: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 15:10:06.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8651" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":148,"skipped":2664,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:10:06.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
W0830 15:10:06.562135      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Aug 30 15:10:07.763: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 30 15:10:07.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0830 15:10:07.763106      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-2790" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":149,"skipped":2671,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:10:07.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
W0830 15:10:07.957211      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up
Aug 30 15:10:07.973: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 30 15:10:12.996: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Aug 30 15:10:13.012: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 30 15:10:13.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1202" for this suite.

• [SLOW TEST:5.409 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":150,"skipped":2673,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:10:13.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 30 15:10:13.452: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3edcd6bc-fdc1-4211-899f-3407ddfe18cc" in namespace "projected-4618" to be "Succeeded or Failed"
W0830 15:10:13.451984      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:10:13.470: INFO: Pod "downwardapi-volume-3edcd6bc-fdc1-4211-899f-3407ddfe18cc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.852404ms
Aug 30 15:10:15.493: INFO: Pod "downwardapi-volume-3edcd6bc-fdc1-4211-899f-3407ddfe18cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041515196s
Aug 30 15:10:17.516: INFO: Pod "downwardapi-volume-3edcd6bc-fdc1-4211-899f-3407ddfe18cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064390348s
Aug 30 15:10:19.534: INFO: Pod "downwardapi-volume-3edcd6bc-fdc1-4211-899f-3407ddfe18cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082262261s
STEP: Saw pod success
Aug 30 15:10:19.534: INFO: Pod "downwardapi-volume-3edcd6bc-fdc1-4211-899f-3407ddfe18cc" satisfied condition "Succeeded or Failed"
Aug 30 15:10:19.549: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-3edcd6bc-fdc1-4211-899f-3407ddfe18cc container client-container: <nil>
STEP: delete the pod
Aug 30 15:10:19.631: INFO: Waiting for pod downwardapi-volume-3edcd6bc-fdc1-4211-899f-3407ddfe18cc to disappear
Aug 30 15:10:19.645: INFO: Pod downwardapi-volume-3edcd6bc-fdc1-4211-899f-3407ddfe18cc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 15:10:19.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4618" for this suite.

• [SLOW TEST:6.461 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":151,"skipped":2674,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:10:19.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 30 15:10:19.889: INFO: observed the pod list
W0830 15:10:19.961579      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 30 15:10:26.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3717" for this suite.

• [SLOW TEST:6.875 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":152,"skipped":2681,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:10:26.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
W0830 15:10:26.812585      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:10:26.813: INFO: Waiting up to 5m0s for pod "pod-e3394fa3-d2f4-4ac7-8c7e-c7617b0f6533" in namespace "emptydir-5298" to be "Succeeded or Failed"
Aug 30 15:10:26.828: INFO: Pod "pod-e3394fa3-d2f4-4ac7-8c7e-c7617b0f6533": Phase="Pending", Reason="", readiness=false. Elapsed: 15.100051ms
Aug 30 15:10:28.846: INFO: Pod "pod-e3394fa3-d2f4-4ac7-8c7e-c7617b0f6533": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033412266s
Aug 30 15:10:30.871: INFO: Pod "pod-e3394fa3-d2f4-4ac7-8c7e-c7617b0f6533": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058794572s
Aug 30 15:10:32.890: INFO: Pod "pod-e3394fa3-d2f4-4ac7-8c7e-c7617b0f6533": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077552304s
STEP: Saw pod success
Aug 30 15:10:32.890: INFO: Pod "pod-e3394fa3-d2f4-4ac7-8c7e-c7617b0f6533" satisfied condition "Succeeded or Failed"
Aug 30 15:10:32.905: INFO: Trying to get logs from node 10.63.224.189 pod pod-e3394fa3-d2f4-4ac7-8c7e-c7617b0f6533 container test-container: <nil>
STEP: delete the pod
Aug 30 15:10:33.013: INFO: Waiting for pod pod-e3394fa3-d2f4-4ac7-8c7e-c7617b0f6533 to disappear
Aug 30 15:10:33.029: INFO: Pod pod-e3394fa3-d2f4-4ac7-8c7e-c7617b0f6533 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 15:10:33.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5298" for this suite.

• [SLOW TEST:6.491 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":153,"skipped":2681,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:10:33.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
W0830 15:10:33.257403      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:10:33.279: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 30 15:10:38.298: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Aug 30 15:10:38.318: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Aug 30 15:10:38.359: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Aug 30 15:10:38.370: INFO: Observed &ReplicaSet event: ADDED
Aug 30 15:10:38.371: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 15:10:38.371: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 15:10:38.372: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 15:10:38.372: INFO: Found replicaset test-rs in namespace replicaset-9516 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 30 15:10:38.372: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Aug 30 15:10:38.372: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 30 15:10:38.400: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Aug 30 15:10:38.408: INFO: Observed &ReplicaSet event: ADDED
Aug 30 15:10:38.408: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 15:10:38.409: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 15:10:38.409: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 15:10:38.409: INFO: Observed replicaset test-rs in namespace replicaset-9516 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 30 15:10:38.410: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 15:10:38.410: INFO: Found replicaset test-rs in namespace replicaset-9516 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 30 15:10:38.410: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 30 15:10:38.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9516" for this suite.

• [SLOW TEST:5.392 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":154,"skipped":2688,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:10:38.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-4884
STEP: creating service affinity-clusterip-transition in namespace services-4884
STEP: creating replication controller affinity-clusterip-transition in namespace services-4884
W0830 15:10:38.721969      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-clusterip-transition" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-clusterip-transition" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-clusterip-transition" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-clusterip-transition" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 15:10:38.722573      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4884, replica count: 3
I0830 15:10:41.775586      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 15:10:41.813: INFO: Creating new exec pod
W0830 15:10:41.864990      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:10:44.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4884 exec execpod-affinityb66vs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Aug 30 15:10:45.396: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 30 15:10:45.396: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 15:10:45.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4884 exec execpod-affinityb66vs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.221.182 80'
Aug 30 15:10:46.119: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.221.182 80\nConnection to 172.21.221.182 80 port [tcp/http] succeeded!\n"
Aug 30 15:10:46.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 15:10:46.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4884 exec execpod-affinityb66vs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.221.182:80/ ; done'
Aug 30 15:10:46.780: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n"
Aug 30 15:10:46.780: INFO: stdout: "\naffinity-clusterip-transition-td4h2\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-td4h2\naffinity-clusterip-transition-td4h2\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9bjqb\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9bjqb\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-td4h2\naffinity-clusterip-transition-td4h2\naffinity-clusterip-transition-9bjqb\naffinity-clusterip-transition-9gsbj"
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-td4h2
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-td4h2
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-td4h2
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9bjqb
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9bjqb
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-td4h2
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-td4h2
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9bjqb
Aug 30 15:10:46.780: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:46.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4884 exec execpod-affinityb66vs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.221.182:80/ ; done'
Aug 30 15:10:47.442: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.221.182:80/\n"
Aug 30 15:10:47.442: INFO: stdout: "\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj\naffinity-clusterip-transition-9gsbj"
Aug 30 15:10:47.442: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.442: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.442: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.442: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.442: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.442: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.442: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.442: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.443: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.443: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.443: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.443: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.443: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.443: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.443: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.443: INFO: Received response from host: affinity-clusterip-transition-9gsbj
Aug 30 15:10:47.443: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4884, will wait for the garbage collector to delete the pods
Aug 30 15:10:47.612: INFO: Deleting ReplicationController affinity-clusterip-transition took: 26.897838ms
Aug 30 15:10:47.713: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.375962ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 15:10:50.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4884" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:12.470 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":155,"skipped":2696,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:10:50.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8656
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8656
W0830 15:10:51.202934      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "externalname-service" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalname-service" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalname-service" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalname-service" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 15:10:51.203030      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8656, replica count: 2
I0830 15:10:54.259432      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 15:10:54.259: INFO: Creating new exec pod
W0830 15:10:54.323079      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:10:59.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8656 exec execpoddr2bh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 30 15:10:59.793: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 30 15:10:59.793: INFO: stdout: "externalname-service-xxkjg"
Aug 30 15:10:59.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8656 exec execpoddr2bh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.10.72 80'
Aug 30 15:11:00.256: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.10.72 80\nConnection to 172.21.10.72 80 port [tcp/http] succeeded!\n"
Aug 30 15:11:00.256: INFO: stdout: ""
Aug 30 15:11:01.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8656 exec execpoddr2bh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.10.72 80'
Aug 30 15:11:01.666: INFO: stderr: "+ echo hostName+ \nnc -v -t -w 2 172.21.10.72 80\nConnection to 172.21.10.72 80 port [tcp/http] succeeded!\n"
Aug 30 15:11:01.666: INFO: stdout: ""
Aug 30 15:11:02.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8656 exec execpoddr2bh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.10.72 80'
Aug 30 15:11:02.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.10.72 80\nConnection to 172.21.10.72 80 port [tcp/http] succeeded!\n"
Aug 30 15:11:02.704: INFO: stdout: ""
Aug 30 15:11:03.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8656 exec execpoddr2bh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.10.72 80'
Aug 30 15:11:03.652: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.10.72 80\nConnection to 172.21.10.72 80 port [tcp/http] succeeded!\n"
Aug 30 15:11:03.652: INFO: stdout: "externalname-service-bcl2d"
Aug 30 15:11:03.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8656 exec execpoddr2bh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.158 30764'
Aug 30 15:11:04.099: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.63.224.158 30764\nConnection to 10.63.224.158 30764 port [tcp/*] succeeded!\n"
Aug 30 15:11:04.099: INFO: stdout: "externalname-service-xxkjg"
Aug 30 15:11:04.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8656 exec execpoddr2bh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.189 30764'
Aug 30 15:11:04.521: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.63.224.189 30764\nConnection to 10.63.224.189 30764 port [tcp/*] succeeded!\n"
Aug 30 15:11:04.521: INFO: stdout: "externalname-service-bcl2d"
Aug 30 15:11:04.521: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 15:11:04.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8656" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:13.768 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":156,"skipped":2732,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:11:04.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-33a2c263-77c2-4565-b0f0-a0b5fb5865b5
STEP: Creating a pod to test consume configMaps
W0830 15:11:04.900391      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:11:04.900: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3d1381f9-533b-4c57-afd3-388ae7c63dc9" in namespace "projected-4387" to be "Succeeded or Failed"
Aug 30 15:11:04.914: INFO: Pod "pod-projected-configmaps-3d1381f9-533b-4c57-afd3-388ae7c63dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.381308ms
Aug 30 15:11:06.937: INFO: Pod "pod-projected-configmaps-3d1381f9-533b-4c57-afd3-388ae7c63dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036326589s
Aug 30 15:11:08.957: INFO: Pod "pod-projected-configmaps-3d1381f9-533b-4c57-afd3-388ae7c63dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056895762s
Aug 30 15:11:10.973: INFO: Pod "pod-projected-configmaps-3d1381f9-533b-4c57-afd3-388ae7c63dc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.072992282s
STEP: Saw pod success
Aug 30 15:11:10.974: INFO: Pod "pod-projected-configmaps-3d1381f9-533b-4c57-afd3-388ae7c63dc9" satisfied condition "Succeeded or Failed"
Aug 30 15:11:10.992: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-configmaps-3d1381f9-533b-4c57-afd3-388ae7c63dc9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 30 15:11:11.109: INFO: Waiting for pod pod-projected-configmaps-3d1381f9-533b-4c57-afd3-388ae7c63dc9 to disappear
Aug 30 15:11:11.154: INFO: Pod pod-projected-configmaps-3d1381f9-533b-4c57-afd3-388ae7c63dc9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 30 15:11:11.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4387" for this suite.

• [SLOW TEST:6.498 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":157,"skipped":2746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:11:11.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-ffdb98aa-7f86-4ac4-9f17-678f38876eec
STEP: Creating a pod to test consume secrets
W0830 15:11:11.457760      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:11:11.458: INFO: Waiting up to 5m0s for pod "pod-secrets-f62f75ed-ed21-49b8-9a82-ddab96c4e14c" in namespace "secrets-6112" to be "Succeeded or Failed"
Aug 30 15:11:11.484: INFO: Pod "pod-secrets-f62f75ed-ed21-49b8-9a82-ddab96c4e14c": Phase="Pending", Reason="", readiness=false. Elapsed: 25.932927ms
Aug 30 15:11:13.499: INFO: Pod "pod-secrets-f62f75ed-ed21-49b8-9a82-ddab96c4e14c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041296145s
Aug 30 15:11:15.515: INFO: Pod "pod-secrets-f62f75ed-ed21-49b8-9a82-ddab96c4e14c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056610496s
Aug 30 15:11:17.546: INFO: Pod "pod-secrets-f62f75ed-ed21-49b8-9a82-ddab96c4e14c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087792592s
STEP: Saw pod success
Aug 30 15:11:17.546: INFO: Pod "pod-secrets-f62f75ed-ed21-49b8-9a82-ddab96c4e14c" satisfied condition "Succeeded or Failed"
Aug 30 15:11:17.559: INFO: Trying to get logs from node 10.63.224.189 pod pod-secrets-f62f75ed-ed21-49b8-9a82-ddab96c4e14c container secret-env-test: <nil>
STEP: delete the pod
Aug 30 15:11:17.632: INFO: Waiting for pod pod-secrets-f62f75ed-ed21-49b8-9a82-ddab96c4e14c to disappear
Aug 30 15:11:17.648: INFO: Pod pod-secrets-f62f75ed-ed21-49b8-9a82-ddab96c4e14c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 30 15:11:17.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6112" for this suite.

• [SLOW TEST:6.497 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":158,"skipped":2781,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:11:17.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:11:17.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Aug 30 15:11:27.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 --namespace=crd-publish-openapi-498 create -f -'
Aug 30 15:11:30.029: INFO: stderr: ""
Aug 30 15:11:30.029: INFO: stdout: "e2e-test-crd-publish-openapi-387-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 30 15:11:30.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 --namespace=crd-publish-openapi-498 delete e2e-test-crd-publish-openapi-387-crds test-foo'
Aug 30 15:11:30.201: INFO: stderr: ""
Aug 30 15:11:30.201: INFO: stdout: "e2e-test-crd-publish-openapi-387-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 30 15:11:30.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 --namespace=crd-publish-openapi-498 apply -f -'
Aug 30 15:11:31.819: INFO: stderr: ""
Aug 30 15:11:31.820: INFO: stdout: "e2e-test-crd-publish-openapi-387-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 30 15:11:31.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 --namespace=crd-publish-openapi-498 delete e2e-test-crd-publish-openapi-387-crds test-foo'
Aug 30 15:11:31.974: INFO: stderr: ""
Aug 30 15:11:31.974: INFO: stdout: "e2e-test-crd-publish-openapi-387-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Aug 30 15:11:31.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 --namespace=crd-publish-openapi-498 create -f -'
Aug 30 15:11:32.377: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 30 15:11:32.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 --namespace=crd-publish-openapi-498 create -f -'
Aug 30 15:11:32.769: INFO: rc: 1
Aug 30 15:11:32.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 --namespace=crd-publish-openapi-498 apply -f -'
Aug 30 15:11:33.159: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Aug 30 15:11:33.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 --namespace=crd-publish-openapi-498 create -f -'
Aug 30 15:11:34.519: INFO: rc: 1
Aug 30 15:11:34.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 --namespace=crd-publish-openapi-498 apply -f -'
Aug 30 15:11:34.940: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 30 15:11:34.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 explain e2e-test-crd-publish-openapi-387-crds'
Aug 30 15:11:35.370: INFO: stderr: ""
Aug 30 15:11:35.370: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-387-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 30 15:11:35.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 explain e2e-test-crd-publish-openapi-387-crds.metadata'
Aug 30 15:11:35.799: INFO: stderr: ""
Aug 30 15:11:35.799: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-387-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 30 15:11:35.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 explain e2e-test-crd-publish-openapi-387-crds.spec'
Aug 30 15:11:36.234: INFO: stderr: ""
Aug 30 15:11:36.234: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-387-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 30 15:11:36.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 explain e2e-test-crd-publish-openapi-387-crds.spec.bars'
Aug 30 15:11:36.668: INFO: stderr: ""
Aug 30 15:11:36.668: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-387-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 30 15:11:36.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-498 explain e2e-test-crd-publish-openapi-387-crds.spec.bars2'
Aug 30 15:11:37.080: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:11:46.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-498" for this suite.

• [SLOW TEST:29.233 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":159,"skipped":2800,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:11:46.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
W0830 15:11:47.130078      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 30 15:13:01.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1593" for this suite.

• [SLOW TEST:74.300 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":160,"skipped":2841,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:13:01.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-2487
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 30 15:13:01.371: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
W0830 15:13:01.499646      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 15:13:01.581523      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 15:13:01.643181      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:13:01.686: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:13:03.712: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 15:13:05.717: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 15:13:07.711: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 15:13:09.707: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 15:13:11.711: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 15:13:13.714: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 15:13:15.718: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 15:13:17.707: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 15:13:19.711: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 15:13:21.706: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 15:13:23.708: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 30 15:13:23.744: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 30 15:13:23.780: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
W0830 15:13:23.843742      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 15:13:23.892713      21 warnings.go:70] would violate PodSecurity "restricted:latest": host namespaces (hostNetwork=true)
Aug 30 15:13:26.006: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 30 15:13:26.006: INFO: Going to poll 172.30.78.51 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 30 15:13:26.028: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.78.51 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2487 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:13:26.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:13:26.029: INFO: ExecWithOptions: Clientset creation
Aug 30 15:13:26.029: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2487/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.78.51+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 15:13:27.325: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 30 15:13:27.325: INFO: Going to poll 172.30.38.252 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 30 15:13:27.356: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.38.252 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2487 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:13:27.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:13:27.358: INFO: ExecWithOptions: Clientset creation
Aug 30 15:13:27.358: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2487/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.38.252+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 15:13:28.634: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 30 15:13:28.634: INFO: Going to poll 172.30.233.205 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 30 15:13:28.654: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.233.205 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2487 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:13:28.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:13:28.657: INFO: ExecWithOptions: Clientset creation
Aug 30 15:13:28.657: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2487/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.233.205+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 15:13:29.966: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 30 15:13:29.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2487" for this suite.

• [SLOW TEST:28.765 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":161,"skipped":2857,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:13:30.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
W0830 15:13:30.310241      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:13:30.330: INFO: The status of Pod pod-secrets-5e83d7ae-6b1c-4921-be61-c9484305fffa is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:13:32.352: INFO: The status of Pod pod-secrets-5e83d7ae-6b1c-4921-be61-c9484305fffa is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:13:34.368: INFO: The status of Pod pod-secrets-5e83d7ae-6b1c-4921-be61-c9484305fffa is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Aug 30 15:13:34.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2928" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":162,"skipped":2887,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:13:34.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 15:13:34.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6769" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":163,"skipped":2895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:13:34.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
W0830 15:13:34.931549      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:13:34.932: INFO: Waiting up to 5m0s for pod "pod-32f66d5b-280a-427f-8381-35baffb2d6db" in namespace "emptydir-5111" to be "Succeeded or Failed"
Aug 30 15:13:34.955: INFO: Pod "pod-32f66d5b-280a-427f-8381-35baffb2d6db": Phase="Pending", Reason="", readiness=false. Elapsed: 23.441098ms
Aug 30 15:13:36.987: INFO: Pod "pod-32f66d5b-280a-427f-8381-35baffb2d6db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055110824s
Aug 30 15:13:39.015: INFO: Pod "pod-32f66d5b-280a-427f-8381-35baffb2d6db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083054247s
Aug 30 15:13:41.038: INFO: Pod "pod-32f66d5b-280a-427f-8381-35baffb2d6db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.106381685s
STEP: Saw pod success
Aug 30 15:13:41.039: INFO: Pod "pod-32f66d5b-280a-427f-8381-35baffb2d6db" satisfied condition "Succeeded or Failed"
Aug 30 15:13:41.059: INFO: Trying to get logs from node 10.63.224.189 pod pod-32f66d5b-280a-427f-8381-35baffb2d6db container test-container: <nil>
STEP: delete the pod
Aug 30 15:13:41.196: INFO: Waiting for pod pod-32f66d5b-280a-427f-8381-35baffb2d6db to disappear
Aug 30 15:13:41.214: INFO: Pod pod-32f66d5b-280a-427f-8381-35baffb2d6db no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 15:13:41.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5111" for this suite.

• [SLOW TEST:6.543 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":164,"skipped":2919,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:13:41.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 30 15:13:41.434: INFO: Waiting up to 5m0s for pod "downward-api-b8de61ed-8d74-4b86-b67e-8f4be374d562" in namespace "downward-api-6233" to be "Succeeded or Failed"
W0830 15:13:41.434491      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:13:41.455: INFO: Pod "downward-api-b8de61ed-8d74-4b86-b67e-8f4be374d562": Phase="Pending", Reason="", readiness=false. Elapsed: 20.866523ms
Aug 30 15:13:43.476: INFO: Pod "downward-api-b8de61ed-8d74-4b86-b67e-8f4be374d562": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041930455s
Aug 30 15:13:45.494: INFO: Pod "downward-api-b8de61ed-8d74-4b86-b67e-8f4be374d562": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059615774s
Aug 30 15:13:47.519: INFO: Pod "downward-api-b8de61ed-8d74-4b86-b67e-8f4be374d562": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.084510994s
STEP: Saw pod success
Aug 30 15:13:47.519: INFO: Pod "downward-api-b8de61ed-8d74-4b86-b67e-8f4be374d562" satisfied condition "Succeeded or Failed"
Aug 30 15:13:47.536: INFO: Trying to get logs from node 10.63.224.189 pod downward-api-b8de61ed-8d74-4b86-b67e-8f4be374d562 container dapi-container: <nil>
STEP: delete the pod
Aug 30 15:13:47.622: INFO: Waiting for pod downward-api-b8de61ed-8d74-4b86-b67e-8f4be374d562 to disappear
Aug 30 15:13:47.637: INFO: Pod downward-api-b8de61ed-8d74-4b86-b67e-8f4be374d562 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 30 15:13:47.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6233" for this suite.

• [SLOW TEST:6.408 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":165,"skipped":2926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:13:47.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
W0830 15:13:47.862835      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:13:47.863: INFO: Waiting up to 5m0s for pod "client-containers-9a4cc04f-e469-48bc-b649-2e50c561b24c" in namespace "containers-4600" to be "Succeeded or Failed"
Aug 30 15:13:47.886: INFO: Pod "client-containers-9a4cc04f-e469-48bc-b649-2e50c561b24c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.080696ms
Aug 30 15:13:49.931: INFO: Pod "client-containers-9a4cc04f-e469-48bc-b649-2e50c561b24c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067756621s
Aug 30 15:13:51.953: INFO: Pod "client-containers-9a4cc04f-e469-48bc-b649-2e50c561b24c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08950713s
Aug 30 15:13:53.990: INFO: Pod "client-containers-9a4cc04f-e469-48bc-b649-2e50c561b24c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.127207774s
STEP: Saw pod success
Aug 30 15:13:53.990: INFO: Pod "client-containers-9a4cc04f-e469-48bc-b649-2e50c561b24c" satisfied condition "Succeeded or Failed"
Aug 30 15:13:54.010: INFO: Trying to get logs from node 10.63.224.189 pod client-containers-9a4cc04f-e469-48bc-b649-2e50c561b24c container agnhost-container: <nil>
STEP: delete the pod
Aug 30 15:13:54.172: INFO: Waiting for pod client-containers-9a4cc04f-e469-48bc-b649-2e50c561b24c to disappear
Aug 30 15:13:54.224: INFO: Pod client-containers-9a4cc04f-e469-48bc-b649-2e50c561b24c no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 30 15:13:54.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4600" for this suite.

• [SLOW TEST:6.596 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":166,"skipped":2961,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:13:54.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:13:54.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:14:02.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4899" for this suite.

• [SLOW TEST:8.020 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":167,"skipped":2975,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:14:02.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:14:02.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: creating the pod
STEP: submitting the pod to kubernetes
W0830 15:14:02.556777      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "main" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "main" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "main" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "main" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:14:02.626: INFO: The status of Pod pod-exec-websocket-2e8b91c2-7d3a-4f89-a9d5-818148cb1f0e is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:14:04.647: INFO: The status of Pod pod-exec-websocket-2e8b91c2-7d3a-4f89-a9d5-818148cb1f0e is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:14:06.651: INFO: The status of Pod pod-exec-websocket-2e8b91c2-7d3a-4f89-a9d5-818148cb1f0e is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 30 15:14:06.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5699" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":168,"skipped":2985,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:14:06.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-e7fbcecb-1796-40c1-9285-552116b0eb50
STEP: Creating a pod to test consume secrets
W0830 15:14:07.135768      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:14:07.136: INFO: Waiting up to 5m0s for pod "pod-secrets-b25bb4a1-cd7a-4fb8-a156-7d06e0ffdeff" in namespace "secrets-2531" to be "Succeeded or Failed"
Aug 30 15:14:07.176: INFO: Pod "pod-secrets-b25bb4a1-cd7a-4fb8-a156-7d06e0ffdeff": Phase="Pending", Reason="", readiness=false. Elapsed: 39.167581ms
Aug 30 15:14:09.207: INFO: Pod "pod-secrets-b25bb4a1-cd7a-4fb8-a156-7d06e0ffdeff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070048743s
Aug 30 15:14:11.227: INFO: Pod "pod-secrets-b25bb4a1-cd7a-4fb8-a156-7d06e0ffdeff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08994084s
Aug 30 15:14:13.245: INFO: Pod "pod-secrets-b25bb4a1-cd7a-4fb8-a156-7d06e0ffdeff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.107839164s
STEP: Saw pod success
Aug 30 15:14:13.245: INFO: Pod "pod-secrets-b25bb4a1-cd7a-4fb8-a156-7d06e0ffdeff" satisfied condition "Succeeded or Failed"
Aug 30 15:14:13.267: INFO: Trying to get logs from node 10.63.224.189 pod pod-secrets-b25bb4a1-cd7a-4fb8-a156-7d06e0ffdeff container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 15:14:13.351: INFO: Waiting for pod pod-secrets-b25bb4a1-cd7a-4fb8-a156-7d06e0ffdeff to disappear
Aug 30 15:14:13.365: INFO: Pod pod-secrets-b25bb4a1-cd7a-4fb8-a156-7d06e0ffdeff no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 30 15:14:13.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2531" for this suite.

• [SLOW TEST:6.484 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":169,"skipped":2990,"failed":0}
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:14:13.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 30 15:14:13.511: INFO: PodSpec: initContainers in spec.initContainers
W0830 15:14:13.587364      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:14:57.791: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f907b345-d5b3-4a02-83b9-c810e6893bdc", GenerateName:"", Namespace:"init-container-276", SelfLink:"", UID:"d2f17592-6156-48ab-a7b1-10a61fe6fc78", ResourceVersion:"103563", Generation:0, CreationTimestamp:time.Date(2022, time.August, 30, 15, 14, 13, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"511202562"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b6905cbc08d2fe172f89374a85f26399b501a2baa3f1cc8776e54272ed9185f8", "cni.projectcalico.org/podIP":"172.30.233.197/32", "cni.projectcalico.org/podIPs":"172.30.233.197/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.233.197\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.233.197\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 30, 15, 14, 13, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003cf0f18), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 30, 15, 14, 14, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003cf0f48), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 30, 15, 14, 14, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003cf0f78), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 30, 15, 14, 15, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003cf0fa8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-wwcrk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc009a9ef40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wwcrk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00cd67500), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wwcrk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00cd67560), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wwcrk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00cd674a0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00b33e7f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.63.224.189", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003317730), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00b33e8b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00b33e8d0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00b33e8ec), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00b33e8f0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005cd60a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 30, 15, 14, 13, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 30, 15, 14, 13, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 30, 15, 14, 13, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 30, 15, 14, 13, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.63.224.189", PodIP:"172.30.233.197", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.233.197"}}, StartTime:time.Date(2022, time.August, 30, 15, 14, 13, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003317810)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003317880)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://98d205e01b0d3ef5275e12ba600f9f9b3cd139744b4dd8bf97e43cafc96b4e39", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc009a9efc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc009a9efa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc00b33e964)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 30 15:14:57.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-276" for this suite.

• [SLOW TEST:44.438 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":170,"skipped":2994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:14:57.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Aug 30 15:14:58.075: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-9012 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 15:14:58.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9012" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":171,"skipped":3040,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:14:58.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
W0830 15:14:59.031204      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:14:59.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 15:15:01.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 15, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 14, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 15, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 14, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 15:15:04.171: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
W0830 15:15:04.508065      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webhook-added-init-container", "example" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webhook-added-init-container", "example" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webhook-added-init-container", "example" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webhook-added-init-container", "example" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:15:04.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1662" for this suite.
STEP: Destroying namespace "webhook-1662-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.565 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":172,"skipped":3043,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:15:04.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:15:05.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:15:08.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-27" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":173,"skipped":3047,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:15:08.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 30 15:15:08.802: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 15:16:09.196: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
W0830 15:16:09.324681      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod0-0-sched-preemption-low-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod0-0-sched-preemption-low-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod0-0-sched-preemption-low-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod0-0-sched-preemption-low-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:16:09.325: INFO: Created pod: pod0-0-sched-preemption-low-priority
W0830 15:16:09.393072      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod0-1-sched-preemption-medium-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod0-1-sched-preemption-medium-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod0-1-sched-preemption-medium-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod0-1-sched-preemption-medium-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:16:09.393: INFO: Created pod: pod0-1-sched-preemption-medium-priority
W0830 15:16:09.460965      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod1-0-sched-preemption-medium-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod1-0-sched-preemption-medium-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod1-0-sched-preemption-medium-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod1-0-sched-preemption-medium-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:16:09.461: INFO: Created pod: pod1-0-sched-preemption-medium-priority
W0830 15:16:09.515126      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod1-1-sched-preemption-medium-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod1-1-sched-preemption-medium-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod1-1-sched-preemption-medium-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod1-1-sched-preemption-medium-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:16:09.515: INFO: Created pod: pod1-1-sched-preemption-medium-priority
W0830 15:16:09.593987      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod2-0-sched-preemption-medium-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod2-0-sched-preemption-medium-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod2-0-sched-preemption-medium-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod2-0-sched-preemption-medium-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:16:09.594: INFO: Created pod: pod2-0-sched-preemption-medium-priority
W0830 15:16:09.654510      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod2-1-sched-preemption-medium-priority" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod2-1-sched-preemption-medium-priority" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod2-1-sched-preemption-medium-priority" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod2-1-sched-preemption-medium-priority" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:16:09.654: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
W0830 15:16:13.838995      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "preemptor-pod" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "preemptor-pod" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "preemptor-pod" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "preemptor-pod" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 30 15:16:20.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-345" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:71.782 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":174,"skipped":3054,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:16:20.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 30 15:16:21.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2180" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":175,"skipped":3072,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:16:21.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:16:21.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-6061
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Aug 30 15:16:28.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-4563" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 30 15:16:28.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6061" for this suite.

• [SLOW TEST:6.643 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":176,"skipped":3082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:16:28.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Aug 30 15:16:28.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3732 create -f -'
Aug 30 15:16:31.861: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"agnhost-primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"agnhost-primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"agnhost-primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"agnhost-primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 15:16:31.861: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 30 15:16:32.880: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 15:16:32.880: INFO: Found 0 / 1
Aug 30 15:16:33.893: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 15:16:33.893: INFO: Found 0 / 1
Aug 30 15:16:34.883: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 15:16:34.883: INFO: Found 1 / 1
Aug 30 15:16:34.883: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 30 15:16:34.903: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 15:16:34.903: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 30 15:16:34.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3732 patch pod agnhost-primary-8d99l -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 30 15:16:35.148: INFO: stderr: ""
Aug 30 15:16:35.148: INFO: stdout: "pod/agnhost-primary-8d99l patched\n"
STEP: checking annotations
Aug 30 15:16:35.165: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 15:16:35.165: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 15:16:35.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3732" for this suite.

• [SLOW TEST:7.047 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1486
    should add annotations for pods in rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":177,"skipped":3139,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:16:35.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-cad152e8-e3f9-4881-9d8f-914f91497c7a in namespace container-probe-10
W0830 15:16:35.395355      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:16:39.460: INFO: Started pod test-webserver-cad152e8-e3f9-4881-9d8f-914f91497c7a in namespace container-probe-10
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 15:16:39.478: INFO: Initial restart count of pod test-webserver-cad152e8-e3f9-4881-9d8f-914f91497c7a is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 30 15:20:40.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-10" for this suite.

• [SLOW TEST:245.502 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":178,"skipped":3151,"failed":0}
SSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:20:40.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Aug 30 15:20:40.945: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Aug 30 15:20:41.065: INFO: waiting for watch events with expected annotations in namespace
Aug 30 15:20:41.065: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Aug 30 15:20:41.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-1150" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":179,"skipped":3158,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:20:41.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:20:41.360: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-6edf5ba3-7244-4eaf-b4d9-5511124ccf57
STEP: Creating the pod
W0830 15:20:41.458141      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 15:20:45.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8519" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":180,"skipped":3160,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:20:45.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Aug 30 15:20:46.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8402" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":181,"skipped":3177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:20:46.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0830 15:20:46.411160      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-readonly-fsf6e9114c-342e-4f60-811b-a0bd15791c63" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-fsf6e9114c-342e-4f60-811b-a0bd15791c63" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-fsf6e9114c-342e-4f60-811b-a0bd15791c63" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-fsf6e9114c-342e-4f60-811b-a0bd15791c63" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:20:46.438: INFO: The status of Pod busybox-readonly-fsf6e9114c-342e-4f60-811b-a0bd15791c63 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:20:48.468: INFO: The status of Pod busybox-readonly-fsf6e9114c-342e-4f60-811b-a0bd15791c63 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:20:50.464: INFO: The status of Pod busybox-readonly-fsf6e9114c-342e-4f60-811b-a0bd15791c63 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 30 15:20:50.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6858" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":182,"skipped":3224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:20:50.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-d44f3acd-55c0-4e8d-840e-30aec86abe90
STEP: Creating secret with name secret-projected-all-test-volume-448cabd4-13b6-427e-9c9b-1bd60f1e34af
STEP: Creating a pod to test Check all projections for projected volume plugin
W0830 15:20:50.846424      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-all-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-all-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-all-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-all-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:20:50.846: INFO: Waiting up to 5m0s for pod "projected-volume-a5c58d6b-cbbd-42ad-a06b-55b4c3594bc7" in namespace "projected-4366" to be "Succeeded or Failed"
Aug 30 15:20:50.879: INFO: Pod "projected-volume-a5c58d6b-cbbd-42ad-a06b-55b4c3594bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.419147ms
Aug 30 15:20:52.899: INFO: Pod "projected-volume-a5c58d6b-cbbd-42ad-a06b-55b4c3594bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05305146s
Aug 30 15:20:54.922: INFO: Pod "projected-volume-a5c58d6b-cbbd-42ad-a06b-55b4c3594bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075982022s
Aug 30 15:20:56.944: INFO: Pod "projected-volume-a5c58d6b-cbbd-42ad-a06b-55b4c3594bc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098031874s
STEP: Saw pod success
Aug 30 15:20:56.945: INFO: Pod "projected-volume-a5c58d6b-cbbd-42ad-a06b-55b4c3594bc7" satisfied condition "Succeeded or Failed"
Aug 30 15:20:56.966: INFO: Trying to get logs from node 10.63.224.189 pod projected-volume-a5c58d6b-cbbd-42ad-a06b-55b4c3594bc7 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 30 15:20:57.055: INFO: Waiting for pod projected-volume-a5c58d6b-cbbd-42ad-a06b-55b4c3594bc7 to disappear
Aug 30 15:20:57.072: INFO: Pod projected-volume-a5c58d6b-cbbd-42ad-a06b-55b4c3594bc7 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Aug 30 15:20:57.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4366" for this suite.

• [SLOW TEST:6.544 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":183,"skipped":3257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:20:57.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
W0830 15:20:57.313577      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 30 15:21:11.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5781" for this suite.

• [SLOW TEST:14.275 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":184,"skipped":3346,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:21:11.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Aug 30 15:21:11.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4008" for this suite.
STEP: Destroying namespace "nspatchtest-69745efc-eeb8-4782-bbfd-d0def226ab5d-2533" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":185,"skipped":3357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:21:11.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:21:11.955: INFO: Create a RollingUpdate DaemonSet
W0830 15:21:11.985987      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:21:11.986: INFO: Check that daemon pods launch on every node of the cluster
Aug 30 15:21:12.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:21:12.060: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:21:13.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:21:13.097: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:21:14.111: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:21:14.111: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:21:15.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 15:21:15.115: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Aug 30 15:21:15.115: INFO: Update the DaemonSet to trigger a rollout
Aug 30 15:21:15.196: INFO: Updating DaemonSet daemon-set
W0830 15:21:15.196750      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:21:19.292: INFO: Roll back the DaemonSet before rollout is complete
Aug 30 15:21:19.327: INFO: Updating DaemonSet daemon-set
Aug 30 15:21:19.327: INFO: Make sure DaemonSet rollback is complete
W0830 15:21:19.327480      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:21:19.346: INFO: Wrong image for pod: daemon-set-t9cqh. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Aug 30 15:21:19.346: INFO: Pod daemon-set-t9cqh is not available
Aug 30 15:21:27.396: INFO: Pod daemon-set-d8tdb is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-906, will wait for the garbage collector to delete the pods
Aug 30 15:21:27.631: INFO: Deleting DaemonSet.extensions daemon-set took: 110.498875ms
Aug 30 15:21:27.732: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.134606ms
Aug 30 15:21:31.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:21:31.168: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 15:21:31.182: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"107250"},"items":null}

Aug 30 15:21:31.202: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"107250"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 30 15:21:31.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-906" for this suite.

• [SLOW TEST:19.623 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":186,"skipped":3396,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:21:31.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 30 15:21:31.578: INFO: Waiting up to 5m0s for pod "pod-99b03936-8864-4519-b8ce-04392c258e82" in namespace "emptydir-1696" to be "Succeeded or Failed"
W0830 15:21:31.578019      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:21:31.601: INFO: Pod "pod-99b03936-8864-4519-b8ce-04392c258e82": Phase="Pending", Reason="", readiness=false. Elapsed: 22.831938ms
Aug 30 15:21:33.640: INFO: Pod "pod-99b03936-8864-4519-b8ce-04392c258e82": Phase="Running", Reason="", readiness=true. Elapsed: 2.061762173s
Aug 30 15:21:35.665: INFO: Pod "pod-99b03936-8864-4519-b8ce-04392c258e82": Phase="Running", Reason="", readiness=false. Elapsed: 4.087262545s
Aug 30 15:21:37.690: INFO: Pod "pod-99b03936-8864-4519-b8ce-04392c258e82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.111973584s
STEP: Saw pod success
Aug 30 15:21:37.690: INFO: Pod "pod-99b03936-8864-4519-b8ce-04392c258e82" satisfied condition "Succeeded or Failed"
Aug 30 15:21:37.708: INFO: Trying to get logs from node 10.63.224.189 pod pod-99b03936-8864-4519-b8ce-04392c258e82 container test-container: <nil>
STEP: delete the pod
Aug 30 15:21:37.847: INFO: Waiting for pod pod-99b03936-8864-4519-b8ce-04392c258e82 to disappear
Aug 30 15:21:37.864: INFO: Pod pod-99b03936-8864-4519-b8ce-04392c258e82 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 15:21:37.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1696" for this suite.

• [SLOW TEST:6.572 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":187,"skipped":3405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:21:37.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
W0830 15:21:38.084337      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 30 15:27:02.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7200" for this suite.

• [SLOW TEST:324.387 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":188,"skipped":3485,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:27:02.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:27:02.427: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: creating the pod
STEP: submitting the pod to kubernetes
W0830 15:27:02.512439      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "main" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "main" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "main" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "main" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:27:02.529: INFO: The status of Pod pod-logs-websocket-9965733b-a31e-434d-9ad7-b023f501f91a is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:27:04.553: INFO: The status of Pod pod-logs-websocket-9965733b-a31e-434d-9ad7-b023f501f91a is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:27:06.554: INFO: The status of Pod pod-logs-websocket-9965733b-a31e-434d-9ad7-b023f501f91a is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 30 15:27:06.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5119" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":189,"skipped":3494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:27:06.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
W0830 15:27:06.932686      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:27:06.932: INFO: Waiting up to 5m0s for pod "pod-fbf5f940-269b-4d32-bb1d-d27fbc4fc248" in namespace "emptydir-9593" to be "Succeeded or Failed"
Aug 30 15:27:06.950: INFO: Pod "pod-fbf5f940-269b-4d32-bb1d-d27fbc4fc248": Phase="Pending", Reason="", readiness=false. Elapsed: 17.624106ms
Aug 30 15:27:08.971: INFO: Pod "pod-fbf5f940-269b-4d32-bb1d-d27fbc4fc248": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038813267s
Aug 30 15:27:10.997: INFO: Pod "pod-fbf5f940-269b-4d32-bb1d-d27fbc4fc248": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064224423s
Aug 30 15:27:13.019: INFO: Pod "pod-fbf5f940-269b-4d32-bb1d-d27fbc4fc248": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.086384706s
STEP: Saw pod success
Aug 30 15:27:13.019: INFO: Pod "pod-fbf5f940-269b-4d32-bb1d-d27fbc4fc248" satisfied condition "Succeeded or Failed"
Aug 30 15:27:13.041: INFO: Trying to get logs from node 10.63.224.189 pod pod-fbf5f940-269b-4d32-bb1d-d27fbc4fc248 container test-container: <nil>
STEP: delete the pod
Aug 30 15:27:13.166: INFO: Waiting for pod pod-fbf5f940-269b-4d32-bb1d-d27fbc4fc248 to disappear
Aug 30 15:27:13.184: INFO: Pod pod-fbf5f940-269b-4d32-bb1d-d27fbc4fc248 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 15:27:13.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9593" for this suite.

• [SLOW TEST:6.516 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":190,"skipped":3518,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:27:13.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
W0830 15:27:13.425046      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for all Replicas to be Ready
Aug 30 15:27:13.440: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 15:27:13.440: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 15:27:13.462: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 15:27:13.462: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 15:27:13.518: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 15:27:13.522: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 15:27:13.616: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 15:27:13.617: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 15:27:15.627: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 30 15:27:15.628: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 30 15:27:15.868: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
W0830 15:27:15.898279      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:27:15.907: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Aug 30 15:27:15.917: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0
Aug 30 15:27:15.917: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0
Aug 30 15:27:15.917: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0
Aug 30 15:27:15.917: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0
Aug 30 15:27:15.917: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0
Aug 30 15:27:15.917: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0
Aug 30 15:27:15.917: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0
Aug 30 15:27:15.917: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 0
Aug 30 15:27:15.918: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:15.918: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:15.918: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:15.918: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:15.919: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:15.919: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:15.945: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:15.945: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:16.016: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:16.016: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:16.062: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:16.062: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:16.114: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:16.114: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:18.937: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:18.937: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:19.007: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
STEP: listing Deployments
Aug 30 15:27:19.032: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
W0830 15:27:19.075138      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:27:19.082: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Aug 30 15:27:19.106: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 15:27:19.128: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 15:27:19.188: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 15:27:19.224: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 15:27:19.255: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 15:27:19.310: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 15:27:21.695: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 15:27:21.975: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 15:27:22.074: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 15:27:22.196: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 15:27:24.727: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Aug 30 15:27:24.837: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:24.837: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:24.837: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:24.840: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:24.840: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:24.840: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 1
Aug 30 15:27:24.840: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:24.847: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 3
Aug 30 15:27:24.847: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:24.847: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 2
Aug 30 15:27:24.848: INFO: observed Deployment test-deployment in namespace deployment-7460 with ReadyReplicas 3
STEP: deleting the Deployment
Aug 30 15:27:24.882: INFO: observed event type MODIFIED
Aug 30 15:27:24.883: INFO: observed event type MODIFIED
Aug 30 15:27:24.883: INFO: observed event type MODIFIED
Aug 30 15:27:24.884: INFO: observed event type MODIFIED
Aug 30 15:27:24.884: INFO: observed event type MODIFIED
Aug 30 15:27:24.885: INFO: observed event type MODIFIED
Aug 30 15:27:24.886: INFO: observed event type MODIFIED
Aug 30 15:27:24.887: INFO: observed event type MODIFIED
Aug 30 15:27:24.887: INFO: observed event type MODIFIED
Aug 30 15:27:24.887: INFO: observed event type MODIFIED
Aug 30 15:27:24.887: INFO: observed event type MODIFIED
Aug 30 15:27:24.889: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 15:27:24.905: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 30 15:27:24.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7460" for this suite.

• [SLOW TEST:11.726 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":191,"skipped":3525,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:27:24.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 15:27:25.198370      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:27:25.204: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc3492d1-9f4f-40a5-8b55-e1d2497e2637" in namespace "downward-api-2252" to be "Succeeded or Failed"
Aug 30 15:27:25.238: INFO: Pod "downwardapi-volume-dc3492d1-9f4f-40a5-8b55-e1d2497e2637": Phase="Pending", Reason="", readiness=false. Elapsed: 34.119619ms
Aug 30 15:27:27.261: INFO: Pod "downwardapi-volume-dc3492d1-9f4f-40a5-8b55-e1d2497e2637": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057294254s
Aug 30 15:27:29.287: INFO: Pod "downwardapi-volume-dc3492d1-9f4f-40a5-8b55-e1d2497e2637": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083049623s
Aug 30 15:27:31.339: INFO: Pod "downwardapi-volume-dc3492d1-9f4f-40a5-8b55-e1d2497e2637": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.135547069s
STEP: Saw pod success
Aug 30 15:27:31.339: INFO: Pod "downwardapi-volume-dc3492d1-9f4f-40a5-8b55-e1d2497e2637" satisfied condition "Succeeded or Failed"
Aug 30 15:27:31.359: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-dc3492d1-9f4f-40a5-8b55-e1d2497e2637 container client-container: <nil>
STEP: delete the pod
Aug 30 15:27:31.509: INFO: Waiting for pod downwardapi-volume-dc3492d1-9f4f-40a5-8b55-e1d2497e2637 to disappear
Aug 30 15:27:31.526: INFO: Pod downwardapi-volume-dc3492d1-9f4f-40a5-8b55-e1d2497e2637 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 15:27:31.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2252" for this suite.

• [SLOW TEST:6.626 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":192,"skipped":3536,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:27:31.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:27:31.713: INFO: Creating ReplicaSet my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628
W0830 15:27:31.732024      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:27:31.770: INFO: Pod name my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628: Found 0 pods out of 1
Aug 30 15:27:36.799: INFO: Pod name my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628: Found 1 pods out of 1
Aug 30 15:27:36.799: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628" is running
Aug 30 15:27:36.816: INFO: Pod "my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628-5w2dt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-30 15:27:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-30 15:27:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-30 15:27:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-30 15:27:31 +0000 UTC Reason: Message:}])
Aug 30 15:27:36.816: INFO: Trying to dial the pod
Aug 30 15:27:41.919: INFO: Controller my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628: Got expected result from replica 1 [my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628-5w2dt]: "my-hostname-basic-3ea7005b-4d0d-4c6d-aaa0-76106a5fc628-5w2dt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 30 15:27:41.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6848" for this suite.

• [SLOW TEST:10.372 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":193,"skipped":3543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:27:41.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-298407fa-975b-4395-9217-19af06203d68
STEP: Creating a pod to test consume secrets
W0830 15:27:42.214434      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:27:42.215: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2f01527e-eb5c-4e31-addb-9b41979108cf" in namespace "projected-9117" to be "Succeeded or Failed"
Aug 30 15:27:42.239: INFO: Pod "pod-projected-secrets-2f01527e-eb5c-4e31-addb-9b41979108cf": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017884ms
Aug 30 15:27:44.259: INFO: Pod "pod-projected-secrets-2f01527e-eb5c-4e31-addb-9b41979108cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0441678s
Aug 30 15:27:46.296: INFO: Pod "pod-projected-secrets-2f01527e-eb5c-4e31-addb-9b41979108cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081441367s
Aug 30 15:27:48.366: INFO: Pod "pod-projected-secrets-2f01527e-eb5c-4e31-addb-9b41979108cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.151472833s
STEP: Saw pod success
Aug 30 15:27:48.366: INFO: Pod "pod-projected-secrets-2f01527e-eb5c-4e31-addb-9b41979108cf" satisfied condition "Succeeded or Failed"
Aug 30 15:27:48.421: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-secrets-2f01527e-eb5c-4e31-addb-9b41979108cf container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 15:27:48.636: INFO: Waiting for pod pod-projected-secrets-2f01527e-eb5c-4e31-addb-9b41979108cf to disappear
Aug 30 15:27:48.654: INFO: Pod pod-projected-secrets-2f01527e-eb5c-4e31-addb-9b41979108cf no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 30 15:27:48.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9117" for this suite.

• [SLOW TEST:6.727 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":194,"skipped":3572,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:27:48.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 30 15:27:49.063: INFO: starting watch
STEP: patching
STEP: updating
Aug 30 15:27:49.121: INFO: waiting for watch events with expected annotations
Aug 30 15:27:49.121: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Aug 30 15:27:49.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2761" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":195,"skipped":3592,"failed":0}
SSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:27:49.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
W0830 15:27:49.629777      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:27:49.630: INFO: Waiting up to 5m0s for pod "client-containers-65dd14f3-c803-45eb-8004-0da208ecbd52" in namespace "containers-6275" to be "Succeeded or Failed"
Aug 30 15:27:49.650: INFO: Pod "client-containers-65dd14f3-c803-45eb-8004-0da208ecbd52": Phase="Pending", Reason="", readiness=false. Elapsed: 20.235866ms
Aug 30 15:27:51.712: INFO: Pod "client-containers-65dd14f3-c803-45eb-8004-0da208ecbd52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082485585s
Aug 30 15:27:53.745: INFO: Pod "client-containers-65dd14f3-c803-45eb-8004-0da208ecbd52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115399188s
Aug 30 15:27:55.771: INFO: Pod "client-containers-65dd14f3-c803-45eb-8004-0da208ecbd52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.140950794s
STEP: Saw pod success
Aug 30 15:27:55.771: INFO: Pod "client-containers-65dd14f3-c803-45eb-8004-0da208ecbd52" satisfied condition "Succeeded or Failed"
Aug 30 15:27:55.790: INFO: Trying to get logs from node 10.63.224.189 pod client-containers-65dd14f3-c803-45eb-8004-0da208ecbd52 container agnhost-container: <nil>
STEP: delete the pod
Aug 30 15:27:55.946: INFO: Waiting for pod client-containers-65dd14f3-c803-45eb-8004-0da208ecbd52 to disappear
Aug 30 15:27:55.962: INFO: Pod client-containers-65dd14f3-c803-45eb-8004-0da208ecbd52 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 30 15:27:55.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6275" for this suite.

• [SLOW TEST:6.608 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":196,"skipped":3598,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:27:56.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
W0830 15:27:56.227065      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:27:56.227: INFO: Waiting up to 5m0s for pod "client-containers-ebc6042f-8ab9-4435-b857-aa84b26af0bb" in namespace "containers-9504" to be "Succeeded or Failed"
Aug 30 15:27:56.296: INFO: Pod "client-containers-ebc6042f-8ab9-4435-b857-aa84b26af0bb": Phase="Pending", Reason="", readiness=false. Elapsed: 69.191494ms
Aug 30 15:27:58.318: INFO: Pod "client-containers-ebc6042f-8ab9-4435-b857-aa84b26af0bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09075261s
Aug 30 15:28:00.348: INFO: Pod "client-containers-ebc6042f-8ab9-4435-b857-aa84b26af0bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.120927426s
Aug 30 15:28:02.379: INFO: Pod "client-containers-ebc6042f-8ab9-4435-b857-aa84b26af0bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.15182963s
STEP: Saw pod success
Aug 30 15:28:02.379: INFO: Pod "client-containers-ebc6042f-8ab9-4435-b857-aa84b26af0bb" satisfied condition "Succeeded or Failed"
Aug 30 15:28:02.403: INFO: Trying to get logs from node 10.63.224.189 pod client-containers-ebc6042f-8ab9-4435-b857-aa84b26af0bb container agnhost-container: <nil>
STEP: delete the pod
Aug 30 15:28:02.511: INFO: Waiting for pod client-containers-ebc6042f-8ab9-4435-b857-aa84b26af0bb to disappear
Aug 30 15:28:02.534: INFO: Pod client-containers-ebc6042f-8ab9-4435-b857-aa84b26af0bb no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 30 15:28:02.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9504" for this suite.

• [SLOW TEST:6.574 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":197,"skipped":3618,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:28:02.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-1748a340-5c44-45a4-83ee-70ec7eb5717a
STEP: Creating a pod to test consume configMaps
W0830 15:28:02.856763      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:28:02.857: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-40827589-7cc9-4e87-86f9-a8205fcbbba8" in namespace "projected-5871" to be "Succeeded or Failed"
Aug 30 15:28:02.914: INFO: Pod "pod-projected-configmaps-40827589-7cc9-4e87-86f9-a8205fcbbba8": Phase="Pending", Reason="", readiness=false. Elapsed: 57.49092ms
Aug 30 15:28:04.935: INFO: Pod "pod-projected-configmaps-40827589-7cc9-4e87-86f9-a8205fcbbba8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077715354s
Aug 30 15:28:06.956: INFO: Pod "pod-projected-configmaps-40827589-7cc9-4e87-86f9-a8205fcbbba8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098732059s
Aug 30 15:28:08.980: INFO: Pod "pod-projected-configmaps-40827589-7cc9-4e87-86f9-a8205fcbbba8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.123217002s
STEP: Saw pod success
Aug 30 15:28:08.980: INFO: Pod "pod-projected-configmaps-40827589-7cc9-4e87-86f9-a8205fcbbba8" satisfied condition "Succeeded or Failed"
Aug 30 15:28:08.998: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-configmaps-40827589-7cc9-4e87-86f9-a8205fcbbba8 container agnhost-container: <nil>
STEP: delete the pod
Aug 30 15:28:09.116: INFO: Waiting for pod pod-projected-configmaps-40827589-7cc9-4e87-86f9-a8205fcbbba8 to disappear
Aug 30 15:28:09.134: INFO: Pod pod-projected-configmaps-40827589-7cc9-4e87-86f9-a8205fcbbba8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 30 15:28:09.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5871" for this suite.

• [SLOW TEST:6.608 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":198,"skipped":3634,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:28:09.219: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-4da7579b-3c5b-4df9-a928-80988780db49
STEP: Creating a pod to test consume configMaps
W0830 15:28:09.493327      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:28:09.493: INFO: Waiting up to 5m0s for pod "pod-configmaps-05366e88-6532-46db-ab17-9ba96fded53f" in namespace "configmap-8331" to be "Succeeded or Failed"
Aug 30 15:28:09.520: INFO: Pod "pod-configmaps-05366e88-6532-46db-ab17-9ba96fded53f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.304683ms
Aug 30 15:28:11.544: INFO: Pod "pod-configmaps-05366e88-6532-46db-ab17-9ba96fded53f": Phase="Running", Reason="", readiness=true. Elapsed: 2.050382657s
Aug 30 15:28:13.561: INFO: Pod "pod-configmaps-05366e88-6532-46db-ab17-9ba96fded53f": Phase="Running", Reason="", readiness=false. Elapsed: 4.067970796s
Aug 30 15:28:15.595: INFO: Pod "pod-configmaps-05366e88-6532-46db-ab17-9ba96fded53f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.101408289s
STEP: Saw pod success
Aug 30 15:28:15.595: INFO: Pod "pod-configmaps-05366e88-6532-46db-ab17-9ba96fded53f" satisfied condition "Succeeded or Failed"
Aug 30 15:28:15.613: INFO: Trying to get logs from node 10.63.224.189 pod pod-configmaps-05366e88-6532-46db-ab17-9ba96fded53f container agnhost-container: <nil>
STEP: delete the pod
Aug 30 15:28:15.766: INFO: Waiting for pod pod-configmaps-05366e88-6532-46db-ab17-9ba96fded53f to disappear
Aug 30 15:28:15.784: INFO: Pod pod-configmaps-05366e88-6532-46db-ab17-9ba96fded53f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 15:28:15.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8331" for this suite.

• [SLOW TEST:6.606 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":199,"skipped":3666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:28:15.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
W0830 15:28:16.128474      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods launch on every node of the cluster.
Aug 30 15:28:16.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:28:16.168: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:28:17.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:28:17.208: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:28:18.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 15:28:18.216: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:28:19.217: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 15:28:19.217: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 30 15:28:19.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 15:28:19.332: INFO: Node 10.63.224.187 is running 0 daemon pod, expected 1
Aug 30 15:28:20.391: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 15:28:20.392: INFO: Node 10.63.224.187 is running 0 daemon pod, expected 1
Aug 30 15:28:21.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 15:28:21.395: INFO: Node 10.63.224.187 is running 0 daemon pod, expected 1
Aug 30 15:28:22.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 15:28:22.379: INFO: Node 10.63.224.187 is running 0 daemon pod, expected 1
Aug 30 15:28:23.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 15:28:23.379: INFO: Node 10.63.224.187 is running 0 daemon pod, expected 1
Aug 30 15:28:24.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 15:28:24.380: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7769, will wait for the garbage collector to delete the pods
Aug 30 15:28:24.483: INFO: Deleting DaemonSet.extensions daemon-set took: 19.392691ms
Aug 30 15:28:24.589: INFO: Terminating DaemonSet.extensions daemon-set pods took: 105.383882ms
Aug 30 15:28:27.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:28:27.414: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 15:28:27.427: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"110819"},"items":null}

Aug 30 15:28:27.440: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"110819"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 30 15:28:27.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7769" for this suite.

• [SLOW TEST:11.710 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":200,"skipped":3734,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:28:27.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-9770
W0830 15:28:27.705605      21 warnings.go:70] would violate PodSecurity "restricted:latest": host namespaces (hostNetwork=true)
Aug 30 15:28:27.737: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:28:29.760: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 30 15:28:29.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-9770 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 30 15:28:30.288: INFO: rc: 7
Aug 30 15:28:30.372: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 30 15:28:30.389: INFO: Pod kube-proxy-mode-detector no longer exists
Aug 30 15:28:30.389: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-9770 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-9770
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9770
W0830 15:28:30.479522      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-nodeport-timeout" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport-timeout" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport-timeout" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport-timeout" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 15:28:30.479650      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9770, replica count: 3
I0830 15:28:33.531740      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 15:28:33.578: INFO: Creating new exec pod
W0830 15:28:33.621851      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:28:36.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-9770 exec execpod-affinity6mgcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Aug 30 15:28:37.109: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Aug 30 15:28:37.109: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 15:28:37.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-9770 exec execpod-affinity6mgcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.200.165 80'
Aug 30 15:28:37.534: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.200.165 80\nConnection to 172.21.200.165 80 port [tcp/http] succeeded!\n"
Aug 30 15:28:37.534: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 15:28:37.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-9770 exec execpod-affinity6mgcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.158 31310'
Aug 30 15:28:37.953: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.63.224.158 31310\nConnection to 10.63.224.158 31310 port [tcp/*] succeeded!\n"
Aug 30 15:28:37.953: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 15:28:37.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-9770 exec execpod-affinity6mgcv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.63.224.187 31310'
Aug 30 15:28:38.391: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 10.63.224.187 31310\nConnection to 10.63.224.187 31310 port [tcp/*] succeeded!\n"
Aug 30 15:28:38.391: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 15:28:38.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-9770 exec execpod-affinity6mgcv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.63.224.158:31310/ ; done'
Aug 30 15:28:39.059: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n"
Aug 30 15:28:39.060: INFO: stdout: "\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p\naffinity-nodeport-timeout-5fg9p"
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Received response from host: affinity-nodeport-timeout-5fg9p
Aug 30 15:28:39.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-9770 exec execpod-affinity6mgcv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.63.224.158:31310/'
Aug 30 15:28:39.444: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n"
Aug 30 15:28:39.444: INFO: stdout: "affinity-nodeport-timeout-5fg9p"
Aug 30 15:28:59.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-9770 exec execpod-affinity6mgcv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.63.224.158:31310/'
Aug 30 15:28:59.817: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.63.224.158:31310/\n"
Aug 30 15:28:59.817: INFO: stdout: "affinity-nodeport-timeout-cks7l"
Aug 30 15:28:59.817: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9770, will wait for the garbage collector to delete the pods
Aug 30 15:28:59.960: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 20.838184ms
Aug 30 15:29:00.160: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.491564ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 15:29:03.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9770" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:35.840 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":201,"skipped":3772,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:29:03.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
W0830 15:29:03.602688      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:29:03.603: INFO: Waiting up to 5m0s for pod "pod-c115f37e-96ef-4e08-bd0b-22ec427275ed" in namespace "emptydir-1493" to be "Succeeded or Failed"
Aug 30 15:29:03.624: INFO: Pod "pod-c115f37e-96ef-4e08-bd0b-22ec427275ed": Phase="Pending", Reason="", readiness=false. Elapsed: 21.888115ms
Aug 30 15:29:05.674: INFO: Pod "pod-c115f37e-96ef-4e08-bd0b-22ec427275ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071341544s
Aug 30 15:29:07.712: INFO: Pod "pod-c115f37e-96ef-4e08-bd0b-22ec427275ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109011095s
Aug 30 15:29:09.732: INFO: Pod "pod-c115f37e-96ef-4e08-bd0b-22ec427275ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.128976621s
STEP: Saw pod success
Aug 30 15:29:09.732: INFO: Pod "pod-c115f37e-96ef-4e08-bd0b-22ec427275ed" satisfied condition "Succeeded or Failed"
Aug 30 15:29:09.752: INFO: Trying to get logs from node 10.63.224.189 pod pod-c115f37e-96ef-4e08-bd0b-22ec427275ed container test-container: <nil>
STEP: delete the pod
Aug 30 15:29:09.858: INFO: Waiting for pod pod-c115f37e-96ef-4e08-bd0b-22ec427275ed to disappear
Aug 30 15:29:09.895: INFO: Pod pod-c115f37e-96ef-4e08-bd0b-22ec427275ed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 15:29:09.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1493" for this suite.

• [SLOW TEST:6.562 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":202,"skipped":3773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:29:09.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 30 15:29:10.095: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 15:29:10.148: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 15:29:10.218: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.158 before test
Aug 30 15:29:10.405: INFO: calico-kube-controllers-867bb5b44d-wqzpr from calico-system started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.405: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 30 15:29:10.405: INFO: calico-node-rfpmd from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.405: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 15:29:10.405: INFO: ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-q4rqr from ibm-system started at 2022-08-30 12:17:08 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.405: INFO: 	Container ibm-cloud-provider-ip-130-198-93-237 ready: true, restart count 0
Aug 30 15:29:10.405: INFO: ibm-file-plugin-ccc878f48-frwb7 from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.405: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 30 15:29:10.405: INFO: ibm-keepalived-watcher-c2b7b from kube-system started at 2022-08-30 12:07:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.405: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 15:29:10.405: INFO: ibm-master-proxy-static-10.63.224.158 from kube-system started at 2022-08-30 12:07:30 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.405: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 15:29:10.405: INFO: 	Container pause ready: true, restart count 0
Aug 30 15:29:10.405: INFO: ibm-storage-watcher-6fcfdb7ccc-5s2xq from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.405: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 30 15:29:10.405: INFO: ibmcloud-block-storage-driver-kx4tx from kube-system started at 2022-08-30 12:07:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.405: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 15:29:10.405: INFO: ibmcloud-block-storage-plugin-7495f48b76-lht72 from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.405: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 30 15:29:10.405: INFO: vpn-59795d4f7c-rf6fw from kube-system started at 2022-08-30 12:18:35 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.405: INFO: 	Container vpn ready: true, restart count 0
Aug 30 15:29:10.406: INFO: cluster-node-tuning-operator-6c4d4b46dd-dsdts from openshift-cluster-node-tuning-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 30 15:29:10.406: INFO: tuned-mpvrs from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container tuned ready: true, restart count 0
Aug 30 15:29:10.406: INFO: cluster-samples-operator-79cb65b9b5-jbclm from openshift-cluster-samples-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 30 15:29:10.406: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 30 15:29:10.406: INFO: cluster-storage-operator-97dcf6b44-krf6f from openshift-cluster-storage-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 30 15:29:10.406: INFO: csi-snapshot-controller-7d8bf4bb58-6lkk4 from openshift-cluster-storage-operator started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 15:29:10.406: INFO: csi-snapshot-controller-operator-7b97dc6dfd-xjp62 from openshift-cluster-storage-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 30 15:29:10.406: INFO: csi-snapshot-webhook-8f4fd6cc6-48czg from openshift-cluster-storage-operator started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container webhook ready: true, restart count 0
Aug 30 15:29:10.406: INFO: console-operator-55bcdb7b48-tgxd4 from openshift-console-operator started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container console-operator ready: true, restart count 1
Aug 30 15:29:10.406: INFO: console-5dd7474d84-js5g9 from openshift-console started at 2022-08-30 12:14:31 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container console ready: true, restart count 0
Aug 30 15:29:10.406: INFO: dns-operator-5549dbd7c9-c4h6t from openshift-dns-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container dns-operator ready: true, restart count 0
Aug 30 15:29:10.406: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.406: INFO: dns-default-v6jl9 from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container dns ready: true, restart count 0
Aug 30 15:29:10.406: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.406: INFO: node-resolver-ppzz9 from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.406: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 15:29:10.407: INFO: cluster-image-registry-operator-cb6448756-qh2wd from openshift-image-registry started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 30 15:29:10.407: INFO: image-registry-7fb4f45578-scb5q from openshift-image-registry started at 2022-08-30 12:18:40 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container registry ready: true, restart count 0
Aug 30 15:29:10.407: INFO: node-ca-4pbx7 from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 15:29:10.407: INFO: ingress-canary-vwqf6 from openshift-ingress-canary started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 15:29:10.407: INFO: ingress-operator-57457886bd-v8vkg from openshift-ingress-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.407: INFO: router-default-bdd65b74f-jwc99 from openshift-ingress started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container router ready: true, restart count 0
Aug 30 15:29:10.407: INFO: insights-operator-6d9b46b7c5-6q6pl from openshift-insights started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container insights-operator ready: true, restart count 1
Aug 30 15:29:10.407: INFO: openshift-kube-proxy-ms7hb from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.407: INFO: kube-storage-version-migrator-operator-fb46b8c8d-n5q2j from openshift-kube-storage-version-migrator-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Aug 30 15:29:10.407: INFO: marketplace-operator-84ff65f9c9-mcwn4 from openshift-marketplace started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 30 15:29:10.407: INFO: cluster-monitoring-operator-677fb4cf4b-wfrk9 from openshift-monitoring started at 2022-08-30 12:09:18 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.407: INFO: kube-state-metrics-b6455c4dc-pknn6 from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (3 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 30 15:29:10.407: INFO: node-exporter-ct2nk from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 15:29:10.407: INFO: prometheus-adapter-67874b74f7-2kkcq from openshift-monitoring started at 2022-08-30 12:14:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 15:29:10.407: INFO: prometheus-operator-admission-webhook-f5f88b968-wkmnq from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 15:29:10.407: INFO: thanos-querier-75bdcf8599-tdj9h from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (6 container statuses recorded)
Aug 30 15:29:10.407: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:29:10.407: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 15:29:10.407: INFO: multus-additional-cni-plugins-fwxxj from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 15:29:10.408: INFO: multus-admission-controller-gvnw7 from openshift-multus started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.408: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 15:29:10.408: INFO: multus-n2s54 from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 15:29:10.408: INFO: network-metrics-daemon-ncgdn from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.408: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 15:29:10.408: INFO: network-check-source-5cb989cf6f-8pbqc from openshift-network-diagnostics started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 30 15:29:10.408: INFO: network-check-target-tqzgf from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 15:29:10.408: INFO: catalog-operator-5d9dd4bb98-hwstf from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 30 15:29:10.408: INFO: olm-operator-757497677b-z5lbc from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container olm-operator ready: true, restart count 0
Aug 30 15:29:10.408: INFO: package-server-manager-784548687f-9vg2b from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container package-server-manager ready: true, restart count 0
Aug 30 15:29:10.408: INFO: packageserver-b5cbb5ff5-2vxb9 from openshift-operator-lifecycle-manager started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 15:29:10.408: INFO: metrics-67dbdb4ffd-gj9lp from openshift-roks-metrics started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container metrics ready: true, restart count 3
Aug 30 15:29:10.408: INFO: push-gateway-58dc4cbdf8-kl5x7 from openshift-roks-metrics started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container push-gateway ready: true, restart count 0
Aug 30 15:29:10.408: INFO: service-ca-operator-5df8fbb45b-fz8bw from openshift-service-ca-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container service-ca-operator ready: true, restart count 1
Aug 30 15:29:10.408: INFO: sonobuoy-e2e-job-a8c1c5c47c2346af from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container e2e ready: true, restart count 0
Aug 30 15:29:10.408: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:29:10.408: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-fngwd from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:29:10.408: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 15:29:10.408: INFO: tigera-operator-7b76886f74-5rs97 from tigera-operator started at 2022-08-30 12:07:46 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.408: INFO: 	Container tigera-operator ready: true, restart count 2
Aug 30 15:29:10.408: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.187 before test
Aug 30 15:29:10.527: INFO: calico-node-bv858 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.527: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 15:29:10.527: INFO: calico-typha-66986f6f4d-mg7gw from calico-system started at 2022-08-30 12:08:59 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.527: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 15:29:10.527: INFO: ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-gq5c7 from ibm-system started at 2022-08-30 12:17:08 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.528: INFO: 	Container ibm-cloud-provider-ip-130-198-93-237 ready: true, restart count 0
Aug 30 15:29:10.528: INFO: ibm-keepalived-watcher-7czrz from kube-system started at 2022-08-30 12:07:36 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.528: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 15:29:10.528: INFO: ibm-master-proxy-static-10.63.224.187 from kube-system started at 2022-08-30 12:07:34 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.528: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 15:29:10.528: INFO: 	Container pause ready: true, restart count 0
Aug 30 15:29:10.528: INFO: ibmcloud-block-storage-driver-m5jbh from kube-system started at 2022-08-30 12:07:42 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.528: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 15:29:10.528: INFO: tuned-c2ph5 from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.528: INFO: 	Container tuned ready: true, restart count 0
Aug 30 15:29:10.528: INFO: csi-snapshot-controller-7d8bf4bb58-gv7xl from openshift-cluster-storage-operator started at 2022-08-30 12:10:04 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.528: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 15:29:10.528: INFO: csi-snapshot-webhook-8f4fd6cc6-pnk9x from openshift-cluster-storage-operator started at 2022-08-30 12:10:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.528: INFO: 	Container webhook ready: true, restart count 0
Aug 30 15:29:10.528: INFO: console-5dd7474d84-t6lg4 from openshift-console started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.528: INFO: 	Container console ready: true, restart count 0
Aug 30 15:29:10.528: INFO: downloads-6f74f6fcbf-sbj8k from openshift-console started at 2022-08-30 12:10:07 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.528: INFO: 	Container download-server ready: true, restart count 0
Aug 30 15:29:10.528: INFO: dns-default-dltcr from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.528: INFO: 	Container dns ready: true, restart count 0
Aug 30 15:29:10.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.528: INFO: node-resolver-n66mp from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.529: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 15:29:10.529: INFO: node-ca-9bj8c from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.529: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 15:29:10.529: INFO: registry-pvc-permissions-2pnhd from openshift-image-registry started at 2022-08-30 12:18:48 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.529: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 30 15:29:10.529: INFO: ingress-canary-zjwth from openshift-ingress-canary started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.529: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 15:29:10.529: INFO: router-default-bdd65b74f-9xp2d from openshift-ingress started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.529: INFO: 	Container router ready: true, restart count 0
Aug 30 15:29:10.529: INFO: openshift-kube-proxy-whcnq from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.529: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 15:29:10.529: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.529: INFO: migrator-59f7fcfc8f-l77kl from openshift-kube-storage-version-migrator started at 2022-08-30 12:10:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.529: INFO: 	Container migrator ready: true, restart count 0
Aug 30 15:29:10.529: INFO: certified-operators-t6598 from openshift-marketplace started at 2022-08-30 13:47:48 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.530: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:29:10.530: INFO: community-operators-btnw8 from openshift-marketplace started at 2022-08-30 12:11:53 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.530: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:29:10.530: INFO: redhat-marketplace-9rltc from openshift-marketplace started at 2022-08-30 12:11:54 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.530: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:29:10.530: INFO: redhat-operators-cnsks from openshift-marketplace started at 2022-08-30 12:11:53 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.530: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:29:10.530: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-08-30 12:14:46 +0000 UTC (6 container statuses recorded)
Aug 30 15:29:10.530: INFO: 	Container alertmanager ready: true, restart count 0
Aug 30 15:29:10.530: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 15:29:10.530: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:29:10.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.530: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 15:29:10.530: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:29:10.530: INFO: node-exporter-gls7c from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.530: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 15:29:10.530: INFO: openshift-state-metrics-7984888fbd-dgxfd from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (3 container statuses recorded)
Aug 30 15:29:10.530: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 15:29:10.530: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 15:29:10.530: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 30 15:29:10.530: INFO: prometheus-adapter-67874b74f7-xwhkv from openshift-monitoring started at 2022-08-30 12:14:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.530: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 15:29:10.530: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-08-30 12:15:07 +0000 UTC (6 container statuses recorded)
Aug 30 15:29:10.530: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 15:29:10.531: INFO: prometheus-operator-admission-webhook-f5f88b968-wjzg5 from openshift-monitoring started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.531: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 15:29:10.531: INFO: thanos-querier-75bdcf8599-tb7gh from openshift-monitoring started at 2022-08-30 12:14:46 +0000 UTC (6 container statuses recorded)
Aug 30 15:29:10.531: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 15:29:10.531: INFO: multus-additional-cni-plugins-8j4kx from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.531: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 15:29:10.531: INFO: multus-admission-controller-xzspk from openshift-multus started at 2022-08-30 12:09:28 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.531: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.531: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 15:29:10.531: INFO: multus-qpmzg from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.531: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 15:29:10.532: INFO: network-metrics-daemon-nlcxx from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.532: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.532: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 15:29:10.532: INFO: network-check-target-wrm4t from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.532: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 15:29:10.532: INFO: network-operator-55b69485bb-qth5v from openshift-network-operator started at 2022-08-30 12:07:47 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.532: INFO: 	Container network-operator ready: true, restart count 1
Aug 30 15:29:10.532: INFO: collect-profiles-27697845-wmsd6 from openshift-operator-lifecycle-manager started at 2022-08-30 14:45:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.532: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 15:29:10.532: INFO: collect-profiles-27697860-qvs8g from openshift-operator-lifecycle-manager started at 2022-08-30 15:00:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.532: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 15:29:10.532: INFO: collect-profiles-27697875-986x4 from openshift-operator-lifecycle-manager started at 2022-08-30 15:15:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.532: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 15:29:10.532: INFO: packageserver-b5cbb5ff5-nc6jk from openshift-operator-lifecycle-manager started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.532: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 15:29:10.532: INFO: sonobuoy from sonobuoy started at 2022-08-30 14:26:02 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.532: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 15:29:10.532: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-7b4kf from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.532: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:29:10.532: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 15:29:10.532: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.189 before test
Aug 30 15:29:10.631: INFO: calico-node-g5sh7 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 15:29:10.631: INFO: calico-typha-66986f6f4d-rd6m6 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 15:29:10.631: INFO: ibm-keepalived-watcher-ctfdm from kube-system started at 2022-08-30 12:08:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 15:29:10.631: INFO: ibm-master-proxy-static-10.63.224.189 from kube-system started at 2022-08-30 12:07:47 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container pause ready: true, restart count 0
Aug 30 15:29:10.631: INFO: ibmcloud-block-storage-driver-xl7s6 from kube-system started at 2022-08-30 12:08:07 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 15:29:10.631: INFO: tuned-p2mrf from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container tuned ready: true, restart count 0
Aug 30 15:29:10.631: INFO: downloads-6f74f6fcbf-gsbhd from openshift-console started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container download-server ready: true, restart count 0
Aug 30 15:29:10.631: INFO: dns-default-zvh4h from openshift-dns started at 2022-08-30 14:53:11 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container dns ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: node-resolver-9tslx from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 15:29:10.631: INFO: node-ca-mq7hh from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 15:29:10.631: INFO: ingress-canary-qxqwq from openshift-ingress-canary started at 2022-08-30 14:52:56 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 15:29:10.631: INFO: openshift-kube-proxy-5zfxw from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-08-30 14:52:56 +0000 UTC (6 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container alertmanager ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: node-exporter-9w9vp from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 15:29:10.631: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-08-30 14:52:58 +0000 UTC (6 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 15:29:10.631: INFO: prometheus-operator-dd6c54897-5s5xx from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 30 15:29:10.631: INFO: telemeter-client-dd46ccd5b-ttrgn from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (3 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container reload ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 30 15:29:10.631: INFO: multus-additional-cni-plugins-gnz55 from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 15:29:10.631: INFO: multus-admission-controller-94qqx from openshift-multus started at 2022-08-30 14:53:21 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 15:29:10.631: INFO: multus-nzfsd from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 15:29:10.631: INFO: network-metrics-daemon-hzwd2 from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 15:29:10.631: INFO: network-check-target-whfqq from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 15:29:10.631: INFO: service-ca-5f4d84b84b-wnqw2 from openshift-service-ca started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container service-ca-controller ready: false, restart count 0
Aug 30 15:29:10.631: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-x4jz5 from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:29:10.631: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:29:10.631: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
W0830 15:29:10.901420      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "restricted-pod" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "restricted-pod" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "restricted-pod" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "restricted-pod" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17102901f66a8209], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 30 15:29:11.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4377" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":203,"skipped":3796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:29:11.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 30 15:29:12.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2525" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":204,"skipped":3844,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:29:12.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-c6082dce-d6cb-4d0a-9d71-763cdb9ed468 in namespace container-probe-4800
W0830 15:29:12.486355      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:29:16.550: INFO: Started pod busybox-c6082dce-d6cb-4d0a-9d71-763cdb9ed468 in namespace container-probe-4800
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 15:29:16.568: INFO: Initial restart count of pod busybox-c6082dce-d6cb-4d0a-9d71-763cdb9ed468 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 30 15:33:17.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4800" for this suite.

• [SLOW TEST:245.437 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":205,"skipped":3846,"failed":0}
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:33:17.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 30 15:33:17.836: INFO: PodSpec: initContainers in spec.initContainers
W0830 15:33:17.901550      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 30 15:33:23.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5847" for this suite.

• [SLOW TEST:6.228 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":206,"skipped":3849,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:33:23.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-9213/configmap-test-a6e57a13-4521-4304-8aa8-88b50c27c76f
STEP: Creating a pod to test consume configMaps
W0830 15:33:24.141259      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:33:24.141: INFO: Waiting up to 5m0s for pod "pod-configmaps-3071b8e0-9d95-4ef0-8928-90e2b8b092b9" in namespace "configmap-9213" to be "Succeeded or Failed"
Aug 30 15:33:24.196: INFO: Pod "pod-configmaps-3071b8e0-9d95-4ef0-8928-90e2b8b092b9": Phase="Pending", Reason="", readiness=false. Elapsed: 55.160283ms
Aug 30 15:33:26.221: INFO: Pod "pod-configmaps-3071b8e0-9d95-4ef0-8928-90e2b8b092b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080262667s
Aug 30 15:33:28.250: INFO: Pod "pod-configmaps-3071b8e0-9d95-4ef0-8928-90e2b8b092b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109561314s
Aug 30 15:33:30.279: INFO: Pod "pod-configmaps-3071b8e0-9d95-4ef0-8928-90e2b8b092b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.138298639s
STEP: Saw pod success
Aug 30 15:33:30.279: INFO: Pod "pod-configmaps-3071b8e0-9d95-4ef0-8928-90e2b8b092b9" satisfied condition "Succeeded or Failed"
Aug 30 15:33:30.297: INFO: Trying to get logs from node 10.63.224.189 pod pod-configmaps-3071b8e0-9d95-4ef0-8928-90e2b8b092b9 container env-test: <nil>
STEP: delete the pod
Aug 30 15:33:30.496: INFO: Waiting for pod pod-configmaps-3071b8e0-9d95-4ef0-8928-90e2b8b092b9 to disappear
Aug 30 15:33:30.515: INFO: Pod pod-configmaps-3071b8e0-9d95-4ef0-8928-90e2b8b092b9 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 15:33:30.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9213" for this suite.

• [SLOW TEST:6.624 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":207,"skipped":3853,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:33:30.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 30 15:33:30.701: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 15:33:30.748: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 15:33:30.808: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.158 before test
Aug 30 15:33:30.996: INFO: calico-kube-controllers-867bb5b44d-wqzpr from calico-system started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.996: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 30 15:33:30.996: INFO: calico-node-rfpmd from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.996: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 15:33:30.996: INFO: ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-q4rqr from ibm-system started at 2022-08-30 12:17:08 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container ibm-cloud-provider-ip-130-198-93-237 ready: true, restart count 0
Aug 30 15:33:30.997: INFO: ibm-file-plugin-ccc878f48-frwb7 from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 30 15:33:30.997: INFO: ibm-keepalived-watcher-c2b7b from kube-system started at 2022-08-30 12:07:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 15:33:30.997: INFO: ibm-master-proxy-static-10.63.224.158 from kube-system started at 2022-08-30 12:07:30 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 15:33:30.997: INFO: 	Container pause ready: true, restart count 0
Aug 30 15:33:30.997: INFO: ibm-storage-watcher-6fcfdb7ccc-5s2xq from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 30 15:33:30.997: INFO: ibmcloud-block-storage-driver-kx4tx from kube-system started at 2022-08-30 12:07:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 15:33:30.997: INFO: ibmcloud-block-storage-plugin-7495f48b76-lht72 from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 30 15:33:30.997: INFO: vpn-59795d4f7c-rf6fw from kube-system started at 2022-08-30 12:18:35 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container vpn ready: true, restart count 0
Aug 30 15:33:30.997: INFO: cluster-node-tuning-operator-6c4d4b46dd-dsdts from openshift-cluster-node-tuning-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 30 15:33:30.997: INFO: tuned-mpvrs from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container tuned ready: true, restart count 0
Aug 30 15:33:30.997: INFO: cluster-samples-operator-79cb65b9b5-jbclm from openshift-cluster-samples-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 30 15:33:30.997: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 30 15:33:30.997: INFO: cluster-storage-operator-97dcf6b44-krf6f from openshift-cluster-storage-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 30 15:33:30.997: INFO: csi-snapshot-controller-7d8bf4bb58-6lkk4 from openshift-cluster-storage-operator started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 15:33:30.997: INFO: csi-snapshot-controller-operator-7b97dc6dfd-xjp62 from openshift-cluster-storage-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 30 15:33:30.997: INFO: csi-snapshot-webhook-8f4fd6cc6-48czg from openshift-cluster-storage-operator started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container webhook ready: true, restart count 0
Aug 30 15:33:30.997: INFO: console-operator-55bcdb7b48-tgxd4 from openshift-console-operator started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container console-operator ready: true, restart count 1
Aug 30 15:33:30.997: INFO: console-5dd7474d84-js5g9 from openshift-console started at 2022-08-30 12:14:31 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container console ready: true, restart count 0
Aug 30 15:33:30.997: INFO: dns-operator-5549dbd7c9-c4h6t from openshift-dns-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container dns-operator ready: true, restart count 0
Aug 30 15:33:30.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:30.997: INFO: dns-default-v6jl9 from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container dns ready: true, restart count 0
Aug 30 15:33:30.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:30.997: INFO: node-resolver-ppzz9 from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 15:33:30.997: INFO: cluster-image-registry-operator-cb6448756-qh2wd from openshift-image-registry started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 30 15:33:30.997: INFO: image-registry-7fb4f45578-scb5q from openshift-image-registry started at 2022-08-30 12:18:40 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container registry ready: true, restart count 0
Aug 30 15:33:30.997: INFO: node-ca-4pbx7 from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 15:33:30.997: INFO: ingress-canary-vwqf6 from openshift-ingress-canary started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 15:33:30.997: INFO: ingress-operator-57457886bd-v8vkg from openshift-ingress-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 30 15:33:30.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:30.997: INFO: router-default-bdd65b74f-jwc99 from openshift-ingress started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container router ready: true, restart count 0
Aug 30 15:33:30.997: INFO: insights-operator-6d9b46b7c5-6q6pl from openshift-insights started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.997: INFO: 	Container insights-operator ready: true, restart count 1
Aug 30 15:33:30.998: INFO: openshift-kube-proxy-ms7hb from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:30.998: INFO: kube-storage-version-migrator-operator-fb46b8c8d-n5q2j from openshift-kube-storage-version-migrator-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Aug 30 15:33:30.998: INFO: marketplace-operator-84ff65f9c9-mcwn4 from openshift-marketplace started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 30 15:33:30.998: INFO: cluster-monitoring-operator-677fb4cf4b-wfrk9 from openshift-monitoring started at 2022-08-30 12:09:18 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:30.998: INFO: kube-state-metrics-b6455c4dc-pknn6 from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (3 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 30 15:33:30.998: INFO: node-exporter-ct2nk from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 15:33:30.998: INFO: prometheus-adapter-67874b74f7-2kkcq from openshift-monitoring started at 2022-08-30 12:14:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 15:33:30.998: INFO: prometheus-operator-admission-webhook-f5f88b968-wkmnq from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 15:33:30.998: INFO: thanos-querier-75bdcf8599-tdj9h from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (6 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 15:33:30.998: INFO: multus-additional-cni-plugins-fwxxj from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 15:33:30.998: INFO: multus-admission-controller-gvnw7 from openshift-multus started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 15:33:30.998: INFO: multus-n2s54 from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 15:33:30.998: INFO: network-metrics-daemon-ncgdn from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 15:33:30.998: INFO: network-check-source-5cb989cf6f-8pbqc from openshift-network-diagnostics started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 30 15:33:30.998: INFO: network-check-target-tqzgf from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 15:33:30.998: INFO: catalog-operator-5d9dd4bb98-hwstf from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 30 15:33:30.998: INFO: olm-operator-757497677b-z5lbc from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container olm-operator ready: true, restart count 0
Aug 30 15:33:30.998: INFO: package-server-manager-784548687f-9vg2b from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container package-server-manager ready: true, restart count 0
Aug 30 15:33:30.998: INFO: packageserver-b5cbb5ff5-2vxb9 from openshift-operator-lifecycle-manager started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 15:33:30.998: INFO: metrics-67dbdb4ffd-gj9lp from openshift-roks-metrics started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container metrics ready: true, restart count 3
Aug 30 15:33:30.998: INFO: push-gateway-58dc4cbdf8-kl5x7 from openshift-roks-metrics started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container push-gateway ready: true, restart count 0
Aug 30 15:33:30.998: INFO: service-ca-operator-5df8fbb45b-fz8bw from openshift-service-ca-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container service-ca-operator ready: true, restart count 1
Aug 30 15:33:30.998: INFO: sonobuoy-e2e-job-a8c1c5c47c2346af from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container e2e ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:33:30.998: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-fngwd from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:33:30.998: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 15:33:30.998: INFO: tigera-operator-7b76886f74-5rs97 from tigera-operator started at 2022-08-30 12:07:46 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:30.998: INFO: 	Container tigera-operator ready: true, restart count 2
Aug 30 15:33:30.998: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.187 before test
Aug 30 15:33:31.125: INFO: calico-node-bv858 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 15:33:31.125: INFO: calico-typha-66986f6f4d-mg7gw from calico-system started at 2022-08-30 12:08:59 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 15:33:31.125: INFO: ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-gq5c7 from ibm-system started at 2022-08-30 12:17:08 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container ibm-cloud-provider-ip-130-198-93-237 ready: true, restart count 0
Aug 30 15:33:31.125: INFO: ibm-keepalived-watcher-7czrz from kube-system started at 2022-08-30 12:07:36 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 15:33:31.125: INFO: ibm-master-proxy-static-10.63.224.187 from kube-system started at 2022-08-30 12:07:34 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container pause ready: true, restart count 0
Aug 30 15:33:31.125: INFO: ibmcloud-block-storage-driver-m5jbh from kube-system started at 2022-08-30 12:07:42 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 15:33:31.125: INFO: tuned-c2ph5 from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container tuned ready: true, restart count 0
Aug 30 15:33:31.125: INFO: csi-snapshot-controller-7d8bf4bb58-gv7xl from openshift-cluster-storage-operator started at 2022-08-30 12:10:04 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 15:33:31.125: INFO: csi-snapshot-webhook-8f4fd6cc6-pnk9x from openshift-cluster-storage-operator started at 2022-08-30 12:10:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container webhook ready: true, restart count 0
Aug 30 15:33:31.125: INFO: console-5dd7474d84-t6lg4 from openshift-console started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container console ready: true, restart count 0
Aug 30 15:33:31.125: INFO: downloads-6f74f6fcbf-sbj8k from openshift-console started at 2022-08-30 12:10:07 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container download-server ready: true, restart count 0
Aug 30 15:33:31.125: INFO: dns-default-dltcr from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container dns ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: node-resolver-n66mp from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 15:33:31.125: INFO: node-ca-9bj8c from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 15:33:31.125: INFO: registry-pvc-permissions-2pnhd from openshift-image-registry started at 2022-08-30 12:18:48 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 30 15:33:31.125: INFO: ingress-canary-zjwth from openshift-ingress-canary started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 15:33:31.125: INFO: router-default-bdd65b74f-9xp2d from openshift-ingress started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container router ready: true, restart count 0
Aug 30 15:33:31.125: INFO: openshift-kube-proxy-whcnq from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: migrator-59f7fcfc8f-l77kl from openshift-kube-storage-version-migrator started at 2022-08-30 12:10:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container migrator ready: true, restart count 0
Aug 30 15:33:31.125: INFO: certified-operators-t6598 from openshift-marketplace started at 2022-08-30 13:47:48 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:33:31.125: INFO: community-operators-btnw8 from openshift-marketplace started at 2022-08-30 12:11:53 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:33:31.125: INFO: redhat-marketplace-9rltc from openshift-marketplace started at 2022-08-30 12:11:54 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:33:31.125: INFO: redhat-operators-cnsks from openshift-marketplace started at 2022-08-30 12:11:53 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:33:31.125: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-08-30 12:14:46 +0000 UTC (6 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container alertmanager ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: node-exporter-gls7c from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 15:33:31.125: INFO: openshift-state-metrics-7984888fbd-dgxfd from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (3 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 30 15:33:31.125: INFO: prometheus-adapter-67874b74f7-xwhkv from openshift-monitoring started at 2022-08-30 12:14:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 15:33:31.125: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-08-30 12:15:07 +0000 UTC (6 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 15:33:31.125: INFO: prometheus-operator-admission-webhook-f5f88b968-wjzg5 from openshift-monitoring started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 15:33:31.125: INFO: thanos-querier-75bdcf8599-tb7gh from openshift-monitoring started at 2022-08-30 12:14:46 +0000 UTC (6 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 15:33:31.125: INFO: multus-additional-cni-plugins-8j4kx from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 15:33:31.125: INFO: multus-admission-controller-xzspk from openshift-multus started at 2022-08-30 12:09:28 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 15:33:31.125: INFO: multus-qpmzg from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 15:33:31.125: INFO: network-metrics-daemon-nlcxx from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 15:33:31.125: INFO: network-check-target-wrm4t from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 15:33:31.125: INFO: network-operator-55b69485bb-qth5v from openshift-network-operator started at 2022-08-30 12:07:47 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container network-operator ready: true, restart count 1
Aug 30 15:33:31.125: INFO: collect-profiles-27697860-qvs8g from openshift-operator-lifecycle-manager started at 2022-08-30 15:00:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 15:33:31.125: INFO: collect-profiles-27697875-986x4 from openshift-operator-lifecycle-manager started at 2022-08-30 15:15:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 15:33:31.125: INFO: collect-profiles-27697890-ppzrv from openshift-operator-lifecycle-manager started at 2022-08-30 15:30:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 15:33:31.125: INFO: packageserver-b5cbb5ff5-nc6jk from openshift-operator-lifecycle-manager started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 15:33:31.125: INFO: sonobuoy from sonobuoy started at 2022-08-30 14:26:02 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 15:33:31.125: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-7b4kf from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.125: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 15:33:31.125: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.189 before test
Aug 30 15:33:31.214: INFO: calico-node-g5sh7 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 15:33:31.214: INFO: calico-typha-66986f6f4d-rd6m6 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 15:33:31.214: INFO: pod-init-142ddafd-69ae-489b-a05e-8032dd19a3fe from init-container-5847 started at 2022-08-30 15:33:17 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container run1 ready: false, restart count 0
Aug 30 15:33:31.214: INFO: ibm-keepalived-watcher-ctfdm from kube-system started at 2022-08-30 12:08:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 15:33:31.214: INFO: ibm-master-proxy-static-10.63.224.189 from kube-system started at 2022-08-30 12:07:47 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container pause ready: true, restart count 0
Aug 30 15:33:31.214: INFO: ibmcloud-block-storage-driver-xl7s6 from kube-system started at 2022-08-30 12:08:07 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 15:33:31.214: INFO: tuned-p2mrf from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container tuned ready: true, restart count 0
Aug 30 15:33:31.214: INFO: downloads-6f74f6fcbf-gsbhd from openshift-console started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container download-server ready: true, restart count 0
Aug 30 15:33:31.214: INFO: dns-default-zvh4h from openshift-dns started at 2022-08-30 14:53:11 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container dns ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.214: INFO: node-resolver-9tslx from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 15:33:31.214: INFO: node-ca-mq7hh from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 15:33:31.214: INFO: ingress-canary-qxqwq from openshift-ingress-canary started at 2022-08-30 14:52:56 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 15:33:31.214: INFO: openshift-kube-proxy-5zfxw from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.214: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-08-30 14:52:56 +0000 UTC (6 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container alertmanager ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:33:31.214: INFO: node-exporter-9w9vp from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 15:33:31.214: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-08-30 14:52:58 +0000 UTC (6 container statuses recorded)
Aug 30 15:33:31.214: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 15:33:31.214: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 15:33:31.214: INFO: prometheus-operator-dd6c54897-5s5xx from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.215: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.215: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 30 15:33:31.215: INFO: telemeter-client-dd46ccd5b-ttrgn from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (3 container statuses recorded)
Aug 30 15:33:31.215: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.215: INFO: 	Container reload ready: true, restart count 0
Aug 30 15:33:31.215: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 30 15:33:31.215: INFO: multus-additional-cni-plugins-gnz55 from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.215: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 15:33:31.215: INFO: multus-admission-controller-94qqx from openshift-multus started at 2022-08-30 14:53:21 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.215: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.215: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 15:33:31.215: INFO: multus-nzfsd from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.215: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 15:33:31.215: INFO: network-metrics-daemon-hzwd2 from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.215: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:33:31.215: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 15:33:31.215: INFO: network-check-target-whfqq from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.215: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 15:33:31.215: INFO: service-ca-5f4d84b84b-wnqw2 from openshift-service-ca started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:33:31.215: INFO: 	Container service-ca-controller ready: false, restart count 0
Aug 30 15:33:31.215: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-x4jz5 from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:33:31.215: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:33:31.215: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node 10.63.224.158
STEP: verifying the node has the label node 10.63.224.187
STEP: verifying the node has the label node 10.63.224.189
Aug 30 15:33:31.694: INFO: Pod calico-kube-controllers-867bb5b44d-wqzpr requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod calico-node-bv858 requesting resource cpu=250m on Node 10.63.224.187
Aug 30 15:33:31.694: INFO: Pod calico-node-g5sh7 requesting resource cpu=250m on Node 10.63.224.189
Aug 30 15:33:31.694: INFO: Pod calico-node-rfpmd requesting resource cpu=250m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod calico-typha-66986f6f4d-mg7gw requesting resource cpu=250m on Node 10.63.224.187
Aug 30 15:33:31.694: INFO: Pod calico-typha-66986f6f4d-rd6m6 requesting resource cpu=250m on Node 10.63.224.189
Aug 30 15:33:31.694: INFO: Pod ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-gq5c7 requesting resource cpu=5m on Node 10.63.224.187
Aug 30 15:33:31.694: INFO: Pod ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-q4rqr requesting resource cpu=5m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod ibm-file-plugin-ccc878f48-frwb7 requesting resource cpu=50m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod ibm-keepalived-watcher-7czrz requesting resource cpu=5m on Node 10.63.224.187
Aug 30 15:33:31.694: INFO: Pod ibm-keepalived-watcher-c2b7b requesting resource cpu=5m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod ibm-keepalived-watcher-ctfdm requesting resource cpu=5m on Node 10.63.224.189
Aug 30 15:33:31.694: INFO: Pod ibm-master-proxy-static-10.63.224.158 requesting resource cpu=26m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod ibm-master-proxy-static-10.63.224.187 requesting resource cpu=26m on Node 10.63.224.187
Aug 30 15:33:31.694: INFO: Pod ibm-master-proxy-static-10.63.224.189 requesting resource cpu=26m on Node 10.63.224.189
Aug 30 15:33:31.694: INFO: Pod ibm-storage-watcher-6fcfdb7ccc-5s2xq requesting resource cpu=50m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod ibmcloud-block-storage-driver-kx4tx requesting resource cpu=50m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod ibmcloud-block-storage-driver-m5jbh requesting resource cpu=50m on Node 10.63.224.187
Aug 30 15:33:31.694: INFO: Pod ibmcloud-block-storage-driver-xl7s6 requesting resource cpu=50m on Node 10.63.224.189
Aug 30 15:33:31.694: INFO: Pod ibmcloud-block-storage-plugin-7495f48b76-lht72 requesting resource cpu=50m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod vpn-59795d4f7c-rf6fw requesting resource cpu=5m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod cluster-node-tuning-operator-6c4d4b46dd-dsdts requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod tuned-c2ph5 requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.694: INFO: Pod tuned-mpvrs requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod tuned-p2mrf requesting resource cpu=10m on Node 10.63.224.189
Aug 30 15:33:31.694: INFO: Pod cluster-samples-operator-79cb65b9b5-jbclm requesting resource cpu=20m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod cluster-storage-operator-97dcf6b44-krf6f requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod csi-snapshot-controller-7d8bf4bb58-6lkk4 requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.694: INFO: Pod csi-snapshot-controller-7d8bf4bb58-gv7xl requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.694: INFO: Pod csi-snapshot-controller-operator-7b97dc6dfd-xjp62 requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod csi-snapshot-webhook-8f4fd6cc6-48czg requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod csi-snapshot-webhook-8f4fd6cc6-pnk9x requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.695: INFO: Pod console-operator-55bcdb7b48-tgxd4 requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod console-5dd7474d84-js5g9 requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod console-5dd7474d84-t6lg4 requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.695: INFO: Pod downloads-6f74f6fcbf-gsbhd requesting resource cpu=10m on Node 10.63.224.189
Aug 30 15:33:31.695: INFO: Pod downloads-6f74f6fcbf-sbj8k requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.695: INFO: Pod dns-operator-5549dbd7c9-c4h6t requesting resource cpu=20m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod dns-default-dltcr requesting resource cpu=60m on Node 10.63.224.187
Aug 30 15:33:31.695: INFO: Pod dns-default-v6jl9 requesting resource cpu=60m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod dns-default-zvh4h requesting resource cpu=60m on Node 10.63.224.189
Aug 30 15:33:31.695: INFO: Pod node-resolver-9tslx requesting resource cpu=5m on Node 10.63.224.189
Aug 30 15:33:31.695: INFO: Pod node-resolver-n66mp requesting resource cpu=5m on Node 10.63.224.187
Aug 30 15:33:31.695: INFO: Pod node-resolver-ppzz9 requesting resource cpu=5m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod cluster-image-registry-operator-cb6448756-qh2wd requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod image-registry-7fb4f45578-scb5q requesting resource cpu=100m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod node-ca-4pbx7 requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod node-ca-9bj8c requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.695: INFO: Pod node-ca-mq7hh requesting resource cpu=10m on Node 10.63.224.189
Aug 30 15:33:31.695: INFO: Pod ingress-canary-qxqwq requesting resource cpu=10m on Node 10.63.224.189
Aug 30 15:33:31.695: INFO: Pod ingress-canary-vwqf6 requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod ingress-canary-zjwth requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.695: INFO: Pod ingress-operator-57457886bd-v8vkg requesting resource cpu=20m on Node 10.63.224.158
Aug 30 15:33:31.695: INFO: Pod router-default-bdd65b74f-9xp2d requesting resource cpu=100m on Node 10.63.224.187
Aug 30 15:33:31.695: INFO: Pod router-default-bdd65b74f-jwc99 requesting resource cpu=100m on Node 10.63.224.158
Aug 30 15:33:31.696: INFO: Pod insights-operator-6d9b46b7c5-6q6pl requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.696: INFO: Pod openshift-kube-proxy-5zfxw requesting resource cpu=110m on Node 10.63.224.189
Aug 30 15:33:31.696: INFO: Pod openshift-kube-proxy-ms7hb requesting resource cpu=110m on Node 10.63.224.158
Aug 30 15:33:31.696: INFO: Pod openshift-kube-proxy-whcnq requesting resource cpu=110m on Node 10.63.224.187
Aug 30 15:33:31.696: INFO: Pod kube-storage-version-migrator-operator-fb46b8c8d-n5q2j requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.696: INFO: Pod migrator-59f7fcfc8f-l77kl requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.696: INFO: Pod certified-operators-t6598 requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.696: INFO: Pod community-operators-btnw8 requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.696: INFO: Pod marketplace-operator-84ff65f9c9-mcwn4 requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.696: INFO: Pod redhat-marketplace-9rltc requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.697: INFO: Pod redhat-operators-cnsks requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.697: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.63.224.187
Aug 30 15:33:31.697: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.63.224.189
Aug 30 15:33:31.697: INFO: Pod cluster-monitoring-operator-677fb4cf4b-wfrk9 requesting resource cpu=11m on Node 10.63.224.158
Aug 30 15:33:31.697: INFO: Pod kube-state-metrics-b6455c4dc-pknn6 requesting resource cpu=4m on Node 10.63.224.158
Aug 30 15:33:31.697: INFO: Pod node-exporter-9w9vp requesting resource cpu=9m on Node 10.63.224.189
Aug 30 15:33:31.697: INFO: Pod node-exporter-ct2nk requesting resource cpu=9m on Node 10.63.224.158
Aug 30 15:33:31.697: INFO: Pod node-exporter-gls7c requesting resource cpu=9m on Node 10.63.224.187
Aug 30 15:33:31.697: INFO: Pod openshift-state-metrics-7984888fbd-dgxfd requesting resource cpu=3m on Node 10.63.224.187
Aug 30 15:33:31.697: INFO: Pod prometheus-adapter-67874b74f7-2kkcq requesting resource cpu=1m on Node 10.63.224.158
Aug 30 15:33:31.697: INFO: Pod prometheus-adapter-67874b74f7-xwhkv requesting resource cpu=1m on Node 10.63.224.187
Aug 30 15:33:31.697: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.63.224.187
Aug 30 15:33:31.697: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.63.224.189
Aug 30 15:33:31.697: INFO: Pod prometheus-operator-admission-webhook-f5f88b968-wjzg5 requesting resource cpu=5m on Node 10.63.224.187
Aug 30 15:33:31.697: INFO: Pod prometheus-operator-admission-webhook-f5f88b968-wkmnq requesting resource cpu=5m on Node 10.63.224.158
Aug 30 15:33:31.697: INFO: Pod prometheus-operator-dd6c54897-5s5xx requesting resource cpu=6m on Node 10.63.224.189
Aug 30 15:33:31.697: INFO: Pod telemeter-client-dd46ccd5b-ttrgn requesting resource cpu=3m on Node 10.63.224.189
Aug 30 15:33:31.697: INFO: Pod thanos-querier-75bdcf8599-tb7gh requesting resource cpu=15m on Node 10.63.224.187
Aug 30 15:33:31.697: INFO: Pod thanos-querier-75bdcf8599-tdj9h requesting resource cpu=15m on Node 10.63.224.158
Aug 30 15:33:31.697: INFO: Pod multus-additional-cni-plugins-8j4kx requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.698: INFO: Pod multus-additional-cni-plugins-fwxxj requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod multus-additional-cni-plugins-gnz55 requesting resource cpu=10m on Node 10.63.224.189
Aug 30 15:33:31.698: INFO: Pod multus-admission-controller-94qqx requesting resource cpu=20m on Node 10.63.224.189
Aug 30 15:33:31.698: INFO: Pod multus-admission-controller-gvnw7 requesting resource cpu=20m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod multus-admission-controller-xzspk requesting resource cpu=20m on Node 10.63.224.187
Aug 30 15:33:31.698: INFO: Pod multus-n2s54 requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod multus-nzfsd requesting resource cpu=10m on Node 10.63.224.189
Aug 30 15:33:31.698: INFO: Pod multus-qpmzg requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.698: INFO: Pod network-metrics-daemon-hzwd2 requesting resource cpu=20m on Node 10.63.224.189
Aug 30 15:33:31.698: INFO: Pod network-metrics-daemon-ncgdn requesting resource cpu=20m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod network-metrics-daemon-nlcxx requesting resource cpu=20m on Node 10.63.224.187
Aug 30 15:33:31.698: INFO: Pod network-check-source-5cb989cf6f-8pbqc requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod network-check-target-tqzgf requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod network-check-target-whfqq requesting resource cpu=10m on Node 10.63.224.189
Aug 30 15:33:31.698: INFO: Pod network-check-target-wrm4t requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.698: INFO: Pod network-operator-55b69485bb-qth5v requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.698: INFO: Pod catalog-operator-5d9dd4bb98-hwstf requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod olm-operator-757497677b-z5lbc requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod package-server-manager-784548687f-9vg2b requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod packageserver-b5cbb5ff5-2vxb9 requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod packageserver-b5cbb5ff5-nc6jk requesting resource cpu=10m on Node 10.63.224.187
Aug 30 15:33:31.698: INFO: Pod metrics-67dbdb4ffd-gj9lp requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod push-gateway-58dc4cbdf8-kl5x7 requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod service-ca-operator-5df8fbb45b-fz8bw requesting resource cpu=10m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod service-ca-5f4d84b84b-wnqw2 requesting resource cpu=10m on Node 10.63.224.189
Aug 30 15:33:31.698: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.63.224.187
Aug 30 15:33:31.698: INFO: Pod sonobuoy-e2e-job-a8c1c5c47c2346af requesting resource cpu=0m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-7b4kf requesting resource cpu=0m on Node 10.63.224.187
Aug 30 15:33:31.698: INFO: Pod sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-fngwd requesting resource cpu=0m on Node 10.63.224.158
Aug 30 15:33:31.698: INFO: Pod sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-x4jz5 requesting resource cpu=0m on Node 10.63.224.189
Aug 30 15:33:31.699: INFO: Pod tigera-operator-7b76886f74-5rs97 requesting resource cpu=100m on Node 10.63.224.158
STEP: Starting Pods to consume most of the cluster CPU.
Aug 30 15:33:31.699: INFO: Creating a pod which consumes cpu=1777m on Node 10.63.224.158
Aug 30 15:33:31.747: INFO: Creating a pod which consumes cpu=1905m on Node 10.63.224.187
W0830 15:33:31.747320      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:33:31.817: INFO: Creating a pod which consumes cpu=2052m on Node 10.63.224.189
W0830 15:33:31.816896      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 15:33:31.894340      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "filler-pod-ff091195-83aa-486e-a006-e350c3c1031f" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "filler-pod-ff091195-83aa-486e-a006-e350c3c1031f" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "filler-pod-ff091195-83aa-486e-a006-e350c3c1031f" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "filler-pod-ff091195-83aa-486e-a006-e350c3c1031f" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e.1710293eb468eace], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3313/filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e to 10.63.224.158 by kube-scheduler-66b794b989-b5tfz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e.1710293ef269eeea], Reason = [AddedInterface], Message = [Add eth0 [172.30.78.43/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e.1710293f05d9f0cd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e.1710293f12417ff3], Reason = [Created], Message = [Created container filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e.1710293f14dda3d5], Reason = [Started], Message = [Started container filler-pod-1b2e9208-6f4e-488c-8e2b-4b98fd0da40e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90.1710293eb8fcd77e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3313/filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90 to 10.63.224.187 by kube-scheduler-66b794b989-b5tfz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90.1710293ef053ac88], Reason = [AddedInterface], Message = [Add eth0 [172.30.38.249/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90.1710293efee220b6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90.1710293f0976fe4a], Reason = [Created], Message = [Created container filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90.1710293f0ae41008], Reason = [Started], Message = [Started container filler-pod-7e6f785d-9432-4463-85a1-0e8ab61d2c90]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff091195-83aa-486e-a006-e350c3c1031f.1710293ebc37b5b8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3313/filler-pod-ff091195-83aa-486e-a006-e350c3c1031f to 10.63.224.189 by kube-scheduler-66b794b989-b5tfz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff091195-83aa-486e-a006-e350c3c1031f.1710293efeed1e54], Reason = [AddedInterface], Message = [Add eth0 [172.30.233.239/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff091195-83aa-486e-a006-e350c3c1031f.1710293f16af498b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff091195-83aa-486e-a006-e350c3c1031f.1710293f24cc4df1], Reason = [Created], Message = [Created container filler-pod-ff091195-83aa-486e-a006-e350c3c1031f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff091195-83aa-486e-a006-e350c3c1031f.1710293f27544c24], Reason = [Started], Message = [Started container filler-pod-ff091195-83aa-486e-a006-e350c3c1031f]
W0830 15:33:36.069472      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "additional-pod" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "additional-pod" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "additional-pod" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "additional-pod" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1710293fb3416792], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.]
STEP: removing the label node off the node 10.63.224.158
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.63.224.187
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.63.224.189
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 30 15:33:37.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3313" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.776 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":208,"skipped":3857,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:33:37.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Aug 30 15:33:37.527: INFO: Creating simple deployment test-deployment-b4hfm
W0830 15:33:37.582240      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:33:37.610: INFO: new replicaset for deployment "test-deployment-b4hfm" is yet to be created
Aug 30 15:33:39.658: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 15, 33, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 33, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 15, 33, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 33, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b4hfm-688c4d6789\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Aug 30 15:33:41.716: INFO: Deployment test-deployment-b4hfm has Conditions: [{Available True 2022-08-30 15:33:40 +0000 UTC 2022-08-30 15:33:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-08-30 15:33:40 +0000 UTC 2022-08-30 15:33:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4hfm-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Aug 30 15:33:41.756: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 15, 33, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 33, 40, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 15, 33, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 33, 37, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-b4hfm-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Aug 30 15:33:41.762: INFO: Observed &Deployment event: ADDED
Aug 30 15:33:41.762: INFO: Observed Deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-30 15:33:37 +0000 UTC 2022-08-30 15:33:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4hfm-688c4d6789"}
Aug 30 15:33:41.763: INFO: Observed &Deployment event: MODIFIED
Aug 30 15:33:41.763: INFO: Observed Deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-30 15:33:37 +0000 UTC 2022-08-30 15:33:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4hfm-688c4d6789"}
Aug 30 15:33:41.763: INFO: Observed Deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-30 15:33:37 +0000 UTC 2022-08-30 15:33:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 30 15:33:41.763: INFO: Observed &Deployment event: MODIFIED
Aug 30 15:33:41.763: INFO: Observed Deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-30 15:33:37 +0000 UTC 2022-08-30 15:33:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 30 15:33:41.763: INFO: Observed Deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-30 15:33:37 +0000 UTC 2022-08-30 15:33:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b4hfm-688c4d6789" is progressing.}
Aug 30 15:33:41.763: INFO: Observed &Deployment event: MODIFIED
Aug 30 15:33:41.763: INFO: Observed Deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-30 15:33:40 +0000 UTC 2022-08-30 15:33:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 30 15:33:41.763: INFO: Observed Deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-30 15:33:40 +0000 UTC 2022-08-30 15:33:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4hfm-688c4d6789" has successfully progressed.}
Aug 30 15:33:41.763: INFO: Observed &Deployment event: MODIFIED
Aug 30 15:33:41.763: INFO: Observed Deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-30 15:33:40 +0000 UTC 2022-08-30 15:33:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 30 15:33:41.763: INFO: Observed Deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-30 15:33:40 +0000 UTC 2022-08-30 15:33:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4hfm-688c4d6789" has successfully progressed.}
Aug 30 15:33:41.763: INFO: Found Deployment test-deployment-b4hfm in namespace deployment-8604 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 30 15:33:41.763: INFO: Deployment test-deployment-b4hfm has an updated status
STEP: patching the Statefulset Status
Aug 30 15:33:41.763: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 30 15:33:41.790: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Aug 30 15:33:41.798: INFO: Observed &Deployment event: ADDED
Aug 30 15:33:41.798: INFO: Observed deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-30 15:33:37 +0000 UTC 2022-08-30 15:33:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4hfm-688c4d6789"}
Aug 30 15:33:41.798: INFO: Observed &Deployment event: MODIFIED
Aug 30 15:33:41.798: INFO: Observed deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-30 15:33:37 +0000 UTC 2022-08-30 15:33:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4hfm-688c4d6789"}
Aug 30 15:33:41.798: INFO: Observed deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-30 15:33:37 +0000 UTC 2022-08-30 15:33:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 30 15:33:41.799: INFO: Observed &Deployment event: MODIFIED
Aug 30 15:33:41.800: INFO: Observed deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-30 15:33:37 +0000 UTC 2022-08-30 15:33:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 30 15:33:41.800: INFO: Observed deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-30 15:33:37 +0000 UTC 2022-08-30 15:33:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b4hfm-688c4d6789" is progressing.}
Aug 30 15:33:41.800: INFO: Observed &Deployment event: MODIFIED
Aug 30 15:33:41.800: INFO: Observed deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-30 15:33:40 +0000 UTC 2022-08-30 15:33:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 30 15:33:41.800: INFO: Observed deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-30 15:33:40 +0000 UTC 2022-08-30 15:33:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4hfm-688c4d6789" has successfully progressed.}
Aug 30 15:33:41.800: INFO: Observed &Deployment event: MODIFIED
Aug 30 15:33:41.800: INFO: Observed deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-30 15:33:40 +0000 UTC 2022-08-30 15:33:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 30 15:33:41.800: INFO: Observed deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-30 15:33:40 +0000 UTC 2022-08-30 15:33:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4hfm-688c4d6789" has successfully progressed.}
Aug 30 15:33:41.800: INFO: Observed deployment test-deployment-b4hfm in namespace deployment-8604 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 30 15:33:41.800: INFO: Observed &Deployment event: MODIFIED
Aug 30 15:33:41.801: INFO: Found deployment test-deployment-b4hfm in namespace deployment-8604 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 30 15:33:41.801: INFO: Deployment test-deployment-b4hfm has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 15:33:41.816: INFO: Deployment "test-deployment-b4hfm":
&Deployment{ObjectMeta:{test-deployment-b4hfm  deployment-8604  b51c63bb-5e36-4f00-bfe6-bf5700f1a707 113225 1 2022-08-30 15:33:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-08-30 15:33:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 15:33:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-08-30 15:33:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dec3f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 30 15:33:41.844: INFO: New ReplicaSet "test-deployment-b4hfm-688c4d6789" of Deployment "test-deployment-b4hfm":
&ReplicaSet{ObjectMeta:{test-deployment-b4hfm-688c4d6789  deployment-8604  58e0d5c6-8e2e-4908-87d9-a221c8c71ab3 113212 1 2022-08-30 15:33:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-b4hfm b51c63bb-5e36-4f00-bfe6-bf5700f1a707 0xc0057b0200 0xc0057b0201}] []  [{kube-controller-manager Update apps/v1 2022-08-30 15:33:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b51c63bb-5e36-4f00-bfe6-bf5700f1a707\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 15:33:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057b02a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 15:33:41.869: INFO: Pod "test-deployment-b4hfm-688c4d6789-nfl96" is available:
&Pod{ObjectMeta:{test-deployment-b4hfm-688c4d6789-nfl96 test-deployment-b4hfm-688c4d6789- deployment-8604  1601c65e-f5ce-4e4d-9fba-b547a2d3e631 113211 0 2022-08-30 15:33:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[cni.projectcalico.org/containerID:3f2c8c4a20947b0a907e6db33047167ae1db204c5ae3890c9d3aba3df466b339 cni.projectcalico.org/podIP:172.30.233.248/32 cni.projectcalico.org/podIPs:172.30.233.248/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.248"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.248"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-b4hfm-688c4d6789 58e0d5c6-8e2e-4908-87d9-a221c8c71ab3 0xc0057b0677 0xc0057b0678}] []  [{kube-controller-manager Update v1 2022-08-30 15:33:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58e0d5c6-8e2e-4908-87d9-a221c8c71ab3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 15:33:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 15:33:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 15:33:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.233.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcxfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcxfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c55,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-csp5h,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 15:33:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 15:33:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 15:33:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 15:33:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.189,PodIP:172.30.233.248,StartTime:2022-08-30 15:33:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 15:33:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://46707bbef8df3843bdca2472dbee180ea463c64b6d7f863b5e8cae52d18462ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.233.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 30 15:33:41.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8604" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":209,"skipped":3858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:33:41.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
W0830 15:33:42.603852      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-crd-conversion-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-crd-conversion-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-crd-conversion-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-crd-conversion-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:33:42.640: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 30 15:33:44.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 15, 33, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 33, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 15, 33, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 33, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-677b6dd845\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 15:33:47.762: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:33:47.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:33:51.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2179" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:9.612 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":210,"skipped":3887,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:33:51.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-fmlf
STEP: Creating a pod to test atomic-volume-subpath
Aug 30 15:33:51.887: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fmlf" in namespace "subpath-6088" to be "Succeeded or Failed"
W0830 15:33:51.887329      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container-subpath-downwardapi-fmlf" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-downwardapi-fmlf" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-downwardapi-fmlf" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-downwardapi-fmlf" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:33:51.911: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Pending", Reason="", readiness=false. Elapsed: 23.776896ms
Aug 30 15:33:53.930: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=true. Elapsed: 2.042776517s
Aug 30 15:33:55.948: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=true. Elapsed: 4.061176992s
Aug 30 15:33:57.968: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=true. Elapsed: 6.080601264s
Aug 30 15:33:59.991: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=true. Elapsed: 8.10359119s
Aug 30 15:34:02.024: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=true. Elapsed: 10.136787325s
Aug 30 15:34:04.052: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=true. Elapsed: 12.164783118s
Aug 30 15:34:06.103: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=true. Elapsed: 14.215543054s
Aug 30 15:34:08.122: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=true. Elapsed: 16.235312892s
Aug 30 15:34:10.160: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=true. Elapsed: 18.27255922s
Aug 30 15:34:12.181: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=true. Elapsed: 20.294008895s
Aug 30 15:34:14.200: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=false. Elapsed: 22.31327884s
Aug 30 15:34:16.221: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Running", Reason="", readiness=false. Elapsed: 24.334121236s
Aug 30 15:34:18.240: INFO: Pod "pod-subpath-test-downwardapi-fmlf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.353251204s
STEP: Saw pod success
Aug 30 15:34:18.240: INFO: Pod "pod-subpath-test-downwardapi-fmlf" satisfied condition "Succeeded or Failed"
Aug 30 15:34:18.258: INFO: Trying to get logs from node 10.63.224.189 pod pod-subpath-test-downwardapi-fmlf container test-container-subpath-downwardapi-fmlf: <nil>
STEP: delete the pod
Aug 30 15:34:18.367: INFO: Waiting for pod pod-subpath-test-downwardapi-fmlf to disappear
Aug 30 15:34:18.391: INFO: Pod pod-subpath-test-downwardapi-fmlf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fmlf
Aug 30 15:34:18.391: INFO: Deleting pod "pod-subpath-test-downwardapi-fmlf" in namespace "subpath-6088"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 30 15:34:18.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6088" for this suite.

• [SLOW TEST:26.923 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":211,"skipped":3906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:34:18.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
W0830 15:34:18.642349      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.capabilities.drop=["ALL"]), restricted volume types (volume "host-etc-hosts" uses restricted volume type "hostPath"), runAsNonRoot != true (pod or containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:34:18.662: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:34:20.688: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:34:22.680: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
W0830 15:34:22.743763      21 warnings.go:70] would violate PodSecurity "restricted:latest": host namespaces (hostNetwork=true), allowPrivilegeEscalation != false (containers "busybox-1", "busybox-2" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "busybox-1", "busybox-2" must set securityContext.capabilities.drop=["ALL"]), restricted volume types (volume "host-etc-hosts" uses restricted volume type "hostPath"), seccompProfile (pod or containers "busybox-1", "busybox-2" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:34:22.760: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:34:24.785: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:34:26.791: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 30 15:34:26.805: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7459 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:34:26.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:34:26.807: INFO: ExecWithOptions: Clientset creation
Aug 30 15:34:26.807: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7459/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 30 15:34:27.085: INFO: Exec stderr: ""
Aug 30 15:34:27.085: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7459 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:34:27.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:34:27.087: INFO: ExecWithOptions: Clientset creation
Aug 30 15:34:27.087: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7459/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 30 15:34:27.395: INFO: Exec stderr: ""
Aug 30 15:34:27.395: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7459 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:34:27.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:34:27.399: INFO: ExecWithOptions: Clientset creation
Aug 30 15:34:27.399: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7459/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 30 15:34:27.663: INFO: Exec stderr: ""
Aug 30 15:34:27.663: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7459 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:34:27.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:34:27.665: INFO: ExecWithOptions: Clientset creation
Aug 30 15:34:27.665: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7459/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 30 15:34:27.936: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 30 15:34:27.936: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7459 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:34:27.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:34:27.938: INFO: ExecWithOptions: Clientset creation
Aug 30 15:34:27.938: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7459/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 30 15:34:28.209: INFO: Exec stderr: ""
Aug 30 15:34:28.209: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7459 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:34:28.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:34:28.210: INFO: ExecWithOptions: Clientset creation
Aug 30 15:34:28.211: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7459/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 30 15:34:28.505: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 30 15:34:28.505: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7459 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:34:28.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:34:28.506: INFO: ExecWithOptions: Clientset creation
Aug 30 15:34:28.506: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7459/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 30 15:34:28.866: INFO: Exec stderr: ""
Aug 30 15:34:28.866: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7459 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:34:28.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:34:28.868: INFO: ExecWithOptions: Clientset creation
Aug 30 15:34:28.868: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7459/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 30 15:34:29.505: INFO: Exec stderr: ""
Aug 30 15:34:29.505: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7459 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:34:29.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:34:29.507: INFO: ExecWithOptions: Clientset creation
Aug 30 15:34:29.507: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7459/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 30 15:34:29.773: INFO: Exec stderr: ""
Aug 30 15:34:29.773: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7459 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 15:34:29.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:34:29.774: INFO: ExecWithOptions: Clientset creation
Aug 30 15:34:29.774: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7459/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 30 15:34:30.155: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Aug 30 15:34:30.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7459" for this suite.

• [SLOW TEST:11.773 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":212,"skipped":3963,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:34:30.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-927dbc4b-e8e4-4e81-bd84-f8fbd7dacd0a
STEP: Creating a pod to test consume secrets
W0830 15:34:30.409821      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:34:30.410: INFO: Waiting up to 5m0s for pod "pod-secrets-47df2e60-48cb-416e-986f-57b3f24c9280" in namespace "secrets-7717" to be "Succeeded or Failed"
Aug 30 15:34:30.437: INFO: Pod "pod-secrets-47df2e60-48cb-416e-986f-57b3f24c9280": Phase="Pending", Reason="", readiness=false. Elapsed: 27.20072ms
Aug 30 15:34:32.455: INFO: Pod "pod-secrets-47df2e60-48cb-416e-986f-57b3f24c9280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045626937s
Aug 30 15:34:34.476: INFO: Pod "pod-secrets-47df2e60-48cb-416e-986f-57b3f24c9280": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066574264s
Aug 30 15:34:36.495: INFO: Pod "pod-secrets-47df2e60-48cb-416e-986f-57b3f24c9280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.085266467s
STEP: Saw pod success
Aug 30 15:34:36.495: INFO: Pod "pod-secrets-47df2e60-48cb-416e-986f-57b3f24c9280" satisfied condition "Succeeded or Failed"
Aug 30 15:34:36.512: INFO: Trying to get logs from node 10.63.224.187 pod pod-secrets-47df2e60-48cb-416e-986f-57b3f24c9280 container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 15:34:36.650: INFO: Waiting for pod pod-secrets-47df2e60-48cb-416e-986f-57b3f24c9280 to disappear
Aug 30 15:34:36.666: INFO: Pod pod-secrets-47df2e60-48cb-416e-986f-57b3f24c9280 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 30 15:34:36.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7717" for this suite.

• [SLOW TEST:6.480 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":213,"skipped":3963,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:34:36.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
W0830 15:34:36.887568      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:34:36.888: INFO: Waiting up to 5m0s for pod "pod-ebef9866-b855-4fd6-bce0-cb43a4bd3199" in namespace "emptydir-9763" to be "Succeeded or Failed"
Aug 30 15:34:36.905: INFO: Pod "pod-ebef9866-b855-4fd6-bce0-cb43a4bd3199": Phase="Pending", Reason="", readiness=false. Elapsed: 16.363252ms
Aug 30 15:34:38.922: INFO: Pod "pod-ebef9866-b855-4fd6-bce0-cb43a4bd3199": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034020569s
Aug 30 15:34:40.941: INFO: Pod "pod-ebef9866-b855-4fd6-bce0-cb43a4bd3199": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05282044s
Aug 30 15:34:42.961: INFO: Pod "pod-ebef9866-b855-4fd6-bce0-cb43a4bd3199": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073152173s
STEP: Saw pod success
Aug 30 15:34:42.961: INFO: Pod "pod-ebef9866-b855-4fd6-bce0-cb43a4bd3199" satisfied condition "Succeeded or Failed"
Aug 30 15:34:42.989: INFO: Trying to get logs from node 10.63.224.187 pod pod-ebef9866-b855-4fd6-bce0-cb43a4bd3199 container test-container: <nil>
STEP: delete the pod
Aug 30 15:34:43.087: INFO: Waiting for pod pod-ebef9866-b855-4fd6-bce0-cb43a4bd3199 to disappear
Aug 30 15:34:43.106: INFO: Pod pod-ebef9866-b855-4fd6-bce0-cb43a4bd3199 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 15:34:43.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9763" for this suite.

• [SLOW TEST:6.431 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":214,"skipped":3984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:34:43.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Aug 30 15:34:43.283: INFO: created test-event-1
Aug 30 15:34:43.300: INFO: created test-event-2
Aug 30 15:34:43.319: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Aug 30 15:34:43.333: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Aug 30 15:34:43.425: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Aug 30 15:34:43.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8009" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":215,"skipped":4025,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:34:43.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 15:34:44.239133      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 15:34:44.272: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 15:34:46.313: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 15, 34, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 34, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 15, 34, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 34, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 15:34:49.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
W0830 15:34:49.547352      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 30 15:34:51.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=webhook-3853 attach --namespace=webhook-3853 to-be-attached-pod -i -c=container1'
Aug 30 15:34:51.893: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:34:51.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3853" for this suite.
STEP: Destroying namespace "webhook-3853-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.640 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":216,"skipped":4026,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:34:52.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 15:34:52.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6220" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":217,"skipped":4027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:34:52.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
W0830 15:34:52.547403      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:34:52.571: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:34:54.590: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:34:56.591: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
W0830 15:34:56.645455      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 30 15:34:57.704: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 30 15:34:57.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1405" for this suite.

• [SLOW TEST:5.550 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":218,"skipped":4094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:34:57.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:34:58.031: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-a4f6bf88-f62e-49f2-aebf-71c9591af589
STEP: Creating configMap with name cm-test-opt-upd-afe78703-90dc-4fef-a867-142540812e56
STEP: Creating the pod
W0830 15:34:58.162257      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:34:58.187: INFO: The status of Pod pod-configmaps-669b8a37-72ed-41e8-9290-f9d13cbe097f is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:35:00.205: INFO: The status of Pod pod-configmaps-669b8a37-72ed-41e8-9290-f9d13cbe097f is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:35:02.221: INFO: The status of Pod pod-configmaps-669b8a37-72ed-41e8-9290-f9d13cbe097f is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-a4f6bf88-f62e-49f2-aebf-71c9591af589
STEP: Updating configmap cm-test-opt-upd-afe78703-90dc-4fef-a867-142540812e56
STEP: Creating configMap with name cm-test-opt-create-774b927f-99c4-48cc-8a19-e0358a2597b6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 15:36:08.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6519" for this suite.

• [SLOW TEST:70.396 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":219,"skipped":4128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:08.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 30 15:36:08.578: INFO: Waiting up to 5m0s for pod "downwardapi-volume-416df1dd-9849-4911-bd3b-cab7acd05233" in namespace "projected-9862" to be "Succeeded or Failed"
W0830 15:36:08.578268      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:36:08.638: INFO: Pod "downwardapi-volume-416df1dd-9849-4911-bd3b-cab7acd05233": Phase="Pending", Reason="", readiness=false. Elapsed: 60.272783ms
Aug 30 15:36:10.659: INFO: Pod "downwardapi-volume-416df1dd-9849-4911-bd3b-cab7acd05233": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081231363s
Aug 30 15:36:12.691: INFO: Pod "downwardapi-volume-416df1dd-9849-4911-bd3b-cab7acd05233": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112527888s
Aug 30 15:36:14.709: INFO: Pod "downwardapi-volume-416df1dd-9849-4911-bd3b-cab7acd05233": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.130715175s
STEP: Saw pod success
Aug 30 15:36:14.710: INFO: Pod "downwardapi-volume-416df1dd-9849-4911-bd3b-cab7acd05233" satisfied condition "Succeeded or Failed"
Aug 30 15:36:14.732: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-416df1dd-9849-4911-bd3b-cab7acd05233 container client-container: <nil>
STEP: delete the pod
Aug 30 15:36:14.922: INFO: Waiting for pod downwardapi-volume-416df1dd-9849-4911-bd3b-cab7acd05233 to disappear
Aug 30 15:36:14.942: INFO: Pod downwardapi-volume-416df1dd-9849-4911-bd3b-cab7acd05233 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 15:36:14.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9862" for this suite.

• [SLOW TEST:6.706 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":220,"skipped":4166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:14.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
W0830 15:36:15.170531      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-false0f5281b8-1a5d-40cc-96ca-790e5bcf42cd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-false0f5281b8-1a5d-40cc-96ca-790e5bcf42cd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-false0f5281b8-1a5d-40cc-96ca-790e5bcf42cd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-false0f5281b8-1a5d-40cc-96ca-790e5bcf42cd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 30 15:36:15.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5253" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":221,"skipped":4188,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:15.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:36:15.523: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
W0830 15:36:15.549044      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:36:15.589: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:15.589: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Aug 30 15:36:15.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:15.735: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:36:16.754: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:16.754: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:36:17.754: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:17.755: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:36:18.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 15:36:18.767: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 30 15:36:18.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 15:36:18.931: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 30 15:36:19.955: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:19.955: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
W0830 15:36:19.977059      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:36:19.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:19.995: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:36:21.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:21.031: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:36:22.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:22.042: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:36:23.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:23.013: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:36:24.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:24.046: INFO: Node 10.63.224.158 is running 0 daemon pod, expected 1
Aug 30 15:36:25.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 15:36:25.049: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9344, will wait for the garbage collector to delete the pods
Aug 30 15:36:25.163: INFO: Deleting DaemonSet.extensions daemon-set took: 27.728059ms
Aug 30 15:36:25.364: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.654535ms
Aug 30 15:36:28.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 15:36:28.293: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 15:36:28.312: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"115436"},"items":null}

Aug 30 15:36:28.332: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"115436"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 30 15:36:28.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9344" for this suite.

• [SLOW TEST:13.213 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":222,"skipped":4191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:28.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 30 15:36:28.783: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 30 15:36:28.831: INFO: starting watch
STEP: patching
STEP: updating
Aug 30 15:36:28.940: INFO: waiting for watch events with expected annotations
Aug 30 15:36:28.940: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Aug 30 15:36:29.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8761" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":223,"skipped":4224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:29.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:36:29.501: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Aug 30 15:36:29.501: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 30 15:36:29.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5732" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":224,"skipped":4250,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:29.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
W0830 15:36:29.724152      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 15:36:29.769209      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 15:36:29.854354      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:36:29.886: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"90173876-1363-4009-8de0-0f9913eb3457", Controller:(*bool)(0xc00c111722), BlockOwnerDeletion:(*bool)(0xc00c111723)}}
Aug 30 15:36:29.929: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2a27cd03-db86-4d1a-85c1-6f2ff88f7edf", Controller:(*bool)(0xc00c111a36), BlockOwnerDeletion:(*bool)(0xc00c111a37)}}
Aug 30 15:36:30.027: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"564bcff1-f023-4c72-b495-61c4243176bc", Controller:(*bool)(0xc00252f996), BlockOwnerDeletion:(*bool)(0xc00252f997)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 30 15:36:35.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4454" for this suite.

• [SLOW TEST:5.581 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":225,"skipped":4257,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:35.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
W0830 15:36:35.298302      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:36:35.298: INFO: Waiting up to 5m0s for pod "pod-dfc00c68-f35e-45e9-a991-c5816a8b9017" in namespace "emptydir-1428" to be "Succeeded or Failed"
Aug 30 15:36:35.315: INFO: Pod "pod-dfc00c68-f35e-45e9-a991-c5816a8b9017": Phase="Pending", Reason="", readiness=false. Elapsed: 17.280534ms
Aug 30 15:36:37.343: INFO: Pod "pod-dfc00c68-f35e-45e9-a991-c5816a8b9017": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044699342s
Aug 30 15:36:39.368: INFO: Pod "pod-dfc00c68-f35e-45e9-a991-c5816a8b9017": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069578278s
Aug 30 15:36:41.395: INFO: Pod "pod-dfc00c68-f35e-45e9-a991-c5816a8b9017": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.096424617s
STEP: Saw pod success
Aug 30 15:36:41.395: INFO: Pod "pod-dfc00c68-f35e-45e9-a991-c5816a8b9017" satisfied condition "Succeeded or Failed"
Aug 30 15:36:41.424: INFO: Trying to get logs from node 10.63.224.189 pod pod-dfc00c68-f35e-45e9-a991-c5816a8b9017 container test-container: <nil>
STEP: delete the pod
Aug 30 15:36:41.524: INFO: Waiting for pod pod-dfc00c68-f35e-45e9-a991-c5816a8b9017 to disappear
Aug 30 15:36:41.542: INFO: Pod pod-dfc00c68-f35e-45e9-a991-c5816a8b9017 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 15:36:41.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1428" for this suite.

• [SLOW TEST:6.456 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":226,"skipped":4266,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:41.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-cd6fa71a-1e9a-4379-8448-b5ab9f8a6006
STEP: Creating a pod to test consume secrets
W0830 15:36:41.791277      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:36:41.791: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-81d8e7e6-e8d9-4d13-af83-10feda0e76d7" in namespace "projected-6429" to be "Succeeded or Failed"
Aug 30 15:36:41.823: INFO: Pod "pod-projected-secrets-81d8e7e6-e8d9-4d13-af83-10feda0e76d7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.213356ms
Aug 30 15:36:43.848: INFO: Pod "pod-projected-secrets-81d8e7e6-e8d9-4d13-af83-10feda0e76d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056554173s
Aug 30 15:36:45.869: INFO: Pod "pod-projected-secrets-81d8e7e6-e8d9-4d13-af83-10feda0e76d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078073564s
Aug 30 15:36:47.895: INFO: Pod "pod-projected-secrets-81d8e7e6-e8d9-4d13-af83-10feda0e76d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.104099762s
STEP: Saw pod success
Aug 30 15:36:47.895: INFO: Pod "pod-projected-secrets-81d8e7e6-e8d9-4d13-af83-10feda0e76d7" satisfied condition "Succeeded or Failed"
Aug 30 15:36:47.921: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-secrets-81d8e7e6-e8d9-4d13-af83-10feda0e76d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 30 15:36:48.044: INFO: Waiting for pod pod-projected-secrets-81d8e7e6-e8d9-4d13-af83-10feda0e76d7 to disappear
Aug 30 15:36:48.061: INFO: Pod pod-projected-secrets-81d8e7e6-e8d9-4d13-af83-10feda0e76d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 30 15:36:48.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6429" for this suite.

• [SLOW TEST:6.504 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":227,"skipped":4281,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:48.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Aug 30 15:36:48.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-862" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":228,"skipped":4285,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:48.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 15:36:49.267333      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 15:36:49.300: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 15:36:52.393: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:36:52.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8154-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:36:55.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8835" for this suite.
STEP: Destroying namespace "webhook-8835-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.756 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":229,"skipped":4287,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:36:56.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
W0830 15:36:56.272843      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:36:56.273: INFO: Waiting up to 5m0s for pod "pod-fa27ea7c-67ff-48e0-833f-d1054909ad66" in namespace "emptydir-1695" to be "Succeeded or Failed"
Aug 30 15:36:56.305: INFO: Pod "pod-fa27ea7c-67ff-48e0-833f-d1054909ad66": Phase="Pending", Reason="", readiness=false. Elapsed: 31.936831ms
Aug 30 15:36:58.371: INFO: Pod "pod-fa27ea7c-67ff-48e0-833f-d1054909ad66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09830317s
Aug 30 15:37:00.401: INFO: Pod "pod-fa27ea7c-67ff-48e0-833f-d1054909ad66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.128152044s
Aug 30 15:37:02.430: INFO: Pod "pod-fa27ea7c-67ff-48e0-833f-d1054909ad66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.156800862s
STEP: Saw pod success
Aug 30 15:37:02.431: INFO: Pod "pod-fa27ea7c-67ff-48e0-833f-d1054909ad66" satisfied condition "Succeeded or Failed"
Aug 30 15:37:02.469: INFO: Trying to get logs from node 10.63.224.189 pod pod-fa27ea7c-67ff-48e0-833f-d1054909ad66 container test-container: <nil>
STEP: delete the pod
Aug 30 15:37:02.562: INFO: Waiting for pod pod-fa27ea7c-67ff-48e0-833f-d1054909ad66 to disappear
Aug 30 15:37:02.580: INFO: Pod pod-fa27ea7c-67ff-48e0-833f-d1054909ad66 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 15:37:02.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1695" for this suite.

• [SLOW TEST:6.560 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":230,"skipped":4292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:37:02.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-e067fd93-f9fa-440c-a4df-492331e5408b in namespace container-probe-6945
W0830 15:37:02.819393      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:37:06.879: INFO: Started pod liveness-e067fd93-f9fa-440c-a4df-492331e5408b in namespace container-probe-6945
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 15:37:06.893: INFO: Initial restart count of pod liveness-e067fd93-f9fa-440c-a4df-492331e5408b is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 30 15:41:08.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6945" for this suite.

• [SLOW TEST:245.491 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":231,"skipped":4326,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:41:08.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-4507
STEP: creating service affinity-clusterip in namespace services-4507
STEP: creating replication controller affinity-clusterip in namespace services-4507
W0830 15:41:08.517187      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-clusterip" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-clusterip" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-clusterip" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-clusterip" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 15:41:08.517862      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4507, replica count: 3
I0830 15:41:11.570967      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 15:41:14.572374      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 15:41:14.597: INFO: Creating new exec pod
W0830 15:41:14.650951      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:41:19.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4507 exec execpod-affinitygcp2r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Aug 30 15:41:20.164: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 30 15:41:20.164: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 15:41:20.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4507 exec execpod-affinitygcp2r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.178.221 80'
Aug 30 15:41:20.545: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.178.221 80\nConnection to 172.21.178.221 80 port [tcp/http] succeeded!\n"
Aug 30 15:41:20.545: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 15:41:20.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4507 exec execpod-affinitygcp2r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.178.221:80/ ; done'
Aug 30 15:41:21.066: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.178.221:80/\n"
Aug 30 15:41:21.066: INFO: stdout: "\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp\naffinity-clusterip-fdlqp"
Aug 30 15:41:21.066: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.066: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Received response from host: affinity-clusterip-fdlqp
Aug 30 15:41:21.067: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4507, will wait for the garbage collector to delete the pods
Aug 30 15:41:21.237: INFO: Deleting ReplicationController affinity-clusterip took: 36.787144ms
Aug 30 15:41:21.438: INFO: Terminating ReplicationController affinity-clusterip pods took: 200.655364ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 15:41:24.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4507" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:16.228 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":232,"skipped":4341,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:41:24.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
W0830 15:41:24.488292      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:41:24.505: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 30 15:41:29.527: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 30 15:41:29.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2696" for this suite.

• [SLOW TEST:5.318 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":233,"skipped":4354,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:41:29.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0830 15:41:29.907069      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 30 15:41:32.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2536" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":234,"skipped":4372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:41:32.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 30 15:41:32.245: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 15:42:32.566: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:42:32.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
W0830 15:42:32.767705      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "without-label" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "without-label" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "without-label" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "without-label" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Explicitly delete pod here to free the resource it takes.
Aug 30 15:42:36.926: INFO: found a healthy node: 10.63.224.189
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
W0830 15:42:37.125299      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 15:42:49.170497      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod2" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod2" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod2" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod2" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 15:42:53.252172      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod3" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod3" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod3" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod3" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:42:57.287: INFO: pods created so far: [1 1 1]
Aug 30 15:42:57.287: INFO: length of pods created so far: 3
W0830 15:42:57.332519      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod4" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod4" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod4" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod4" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:43:03.351: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Aug 30 15:43:10.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9929" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 30 15:43:10.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8424" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:98.728 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":235,"skipped":4401,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:43:10.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
W0830 15:43:11.132034      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:43:11.138: INFO: observed Pod pod-test in namespace pods-172 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 30 15:43:11.156: INFO: observed Pod pod-test in namespace pods-172 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC  }]
Aug 30 15:43:11.209: INFO: observed Pod pod-test in namespace pods-172 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC  }]
Aug 30 15:43:12.325: INFO: observed Pod pod-test in namespace pods-172 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC  }]
Aug 30 15:43:12.432: INFO: observed Pod pod-test in namespace pods-172 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC  }]
Aug 30 15:43:13.858: INFO: Found Pod pod-test in namespace pods-172 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 15:43:11 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
W0830 15:43:13.934455      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Aug 30 15:43:14.078: INFO: observed event type MODIFIED
Aug 30 15:43:15.901: INFO: observed event type MODIFIED
Aug 30 15:43:16.309: INFO: observed event type MODIFIED
Aug 30 15:43:18.012: INFO: observed event type MODIFIED
Aug 30 15:43:18.061: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 30 15:43:18.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-172" for this suite.

• [SLOW TEST:7.353 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":236,"skipped":4422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:43:18.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 30 15:43:18.338: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2526  ef80ddf6-7266-4ade-9622-0a0d6f9fbcd0 118895 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 15:43:18.339: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2526  ef80ddf6-7266-4ade-9622-0a0d6f9fbcd0 118895 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 30 15:43:18.398: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2526  ef80ddf6-7266-4ade-9622-0a0d6f9fbcd0 118901 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 15:43:18.398: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2526  ef80ddf6-7266-4ade-9622-0a0d6f9fbcd0 118901 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 30 15:43:18.439: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2526  ef80ddf6-7266-4ade-9622-0a0d6f9fbcd0 118905 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 15:43:18.440: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2526  ef80ddf6-7266-4ade-9622-0a0d6f9fbcd0 118905 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 30 15:43:18.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2526  ef80ddf6-7266-4ade-9622-0a0d6f9fbcd0 118908 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 15:43:18.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2526  ef80ddf6-7266-4ade-9622-0a0d6f9fbcd0 118908 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 30 15:43:18.505: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2526  9f1aec6d-0f10-427f-ae1f-1538854c42af 118909 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 15:43:18.506: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2526  9f1aec6d-0f10-427f-ae1f-1538854c42af 118909 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 30 15:43:28.544: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2526  9f1aec6d-0f10-427f-ae1f-1538854c42af 118995 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 15:43:28.544: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2526  9f1aec6d-0f10-427f-ae1f-1538854c42af 118995 0 2022-08-30 15:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-30 15:43:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 30 15:43:38.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2526" for this suite.

• [SLOW TEST:20.434 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":237,"skipped":4445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:43:38.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 30 15:44:06.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6712" for this suite.

• [SLOW TEST:28.355 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":238,"skipped":4498,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:44:06.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
W0830 15:44:07.124058      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:44:11.164: INFO: Deleting pod "var-expansion-d81eee06-2771-419c-b4eb-baa4c2c8eac1" in namespace "var-expansion-7024"
Aug 30 15:44:11.201: INFO: Wait up to 5m0s for pod "var-expansion-d81eee06-2771-419c-b4eb-baa4c2c8eac1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 30 15:44:13.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7024" for this suite.

• [SLOW TEST:6.330 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":239,"skipped":4550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:44:13.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-c9c74fb8-f4c9-4786-8b9f-14fcce2ba55e
STEP: Creating a pod to test consume configMaps
Aug 30 15:44:13.477: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff34c95e-7313-46e6-a08c-14fe85438e23" in namespace "configmap-2225" to be "Succeeded or Failed"
W0830 15:44:13.477383      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:44:13.509: INFO: Pod "pod-configmaps-ff34c95e-7313-46e6-a08c-14fe85438e23": Phase="Pending", Reason="", readiness=false. Elapsed: 31.778734ms
Aug 30 15:44:15.526: INFO: Pod "pod-configmaps-ff34c95e-7313-46e6-a08c-14fe85438e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048760356s
Aug 30 15:44:17.563: INFO: Pod "pod-configmaps-ff34c95e-7313-46e6-a08c-14fe85438e23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086052535s
Aug 30 15:44:19.582: INFO: Pod "pod-configmaps-ff34c95e-7313-46e6-a08c-14fe85438e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.104938466s
STEP: Saw pod success
Aug 30 15:44:19.582: INFO: Pod "pod-configmaps-ff34c95e-7313-46e6-a08c-14fe85438e23" satisfied condition "Succeeded or Failed"
Aug 30 15:44:19.600: INFO: Trying to get logs from node 10.63.224.189 pod pod-configmaps-ff34c95e-7313-46e6-a08c-14fe85438e23 container agnhost-container: <nil>
STEP: delete the pod
Aug 30 15:44:19.788: INFO: Waiting for pod pod-configmaps-ff34c95e-7313-46e6-a08c-14fe85438e23 to disappear
Aug 30 15:44:19.806: INFO: Pod pod-configmaps-ff34c95e-7313-46e6-a08c-14fe85438e23 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 15:44:19.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2225" for this suite.

• [SLOW TEST:6.600 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":240,"skipped":4582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:44:19.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-e09e34eb-80d3-4c9f-bc1d-d4ca223803af
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 15:44:20.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5842" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":241,"skipped":4663,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:44:20.066: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 15:44:20.978754      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 15:44:21.014: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 15:44:23.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 15, 44, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 44, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 15, 44, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 15, 44, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 15:44:26.112: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:44:26.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7331" for this suite.
STEP: Destroying namespace "webhook-7331-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.998 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":242,"skipped":4683,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:44:27.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:44:27.187: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-4608e123-64b0-45a3-8dd1-cf0ab4a85812
STEP: Creating secret with name s-test-opt-upd-c71c0d5c-14c2-4333-b5e2-3ddca4674fd5
STEP: Creating the pod
W0830 15:44:27.365347      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:44:27.393: INFO: The status of Pod pod-projected-secrets-302781fc-2f66-416f-8ac3-04e95d68e3a5 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:44:29.420: INFO: The status of Pod pod-projected-secrets-302781fc-2f66-416f-8ac3-04e95d68e3a5 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:44:31.418: INFO: The status of Pod pod-projected-secrets-302781fc-2f66-416f-8ac3-04e95d68e3a5 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-4608e123-64b0-45a3-8dd1-cf0ab4a85812
STEP: Updating secret s-test-opt-upd-c71c0d5c-14c2-4333-b5e2-3ddca4674fd5
STEP: Creating secret with name s-test-opt-create-9f2e895b-905b-4797-a1d8-0e8b7e89e851
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 30 15:45:59.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4153" for this suite.

• [SLOW TEST:92.882 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":243,"skipped":4699,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:45:59.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Aug 30 15:46:00.104: INFO: Major version: 1
STEP: Confirm minor version
Aug 30 15:46:00.104: INFO: cleanMinorVersion: 24
Aug 30 15:46:00.104: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Aug 30 15:46:00.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6184" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":244,"skipped":4735,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:46:00.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0830 15:46:00.374619      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-user-65534-65319e1d-e3ba-4007-a7f8-18ea1e060ab4" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-65319e1d-e3ba-4007-a7f8-18ea1e060ab4" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "busybox-user-65534-65319e1d-e3ba-4007-a7f8-18ea1e060ab4" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:46:00.374: INFO: Waiting up to 5m0s for pod "busybox-user-65534-65319e1d-e3ba-4007-a7f8-18ea1e060ab4" in namespace "security-context-test-2427" to be "Succeeded or Failed"
Aug 30 15:46:00.421: INFO: Pod "busybox-user-65534-65319e1d-e3ba-4007-a7f8-18ea1e060ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 46.176207ms
Aug 30 15:46:02.438: INFO: Pod "busybox-user-65534-65319e1d-e3ba-4007-a7f8-18ea1e060ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063716149s
Aug 30 15:46:04.462: INFO: Pod "busybox-user-65534-65319e1d-e3ba-4007-a7f8-18ea1e060ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087394526s
Aug 30 15:46:06.483: INFO: Pod "busybox-user-65534-65319e1d-e3ba-4007-a7f8-18ea1e060ab4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.108188836s
Aug 30 15:46:06.483: INFO: Pod "busybox-user-65534-65319e1d-e3ba-4007-a7f8-18ea1e060ab4" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 30 15:46:06.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2427" for this suite.

• [SLOW TEST:6.379 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:52
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":245,"skipped":4749,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:46:06.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1294
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
W0830 15:46:06.771555      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:46:06.898: INFO: Found 0 stateful pods, waiting for 3
Aug 30 15:46:16.925: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 15:46:16.925: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 15:46:16.925: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Aug 30 15:46:17.061: INFO: Updating stateful set ss2
W0830 15:46:17.061665      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
W0830 15:46:27.243295      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:46:27.243: INFO: Updating stateful set ss2
Aug 30 15:46:27.304: INFO: Waiting for Pod statefulset-1294/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Aug 30 15:46:37.535: INFO: Found 1 stateful pods, waiting for 3
Aug 30 15:46:47.555: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 15:46:47.555: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 15:46:47.555: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 30 15:46:47.647: INFO: Updating stateful set ss2
W0830 15:46:47.647910      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:46:47.685: INFO: Waiting for Pod statefulset-1294/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Aug 30 15:46:57.790: INFO: Updating stateful set ss2
W0830 15:46:57.790632      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:46:57.841: INFO: Waiting for StatefulSet statefulset-1294/ss2 to complete update
Aug 30 15:46:57.841: INFO: Waiting for Pod statefulset-1294/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 30 15:47:07.909: INFO: Deleting all statefulset in ns statefulset-1294
Aug 30 15:47:07.927: INFO: Scaling statefulset ss2 to 0
W0830 15:47:08.002416      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:47:18.049: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 15:47:18.065: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 30 15:47:18.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1294" for this suite.

• [SLOW TEST:71.631 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":246,"skipped":4776,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:47:18.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-d76df252-9628-4f4b-b7c8-f5aefe52a208
STEP: Creating a pod to test consume configMaps
W0830 15:47:18.370196      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:47:18.370: INFO: Waiting up to 5m0s for pod "pod-configmaps-9a885f07-d69b-4dc3-9fa6-919f1d9f9e02" in namespace "configmap-2693" to be "Succeeded or Failed"
Aug 30 15:47:18.390: INFO: Pod "pod-configmaps-9a885f07-d69b-4dc3-9fa6-919f1d9f9e02": Phase="Pending", Reason="", readiness=false. Elapsed: 19.931732ms
Aug 30 15:47:20.409: INFO: Pod "pod-configmaps-9a885f07-d69b-4dc3-9fa6-919f1d9f9e02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039001059s
Aug 30 15:47:22.440: INFO: Pod "pod-configmaps-9a885f07-d69b-4dc3-9fa6-919f1d9f9e02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070064816s
Aug 30 15:47:24.472: INFO: Pod "pod-configmaps-9a885f07-d69b-4dc3-9fa6-919f1d9f9e02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.102129482s
STEP: Saw pod success
Aug 30 15:47:24.472: INFO: Pod "pod-configmaps-9a885f07-d69b-4dc3-9fa6-919f1d9f9e02" satisfied condition "Succeeded or Failed"
Aug 30 15:47:24.489: INFO: Trying to get logs from node 10.63.224.189 pod pod-configmaps-9a885f07-d69b-4dc3-9fa6-919f1d9f9e02 container agnhost-container: <nil>
STEP: delete the pod
Aug 30 15:47:24.614: INFO: Waiting for pod pod-configmaps-9a885f07-d69b-4dc3-9fa6-919f1d9f9e02 to disappear
Aug 30 15:47:24.630: INFO: Pod pod-configmaps-9a885f07-d69b-4dc3-9fa6-919f1d9f9e02 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 15:47:24.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2693" for this suite.

• [SLOW TEST:6.510 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":247,"skipped":4795,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:47:24.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
W0830 15:47:24.851837      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:47:24.871: INFO: The status of Pod labelsupdate8360cb9e-4f1e-4dbe-b00b-7aa672770c8a is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:47:26.900: INFO: The status of Pod labelsupdate8360cb9e-4f1e-4dbe-b00b-7aa672770c8a is Running (Ready = true)
Aug 30 15:47:27.543: INFO: Successfully updated pod "labelsupdate8360cb9e-4f1e-4dbe-b00b-7aa672770c8a"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 15:47:29.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-776" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":248,"skipped":4812,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:47:29.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:47:29.821: INFO: Got root ca configmap in namespace "svcaccounts-917"
Aug 30 15:47:29.886: INFO: Deleted root ca configmap in namespace "svcaccounts-917"
STEP: waiting for a new root ca configmap created
Aug 30 15:47:30.406: INFO: Recreated root ca configmap in namespace "svcaccounts-917"
Aug 30 15:47:30.432: INFO: Updated root ca configmap in namespace "svcaccounts-917"
STEP: waiting for the root ca configmap reconciled
Aug 30 15:47:30.953: INFO: Reconciled root ca configmap in namespace "svcaccounts-917"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 30 15:47:30.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-917" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":249,"skipped":4821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:47:31.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5092
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5092
STEP: creating replication controller externalsvc in namespace services-5092
W0830 15:47:31.236614      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "externalsvc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalsvc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalsvc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalsvc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 15:47:31.236998      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5092, replica count: 2
I0830 15:47:34.290720      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 30 15:47:34.386: INFO: Creating new exec pod
W0830 15:47:34.461376      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:47:38.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-5092 exec execpod9zv4z -- /bin/sh -x -c nslookup nodeport-service.services-5092.svc.cluster.local'
Aug 30 15:47:39.006: INFO: stderr: "+ nslookup nodeport-service.services-5092.svc.cluster.local\n"
Aug 30 15:47:39.006: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-5092.svc.cluster.local\tcanonical name = externalsvc.services-5092.svc.cluster.local.\nName:\texternalsvc.services-5092.svc.cluster.local\nAddress: 172.21.182.32\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5092, will wait for the garbage collector to delete the pods
Aug 30 15:47:39.093: INFO: Deleting ReplicationController externalsvc took: 19.580254ms
Aug 30 15:47:39.193: INFO: Terminating ReplicationController externalsvc pods took: 100.323259ms
Aug 30 15:47:42.158: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 15:47:42.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5092" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:11.237 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":250,"skipped":4844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:47:42.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 30 15:47:42.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 30 15:48:17.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 15:48:27.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:49:04.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2345" for this suite.

• [SLOW TEST:82.802 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":251,"skipped":4871,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:49:05.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 15:49:05.232638      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:49:05.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec0fc2b6-e173-41a4-a9d8-2786d7a49bc3" in namespace "projected-4747" to be "Succeeded or Failed"
Aug 30 15:49:05.243: INFO: Pod "downwardapi-volume-ec0fc2b6-e173-41a4-a9d8-2786d7a49bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.593111ms
Aug 30 15:49:07.256: INFO: Pod "downwardapi-volume-ec0fc2b6-e173-41a4-a9d8-2786d7a49bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023115573s
Aug 30 15:49:09.269: INFO: Pod "downwardapi-volume-ec0fc2b6-e173-41a4-a9d8-2786d7a49bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036296645s
Aug 30 15:49:11.283: INFO: Pod "downwardapi-volume-ec0fc2b6-e173-41a4-a9d8-2786d7a49bc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050138627s
STEP: Saw pod success
Aug 30 15:49:11.283: INFO: Pod "downwardapi-volume-ec0fc2b6-e173-41a4-a9d8-2786d7a49bc3" satisfied condition "Succeeded or Failed"
Aug 30 15:49:11.292: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-ec0fc2b6-e173-41a4-a9d8-2786d7a49bc3 container client-container: <nil>
STEP: delete the pod
Aug 30 15:49:11.386: INFO: Waiting for pod downwardapi-volume-ec0fc2b6-e173-41a4-a9d8-2786d7a49bc3 to disappear
Aug 30 15:49:11.397: INFO: Pod downwardapi-volume-ec0fc2b6-e173-41a4-a9d8-2786d7a49bc3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 15:49:11.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4747" for this suite.

• [SLOW TEST:6.399 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":252,"skipped":4877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:49:11.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-7a90d8b8-d31b-4128-8dc4-6cd372037afc
STEP: Creating a pod to test consume configMaps
W0830 15:49:11.617044      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:49:11.618: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07337cb5-c131-4ced-91a7-ec97c84feb2e" in namespace "projected-110" to be "Succeeded or Failed"
Aug 30 15:49:11.629: INFO: Pod "pod-projected-configmaps-07337cb5-c131-4ced-91a7-ec97c84feb2e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.864396ms
Aug 30 15:49:13.647: INFO: Pod "pod-projected-configmaps-07337cb5-c131-4ced-91a7-ec97c84feb2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028997872s
Aug 30 15:49:15.660: INFO: Pod "pod-projected-configmaps-07337cb5-c131-4ced-91a7-ec97c84feb2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042752648s
STEP: Saw pod success
Aug 30 15:49:15.660: INFO: Pod "pod-projected-configmaps-07337cb5-c131-4ced-91a7-ec97c84feb2e" satisfied condition "Succeeded or Failed"
Aug 30 15:49:15.671: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-configmaps-07337cb5-c131-4ced-91a7-ec97c84feb2e container agnhost-container: <nil>
STEP: delete the pod
Aug 30 15:49:15.737: INFO: Waiting for pod pod-projected-configmaps-07337cb5-c131-4ced-91a7-ec97c84feb2e to disappear
Aug 30 15:49:15.747: INFO: Pod pod-projected-configmaps-07337cb5-c131-4ced-91a7-ec97c84feb2e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 30 15:49:15.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-110" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":253,"skipped":4907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:49:15.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 30 15:49:15.896: INFO: PodSpec: initContainers in spec.initContainers
W0830 15:49:15.949801      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 30 15:49:20.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9234" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":254,"skipped":4939,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:49:20.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-b837c31f-13fc-407f-8d9e-3f88bb08af93
STEP: Creating a pod to test consume secrets
W0830 15:49:20.954491      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:49:20.955: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ccd217d9-985a-4fd1-bc4b-07fe63495dfa" in namespace "projected-2072" to be "Succeeded or Failed"
Aug 30 15:49:20.985: INFO: Pod "pod-projected-secrets-ccd217d9-985a-4fd1-bc4b-07fe63495dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 29.687705ms
Aug 30 15:49:23.003: INFO: Pod "pod-projected-secrets-ccd217d9-985a-4fd1-bc4b-07fe63495dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048660701s
Aug 30 15:49:25.017: INFO: Pod "pod-projected-secrets-ccd217d9-985a-4fd1-bc4b-07fe63495dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062114167s
Aug 30 15:49:27.037: INFO: Pod "pod-projected-secrets-ccd217d9-985a-4fd1-bc4b-07fe63495dfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081976214s
STEP: Saw pod success
Aug 30 15:49:27.037: INFO: Pod "pod-projected-secrets-ccd217d9-985a-4fd1-bc4b-07fe63495dfa" satisfied condition "Succeeded or Failed"
Aug 30 15:49:27.045: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-secrets-ccd217d9-985a-4fd1-bc4b-07fe63495dfa container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 30 15:49:27.185: INFO: Waiting for pod pod-projected-secrets-ccd217d9-985a-4fd1-bc4b-07fe63495dfa to disappear
Aug 30 15:49:27.193: INFO: Pod pod-projected-secrets-ccd217d9-985a-4fd1-bc4b-07fe63495dfa no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 30 15:49:27.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2072" for this suite.

• [SLOW TEST:6.526 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":255,"skipped":4940,"failed":0}
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:49:27.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
W0830 15:49:27.414300      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 15:49:27.441357      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Aug 30 15:49:27.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4764" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":256,"skipped":4940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:49:27.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 30 15:49:27.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-6745 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 30 15:49:27.925: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 15:49:27.925: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 30 15:49:32.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-6745 get pod e2e-test-httpd-pod -o json'
Aug 30 15:49:33.139: INFO: stderr: ""
Aug 30 15:49:33.139: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"48357a1837af97978c7411052090ff2d138902e715c34ad800902c446d505b8e\",\n            \"cni.projectcalico.org/podIP\": \"172.30.233.225/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.233.225/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.233.225\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.233.225\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2022-08-30T15:49:27Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6745\",\n        \"resourceVersion\": \"122786\",\n        \"uid\": \"6e425bd5-9658-4445-88a7-26692c97d5c1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-lm296\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.63.224.189\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c60,c0\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-lm296\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-30T15:49:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-30T15:49:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-30T15:49:29Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-30T15:49:27Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://0f4e8850941a7709648e4d41d30864312f34dc92cc6ae8542589c50eb5d378f7\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-08-30T15:49:29Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.63.224.189\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.233.225\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.233.225\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-08-30T15:49:27Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 30 15:49:33.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-6745 replace -f -'
Aug 30 15:49:34.631: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 15:49:34.631: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Aug 30 15:49:34.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-6745 delete pods e2e-test-httpd-pod'
Aug 30 15:49:36.922: INFO: stderr: ""
Aug 30 15:49:36.922: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 15:49:36.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6745" for this suite.

• [SLOW TEST:9.444 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":257,"skipped":4967,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:49:36.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename conformance-tests
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Aug 30 15:49:37.067: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Aug 30 15:49:37.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-1341" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":258,"skipped":5005,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:49:37.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:49:37.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:49:38.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9725" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":259,"skipped":5016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:49:38.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
W0830 15:49:45.629464      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
W0830 15:49:47.683116      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a pod cannot update its resource requirements
W0830 15:49:47.714191      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 30 15:49:51.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4430" for this suite.

• [SLOW TEST:13.431 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":260,"skipped":5046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:49:51.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
W0830 15:49:52.083702      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Aug 30 15:50:12.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5244" for this suite.
STEP: Destroying namespace "nsdeletetest-224" for this suite.
Aug 30 15:50:12.329: INFO: Namespace nsdeletetest-224 was already deleted
STEP: Destroying namespace "nsdeletetest-1296" for this suite.

• [SLOW TEST:20.542 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":261,"skipped":5121,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:50:12.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 30 15:50:12.550: INFO: Waiting up to 5m0s for pod "pod-5820c6da-305b-4d93-8ae7-cb33b56b6fcb" in namespace "emptydir-8034" to be "Succeeded or Failed"
W0830 15:50:12.549710      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:50:12.614: INFO: Pod "pod-5820c6da-305b-4d93-8ae7-cb33b56b6fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 64.419024ms
Aug 30 15:50:14.647: INFO: Pod "pod-5820c6da-305b-4d93-8ae7-cb33b56b6fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097469926s
Aug 30 15:50:16.658: INFO: Pod "pod-5820c6da-305b-4d93-8ae7-cb33b56b6fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108675197s
Aug 30 15:50:18.673: INFO: Pod "pod-5820c6da-305b-4d93-8ae7-cb33b56b6fcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.122706339s
STEP: Saw pod success
Aug 30 15:50:18.673: INFO: Pod "pod-5820c6da-305b-4d93-8ae7-cb33b56b6fcb" satisfied condition "Succeeded or Failed"
Aug 30 15:50:18.683: INFO: Trying to get logs from node 10.63.224.189 pod pod-5820c6da-305b-4d93-8ae7-cb33b56b6fcb container test-container: <nil>
STEP: delete the pod
Aug 30 15:50:18.740: INFO: Waiting for pod pod-5820c6da-305b-4d93-8ae7-cb33b56b6fcb to disappear
Aug 30 15:50:18.750: INFO: Pod pod-5820c6da-305b-4d93-8ae7-cb33b56b6fcb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 15:50:18.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8034" for this suite.

• [SLOW TEST:6.385 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":262,"skipped":5123,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:50:18.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
W0830 15:50:18.945696      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:50:18.945: INFO: Waiting up to 5m0s for pod "pod-02ae00e3-6130-419e-b385-1140013ba35b" in namespace "emptydir-1122" to be "Succeeded or Failed"
Aug 30 15:50:18.961: INFO: Pod "pod-02ae00e3-6130-419e-b385-1140013ba35b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.231892ms
Aug 30 15:50:20.975: INFO: Pod "pod-02ae00e3-6130-419e-b385-1140013ba35b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029067948s
Aug 30 15:50:22.992: INFO: Pod "pod-02ae00e3-6130-419e-b385-1140013ba35b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046103147s
Aug 30 15:50:25.008: INFO: Pod "pod-02ae00e3-6130-419e-b385-1140013ba35b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06236486s
STEP: Saw pod success
Aug 30 15:50:25.008: INFO: Pod "pod-02ae00e3-6130-419e-b385-1140013ba35b" satisfied condition "Succeeded or Failed"
Aug 30 15:50:25.019: INFO: Trying to get logs from node 10.63.224.189 pod pod-02ae00e3-6130-419e-b385-1140013ba35b container test-container: <nil>
STEP: delete the pod
Aug 30 15:50:25.077: INFO: Waiting for pod pod-02ae00e3-6130-419e-b385-1140013ba35b to disappear
Aug 30 15:50:25.087: INFO: Pod pod-02ae00e3-6130-419e-b385-1140013ba35b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 15:50:25.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1122" for this suite.

• [SLOW TEST:6.337 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":263,"skipped":5127,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:50:25.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
W0830 15:50:25.274262      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:50:25.274: INFO: Waiting up to 5m0s for pod "downward-api-44fb5edb-52aa-42ec-b6d0-2c9c44edffbc" in namespace "downward-api-3513" to be "Succeeded or Failed"
Aug 30 15:50:25.286: INFO: Pod "downward-api-44fb5edb-52aa-42ec-b6d0-2c9c44edffbc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.080976ms
Aug 30 15:50:27.299: INFO: Pod "downward-api-44fb5edb-52aa-42ec-b6d0-2c9c44edffbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024786548s
Aug 30 15:50:29.313: INFO: Pod "downward-api-44fb5edb-52aa-42ec-b6d0-2c9c44edffbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038845354s
STEP: Saw pod success
Aug 30 15:50:29.313: INFO: Pod "downward-api-44fb5edb-52aa-42ec-b6d0-2c9c44edffbc" satisfied condition "Succeeded or Failed"
Aug 30 15:50:29.321: INFO: Trying to get logs from node 10.63.224.189 pod downward-api-44fb5edb-52aa-42ec-b6d0-2c9c44edffbc container dapi-container: <nil>
STEP: delete the pod
Aug 30 15:50:29.453: INFO: Waiting for pod downward-api-44fb5edb-52aa-42ec-b6d0-2c9c44edffbc to disappear
Aug 30 15:50:29.466: INFO: Pod downward-api-44fb5edb-52aa-42ec-b6d0-2c9c44edffbc no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 30 15:50:29.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3513" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":264,"skipped":5129,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:50:29.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:50:29.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Aug 30 15:50:40.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-9485 --namespace=crd-publish-openapi-9485 create -f -'
Aug 30 15:50:42.513: INFO: stderr: ""
Aug 30 15:50:42.513: INFO: stdout: "e2e-test-crd-publish-openapi-1048-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 30 15:50:42.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-9485 --namespace=crd-publish-openapi-9485 delete e2e-test-crd-publish-openapi-1048-crds test-cr'
Aug 30 15:50:42.661: INFO: stderr: ""
Aug 30 15:50:42.661: INFO: stdout: "e2e-test-crd-publish-openapi-1048-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 30 15:50:42.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-9485 --namespace=crd-publish-openapi-9485 apply -f -'
Aug 30 15:50:44.458: INFO: stderr: ""
Aug 30 15:50:44.458: INFO: stdout: "e2e-test-crd-publish-openapi-1048-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 30 15:50:44.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-9485 --namespace=crd-publish-openapi-9485 delete e2e-test-crd-publish-openapi-1048-crds test-cr'
Aug 30 15:50:44.635: INFO: stderr: ""
Aug 30 15:50:44.635: INFO: stdout: "e2e-test-crd-publish-openapi-1048-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 30 15:50:44.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=crd-publish-openapi-9485 explain e2e-test-crd-publish-openapi-1048-crds'
Aug 30 15:50:46.069: INFO: stderr: ""
Aug 30 15:50:46.070: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1048-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 15:50:55.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9485" for this suite.

• [SLOW TEST:26.497 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":265,"skipped":5137,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:50:56.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:50:56.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-367 version'
Aug 30 15:50:56.237: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 30 15:50:56.238: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0\", GitCommit:\"4ce5a8954017644c5420bae81d72b09b735c21f0\", GitTreeState:\"clean\", BuildDate:\"2022-05-03T13:46:05Z\", GoVersion:\"go1.18.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0+9546431\", GitCommit:\"0a57f1f59bda75ea2cf13d9f3b4ac5d202134f2d\", GitTreeState:\"clean\", BuildDate:\"2022-07-08T19:55:26Z\", GoVersion:\"go1.18.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 15:50:56.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-367" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":266,"skipped":5150,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:50:56.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
W0830 15:51:00.741288      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
W0830 15:51:06.925393      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 30 15:51:13.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2804" for this suite.

• [SLOW TEST:16.733 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":267,"skipped":5164,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:51:13.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 15:51:13.233: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e0ab44b7-9375-45b6-b77f-1b15c92fbebe
STEP: Creating the pod
W0830 15:51:13.401440      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:51:13.433: INFO: The status of Pod pod-projected-configmaps-53490a2c-3e64-40ba-bc5b-7872aadf5a9a is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:51:15.452: INFO: The status of Pod pod-projected-configmaps-53490a2c-3e64-40ba-bc5b-7872aadf5a9a is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:51:17.457: INFO: The status of Pod pod-projected-configmaps-53490a2c-3e64-40ba-bc5b-7872aadf5a9a is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-e0ab44b7-9375-45b6-b77f-1b15c92fbebe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 30 15:52:43.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9223" for this suite.

• [SLOW TEST:90.461 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":268,"skipped":5166,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:52:43.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Aug 30 15:52:43.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 create -f -'
Aug 30 15:52:44.503: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"update-demo\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"update-demo\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"update-demo\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"update-demo\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 15:52:44.503: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 30 15:52:44.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 15:52:44.654: INFO: stderr: ""
Aug 30 15:52:44.654: INFO: stdout: "update-demo-nautilus-8pk8j "
STEP: Replicas for name=update-demo: expected=2 actual=1
Aug 30 15:52:49.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 15:52:49.793: INFO: stderr: ""
Aug 30 15:52:49.793: INFO: stdout: "update-demo-nautilus-8pk8j update-demo-nautilus-s6fs4 "
Aug 30 15:52:49.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-8pk8j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 15:52:49.944: INFO: stderr: ""
Aug 30 15:52:49.944: INFO: stdout: ""
Aug 30 15:52:49.944: INFO: update-demo-nautilus-8pk8j is created but not running
Aug 30 15:52:54.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 15:52:55.095: INFO: stderr: ""
Aug 30 15:52:55.095: INFO: stdout: "update-demo-nautilus-8pk8j update-demo-nautilus-s6fs4 "
Aug 30 15:52:55.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-8pk8j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 15:52:55.226: INFO: stderr: ""
Aug 30 15:52:55.226: INFO: stdout: ""
Aug 30 15:52:55.226: INFO: update-demo-nautilus-8pk8j is created but not running
Aug 30 15:53:00.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 15:53:00.351: INFO: stderr: ""
Aug 30 15:53:00.351: INFO: stdout: "update-demo-nautilus-8pk8j update-demo-nautilus-s6fs4 "
Aug 30 15:53:00.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-8pk8j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 15:53:00.470: INFO: stderr: ""
Aug 30 15:53:00.470: INFO: stdout: "true"
Aug 30 15:53:00.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-8pk8j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 15:53:00.590: INFO: stderr: ""
Aug 30 15:53:00.591: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 30 15:53:00.591: INFO: validating pod update-demo-nautilus-8pk8j
Aug 30 15:53:00.617: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 15:53:00.617: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 15:53:00.617: INFO: update-demo-nautilus-8pk8j is verified up and running
Aug 30 15:53:00.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-s6fs4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 15:53:00.753: INFO: stderr: ""
Aug 30 15:53:00.753: INFO: stdout: "true"
Aug 30 15:53:00.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-s6fs4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 15:53:00.909: INFO: stderr: ""
Aug 30 15:53:00.909: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 30 15:53:00.909: INFO: validating pod update-demo-nautilus-s6fs4
Aug 30 15:53:00.938: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 15:53:00.938: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 15:53:00.938: INFO: update-demo-nautilus-s6fs4 is verified up and running
STEP: scaling down the replication controller
Aug 30 15:53:00.947: INFO: scanned /root for discovery docs: <nil>
Aug 30 15:53:00.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 30 15:53:02.188: INFO: stderr: ""
Aug 30 15:53:02.188: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 30 15:53:02.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 15:53:02.345: INFO: stderr: ""
Aug 30 15:53:02.345: INFO: stdout: "update-demo-nautilus-8pk8j update-demo-nautilus-s6fs4 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 30 15:53:07.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 15:53:07.509: INFO: stderr: ""
Aug 30 15:53:07.509: INFO: stdout: "update-demo-nautilus-s6fs4 "
Aug 30 15:53:07.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-s6fs4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 15:53:07.650: INFO: stderr: ""
Aug 30 15:53:07.650: INFO: stdout: "true"
Aug 30 15:53:07.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-s6fs4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 15:53:07.748: INFO: stderr: ""
Aug 30 15:53:07.748: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 30 15:53:07.748: INFO: validating pod update-demo-nautilus-s6fs4
Aug 30 15:53:07.772: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 15:53:07.772: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 15:53:07.772: INFO: update-demo-nautilus-s6fs4 is verified up and running
STEP: scaling up the replication controller
Aug 30 15:53:07.782: INFO: scanned /root for discovery docs: <nil>
Aug 30 15:53:07.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 30 15:53:08.997: INFO: stderr: ""
Aug 30 15:53:08.998: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 30 15:53:08.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 15:53:09.152: INFO: stderr: ""
Aug 30 15:53:09.152: INFO: stdout: "update-demo-nautilus-6mrpj update-demo-nautilus-s6fs4 "
Aug 30 15:53:09.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-6mrpj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 15:53:09.295: INFO: stderr: ""
Aug 30 15:53:09.295: INFO: stdout: ""
Aug 30 15:53:09.295: INFO: update-demo-nautilus-6mrpj is created but not running
Aug 30 15:53:14.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 15:53:14.468: INFO: stderr: ""
Aug 30 15:53:14.468: INFO: stdout: "update-demo-nautilus-6mrpj update-demo-nautilus-s6fs4 "
Aug 30 15:53:14.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-6mrpj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 15:53:14.594: INFO: stderr: ""
Aug 30 15:53:14.595: INFO: stdout: "true"
Aug 30 15:53:14.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-6mrpj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 15:53:14.699: INFO: stderr: ""
Aug 30 15:53:14.699: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 30 15:53:14.699: INFO: validating pod update-demo-nautilus-6mrpj
Aug 30 15:53:14.732: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 15:53:14.732: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 15:53:14.732: INFO: update-demo-nautilus-6mrpj is verified up and running
Aug 30 15:53:14.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-s6fs4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 15:53:14.877: INFO: stderr: ""
Aug 30 15:53:14.877: INFO: stdout: "true"
Aug 30 15:53:14.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods update-demo-nautilus-s6fs4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 15:53:15.077: INFO: stderr: ""
Aug 30 15:53:15.077: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 30 15:53:15.088: INFO: validating pod update-demo-nautilus-s6fs4
Aug 30 15:53:15.108: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 15:53:15.108: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 15:53:15.108: INFO: update-demo-nautilus-s6fs4 is verified up and running
STEP: using delete to clean up resources
Aug 30 15:53:15.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 delete --grace-period=0 --force -f -'
Aug 30 15:53:15.337: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 15:53:15.337: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 30 15:53:15.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get rc,svc -l name=update-demo --no-headers'
Aug 30 15:53:15.548: INFO: stderr: "No resources found in kubectl-7662 namespace.\n"
Aug 30 15:53:15.548: INFO: stdout: ""
Aug 30 15:53:15.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-7662 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 15:53:15.660: INFO: stderr: ""
Aug 30 15:53:15.660: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 15:53:15.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7662" for this suite.

• [SLOW TEST:32.142 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":269,"skipped":5181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:53:15.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
W0830 15:53:15.894638      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 30 15:53:21.004: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 30 15:53:21.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2269" for this suite.

• [SLOW TEST:5.395 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":270,"skipped":5203,"failed":0}
S
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:53:21.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0830 15:53:21.268224      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:53:21.282: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:53:23.307: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Pending, waiting for it to be Running (with Ready = true)
Aug 30 15:53:25.309: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Running (Ready = false)
Aug 30 15:53:27.298: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Running (Ready = false)
Aug 30 15:53:29.294: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Running (Ready = false)
Aug 30 15:53:31.297: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Running (Ready = false)
Aug 30 15:53:33.301: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Running (Ready = false)
Aug 30 15:53:35.301: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Running (Ready = false)
Aug 30 15:53:37.306: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Running (Ready = false)
Aug 30 15:53:39.300: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Running (Ready = false)
Aug 30 15:53:41.298: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Running (Ready = false)
Aug 30 15:53:43.299: INFO: The status of Pod test-webserver-24c6699e-0a08-479c-9b14-ec9faa581e1b is Running (Ready = true)
Aug 30 15:53:43.315: INFO: Container started at 2022-08-30 15:53:23 +0000 UTC, pod became ready at 2022-08-30 15:53:41 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 30 15:53:43.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6187" for this suite.

• [SLOW TEST:22.251 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":271,"skipped":5204,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:53:43.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-27b1da37-8c15-41dd-8458-308ea6508665 in namespace container-probe-5689
W0830 15:53:43.543616      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 15:53:47.573: INFO: Started pod liveness-27b1da37-8c15-41dd-8458-308ea6508665 in namespace container-probe-5689
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 15:53:47.584: INFO: Initial restart count of pod liveness-27b1da37-8c15-41dd-8458-308ea6508665 is 0
Aug 30 15:54:05.744: INFO: Restart count of pod container-probe-5689/liveness-27b1da37-8c15-41dd-8458-308ea6508665 is now 1 (18.159345198s elapsed)
Aug 30 15:54:25.927: INFO: Restart count of pod container-probe-5689/liveness-27b1da37-8c15-41dd-8458-308ea6508665 is now 2 (38.342670805s elapsed)
Aug 30 15:54:46.105: INFO: Restart count of pod container-probe-5689/liveness-27b1da37-8c15-41dd-8458-308ea6508665 is now 3 (58.520274763s elapsed)
Aug 30 15:55:06.277: INFO: Restart count of pod container-probe-5689/liveness-27b1da37-8c15-41dd-8458-308ea6508665 is now 4 (1m18.692324104s elapsed)
Aug 30 15:56:08.913: INFO: Restart count of pod container-probe-5689/liveness-27b1da37-8c15-41dd-8458-308ea6508665 is now 5 (2m21.328287837s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 30 15:56:08.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5689" for this suite.

• [SLOW TEST:145.664 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":272,"skipped":5212,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 15:56:09.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 30 15:56:09.144: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 15:56:09.181: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 15:56:09.222: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.158 before test
Aug 30 15:56:09.333: INFO: calico-kube-controllers-867bb5b44d-wqzpr from calico-system started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 30 15:56:09.333: INFO: calico-node-rfpmd from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 15:56:09.333: INFO: ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-q4rqr from ibm-system started at 2022-08-30 12:17:08 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container ibm-cloud-provider-ip-130-198-93-237 ready: true, restart count 0
Aug 30 15:56:09.333: INFO: ibm-file-plugin-ccc878f48-frwb7 from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 30 15:56:09.333: INFO: ibm-keepalived-watcher-c2b7b from kube-system started at 2022-08-30 12:07:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 15:56:09.333: INFO: ibm-master-proxy-static-10.63.224.158 from kube-system started at 2022-08-30 12:07:30 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 15:56:09.333: INFO: 	Container pause ready: true, restart count 0
Aug 30 15:56:09.333: INFO: ibm-storage-watcher-6fcfdb7ccc-5s2xq from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 30 15:56:09.333: INFO: ibmcloud-block-storage-driver-kx4tx from kube-system started at 2022-08-30 12:07:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 15:56:09.333: INFO: ibmcloud-block-storage-plugin-7495f48b76-lht72 from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 30 15:56:09.333: INFO: vpn-59795d4f7c-rf6fw from kube-system started at 2022-08-30 12:18:35 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container vpn ready: true, restart count 0
Aug 30 15:56:09.333: INFO: cluster-node-tuning-operator-6c4d4b46dd-dsdts from openshift-cluster-node-tuning-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 30 15:56:09.333: INFO: tuned-mpvrs from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container tuned ready: true, restart count 0
Aug 30 15:56:09.333: INFO: cluster-samples-operator-79cb65b9b5-jbclm from openshift-cluster-samples-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 30 15:56:09.333: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 30 15:56:09.333: INFO: cluster-storage-operator-97dcf6b44-krf6f from openshift-cluster-storage-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 30 15:56:09.333: INFO: csi-snapshot-controller-7d8bf4bb58-6lkk4 from openshift-cluster-storage-operator started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 15:56:09.333: INFO: csi-snapshot-controller-operator-7b97dc6dfd-xjp62 from openshift-cluster-storage-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 30 15:56:09.333: INFO: csi-snapshot-webhook-8f4fd6cc6-48czg from openshift-cluster-storage-operator started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container webhook ready: true, restart count 0
Aug 30 15:56:09.333: INFO: console-operator-55bcdb7b48-tgxd4 from openshift-console-operator started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container console-operator ready: true, restart count 1
Aug 30 15:56:09.333: INFO: console-5dd7474d84-js5g9 from openshift-console started at 2022-08-30 12:14:31 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container console ready: true, restart count 0
Aug 30 15:56:09.333: INFO: dns-operator-5549dbd7c9-c4h6t from openshift-dns-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.333: INFO: 	Container dns-operator ready: true, restart count 0
Aug 30 15:56:09.333: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: dns-default-v6jl9 from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container dns ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: node-resolver-ppzz9 from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 15:56:09.334: INFO: cluster-image-registry-operator-cb6448756-qh2wd from openshift-image-registry started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 30 15:56:09.334: INFO: image-registry-7fb4f45578-scb5q from openshift-image-registry started at 2022-08-30 12:18:40 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container registry ready: true, restart count 0
Aug 30 15:56:09.334: INFO: node-ca-4pbx7 from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 15:56:09.334: INFO: ingress-canary-vwqf6 from openshift-ingress-canary started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 15:56:09.334: INFO: ingress-operator-57457886bd-v8vkg from openshift-ingress-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: router-default-bdd65b74f-jwc99 from openshift-ingress started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container router ready: true, restart count 0
Aug 30 15:56:09.334: INFO: insights-operator-6d9b46b7c5-6q6pl from openshift-insights started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container insights-operator ready: true, restart count 1
Aug 30 15:56:09.334: INFO: openshift-kube-proxy-ms7hb from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: kube-storage-version-migrator-operator-fb46b8c8d-n5q2j from openshift-kube-storage-version-migrator-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Aug 30 15:56:09.334: INFO: marketplace-operator-84ff65f9c9-mcwn4 from openshift-marketplace started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 30 15:56:09.334: INFO: cluster-monitoring-operator-677fb4cf4b-wfrk9 from openshift-monitoring started at 2022-08-30 12:09:18 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: kube-state-metrics-b6455c4dc-pknn6 from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (3 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 30 15:56:09.334: INFO: node-exporter-ct2nk from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 15:56:09.334: INFO: prometheus-adapter-67874b74f7-2kkcq from openshift-monitoring started at 2022-08-30 12:14:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 15:56:09.334: INFO: prometheus-operator-admission-webhook-f5f88b968-wkmnq from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 15:56:09.334: INFO: thanos-querier-75bdcf8599-tdj9h from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (6 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 15:56:09.334: INFO: multus-additional-cni-plugins-fwxxj from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 15:56:09.334: INFO: multus-admission-controller-gvnw7 from openshift-multus started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 15:56:09.334: INFO: multus-n2s54 from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 15:56:09.334: INFO: network-metrics-daemon-ncgdn from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 15:56:09.334: INFO: network-check-source-5cb989cf6f-8pbqc from openshift-network-diagnostics started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 30 15:56:09.334: INFO: network-check-target-tqzgf from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 15:56:09.334: INFO: catalog-operator-5d9dd4bb98-hwstf from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 30 15:56:09.334: INFO: olm-operator-757497677b-z5lbc from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container olm-operator ready: true, restart count 0
Aug 30 15:56:09.334: INFO: package-server-manager-784548687f-9vg2b from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container package-server-manager ready: true, restart count 0
Aug 30 15:56:09.334: INFO: packageserver-b5cbb5ff5-2vxb9 from openshift-operator-lifecycle-manager started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 15:56:09.334: INFO: metrics-67dbdb4ffd-gj9lp from openshift-roks-metrics started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container metrics ready: true, restart count 3
Aug 30 15:56:09.334: INFO: push-gateway-58dc4cbdf8-kl5x7 from openshift-roks-metrics started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container push-gateway ready: true, restart count 0
Aug 30 15:56:09.334: INFO: service-ca-operator-5df8fbb45b-fz8bw from openshift-service-ca-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container service-ca-operator ready: true, restart count 1
Aug 30 15:56:09.334: INFO: sonobuoy-e2e-job-a8c1c5c47c2346af from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container e2e ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:56:09.334: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-fngwd from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:56:09.334: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 15:56:09.334: INFO: tigera-operator-7b76886f74-5rs97 from tigera-operator started at 2022-08-30 12:07:46 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.334: INFO: 	Container tigera-operator ready: true, restart count 2
Aug 30 15:56:09.334: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.187 before test
Aug 30 15:56:09.449: INFO: calico-node-bv858 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.449: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 15:56:09.449: INFO: calico-typha-66986f6f4d-mg7gw from calico-system started at 2022-08-30 12:08:59 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.449: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 15:56:09.449: INFO: ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-gq5c7 from ibm-system started at 2022-08-30 12:17:08 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.449: INFO: 	Container ibm-cloud-provider-ip-130-198-93-237 ready: true, restart count 0
Aug 30 15:56:09.449: INFO: ibm-keepalived-watcher-7czrz from kube-system started at 2022-08-30 12:07:36 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.449: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 15:56:09.449: INFO: ibm-master-proxy-static-10.63.224.187 from kube-system started at 2022-08-30 12:07:34 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container pause ready: true, restart count 0
Aug 30 15:56:09.450: INFO: ibmcloud-block-storage-driver-m5jbh from kube-system started at 2022-08-30 12:07:42 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 15:56:09.450: INFO: tuned-c2ph5 from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container tuned ready: true, restart count 0
Aug 30 15:56:09.450: INFO: csi-snapshot-controller-7d8bf4bb58-gv7xl from openshift-cluster-storage-operator started at 2022-08-30 12:10:04 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 15:56:09.450: INFO: csi-snapshot-webhook-8f4fd6cc6-pnk9x from openshift-cluster-storage-operator started at 2022-08-30 12:10:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container webhook ready: true, restart count 0
Aug 30 15:56:09.450: INFO: console-5dd7474d84-t6lg4 from openshift-console started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container console ready: true, restart count 0
Aug 30 15:56:09.450: INFO: downloads-6f74f6fcbf-sbj8k from openshift-console started at 2022-08-30 12:10:07 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container download-server ready: true, restart count 0
Aug 30 15:56:09.450: INFO: dns-default-dltcr from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container dns ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: node-resolver-n66mp from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 15:56:09.450: INFO: node-ca-9bj8c from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 15:56:09.450: INFO: registry-pvc-permissions-2pnhd from openshift-image-registry started at 2022-08-30 12:18:48 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 30 15:56:09.450: INFO: ingress-canary-zjwth from openshift-ingress-canary started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 15:56:09.450: INFO: router-default-bdd65b74f-9xp2d from openshift-ingress started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container router ready: true, restart count 0
Aug 30 15:56:09.450: INFO: openshift-kube-proxy-whcnq from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: migrator-59f7fcfc8f-l77kl from openshift-kube-storage-version-migrator started at 2022-08-30 12:10:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container migrator ready: true, restart count 0
Aug 30 15:56:09.450: INFO: certified-operators-t6598 from openshift-marketplace started at 2022-08-30 13:47:48 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:56:09.450: INFO: community-operators-btnw8 from openshift-marketplace started at 2022-08-30 12:11:53 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:56:09.450: INFO: redhat-marketplace-9rltc from openshift-marketplace started at 2022-08-30 12:11:54 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:56:09.450: INFO: redhat-operators-cnsks from openshift-marketplace started at 2022-08-30 12:11:53 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 15:56:09.450: INFO: redhat-operators-szsxg from openshift-marketplace started at 2022-08-30 15:55:42 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container registry-server ready: false, restart count 0
Aug 30 15:56:09.450: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-08-30 12:14:46 +0000 UTC (6 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container alertmanager ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: node-exporter-gls7c from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 15:56:09.450: INFO: openshift-state-metrics-7984888fbd-dgxfd from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (3 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 30 15:56:09.450: INFO: prometheus-adapter-67874b74f7-xwhkv from openshift-monitoring started at 2022-08-30 12:14:39 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 15:56:09.450: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-08-30 12:15:07 +0000 UTC (6 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 15:56:09.450: INFO: prometheus-operator-admission-webhook-f5f88b968-wjzg5 from openshift-monitoring started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 15:56:09.450: INFO: thanos-querier-75bdcf8599-tb7gh from openshift-monitoring started at 2022-08-30 12:14:46 +0000 UTC (6 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 15:56:09.450: INFO: multus-additional-cni-plugins-8j4kx from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 15:56:09.450: INFO: multus-admission-controller-xzspk from openshift-multus started at 2022-08-30 12:09:28 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 15:56:09.450: INFO: multus-qpmzg from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 15:56:09.450: INFO: network-metrics-daemon-nlcxx from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 15:56:09.450: INFO: network-check-target-wrm4t from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 15:56:09.450: INFO: network-operator-55b69485bb-qth5v from openshift-network-operator started at 2022-08-30 12:07:47 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container network-operator ready: true, restart count 1
Aug 30 15:56:09.450: INFO: collect-profiles-27697875-986x4 from openshift-operator-lifecycle-manager started at 2022-08-30 15:15:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 15:56:09.450: INFO: collect-profiles-27697890-ppzrv from openshift-operator-lifecycle-manager started at 2022-08-30 15:30:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 15:56:09.450: INFO: collect-profiles-27697905-dflw2 from openshift-operator-lifecycle-manager started at 2022-08-30 15:45:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 15:56:09.450: INFO: packageserver-b5cbb5ff5-nc6jk from openshift-operator-lifecycle-manager started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 15:56:09.450: INFO: sonobuoy from sonobuoy started at 2022-08-30 14:26:02 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 15:56:09.450: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-7b4kf from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.450: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 15:56:09.450: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.189 before test
Aug 30 15:56:09.562: INFO: calico-node-g5sh7 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 15:56:09.562: INFO: calico-typha-66986f6f4d-rd6m6 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 15:56:09.562: INFO: ibm-keepalived-watcher-ctfdm from kube-system started at 2022-08-30 12:08:00 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 15:56:09.562: INFO: ibm-master-proxy-static-10.63.224.189 from kube-system started at 2022-08-30 12:07:47 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container pause ready: true, restart count 0
Aug 30 15:56:09.562: INFO: ibmcloud-block-storage-driver-xl7s6 from kube-system started at 2022-08-30 12:08:07 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 15:56:09.562: INFO: tuned-p2mrf from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container tuned ready: true, restart count 0
Aug 30 15:56:09.562: INFO: downloads-6f74f6fcbf-gsbhd from openshift-console started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container download-server ready: true, restart count 0
Aug 30 15:56:09.562: INFO: dns-default-zvh4h from openshift-dns started at 2022-08-30 14:53:11 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container dns ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: node-resolver-9tslx from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 15:56:09.562: INFO: node-ca-mq7hh from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 15:56:09.562: INFO: ingress-canary-qxqwq from openshift-ingress-canary started at 2022-08-30 14:52:56 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 15:56:09.562: INFO: openshift-kube-proxy-5zfxw from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-08-30 14:52:56 +0000 UTC (6 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container alertmanager ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: node-exporter-9w9vp from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 15:56:09.562: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-08-30 14:52:58 +0000 UTC (6 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 15:56:09.562: INFO: prometheus-operator-dd6c54897-5s5xx from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 30 15:56:09.562: INFO: telemeter-client-dd46ccd5b-ttrgn from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (3 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container reload ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 30 15:56:09.562: INFO: multus-additional-cni-plugins-gnz55 from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 15:56:09.562: INFO: multus-admission-controller-94qqx from openshift-multus started at 2022-08-30 14:53:21 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 15:56:09.562: INFO: multus-nzfsd from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 15:56:09.562: INFO: network-metrics-daemon-hzwd2 from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 15:56:09.562: INFO: network-check-target-whfqq from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 15:56:09.562: INFO: service-ca-5f4d84b84b-wnqw2 from openshift-service-ca started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container service-ca-controller ready: false, restart count 0
Aug 30 15:56:09.562: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-x4jz5 from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 15:56:09.562: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 15:56:09.562: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
W0830 15:56:09.636636      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "without-label" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "without-label" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "without-label" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "without-label" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-93f4a7e1-b59c-434d-8790-b923db8105ca 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
W0830 15:56:13.858523      21 warnings.go:70] would violate PodSecurity "restricted:latest": hostPort (container "agnhost" uses hostPort 54322)
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.63.224.189 on the node which pod4 resides and expect not scheduled
W0830 15:56:17.954285      21 warnings.go:70] would violate PodSecurity "restricted:latest": hostPort (container "agnhost" uses hostPort 54322)
STEP: removing the label kubernetes.io/e2e-93f4a7e1-b59c-434d-8790-b923db8105ca off the node 10.63.224.189
STEP: verifying the node doesn't have the label kubernetes.io/e2e-93f4a7e1-b59c-434d-8790-b923db8105ca
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 30 16:01:18.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7623" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:309.100 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":273,"skipped":5215,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:01:18.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-e7ed8597-6f95-4c91-af45-cabfbc18bbdd
STEP: Creating a pod to test consume secrets
Aug 30 16:01:18.286: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8ebbe164-9b7c-4541-af2d-01654cd69c21" in namespace "projected-8628" to be "Succeeded or Failed"
W0830 16:01:18.285784      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:01:18.302: INFO: Pod "pod-projected-secrets-8ebbe164-9b7c-4541-af2d-01654cd69c21": Phase="Pending", Reason="", readiness=false. Elapsed: 15.819494ms
Aug 30 16:01:20.322: INFO: Pod "pod-projected-secrets-8ebbe164-9b7c-4541-af2d-01654cd69c21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03643492s
Aug 30 16:01:22.343: INFO: Pod "pod-projected-secrets-8ebbe164-9b7c-4541-af2d-01654cd69c21": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056980505s
Aug 30 16:01:24.362: INFO: Pod "pod-projected-secrets-8ebbe164-9b7c-4541-af2d-01654cd69c21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075907249s
STEP: Saw pod success
Aug 30 16:01:24.362: INFO: Pod "pod-projected-secrets-8ebbe164-9b7c-4541-af2d-01654cd69c21" satisfied condition "Succeeded or Failed"
Aug 30 16:01:24.374: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-secrets-8ebbe164-9b7c-4541-af2d-01654cd69c21 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 30 16:01:24.531: INFO: Waiting for pod pod-projected-secrets-8ebbe164-9b7c-4541-af2d-01654cd69c21 to disappear
Aug 30 16:01:24.544: INFO: Pod pod-projected-secrets-8ebbe164-9b7c-4541-af2d-01654cd69c21 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 30 16:01:24.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8628" for this suite.

• [SLOW TEST:6.463 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":274,"skipped":5218,"failed":0}
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:01:24.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
W0830 16:01:24.827577      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:01:24.828: INFO: Waiting up to 5m0s for pod "var-expansion-ea523db3-e8e8-4b20-81ac-31e20c3d118c" in namespace "var-expansion-6091" to be "Succeeded or Failed"
Aug 30 16:01:24.843: INFO: Pod "var-expansion-ea523db3-e8e8-4b20-81ac-31e20c3d118c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.667462ms
Aug 30 16:01:26.861: INFO: Pod "var-expansion-ea523db3-e8e8-4b20-81ac-31e20c3d118c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032977613s
Aug 30 16:01:28.878: INFO: Pod "var-expansion-ea523db3-e8e8-4b20-81ac-31e20c3d118c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050749666s
Aug 30 16:01:30.894: INFO: Pod "var-expansion-ea523db3-e8e8-4b20-81ac-31e20c3d118c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066753047s
STEP: Saw pod success
Aug 30 16:01:30.894: INFO: Pod "var-expansion-ea523db3-e8e8-4b20-81ac-31e20c3d118c" satisfied condition "Succeeded or Failed"
Aug 30 16:01:30.907: INFO: Trying to get logs from node 10.63.224.189 pod var-expansion-ea523db3-e8e8-4b20-81ac-31e20c3d118c container dapi-container: <nil>
STEP: delete the pod
Aug 30 16:01:30.992: INFO: Waiting for pod var-expansion-ea523db3-e8e8-4b20-81ac-31e20c3d118c to disappear
Aug 30 16:01:31.002: INFO: Pod var-expansion-ea523db3-e8e8-4b20-81ac-31e20c3d118c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 30 16:01:31.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6091" for this suite.

• [SLOW TEST:6.451 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":275,"skipped":5218,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:01:31.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-2965
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 30 16:01:31.127: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
W0830 16:01:31.239340      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 16:01:31.320542      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 16:01:31.370653      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:01:31.388: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:01:33.410: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:01:35.410: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:01:37.402: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:01:39.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:01:41.402: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:01:43.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:01:45.404: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:01:47.405: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:01:49.400: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:01:51.412: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:01:53.403: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 30 16:01:53.432: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 30 16:01:53.478: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
W0830 16:01:53.565551      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:01:57.603: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 30 16:01:57.603: INFO: Breadth first check of 172.30.78.47 on host 10.63.224.158...
Aug 30 16:01:57.616: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.233.198:9080/dial?request=hostname&protocol=udp&host=172.30.78.47&port=8081&tries=1'] Namespace:pod-network-test-2965 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 16:01:57.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 16:01:57.617: INFO: ExecWithOptions: Clientset creation
Aug 30 16:01:57.617: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2965/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.233.198%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.78.47%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 16:01:57.915: INFO: Waiting for responses: map[]
Aug 30 16:01:57.915: INFO: reached 172.30.78.47 after 0/1 tries
Aug 30 16:01:57.915: INFO: Breadth first check of 172.30.38.199 on host 10.63.224.187...
Aug 30 16:01:57.931: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.233.198:9080/dial?request=hostname&protocol=udp&host=172.30.38.199&port=8081&tries=1'] Namespace:pod-network-test-2965 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 16:01:57.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 16:01:57.933: INFO: ExecWithOptions: Clientset creation
Aug 30 16:01:57.933: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2965/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.233.198%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.38.199%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 16:01:58.227: INFO: Waiting for responses: map[]
Aug 30 16:01:58.227: INFO: reached 172.30.38.199 after 0/1 tries
Aug 30 16:01:58.227: INFO: Breadth first check of 172.30.233.237 on host 10.63.224.189...
Aug 30 16:01:58.241: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.233.198:9080/dial?request=hostname&protocol=udp&host=172.30.233.237&port=8081&tries=1'] Namespace:pod-network-test-2965 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 16:01:58.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 16:01:58.243: INFO: ExecWithOptions: Clientset creation
Aug 30 16:01:58.244: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2965/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.233.198%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.233.237%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 16:01:58.553: INFO: Waiting for responses: map[]
Aug 30 16:01:58.553: INFO: reached 172.30.233.237 after 0/1 tries
Aug 30 16:01:58.553: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 30 16:01:58.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2965" for this suite.

• [SLOW TEST:27.552 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":276,"skipped":5220,"failed":0}
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:01:58.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
W0830 16:02:02.894420      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
W0830 16:02:09.090222      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 30 16:02:15.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3119" for this suite.

• [SLOW TEST:16.663 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":277,"skipped":5220,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:02:15.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6556
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6556
W0830 16:02:15.525626      21 warnings.go:70] would violate PodSecurity "restricted:latest": hostPort (container "webserver" uses hostPort 21017)
STEP: Waiting until pod test-pod will start running in namespace statefulset-6556
STEP: Creating statefulset with conflicting port in namespace statefulset-6556
W0830 16:02:19.587516      21 warnings.go:70] would violate PodSecurity "restricted:latest": hostPort (container "webserver" uses hostPort 21017), allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6556
Aug 30 16:02:19.669: INFO: Observed stateful pod in namespace: statefulset-6556, name: ss-0, uid: aed93983-f746-4cc2-a128-77213ca02366, status phase: Pending. Waiting for statefulset controller to delete.
Aug 30 16:02:19.730: INFO: Observed stateful pod in namespace: statefulset-6556, name: ss-0, uid: aed93983-f746-4cc2-a128-77213ca02366, status phase: Failed. Waiting for statefulset controller to delete.
Aug 30 16:02:19.760: INFO: Observed stateful pod in namespace: statefulset-6556, name: ss-0, uid: aed93983-f746-4cc2-a128-77213ca02366, status phase: Failed. Waiting for statefulset controller to delete.
Aug 30 16:02:19.773: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6556
STEP: Removing pod with conflicting port in namespace statefulset-6556
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6556 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 30 16:02:23.926: INFO: Deleting all statefulset in ns statefulset-6556
Aug 30 16:02:23.941: INFO: Scaling statefulset ss to 0
W0830 16:02:23.995207      21 warnings.go:70] would violate PodSecurity "restricted:latest": hostPort (container "webserver" uses hostPort 21017), allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:02:34.035: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 16:02:34.051: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 30 16:02:34.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6556" for this suite.

• [SLOW TEST:18.919 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":278,"skipped":5220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:02:34.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
W0830 16:02:34.284221      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:02:34.293: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 30 16:02:39.309: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 30 16:02:39.309: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 30 16:02:41.329: INFO: Creating deployment "test-rollover-deployment"
W0830 16:02:41.360843      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "redis-slave" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "redis-slave" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "redis-slave" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "redis-slave" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:02:41.375: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 30 16:02:43.413: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 30 16:02:43.439: INFO: Ensure that both replica sets have 1 created replica
Aug 30 16:02:43.465: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 30 16:02:43.526: INFO: Updating deployment test-rollover-deployment
Aug 30 16:02:43.527: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
W0830 16:02:43.526705      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:02:45.555: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 30 16:02:45.584: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 30 16:02:45.613: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:02:45.614: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:02:47.646: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:02:47.646: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:02:49.647: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:02:49.647: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:02:51.673: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:02:51.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:02:53.644: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:02:53.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:02:55.653: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 16:02:55.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 2, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 2, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:02:57.647: INFO: 
Aug 30 16:02:57.647: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 16:02:57.691: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4141  e4d7d54f-89d6-4c99-9f25-e9554f3f72a4 129131 2 2022-08-30 16:02:41 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-30 16:02:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b8e468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-30 16:02:41 +0000 UTC,LastTransitionTime:2022-08-30 16:02:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-77745f886c" has successfully progressed.,LastUpdateTime:2022-08-30 16:02:56 +0000 UTC,LastTransitionTime:2022-08-30 16:02:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 30 16:02:57.705: INFO: New ReplicaSet "test-rollover-deployment-77745f886c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-77745f886c  deployment-4141  ab8a619c-ff08-4f2d-be7a-13af1a00cb84 129121 2 2022-08-30 16:02:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e4d7d54f-89d6-4c99-9f25-e9554f3f72a4 0xc005ccbcc7 0xc005ccbcc8}] []  [{kube-controller-manager Update apps/v1 2022-08-30 16:02:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4d7d54f-89d6-4c99-9f25-e9554f3f72a4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:02:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 77745f886c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ccbd88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:02:57.706: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 30 16:02:57.706: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4141  00afa8e6-7d5b-426e-a96b-7934a22664ba 129130 2 2022-08-30 16:02:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e4d7d54f-89d6-4c99-9f25-e9554f3f72a4 0xc005ccbb97 0xc005ccbb98}] []  [{e2e.test Update apps/v1 2022-08-30 16:02:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4d7d54f-89d6-4c99-9f25-e9554f3f72a4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:02:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005ccbc58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:02:57.707: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-4141  4d9adabb-d9be-472d-9aa3-54fc83d8a88e 129030 2 2022-08-30 16:02:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e4d7d54f-89d6-4c99-9f25-e9554f3f72a4 0xc005ccbdf0 0xc005ccbdf1}] []  [{kube-controller-manager Update apps/v1 2022-08-30 16:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4d7d54f-89d6-4c99-9f25-e9554f3f72a4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:02:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ccbe98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:02:57.722: INFO: Pod "test-rollover-deployment-77745f886c-lrknv" is available:
&Pod{ObjectMeta:{test-rollover-deployment-77745f886c-lrknv test-rollover-deployment-77745f886c- deployment-4141  621f1bc3-e0f7-4e1b-8cbc-21243d4eb34d 129070 0 2022-08-30 16:02:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[cni.projectcalico.org/containerID:3aad5794424fdf8e5b532ba569c1c96b8244cb56ba5f412f3559fae987aab99e cni.projectcalico.org/podIP:172.30.233.211/32 cni.projectcalico.org/podIPs:172.30.233.211/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.211"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.211"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-77745f886c ab8a619c-ff08-4f2d-be7a-13af1a00cb84 0xc0023fe857 0xc0023fe858}] []  [{kube-controller-manager Update v1 2022-08-30 16:02:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab8a619c-ff08-4f2d-be7a-13af1a00cb84\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 16:02:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:02:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 16:02:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.233.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4r626,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4r626,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c61,c60,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wp87w,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:02:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:02:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:02:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:02:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.189,PodIP:172.30.233.211,StartTime:2022-08-30 16:02:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 16:02:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:c8cf9027e6db0e7de9b172400b209da8fe7aac863d19352723d2457847457403,ContainerID:cri-o://2b0cfdbb6753e4eebe35082f0e253055a561f69fa31081537e9d82178026eb3f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.233.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 30 16:02:57.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4141" for this suite.

• [SLOW TEST:23.592 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":279,"skipped":5250,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:02:57.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 30 16:02:58.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1654" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":280,"skipped":5252,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:02:58.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-kd9bp in namespace proxy-5752
W0830 16:02:58.262129      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "proxy-service-kd9bp" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "proxy-service-kd9bp" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "proxy-service-kd9bp" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "proxy-service-kd9bp" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 16:02:58.262484      21 runners.go:193] Created replication controller with name: proxy-service-kd9bp, namespace: proxy-5752, replica count: 1
I0830 16:02:59.314435      21 runners.go:193] proxy-service-kd9bp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 16:03:00.317166      21 runners.go:193] proxy-service-kd9bp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 16:03:01.318296      21 runners.go:193] proxy-service-kd9bp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0830 16:03:02.321084      21 runners.go:193] proxy-service-kd9bp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 16:03:02.344: INFO: setup took 4.152773917s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 30 16:03:02.388: INFO: (0) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 43.300785ms)
Aug 30 16:03:02.389: INFO: (0) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 44.166195ms)
Aug 30 16:03:02.431: INFO: (0) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 86.252539ms)
Aug 30 16:03:02.432: INFO: (0) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 87.29242ms)
Aug 30 16:03:02.432: INFO: (0) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 86.551085ms)
Aug 30 16:03:02.433: INFO: (0) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 88.030972ms)
Aug 30 16:03:02.433: INFO: (0) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 88.206207ms)
Aug 30 16:03:02.433: INFO: (0) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 87.7962ms)
Aug 30 16:03:02.438: INFO: (0) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 93.392127ms)
Aug 30 16:03:02.439: INFO: (0) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 94.757438ms)
Aug 30 16:03:02.440: INFO: (0) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 95.0504ms)
Aug 30 16:03:02.457: INFO: (0) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 112.218497ms)
Aug 30 16:03:02.457: INFO: (0) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 112.739895ms)
Aug 30 16:03:02.458: INFO: (0) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 112.682731ms)
Aug 30 16:03:02.458: INFO: (0) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 112.95367ms)
Aug 30 16:03:02.458: INFO: (0) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 113.290695ms)
Aug 30 16:03:02.480: INFO: (1) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 20.588649ms)
Aug 30 16:03:02.482: INFO: (1) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 21.515876ms)
Aug 30 16:03:02.484: INFO: (1) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 24.169679ms)
Aug 30 16:03:02.485: INFO: (1) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 25.035106ms)
Aug 30 16:03:02.486: INFO: (1) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 25.464357ms)
Aug 30 16:03:02.487: INFO: (1) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 27.380973ms)
Aug 30 16:03:02.492: INFO: (1) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 31.745638ms)
Aug 30 16:03:02.493: INFO: (1) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 33.869268ms)
Aug 30 16:03:02.493: INFO: (1) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 33.839054ms)
Aug 30 16:03:02.495: INFO: (1) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 34.737053ms)
Aug 30 16:03:02.496: INFO: (1) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 35.438857ms)
Aug 30 16:03:02.508: INFO: (1) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 48.529703ms)
Aug 30 16:03:02.511: INFO: (1) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 51.90339ms)
Aug 30 16:03:02.511: INFO: (1) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 51.902096ms)
Aug 30 16:03:02.513: INFO: (1) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 54.099594ms)
Aug 30 16:03:02.520: INFO: (1) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 60.878398ms)
Aug 30 16:03:02.543: INFO: (2) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 22.927665ms)
Aug 30 16:03:02.544: INFO: (2) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 23.17616ms)
Aug 30 16:03:02.547: INFO: (2) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 25.789014ms)
Aug 30 16:03:02.547: INFO: (2) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 25.937378ms)
Aug 30 16:03:02.548: INFO: (2) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 27.150664ms)
Aug 30 16:03:02.548: INFO: (2) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 27.34257ms)
Aug 30 16:03:02.551: INFO: (2) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 29.973808ms)
Aug 30 16:03:02.552: INFO: (2) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 31.141133ms)
Aug 30 16:03:02.553: INFO: (2) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 31.752727ms)
Aug 30 16:03:02.553: INFO: (2) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 32.996029ms)
Aug 30 16:03:02.554: INFO: (2) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 33.177541ms)
Aug 30 16:03:02.557: INFO: (2) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 35.969793ms)
Aug 30 16:03:02.558: INFO: (2) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 36.996706ms)
Aug 30 16:03:02.559: INFO: (2) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 37.667677ms)
Aug 30 16:03:02.560: INFO: (2) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 38.574902ms)
Aug 30 16:03:02.570: INFO: (2) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 49.444537ms)
Aug 30 16:03:02.596: INFO: (3) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 24.666002ms)
Aug 30 16:03:02.596: INFO: (3) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 25.495045ms)
Aug 30 16:03:02.597: INFO: (3) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 25.43468ms)
Aug 30 16:03:02.597: INFO: (3) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 26.028419ms)
Aug 30 16:03:02.599: INFO: (3) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 27.143797ms)
Aug 30 16:03:02.599: INFO: (3) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 27.070732ms)
Aug 30 16:03:02.599: INFO: (3) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 27.28827ms)
Aug 30 16:03:02.601: INFO: (3) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 30.118911ms)
Aug 30 16:03:02.601: INFO: (3) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 29.799705ms)
Aug 30 16:03:02.603: INFO: (3) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 31.201027ms)
Aug 30 16:03:02.611: INFO: (3) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 39.731943ms)
Aug 30 16:03:02.611: INFO: (3) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 39.593524ms)
Aug 30 16:03:02.612: INFO: (3) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 41.321852ms)
Aug 30 16:03:02.613: INFO: (3) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 41.546596ms)
Aug 30 16:03:02.614: INFO: (3) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 41.52829ms)
Aug 30 16:03:02.614: INFO: (3) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 42.401981ms)
Aug 30 16:03:02.638: INFO: (4) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 22.691555ms)
Aug 30 16:03:02.643: INFO: (4) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 24.968184ms)
Aug 30 16:03:02.643: INFO: (4) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 25.397782ms)
Aug 30 16:03:02.644: INFO: (4) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 27.013914ms)
Aug 30 16:03:02.644: INFO: (4) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 24.869678ms)
Aug 30 16:03:02.646: INFO: (4) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 29.030667ms)
Aug 30 16:03:02.647: INFO: (4) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 29.969448ms)
Aug 30 16:03:02.647: INFO: (4) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 32.983412ms)
Aug 30 16:03:02.648: INFO: (4) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 30.822369ms)
Aug 30 16:03:02.648: INFO: (4) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 32.728457ms)
Aug 30 16:03:02.662: INFO: (4) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 45.760484ms)
Aug 30 16:03:02.663: INFO: (4) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 48.263633ms)
Aug 30 16:03:02.664: INFO: (4) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 46.04811ms)
Aug 30 16:03:02.664: INFO: (4) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 45.804419ms)
Aug 30 16:03:02.664: INFO: (4) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 45.873686ms)
Aug 30 16:03:02.665: INFO: (4) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 47.254804ms)
Aug 30 16:03:02.691: INFO: (5) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 24.212167ms)
Aug 30 16:03:02.692: INFO: (5) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 26.894683ms)
Aug 30 16:03:02.692: INFO: (5) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 25.281646ms)
Aug 30 16:03:02.693: INFO: (5) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 25.640709ms)
Aug 30 16:03:02.692: INFO: (5) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 25.192349ms)
Aug 30 16:03:02.698: INFO: (5) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 31.387091ms)
Aug 30 16:03:02.699: INFO: (5) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 31.470126ms)
Aug 30 16:03:02.699: INFO: (5) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 32.308455ms)
Aug 30 16:03:02.699: INFO: (5) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 32.147856ms)
Aug 30 16:03:02.699: INFO: (5) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 32.746204ms)
Aug 30 16:03:02.700: INFO: (5) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 34.03448ms)
Aug 30 16:03:02.716: INFO: (5) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 51.296173ms)
Aug 30 16:03:02.718: INFO: (5) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 51.424614ms)
Aug 30 16:03:02.718: INFO: (5) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 52.702388ms)
Aug 30 16:03:02.719: INFO: (5) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 51.527302ms)
Aug 30 16:03:02.719: INFO: (5) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 52.503655ms)
Aug 30 16:03:02.740: INFO: (6) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 20.602033ms)
Aug 30 16:03:02.741: INFO: (6) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 22.171571ms)
Aug 30 16:03:02.744: INFO: (6) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 24.187202ms)
Aug 30 16:03:02.747: INFO: (6) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 27.68922ms)
Aug 30 16:03:02.750: INFO: (6) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 29.800587ms)
Aug 30 16:03:02.752: INFO: (6) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 32.66855ms)
Aug 30 16:03:02.767: INFO: (6) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 47.134843ms)
Aug 30 16:03:02.767: INFO: (6) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 47.72503ms)
Aug 30 16:03:02.767: INFO: (6) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 47.663651ms)
Aug 30 16:03:02.767: INFO: (6) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 47.73576ms)
Aug 30 16:03:02.775: INFO: (6) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 55.17096ms)
Aug 30 16:03:02.776: INFO: (6) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 55.813324ms)
Aug 30 16:03:02.777: INFO: (6) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 57.148346ms)
Aug 30 16:03:02.785: INFO: (6) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 65.868013ms)
Aug 30 16:03:02.785: INFO: (6) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 66.15662ms)
Aug 30 16:03:02.786: INFO: (6) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 66.557439ms)
Aug 30 16:03:02.830: INFO: (7) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 41.941849ms)
Aug 30 16:03:02.830: INFO: (7) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 42.393972ms)
Aug 30 16:03:02.830: INFO: (7) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 42.554173ms)
Aug 30 16:03:02.830: INFO: (7) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 42.307719ms)
Aug 30 16:03:02.831: INFO: (7) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 43.491401ms)
Aug 30 16:03:02.831: INFO: (7) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 42.625266ms)
Aug 30 16:03:02.831: INFO: (7) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 42.866762ms)
Aug 30 16:03:02.831: INFO: (7) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 42.526189ms)
Aug 30 16:03:02.831: INFO: (7) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 42.78399ms)
Aug 30 16:03:02.831: INFO: (7) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 44.156631ms)
Aug 30 16:03:02.831: INFO: (7) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 44.388542ms)
Aug 30 16:03:02.836: INFO: (7) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 47.720441ms)
Aug 30 16:03:02.838: INFO: (7) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 51.518069ms)
Aug 30 16:03:02.838: INFO: (7) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 51.028229ms)
Aug 30 16:03:02.838: INFO: (7) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 51.475304ms)
Aug 30 16:03:02.839: INFO: (7) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 52.018309ms)
Aug 30 16:03:02.866: INFO: (8) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 26.449278ms)
Aug 30 16:03:02.867: INFO: (8) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 27.427627ms)
Aug 30 16:03:02.867: INFO: (8) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 26.664697ms)
Aug 30 16:03:02.868: INFO: (8) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 27.184728ms)
Aug 30 16:03:02.868: INFO: (8) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 27.417278ms)
Aug 30 16:03:02.870: INFO: (8) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 29.423754ms)
Aug 30 16:03:02.872: INFO: (8) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 30.895662ms)
Aug 30 16:03:02.872: INFO: (8) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 31.649748ms)
Aug 30 16:03:02.872: INFO: (8) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 31.509154ms)
Aug 30 16:03:02.872: INFO: (8) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 31.208584ms)
Aug 30 16:03:02.880: INFO: (8) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 39.566522ms)
Aug 30 16:03:02.881: INFO: (8) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 40.692272ms)
Aug 30 16:03:02.881: INFO: (8) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 41.325433ms)
Aug 30 16:03:02.882: INFO: (8) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 41.510148ms)
Aug 30 16:03:02.883: INFO: (8) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 43.142946ms)
Aug 30 16:03:02.885: INFO: (8) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 44.845783ms)
Aug 30 16:03:02.914: INFO: (9) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 28.467566ms)
Aug 30 16:03:02.917: INFO: (9) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 30.180488ms)
Aug 30 16:03:02.918: INFO: (9) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 32.506245ms)
Aug 30 16:03:02.921: INFO: (9) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 35.06577ms)
Aug 30 16:03:02.921: INFO: (9) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 33.342812ms)
Aug 30 16:03:02.923: INFO: (9) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 35.751432ms)
Aug 30 16:03:02.923: INFO: (9) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 36.593847ms)
Aug 30 16:03:02.923: INFO: (9) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 37.586868ms)
Aug 30 16:03:02.923: INFO: (9) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 35.942896ms)
Aug 30 16:03:02.924: INFO: (9) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 37.227303ms)
Aug 30 16:03:02.927: INFO: (9) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 39.243138ms)
Aug 30 16:03:02.931: INFO: (9) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 43.467166ms)
Aug 30 16:03:02.932: INFO: (9) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 45.588832ms)
Aug 30 16:03:02.932: INFO: (9) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 43.994331ms)
Aug 30 16:03:02.932: INFO: (9) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 45.112424ms)
Aug 30 16:03:02.932: INFO: (9) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 45.998049ms)
Aug 30 16:03:02.954: INFO: (10) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 21.430685ms)
Aug 30 16:03:02.956: INFO: (10) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 22.789867ms)
Aug 30 16:03:02.956: INFO: (10) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 23.797537ms)
Aug 30 16:03:02.956: INFO: (10) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 23.098641ms)
Aug 30 16:03:02.956: INFO: (10) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 22.842775ms)
Aug 30 16:03:02.974: INFO: (10) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 37.651117ms)
Aug 30 16:03:02.975: INFO: (10) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 38.300712ms)
Aug 30 16:03:02.975: INFO: (10) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 38.51481ms)
Aug 30 16:03:02.978: INFO: (10) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 41.625325ms)
Aug 30 16:03:02.978: INFO: (10) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 41.115463ms)
Aug 30 16:03:02.979: INFO: (10) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 45.991117ms)
Aug 30 16:03:02.982: INFO: (10) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 46.12821ms)
Aug 30 16:03:02.986: INFO: (10) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 48.887871ms)
Aug 30 16:03:02.987: INFO: (10) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 51.421714ms)
Aug 30 16:03:02.987: INFO: (10) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 54.076881ms)
Aug 30 16:03:03.012: INFO: (10) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 75.075293ms)
Aug 30 16:03:03.035: INFO: (11) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 22.012708ms)
Aug 30 16:03:03.037: INFO: (11) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 24.050495ms)
Aug 30 16:03:03.037: INFO: (11) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 23.945798ms)
Aug 30 16:03:03.037: INFO: (11) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 23.527274ms)
Aug 30 16:03:03.041: INFO: (11) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 26.658943ms)
Aug 30 16:03:03.041: INFO: (11) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 26.090638ms)
Aug 30 16:03:03.041: INFO: (11) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 26.733079ms)
Aug 30 16:03:03.042: INFO: (11) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 27.666645ms)
Aug 30 16:03:03.042: INFO: (11) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 27.063589ms)
Aug 30 16:03:03.042: INFO: (11) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 27.226582ms)
Aug 30 16:03:03.046: INFO: (11) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 32.268717ms)
Aug 30 16:03:03.046: INFO: (11) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 32.220488ms)
Aug 30 16:03:03.048: INFO: (11) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 35.271095ms)
Aug 30 16:03:03.051: INFO: (11) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 37.550294ms)
Aug 30 16:03:03.055: INFO: (11) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 40.690446ms)
Aug 30 16:03:03.056: INFO: (11) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 41.22497ms)
Aug 30 16:03:03.077: INFO: (12) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 20.508419ms)
Aug 30 16:03:03.080: INFO: (12) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 23.173036ms)
Aug 30 16:03:03.082: INFO: (12) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 25.100661ms)
Aug 30 16:03:03.082: INFO: (12) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 25.454416ms)
Aug 30 16:03:03.082: INFO: (12) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 24.580644ms)
Aug 30 16:03:03.082: INFO: (12) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 25.912633ms)
Aug 30 16:03:03.083: INFO: (12) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 25.982153ms)
Aug 30 16:03:03.085: INFO: (12) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 27.3938ms)
Aug 30 16:03:03.086: INFO: (12) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 28.328767ms)
Aug 30 16:03:03.087: INFO: (12) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 29.458873ms)
Aug 30 16:03:03.088: INFO: (12) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 30.60852ms)
Aug 30 16:03:03.089: INFO: (12) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 32.601338ms)
Aug 30 16:03:03.091: INFO: (12) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 34.126871ms)
Aug 30 16:03:03.092: INFO: (12) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 34.717367ms)
Aug 30 16:03:03.095: INFO: (12) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 38.009457ms)
Aug 30 16:03:03.096: INFO: (12) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 38.575107ms)
Aug 30 16:03:03.117: INFO: (13) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 21.155656ms)
Aug 30 16:03:03.119: INFO: (13) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 22.742623ms)
Aug 30 16:03:03.120: INFO: (13) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 24.05828ms)
Aug 30 16:03:03.122: INFO: (13) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 25.263666ms)
Aug 30 16:03:03.122: INFO: (13) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 26.327338ms)
Aug 30 16:03:03.125: INFO: (13) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 29.164449ms)
Aug 30 16:03:03.126: INFO: (13) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 29.941121ms)
Aug 30 16:03:03.129: INFO: (13) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 32.456449ms)
Aug 30 16:03:03.129: INFO: (13) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 32.43547ms)
Aug 30 16:03:03.129: INFO: (13) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 32.567533ms)
Aug 30 16:03:03.132: INFO: (13) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 35.423416ms)
Aug 30 16:03:03.132: INFO: (13) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 36.174069ms)
Aug 30 16:03:03.132: INFO: (13) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 35.998224ms)
Aug 30 16:03:03.139: INFO: (13) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 42.290012ms)
Aug 30 16:03:03.139: INFO: (13) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 42.685399ms)
Aug 30 16:03:03.139: INFO: (13) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 42.428899ms)
Aug 30 16:03:03.178: INFO: (14) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 37.643081ms)
Aug 30 16:03:03.178: INFO: (14) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 37.934053ms)
Aug 30 16:03:03.178: INFO: (14) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 38.804349ms)
Aug 30 16:03:03.178: INFO: (14) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 38.864112ms)
Aug 30 16:03:03.179: INFO: (14) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 38.343623ms)
Aug 30 16:03:03.179: INFO: (14) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 39.114717ms)
Aug 30 16:03:03.179: INFO: (14) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 38.994692ms)
Aug 30 16:03:03.179: INFO: (14) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 38.693604ms)
Aug 30 16:03:03.179: INFO: (14) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 38.637786ms)
Aug 30 16:03:03.179: INFO: (14) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 39.096665ms)
Aug 30 16:03:03.179: INFO: (14) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 39.013727ms)
Aug 30 16:03:03.200: INFO: (14) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 60.284326ms)
Aug 30 16:03:03.200: INFO: (14) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 60.760813ms)
Aug 30 16:03:03.200: INFO: (14) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 60.634104ms)
Aug 30 16:03:03.200: INFO: (14) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 60.422683ms)
Aug 30 16:03:03.200: INFO: (14) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 60.77529ms)
Aug 30 16:03:03.219: INFO: (15) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 18.667576ms)
Aug 30 16:03:03.223: INFO: (15) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 21.392008ms)
Aug 30 16:03:03.224: INFO: (15) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 22.585063ms)
Aug 30 16:03:03.231: INFO: (15) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 30.071078ms)
Aug 30 16:03:03.232: INFO: (15) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 29.73031ms)
Aug 30 16:03:03.233: INFO: (15) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 30.572966ms)
Aug 30 16:03:03.233: INFO: (15) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 31.695518ms)
Aug 30 16:03:03.233: INFO: (15) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 31.215515ms)
Aug 30 16:03:03.233: INFO: (15) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 30.209209ms)
Aug 30 16:03:03.234: INFO: (15) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 31.315584ms)
Aug 30 16:03:03.244: INFO: (15) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 43.086521ms)
Aug 30 16:03:03.244: INFO: (15) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 43.585894ms)
Aug 30 16:03:03.245: INFO: (15) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 43.015961ms)
Aug 30 16:03:03.245: INFO: (15) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 43.900673ms)
Aug 30 16:03:03.247: INFO: (15) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 44.703443ms)
Aug 30 16:03:03.247: INFO: (15) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 45.026698ms)
Aug 30 16:03:03.268: INFO: (16) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 19.795857ms)
Aug 30 16:03:03.268: INFO: (16) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 19.688823ms)
Aug 30 16:03:03.268: INFO: (16) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 20.504493ms)
Aug 30 16:03:03.272: INFO: (16) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 23.400073ms)
Aug 30 16:03:03.273: INFO: (16) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 24.055195ms)
Aug 30 16:03:03.273: INFO: (16) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 24.471126ms)
Aug 30 16:03:03.273: INFO: (16) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 24.788551ms)
Aug 30 16:03:03.287: INFO: (16) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 37.861435ms)
Aug 30 16:03:03.287: INFO: (16) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 37.918239ms)
Aug 30 16:03:03.287: INFO: (16) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 38.063035ms)
Aug 30 16:03:03.287: INFO: (16) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 39.34179ms)
Aug 30 16:03:03.287: INFO: (16) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 39.160903ms)
Aug 30 16:03:03.289: INFO: (16) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 40.136047ms)
Aug 30 16:03:03.293: INFO: (16) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 43.903462ms)
Aug 30 16:03:03.293: INFO: (16) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 44.460862ms)
Aug 30 16:03:03.293: INFO: (16) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 44.070977ms)
Aug 30 16:03:03.316: INFO: (17) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 22.255079ms)
Aug 30 16:03:03.322: INFO: (17) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 27.981794ms)
Aug 30 16:03:03.322: INFO: (17) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 28.559644ms)
Aug 30 16:03:03.322: INFO: (17) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 28.121956ms)
Aug 30 16:03:03.323: INFO: (17) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 29.329514ms)
Aug 30 16:03:03.324: INFO: (17) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 29.436363ms)
Aug 30 16:03:03.324: INFO: (17) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 29.843026ms)
Aug 30 16:03:03.324: INFO: (17) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 30.040991ms)
Aug 30 16:03:03.324: INFO: (17) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 29.820973ms)
Aug 30 16:03:03.324: INFO: (17) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 30.112043ms)
Aug 30 16:03:03.330: INFO: (17) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 35.909973ms)
Aug 30 16:03:03.331: INFO: (17) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 36.752688ms)
Aug 30 16:03:03.331: INFO: (17) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 37.063309ms)
Aug 30 16:03:03.332: INFO: (17) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 38.074206ms)
Aug 30 16:03:03.333: INFO: (17) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 38.860703ms)
Aug 30 16:03:03.333: INFO: (17) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 38.831894ms)
Aug 30 16:03:03.356: INFO: (18) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 23.126855ms)
Aug 30 16:03:03.361: INFO: (18) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 27.534962ms)
Aug 30 16:03:03.364: INFO: (18) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 30.070409ms)
Aug 30 16:03:03.364: INFO: (18) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 28.196942ms)
Aug 30 16:03:03.366: INFO: (18) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 29.621476ms)
Aug 30 16:03:03.366: INFO: (18) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 31.180137ms)
Aug 30 16:03:03.370: INFO: (18) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 35.835712ms)
Aug 30 16:03:03.371: INFO: (18) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 36.255307ms)
Aug 30 16:03:03.371: INFO: (18) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 34.557823ms)
Aug 30 16:03:03.371: INFO: (18) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 35.04658ms)
Aug 30 16:03:03.373: INFO: (18) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 37.772504ms)
Aug 30 16:03:03.381: INFO: (18) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 46.808382ms)
Aug 30 16:03:03.381: INFO: (18) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 43.540768ms)
Aug 30 16:03:03.381: INFO: (18) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 44.320284ms)
Aug 30 16:03:03.381: INFO: (18) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 46.742502ms)
Aug 30 16:03:03.382: INFO: (18) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 44.231324ms)
Aug 30 16:03:03.401: INFO: (19) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 19.582913ms)
Aug 30 16:03:03.406: INFO: (19) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">test<... (200; 23.779811ms)
Aug 30 16:03:03.412: INFO: (19) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 28.245227ms)
Aug 30 16:03:03.413: INFO: (19) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:460/proxy/: tls baz (200; 27.771942ms)
Aug 30 16:03:03.413: INFO: (19) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:160/proxy/: foo (200; 28.961156ms)
Aug 30 16:03:03.419: INFO: (19) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:1080/proxy/rewriteme">... (200; 33.914628ms)
Aug 30 16:03:03.419: INFO: (19) /api/v1/namespaces/proxy-5752/pods/http:proxy-service-kd9bp-qh5z7:162/proxy/: bar (200; 34.435279ms)
Aug 30 16:03:03.419: INFO: (19) /api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/proxy-service-kd9bp-qh5z7/proxy/rewriteme">test</a> (200; 36.008518ms)
Aug 30 16:03:03.420: INFO: (19) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:462/proxy/: tls qux (200; 35.476338ms)
Aug 30 16:03:03.420: INFO: (19) /api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/: <a href="/api/v1/namespaces/proxy-5752/pods/https:proxy-service-kd9bp-qh5z7:443/proxy/tlsrewritem... (200; 34.746929ms)
Aug 30 16:03:03.443: INFO: (19) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname2/proxy/: tls qux (200; 60.589832ms)
Aug 30 16:03:03.449: INFO: (19) /api/v1/namespaces/proxy-5752/services/https:proxy-service-kd9bp:tlsportname1/proxy/: tls baz (200; 66.091156ms)
Aug 30 16:03:03.450: INFO: (19) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname2/proxy/: bar (200; 64.803539ms)
Aug 30 16:03:03.453: INFO: (19) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname1/proxy/: foo (200; 70.711999ms)
Aug 30 16:03:03.453: INFO: (19) /api/v1/namespaces/proxy-5752/services/http:proxy-service-kd9bp:portname2/proxy/: bar (200; 68.135311ms)
Aug 30 16:03:03.454: INFO: (19) /api/v1/namespaces/proxy-5752/services/proxy-service-kd9bp:portname1/proxy/: foo (200; 69.25329ms)
STEP: deleting ReplicationController proxy-service-kd9bp in namespace proxy-5752, will wait for the garbage collector to delete the pods
Aug 30 16:03:03.568: INFO: Deleting ReplicationController proxy-service-kd9bp took: 22.37597ms
Aug 30 16:03:03.769: INFO: Terminating ReplicationController proxy-service-kd9bp pods took: 200.722684ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Aug 30 16:03:05.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5752" for this suite.

• [SLOW TEST:7.721 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":281,"skipped":5273,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:03:05.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Aug 30 16:03:05.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-4307 api-versions'
Aug 30 16:03:06.115: INFO: stderr: ""
Aug 30 16:03:06.115: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 16:03:06.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4307" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":282,"skipped":5283,"failed":0}
SSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:03:06.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Aug 30 16:03:06.278: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 16:04:06.540: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 16:04:06.564: INFO: Starting informer...
STEP: Starting pods...
W0830 16:04:06.633738      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:04:06.866: INFO: Pod1 is running on 10.63.224.189. Tainting Node
W0830 16:04:06.907268      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:04:09.169: INFO: Pod2 is running on 10.63.224.189. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 30 16:04:15.761: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 30 16:04:35.845: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Aug 30 16:04:35.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4721" for this suite.

• [SLOW TEST:89.868 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":283,"skipped":5291,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:04:36.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 16:04:36.270021      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:04:36.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5583312b-cf11-43da-a19d-79d15d31e966" in namespace "projected-6215" to be "Succeeded or Failed"
Aug 30 16:04:36.289: INFO: Pod "downwardapi-volume-5583312b-cf11-43da-a19d-79d15d31e966": Phase="Pending", Reason="", readiness=false. Elapsed: 18.816482ms
Aug 30 16:04:38.324: INFO: Pod "downwardapi-volume-5583312b-cf11-43da-a19d-79d15d31e966": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053877835s
Aug 30 16:04:40.353: INFO: Pod "downwardapi-volume-5583312b-cf11-43da-a19d-79d15d31e966": Phase="Pending", Reason="", readiness=false. Elapsed: 4.082788142s
Aug 30 16:04:42.372: INFO: Pod "downwardapi-volume-5583312b-cf11-43da-a19d-79d15d31e966": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.102454931s
STEP: Saw pod success
Aug 30 16:04:42.372: INFO: Pod "downwardapi-volume-5583312b-cf11-43da-a19d-79d15d31e966" satisfied condition "Succeeded or Failed"
Aug 30 16:04:42.385: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-5583312b-cf11-43da-a19d-79d15d31e966 container client-container: <nil>
STEP: delete the pod
Aug 30 16:04:42.509: INFO: Waiting for pod downwardapi-volume-5583312b-cf11-43da-a19d-79d15d31e966 to disappear
Aug 30 16:04:42.524: INFO: Pod downwardapi-volume-5583312b-cf11-43da-a19d-79d15d31e966 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 16:04:42.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6215" for this suite.

• [SLOW TEST:6.523 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":284,"skipped":5297,"failed":0}
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:04:42.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-2221
W0830 16:04:42.731123      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2221
W0830 16:04:44.823960      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "tester" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "tester" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "tester" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "tester" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Deleting pre-stop pod
Aug 30 16:04:51.924: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Aug 30 16:04:51.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2221" for this suite.

• [SLOW TEST:9.445 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":285,"skipped":5297,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:04:52.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 16:04:52.754922      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 16:04:52.782: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:04:55.906: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 16:04:56.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9718" for this suite.
STEP: Destroying namespace "webhook-9718-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":286,"skipped":5313,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:04:56.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
W0830 16:04:56.722236      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:04:58.771: INFO: Deleting pod "var-expansion-94a2542e-42a4-4d5e-adcc-29b0feaea7e0" in namespace "var-expansion-3936"
Aug 30 16:04:58.802: INFO: Wait up to 5m0s for pod "var-expansion-94a2542e-42a4-4d5e-adcc-29b0feaea7e0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 30 16:05:02.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3936" for this suite.

• [SLOW TEST:6.407 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":287,"skipped":5328,"failed":0}
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:05:02.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 16:05:03.000: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
W0830 16:05:04.080358      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
W0830 16:05:05.153043      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:05:05.153: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 30 16:05:06.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8008" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":288,"skipped":5328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:05:06.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
W0830 16:05:06.398030      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:05:06.398: INFO: created test-pod-1
W0830 16:05:06.495557      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:05:06.496: INFO: created test-pod-2
W0830 16:05:06.600217      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:05:06.601: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Aug 30 16:05:06.601: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-8055' to be running and ready
Aug 30 16:05:06.647: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 16:05:06.648: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 16:05:06.648: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 16:05:06.648: INFO: 0 / 3 pods in namespace 'pods-8055' are running and ready (0 seconds elapsed)
Aug 30 16:05:06.648: INFO: expected 0 pod replicas in namespace 'pods-8055', 0 are Running and Ready.
Aug 30 16:05:06.648: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Aug 30 16:05:06.648: INFO: test-pod-1  10.63.224.189  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  }]
Aug 30 16:05:06.648: INFO: test-pod-2  10.63.224.189  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  }]
Aug 30 16:05:06.649: INFO: test-pod-3  10.63.224.189  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  }]
Aug 30 16:05:06.649: INFO: 
Aug 30 16:05:08.697: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 16:05:08.698: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 16:05:08.698: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 16:05:08.698: INFO: 0 / 3 pods in namespace 'pods-8055' are running and ready (2 seconds elapsed)
Aug 30 16:05:08.698: INFO: expected 0 pod replicas in namespace 'pods-8055', 0 are Running and Ready.
Aug 30 16:05:08.698: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Aug 30 16:05:08.698: INFO: test-pod-1  10.63.224.189  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  }]
Aug 30 16:05:08.698: INFO: test-pod-2  10.63.224.189  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  }]
Aug 30 16:05:08.698: INFO: test-pod-3  10.63.224.189  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-30 16:05:06 +0000 UTC  }]
Aug 30 16:05:08.698: INFO: 
Aug 30 16:05:10.702: INFO: 3 / 3 pods in namespace 'pods-8055' are running and ready (4 seconds elapsed)
Aug 30 16:05:10.702: INFO: expected 0 pod replicas in namespace 'pods-8055', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Aug 30 16:05:10.799: INFO: Pod quantity 3 is different from expected quantity 0
Aug 30 16:05:11.820: INFO: Pod quantity 3 is different from expected quantity 0
Aug 30 16:05:12.814: INFO: Pod quantity 3 is different from expected quantity 0
Aug 30 16:05:13.817: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 30 16:05:14.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8055" for this suite.

• [SLOW TEST:8.642 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":289,"skipped":5363,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:05:14.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
W0830 16:05:15.147381      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: reading a file in the container
Aug 30 16:05:17.209: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9785 pod-service-account-1e7976dd-d16c-4f57-8d07-872a35d5c3ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 30 16:05:17.610: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9785 pod-service-account-1e7976dd-d16c-4f57-8d07-872a35d5c3ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 30 16:05:18.042: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9785 pod-service-account-1e7976dd-d16c-4f57-8d07-872a35d5c3ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 30 16:05:18.483: INFO: Got root ca configmap in namespace "svcaccounts-9785"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 30 16:05:18.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9785" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":290,"skipped":5366,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:05:18.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Aug 30 16:05:18.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 create -f -'
Aug 30 16:05:19.217: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"update-demo\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"update-demo\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"update-demo\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"update-demo\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 16:05:19.218: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 30 16:05:19.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 16:05:19.403: INFO: stderr: ""
Aug 30 16:05:19.403: INFO: stdout: "update-demo-nautilus-gwfts update-demo-nautilus-rstzn "
Aug 30 16:05:19.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 get pods update-demo-nautilus-gwfts -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 16:05:19.539: INFO: stderr: ""
Aug 30 16:05:19.539: INFO: stdout: ""
Aug 30 16:05:19.539: INFO: update-demo-nautilus-gwfts is created but not running
Aug 30 16:05:24.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 16:05:24.684: INFO: stderr: ""
Aug 30 16:05:24.684: INFO: stdout: "update-demo-nautilus-gwfts update-demo-nautilus-rstzn "
Aug 30 16:05:24.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 get pods update-demo-nautilus-gwfts -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 16:05:24.813: INFO: stderr: ""
Aug 30 16:05:24.813: INFO: stdout: "true"
Aug 30 16:05:24.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 get pods update-demo-nautilus-gwfts -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 16:05:24.931: INFO: stderr: ""
Aug 30 16:05:24.931: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 30 16:05:24.931: INFO: validating pod update-demo-nautilus-gwfts
Aug 30 16:05:24.958: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 16:05:24.958: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 16:05:24.958: INFO: update-demo-nautilus-gwfts is verified up and running
Aug 30 16:05:24.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 get pods update-demo-nautilus-rstzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 16:05:25.131: INFO: stderr: ""
Aug 30 16:05:25.131: INFO: stdout: "true"
Aug 30 16:05:25.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 get pods update-demo-nautilus-rstzn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 16:05:25.367: INFO: stderr: ""
Aug 30 16:05:25.367: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 30 16:05:25.367: INFO: validating pod update-demo-nautilus-rstzn
Aug 30 16:05:25.459: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 16:05:25.459: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 16:05:25.459: INFO: update-demo-nautilus-rstzn is verified up and running
STEP: using delete to clean up resources
Aug 30 16:05:25.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 delete --grace-period=0 --force -f -'
Aug 30 16:05:25.625: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 16:05:25.625: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 30 16:05:25.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 get rc,svc -l name=update-demo --no-headers'
Aug 30 16:05:25.929: INFO: stderr: "No resources found in kubectl-1141 namespace.\n"
Aug 30 16:05:25.929: INFO: stdout: ""
Aug 30 16:05:25.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-1141 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 16:05:26.047: INFO: stderr: ""
Aug 30 16:05:26.047: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 16:05:26.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1141" for this suite.

• [SLOW TEST:7.530 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":291,"skipped":5376,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:05:26.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6169
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6169
W0830 16:05:26.333061      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "externalname-service" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalname-service" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalname-service" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalname-service" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 16:05:26.333138      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6169, replica count: 2
Aug 30 16:05:29.385: INFO: Creating new exec pod
I0830 16:05:29.384930      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
W0830 16:05:29.432810      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:05:34.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-6169 exec execpodlt526 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 30 16:05:34.884: INFO: stderr: "+ echo hostName+ \nnc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 30 16:05:34.884: INFO: stdout: "externalname-service-425gm"
Aug 30 16:05:34.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-6169 exec execpodlt526 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.121.90 80'
Aug 30 16:05:35.290: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.121.90 80\nConnection to 172.21.121.90 80 port [tcp/http] succeeded!\n"
Aug 30 16:05:35.290: INFO: stdout: "externalname-service-xmhbg"
Aug 30 16:05:35.290: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 16:05:35.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6169" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:9.321 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":292,"skipped":5388,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:05:35.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0830 16:05:35.819011      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-scheduling-42c65429-ece8-4d8c-a421-271de080834f" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-scheduling-42c65429-ece8-4d8c-a421-271de080834f" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-scheduling-42c65429-ece8-4d8c-a421-271de080834f" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-scheduling-42c65429-ece8-4d8c-a421-271de080834f" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:05:35.845: INFO: The status of Pod busybox-scheduling-42c65429-ece8-4d8c-a421-271de080834f is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:05:37.860: INFO: The status of Pod busybox-scheduling-42c65429-ece8-4d8c-a421-271de080834f is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:05:39.869: INFO: The status of Pod busybox-scheduling-42c65429-ece8-4d8c-a421-271de080834f is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 30 16:05:39.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5279" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":293,"skipped":5391,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:05:39.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
W0830 16:05:40.137221      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
Aug 30 16:07:00.212: INFO: Warning: Found 0 jobs in namespace cronjob-7239
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 30 16:07:02.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7239" for this suite.

• [SLOW TEST:82.299 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":294,"skipped":5404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:07:02.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Aug 30 16:07:03.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7368" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":295,"skipped":5436,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:07:03.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Aug 30 16:07:03.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3573 create -f -'
Aug 30 16:07:04.011: INFO: stderr: "Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"pause\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"pause\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"pause\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"pause\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Aug 30 16:07:04.011: INFO: stdout: "pod/pause created\n"
Aug 30 16:07:04.011: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 30 16:07:04.036: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3573" to be "running and ready"
Aug 30 16:07:04.065: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 19.127108ms
Aug 30 16:07:06.079: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.033841568s
Aug 30 16:07:06.079: INFO: Pod "pause" satisfied condition "running and ready"
Aug 30 16:07:06.080: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 30 16:07:06.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3573 label pods pause testing-label=testing-label-value'
Aug 30 16:07:06.267: INFO: stderr: ""
Aug 30 16:07:06.267: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 30 16:07:06.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3573 get pod pause -L testing-label'
Aug 30 16:07:06.396: INFO: stderr: ""
Aug 30 16:07:06.396: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 30 16:07:06.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3573 label pods pause testing-label-'
Aug 30 16:07:06.603: INFO: stderr: ""
Aug 30 16:07:06.603: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 30 16:07:06.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3573 get pod pause -L testing-label'
Aug 30 16:07:06.723: INFO: stderr: ""
Aug 30 16:07:06.723: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Aug 30 16:07:06.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3573 delete --grace-period=0 --force -f -'
Aug 30 16:07:06.876: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 16:07:06.876: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 30 16:07:06.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3573 get rc,svc -l name=pause --no-headers'
Aug 30 16:07:07.010: INFO: stderr: "No resources found in kubectl-3573 namespace.\n"
Aug 30 16:07:07.010: INFO: stdout: ""
Aug 30 16:07:07.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-3573 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 16:07:07.138: INFO: stderr: ""
Aug 30 16:07:07.138: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 16:07:07.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3573" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":296,"skipped":5446,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:07:07.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 30 16:07:18.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4143" for this suite.

• [SLOW TEST:11.636 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":297,"skipped":5447,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:07:18.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
W0830 16:07:19.023171      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: creating a file in subpath
Aug 30 16:07:21.068: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4898 PodName:var-expansion-a79df161-dfe5-419f-a4b8-5e49480b3510 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 16:07:21.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 16:07:21.069: INFO: ExecWithOptions: Clientset creation
Aug 30 16:07:21.087: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-4898/pods/var-expansion-a79df161-dfe5-419f-a4b8-5e49480b3510/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Aug 30 16:07:21.382: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4898 PodName:var-expansion-a79df161-dfe5-419f-a4b8-5e49480b3510 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 16:07:21.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 16:07:21.389: INFO: ExecWithOptions: Clientset creation
Aug 30 16:07:21.390: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-4898/pods/var-expansion-a79df161-dfe5-419f-a4b8-5e49480b3510/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Aug 30 16:07:22.265: INFO: Successfully updated pod "var-expansion-a79df161-dfe5-419f-a4b8-5e49480b3510"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Aug 30 16:07:22.278: INFO: Deleting pod "var-expansion-a79df161-dfe5-419f-a4b8-5e49480b3510" in namespace "var-expansion-4898"
Aug 30 16:07:22.321: INFO: Wait up to 5m0s for pod "var-expansion-a79df161-dfe5-419f-a4b8-5e49480b3510" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 30 16:07:56.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4898" for this suite.

• [SLOW TEST:37.610 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":298,"skipped":5460,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:07:56.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
W0830 16:07:56.674672      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Aug 30 16:08:37.158: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 30 16:08:37.159: INFO: Deleting pod "simpletest.rc-28zrl" in namespace "gc-9926"
W0830 16:08:37.158814      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 30 16:08:37.195: INFO: Deleting pod "simpletest.rc-2r9pt" in namespace "gc-9926"
Aug 30 16:08:37.239: INFO: Deleting pod "simpletest.rc-2s2w2" in namespace "gc-9926"
Aug 30 16:08:37.288: INFO: Deleting pod "simpletest.rc-2x6fx" in namespace "gc-9926"
Aug 30 16:08:37.323: INFO: Deleting pod "simpletest.rc-42kss" in namespace "gc-9926"
Aug 30 16:08:37.363: INFO: Deleting pod "simpletest.rc-44glg" in namespace "gc-9926"
Aug 30 16:08:37.405: INFO: Deleting pod "simpletest.rc-47hp4" in namespace "gc-9926"
Aug 30 16:08:37.455: INFO: Deleting pod "simpletest.rc-4jbck" in namespace "gc-9926"
Aug 30 16:08:37.540: INFO: Deleting pod "simpletest.rc-4n49s" in namespace "gc-9926"
Aug 30 16:08:37.572: INFO: Deleting pod "simpletest.rc-584wn" in namespace "gc-9926"
Aug 30 16:08:37.647: INFO: Deleting pod "simpletest.rc-59ctw" in namespace "gc-9926"
Aug 30 16:08:37.687: INFO: Deleting pod "simpletest.rc-5fxrw" in namespace "gc-9926"
Aug 30 16:08:37.723: INFO: Deleting pod "simpletest.rc-5grht" in namespace "gc-9926"
Aug 30 16:08:37.762: INFO: Deleting pod "simpletest.rc-5jpv6" in namespace "gc-9926"
Aug 30 16:08:37.820: INFO: Deleting pod "simpletest.rc-5w2zq" in namespace "gc-9926"
Aug 30 16:08:37.868: INFO: Deleting pod "simpletest.rc-5wcrk" in namespace "gc-9926"
Aug 30 16:08:37.915: INFO: Deleting pod "simpletest.rc-6l6pf" in namespace "gc-9926"
Aug 30 16:08:37.976: INFO: Deleting pod "simpletest.rc-6rpbd" in namespace "gc-9926"
Aug 30 16:08:38.039: INFO: Deleting pod "simpletest.rc-76krp" in namespace "gc-9926"
Aug 30 16:08:38.135: INFO: Deleting pod "simpletest.rc-7qvh6" in namespace "gc-9926"
Aug 30 16:08:38.179: INFO: Deleting pod "simpletest.rc-84gfs" in namespace "gc-9926"
Aug 30 16:08:38.228: INFO: Deleting pod "simpletest.rc-89tkz" in namespace "gc-9926"
Aug 30 16:08:38.283: INFO: Deleting pod "simpletest.rc-8d8sc" in namespace "gc-9926"
Aug 30 16:08:38.327: INFO: Deleting pod "simpletest.rc-8nd22" in namespace "gc-9926"
Aug 30 16:08:38.383: INFO: Deleting pod "simpletest.rc-9p8xh" in namespace "gc-9926"
Aug 30 16:08:38.412: INFO: Deleting pod "simpletest.rc-9wqt7" in namespace "gc-9926"
Aug 30 16:08:38.451: INFO: Deleting pod "simpletest.rc-9zlbw" in namespace "gc-9926"
Aug 30 16:08:38.494: INFO: Deleting pod "simpletest.rc-b5hsl" in namespace "gc-9926"
Aug 30 16:08:38.548: INFO: Deleting pod "simpletest.rc-bpv9x" in namespace "gc-9926"
Aug 30 16:08:38.586: INFO: Deleting pod "simpletest.rc-cc5x2" in namespace "gc-9926"
Aug 30 16:08:38.617: INFO: Deleting pod "simpletest.rc-cdjmc" in namespace "gc-9926"
Aug 30 16:08:38.661: INFO: Deleting pod "simpletest.rc-cjpj6" in namespace "gc-9926"
Aug 30 16:08:38.740: INFO: Deleting pod "simpletest.rc-dh7p4" in namespace "gc-9926"
Aug 30 16:08:38.777: INFO: Deleting pod "simpletest.rc-dl7xc" in namespace "gc-9926"
Aug 30 16:08:38.808: INFO: Deleting pod "simpletest.rc-dnxlp" in namespace "gc-9926"
Aug 30 16:08:38.888: INFO: Deleting pod "simpletest.rc-drfqw" in namespace "gc-9926"
Aug 30 16:08:38.936: INFO: Deleting pod "simpletest.rc-f25vq" in namespace "gc-9926"
Aug 30 16:08:38.970: INFO: Deleting pod "simpletest.rc-f6gmz" in namespace "gc-9926"
Aug 30 16:08:39.011: INFO: Deleting pod "simpletest.rc-fgmds" in namespace "gc-9926"
Aug 30 16:08:39.052: INFO: Deleting pod "simpletest.rc-fl65w" in namespace "gc-9926"
Aug 30 16:08:39.094: INFO: Deleting pod "simpletest.rc-fmcnh" in namespace "gc-9926"
Aug 30 16:08:39.170: INFO: Deleting pod "simpletest.rc-ft2cn" in namespace "gc-9926"
Aug 30 16:08:39.218: INFO: Deleting pod "simpletest.rc-g2gpz" in namespace "gc-9926"
Aug 30 16:08:39.253: INFO: Deleting pod "simpletest.rc-g74rc" in namespace "gc-9926"
Aug 30 16:08:39.316: INFO: Deleting pod "simpletest.rc-gd6nb" in namespace "gc-9926"
Aug 30 16:08:39.368: INFO: Deleting pod "simpletest.rc-ghkfj" in namespace "gc-9926"
Aug 30 16:08:39.444: INFO: Deleting pod "simpletest.rc-gr26v" in namespace "gc-9926"
Aug 30 16:08:39.488: INFO: Deleting pod "simpletest.rc-gtmpn" in namespace "gc-9926"
Aug 30 16:08:39.517: INFO: Deleting pod "simpletest.rc-gznvz" in namespace "gc-9926"
Aug 30 16:08:39.562: INFO: Deleting pod "simpletest.rc-h4r4k" in namespace "gc-9926"
Aug 30 16:08:39.609: INFO: Deleting pod "simpletest.rc-hdnbf" in namespace "gc-9926"
Aug 30 16:08:39.652: INFO: Deleting pod "simpletest.rc-hfkvx" in namespace "gc-9926"
Aug 30 16:08:39.688: INFO: Deleting pod "simpletest.rc-hk696" in namespace "gc-9926"
Aug 30 16:08:39.729: INFO: Deleting pod "simpletest.rc-hnpj9" in namespace "gc-9926"
Aug 30 16:08:39.762: INFO: Deleting pod "simpletest.rc-hp97g" in namespace "gc-9926"
Aug 30 16:08:39.798: INFO: Deleting pod "simpletest.rc-hqrvx" in namespace "gc-9926"
Aug 30 16:08:39.893: INFO: Deleting pod "simpletest.rc-hsxsm" in namespace "gc-9926"
Aug 30 16:08:39.982: INFO: Deleting pod "simpletest.rc-htq29" in namespace "gc-9926"
Aug 30 16:08:40.017: INFO: Deleting pod "simpletest.rc-j5tvt" in namespace "gc-9926"
Aug 30 16:08:40.086: INFO: Deleting pod "simpletest.rc-j72sf" in namespace "gc-9926"
Aug 30 16:08:40.137: INFO: Deleting pod "simpletest.rc-jjpn6" in namespace "gc-9926"
Aug 30 16:08:40.198: INFO: Deleting pod "simpletest.rc-kfdv8" in namespace "gc-9926"
Aug 30 16:08:40.279: INFO: Deleting pod "simpletest.rc-kmkm6" in namespace "gc-9926"
Aug 30 16:08:40.337: INFO: Deleting pod "simpletest.rc-kvdj8" in namespace "gc-9926"
Aug 30 16:08:40.374: INFO: Deleting pod "simpletest.rc-kvm95" in namespace "gc-9926"
Aug 30 16:08:40.416: INFO: Deleting pod "simpletest.rc-kxrtv" in namespace "gc-9926"
Aug 30 16:08:40.469: INFO: Deleting pod "simpletest.rc-l69pd" in namespace "gc-9926"
Aug 30 16:08:40.541: INFO: Deleting pod "simpletest.rc-ld5bc" in namespace "gc-9926"
Aug 30 16:08:40.610: INFO: Deleting pod "simpletest.rc-lqpzd" in namespace "gc-9926"
Aug 30 16:08:40.653: INFO: Deleting pod "simpletest.rc-lsv7l" in namespace "gc-9926"
Aug 30 16:08:40.695: INFO: Deleting pod "simpletest.rc-m7tnv" in namespace "gc-9926"
Aug 30 16:08:40.733: INFO: Deleting pod "simpletest.rc-mhvt5" in namespace "gc-9926"
Aug 30 16:08:40.777: INFO: Deleting pod "simpletest.rc-mtn9s" in namespace "gc-9926"
Aug 30 16:08:40.824: INFO: Deleting pod "simpletest.rc-mzh29" in namespace "gc-9926"
Aug 30 16:08:40.892: INFO: Deleting pod "simpletest.rc-n68dr" in namespace "gc-9926"
Aug 30 16:08:40.931: INFO: Deleting pod "simpletest.rc-ngj7q" in namespace "gc-9926"
Aug 30 16:08:40.980: INFO: Deleting pod "simpletest.rc-npsdb" in namespace "gc-9926"
Aug 30 16:08:41.016: INFO: Deleting pod "simpletest.rc-pqpdv" in namespace "gc-9926"
Aug 30 16:08:41.054: INFO: Deleting pod "simpletest.rc-pz58t" in namespace "gc-9926"
Aug 30 16:08:41.096: INFO: Deleting pod "simpletest.rc-r8qfb" in namespace "gc-9926"
Aug 30 16:08:41.145: INFO: Deleting pod "simpletest.rc-rf6vd" in namespace "gc-9926"
Aug 30 16:08:41.187: INFO: Deleting pod "simpletest.rc-rn2k6" in namespace "gc-9926"
Aug 30 16:08:41.230: INFO: Deleting pod "simpletest.rc-rwc5f" in namespace "gc-9926"
Aug 30 16:08:41.275: INFO: Deleting pod "simpletest.rc-s6mlm" in namespace "gc-9926"
Aug 30 16:08:41.306: INFO: Deleting pod "simpletest.rc-s8x5r" in namespace "gc-9926"
Aug 30 16:08:41.343: INFO: Deleting pod "simpletest.rc-tpw95" in namespace "gc-9926"
Aug 30 16:08:41.384: INFO: Deleting pod "simpletest.rc-trmk2" in namespace "gc-9926"
Aug 30 16:08:41.427: INFO: Deleting pod "simpletest.rc-txx2m" in namespace "gc-9926"
Aug 30 16:08:41.468: INFO: Deleting pod "simpletest.rc-v5ftz" in namespace "gc-9926"
Aug 30 16:08:41.509: INFO: Deleting pod "simpletest.rc-v6fxg" in namespace "gc-9926"
Aug 30 16:08:41.627: INFO: Deleting pod "simpletest.rc-vgmr5" in namespace "gc-9926"
Aug 30 16:08:41.727: INFO: Deleting pod "simpletest.rc-vz9bp" in namespace "gc-9926"
Aug 30 16:08:41.784: INFO: Deleting pod "simpletest.rc-wmnkf" in namespace "gc-9926"
Aug 30 16:08:41.849: INFO: Deleting pod "simpletest.rc-xb6ns" in namespace "gc-9926"
Aug 30 16:08:41.888: INFO: Deleting pod "simpletest.rc-z5x5f" in namespace "gc-9926"
Aug 30 16:08:41.932: INFO: Deleting pod "simpletest.rc-zh2tw" in namespace "gc-9926"
Aug 30 16:08:41.964: INFO: Deleting pod "simpletest.rc-zmfbh" in namespace "gc-9926"
Aug 30 16:08:41.997: INFO: Deleting pod "simpletest.rc-zwq54" in namespace "gc-9926"
Aug 30 16:08:42.091: INFO: Deleting pod "simpletest.rc-zxbg9" in namespace "gc-9926"
Aug 30 16:08:42.127: INFO: Deleting pod "simpletest.rc-zzdxr" in namespace "gc-9926"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 30 16:08:42.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9926" for this suite.

• [SLOW TEST:45.813 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":299,"skipped":5468,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:08:42.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
W0830 16:08:42.461143      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: create the rc2
W0830 16:08:42.492652      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 30 16:08:54.295: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0830 16:08:54.295824      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 30 16:08:54.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-2662f" in namespace "gc-1162"
Aug 30 16:08:54.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fwtf" in namespace "gc-1162"
Aug 30 16:08:54.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gx4r" in namespace "gc-1162"
Aug 30 16:08:54.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-2x5f8" in namespace "gc-1162"
Aug 30 16:08:54.473: INFO: Deleting pod "simpletest-rc-to-be-deleted-47vld" in namespace "gc-1162"
Aug 30 16:08:54.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-49gwp" in namespace "gc-1162"
Aug 30 16:08:54.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dn5q" in namespace "gc-1162"
Aug 30 16:08:54.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w7dc" in namespace "gc-1162"
Aug 30 16:08:54.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rnmw" in namespace "gc-1162"
Aug 30 16:08:54.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-62kp6" in namespace "gc-1162"
Aug 30 16:08:54.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zrlq" in namespace "gc-1162"
Aug 30 16:08:54.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-76hnw" in namespace "gc-1162"
Aug 30 16:08:54.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-76s4p" in namespace "gc-1162"
Aug 30 16:08:54.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-77xvf" in namespace "gc-1162"
Aug 30 16:08:54.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vpck" in namespace "gc-1162"
Aug 30 16:08:55.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-86tbz" in namespace "gc-1162"
Aug 30 16:08:55.056: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f5md" in namespace "gc-1162"
Aug 30 16:08:55.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kxm4" in namespace "gc-1162"
Aug 30 16:08:55.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ncwr" in namespace "gc-1162"
Aug 30 16:08:55.178: INFO: Deleting pod "simpletest-rc-to-be-deleted-8s4bx" in namespace "gc-1162"
Aug 30 16:08:55.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b6nl" in namespace "gc-1162"
Aug 30 16:08:55.266: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cjv9" in namespace "gc-1162"
Aug 30 16:08:55.303: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sdrd" in namespace "gc-1162"
Aug 30 16:08:55.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-b45tm" in namespace "gc-1162"
Aug 30 16:08:55.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5t5t" in namespace "gc-1162"
Aug 30 16:08:55.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9s6p" in namespace "gc-1162"
Aug 30 16:08:55.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgmk9" in namespace "gc-1162"
Aug 30 16:08:55.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvl7k" in namespace "gc-1162"
Aug 30 16:08:55.555: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwk6d" in namespace "gc-1162"
Aug 30 16:08:55.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8c4b" in namespace "gc-1162"
Aug 30 16:08:55.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-f58qc" in namespace "gc-1162"
Aug 30 16:08:55.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6dcs" in namespace "gc-1162"
Aug 30 16:08:55.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8knh" in namespace "gc-1162"
Aug 30 16:08:55.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjjt6" in namespace "gc-1162"
Aug 30 16:08:55.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs7zn" in namespace "gc-1162"
Aug 30 16:08:55.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-g662g" in namespace "gc-1162"
Aug 30 16:08:56.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9jd7" in namespace "gc-1162"
Aug 30 16:08:56.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnprp" in namespace "gc-1162"
Aug 30 16:08:56.166: INFO: Deleting pod "simpletest-rc-to-be-deleted-gp2ch" in namespace "gc-1162"
Aug 30 16:08:56.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2n8g" in namespace "gc-1162"
Aug 30 16:08:56.228: INFO: Deleting pod "simpletest-rc-to-be-deleted-h662z" in namespace "gc-1162"
Aug 30 16:08:56.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdbmt" in namespace "gc-1162"
Aug 30 16:08:56.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdh7h" in namespace "gc-1162"
Aug 30 16:08:56.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-hftvs" in namespace "gc-1162"
Aug 30 16:08:56.386: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgvh2" in namespace "gc-1162"
Aug 30 16:08:56.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9n8f" in namespace "gc-1162"
Aug 30 16:08:56.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcrmp" in namespace "gc-1162"
Aug 30 16:08:56.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-jrqcd" in namespace "gc-1162"
Aug 30 16:08:56.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-k55kt" in namespace "gc-1162"
Aug 30 16:08:56.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-k6zxs" in namespace "gc-1162"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 30 16:08:56.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1162" for this suite.

• [SLOW TEST:14.500 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":300,"skipped":5479,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:08:56.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
W0830 16:08:57.748258      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:08:57.782: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 16:08:59.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:09:01.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:09:03.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:09:05.874: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:09:07.869: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:09:09.890: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:09:11.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 8, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:09:14.926: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 16:09:15.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4283" for this suite.
STEP: Destroying namespace "webhook-4283-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:18.825 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":301,"skipped":5498,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:09:15.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
W0830 16:09:15.785065      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-falsefaf10ae0-2323-439e-a182-a35d4a10c03d" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-falsefaf10ae0-2323-439e-a182-a35d4a10c03d" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-falsefaf10ae0-2323-439e-a182-a35d4a10c03d" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-falsefaf10ae0-2323-439e-a182-a35d4a10c03d" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 30 16:09:19.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2454" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":302,"skipped":5509,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:09:19.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 16:09:20.032: INFO: Creating deployment "test-recreate-deployment"
W0830 16:09:20.093128      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:09:20.093: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 30 16:09:20.124: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 30 16:09:22.154: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 30 16:09:22.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 9, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 9, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 9, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 9, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-848969dbcd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:09:24.189: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
W0830 16:09:24.229468      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:09:24.229: INFO: Updating deployment test-recreate-deployment
Aug 30 16:09:24.229: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 16:09:24.548: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1774  e7d81a02-7698-44a2-ae03-d0f25b560aa1 138518 2 2022-08-30 16:09:20 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-30 16:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c111078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-30 16:09:24 +0000 UTC,LastTransitionTime:2022-08-30 16:09:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2022-08-30 16:09:24 +0000 UTC,LastTransitionTime:2022-08-30 16:09:20 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 30 16:09:24.564: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-1774  4bdb0f3b-c34c-42cc-bcae-b15bf4fc4755 138516 1 2022-08-30 16:09:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e7d81a02-7698-44a2-ae03-d0f25b560aa1 0xc00350f520 0xc00350f521}] []  [{kube-controller-manager Update apps/v1 2022-08-30 16:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7d81a02-7698-44a2-ae03-d0f25b560aa1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:09:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00350f5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:09:24.564: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 30 16:09:24.564: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-848969dbcd  deployment-1774  556c13f4-523f-4dca-9b7c-09fb71545daa 138505 2 2022-08-30 16:09:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:848969dbcd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e7d81a02-7698-44a2-ae03-d0f25b560aa1 0xc00350f407 0xc00350f408}] []  [{kube-controller-manager Update apps/v1 2022-08-30 16:09:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7d81a02-7698-44a2-ae03-d0f25b560aa1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:09:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 848969dbcd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:848969dbcd] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00350f4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:09:24.579: INFO: Pod "test-recreate-deployment-cd8586fc7-cw6s9" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-cw6s9 test-recreate-deployment-cd8586fc7- deployment-1774  9feaf4cc-82e9-447a-bc14-0a75bf52c20c 138517 0 2022-08-30 16:09:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 4bdb0f3b-c34c-42cc-bcae-b15bf4fc4755 0xc00252e017 0xc00252e018}] []  [{kube-controller-manager Update v1 2022-08-30 16:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4bdb0f3b-c34c-42cc-bcae-b15bf4fc4755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-30 16:09:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxwq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxwq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-pnmcq,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:09:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:09:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.189,PodIP:,StartTime:2022-08-30 16:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 30 16:09:24.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1774" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":303,"skipped":5512,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:09:24.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
W0830 16:09:24.811224      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 30 16:09:29.934: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 30 16:09:30.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8719" for this suite.

• [SLOW TEST:5.423 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":304,"skipped":5517,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:09:30.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-458216d6-1ef2-403c-ae6d-7ba954fbfb6f in namespace container-probe-7165
W0830 16:09:30.258386      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:09:34.293: INFO: Started pod busybox-458216d6-1ef2-403c-ae6d-7ba954fbfb6f in namespace container-probe-7165
STEP: checking the pod's current state and verifying that restartCount is present
Aug 30 16:09:34.308: INFO: Initial restart count of pod busybox-458216d6-1ef2-403c-ae6d-7ba954fbfb6f is 0
Aug 30 16:10:22.833: INFO: Restart count of pod container-probe-7165/busybox-458216d6-1ef2-403c-ae6d-7ba954fbfb6f is now 1 (48.524919947s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 30 16:10:22.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7165" for this suite.

• [SLOW TEST:52.857 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":305,"skipped":5520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:10:22.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Aug 30 16:10:23.039: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=kubectl-4942 proxy --unix-socket=/tmp/kubectl-proxy-unix513266076/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 30 16:10:23.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4942" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":306,"skipped":5547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:10:23.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 16:10:23.399528      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:10:23.402: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8349220f-0dea-4568-ae5f-1bdab944140e" in namespace "downward-api-1859" to be "Succeeded or Failed"
Aug 30 16:10:23.424: INFO: Pod "downwardapi-volume-8349220f-0dea-4568-ae5f-1bdab944140e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.320986ms
Aug 30 16:10:25.441: INFO: Pod "downwardapi-volume-8349220f-0dea-4568-ae5f-1bdab944140e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038899573s
Aug 30 16:10:27.461: INFO: Pod "downwardapi-volume-8349220f-0dea-4568-ae5f-1bdab944140e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058974601s
Aug 30 16:10:29.478: INFO: Pod "downwardapi-volume-8349220f-0dea-4568-ae5f-1bdab944140e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075956838s
STEP: Saw pod success
Aug 30 16:10:29.478: INFO: Pod "downwardapi-volume-8349220f-0dea-4568-ae5f-1bdab944140e" satisfied condition "Succeeded or Failed"
Aug 30 16:10:29.491: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-8349220f-0dea-4568-ae5f-1bdab944140e container client-container: <nil>
STEP: delete the pod
Aug 30 16:10:29.624: INFO: Waiting for pod downwardapi-volume-8349220f-0dea-4568-ae5f-1bdab944140e to disappear
Aug 30 16:10:29.635: INFO: Pod downwardapi-volume-8349220f-0dea-4568-ae5f-1bdab944140e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 16:10:29.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1859" for this suite.

• [SLOW TEST:6.505 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":307,"skipped":5600,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:10:29.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 30 16:10:32.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-13" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":308,"skipped":5613,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:10:32.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
W0830 16:10:32.379667      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:10:32.398: INFO: The status of Pod labelsupdate7bdf3b73-6a77-49c1-8643-eb09aa4377b8 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:10:34.424: INFO: The status of Pod labelsupdate7bdf3b73-6a77-49c1-8643-eb09aa4377b8 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:10:36.420: INFO: The status of Pod labelsupdate7bdf3b73-6a77-49c1-8643-eb09aa4377b8 is Running (Ready = true)
Aug 30 16:10:37.118: INFO: Successfully updated pod "labelsupdate7bdf3b73-6a77-49c1-8643-eb09aa4377b8"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 16:12:02.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8725" for this suite.

• [SLOW TEST:90.861 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":309,"skipped":5619,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:12:03.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 16:12:03.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2355" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":310,"skipped":5621,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:12:03.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
W0830 16:12:04.141302      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:12:04.166: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 16:12:06.217: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 12, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 12, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 12, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 12, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:12:09.314: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 16:12:09.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1982" for this suite.
STEP: Destroying namespace "webhook-1982-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.442 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":311,"skipped":5632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:12:09.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 16:12:10.035: INFO: Creating deployment "webserver-deployment"
W0830 16:12:10.062414      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:12:10.062: INFO: Waiting for observed generation 1
Aug 30 16:12:12.088: INFO: Waiting for all required pods to come up
Aug 30 16:12:12.115: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 30 16:12:14.172: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 30 16:12:14.198: INFO: Updating deployment "webserver-deployment" with a non-existent image
W0830 16:12:14.316184      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:12:14.316: INFO: Updating deployment webserver-deployment
Aug 30 16:12:14.316: INFO: Waiting for observed generation 2
Aug 30 16:12:16.345: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 30 16:12:16.365: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 30 16:12:16.378: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 30 16:12:16.419: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 30 16:12:16.419: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 30 16:12:16.435: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 30 16:12:16.467: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 30 16:12:16.467: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
W0830 16:12:16.501095      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:12:16.501: INFO: Updating deployment webserver-deployment
Aug 30 16:12:16.501: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 30 16:12:16.524: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 30 16:12:16.556: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 16:12:16.586: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2466  23814dd5-eff0-4692-891c-8af3c6222a6c 140320 3 2022-08-30 16:12:10 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-30 16:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00412dcb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2022-08-30 16:12:14 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-30 16:12:16 +0000 UTC,LastTransitionTime:2022-08-30 16:12:16 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 30 16:12:16.597: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-2466  cd0fc58c-b0fd-4953-9cab-25eb4459aaea 140318 3 2022-08-30 16:12:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 23814dd5-eff0-4692-891c-8af3c6222a6c 0xc0033a1287 0xc0033a1288}] []  [{kube-controller-manager Update apps/v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23814dd5-eff0-4692-891c-8af3c6222a6c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033a1328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:12:16.598: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 30 16:12:16.598: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-2466  331f760d-96ea-4684-b449-ecc20a22cdfc 140313 3 2022-08-30 16:12:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 23814dd5-eff0-4692-891c-8af3c6222a6c 0xc0033a1197 0xc0033a1198}] []  [{kube-controller-manager Update apps/v1 2022-08-30 16:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23814dd5-eff0-4692-891c-8af3c6222a6c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:12:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033a1228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:12:16.620: INFO: Pod "webserver-deployment-55df494869-82v9w" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-82v9w webserver-deployment-55df494869- deployment-2466  e198e887-0dac-490f-a24e-dd128c2d2ddc 140183 0 2022-08-30 16:12:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:cd957c0779926d3d486c89474b1b18d2ff5056523ba423350fca045be79ffa2c cni.projectcalico.org/podIP:172.30.38.232/32 cni.projectcalico.org/podIPs:172.30.38.232/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.232"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.232"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 331f760d-96ea-4684-b449-ecc20a22cdfc 0xc007e24047 0xc007e24048}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"331f760d-96ea-4684-b449-ecc20a22cdfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 16:12:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pt72h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pt72h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.187,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.187,PodIP:172.30.38.232,StartTime:2022-08-30 16:12:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 16:12:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://09c20586e1db572fb8dd0a26fabfa6dc169f112eb5f890db6330ae8a77c15e0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.621: INFO: Pod "webserver-deployment-55df494869-dmsft" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-dmsft webserver-deployment-55df494869- deployment-2466  31783a4b-ac39-4342-969d-aeb645957a75 140158 0 2022-08-30 16:12:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:147d99ff5a23c501d639596769dbf44d65040cc8648eb634c15b05d581892a3f cni.projectcalico.org/podIP:172.30.233.235/32 cni.projectcalico.org/podIPs:172.30.233.235/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.235"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.235"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 331f760d-96ea-4684-b449-ecc20a22cdfc 0xc007e242c7 0xc007e242c8}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"331f760d-96ea-4684-b449-ecc20a22cdfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 16:12:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.233.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4fl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4fl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.189,PodIP:172.30.233.235,StartTime:2022-08-30 16:12:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 16:12:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://485112646ce9f660bc1e2d39cc6ad777ac32c244dd401364f27f258997d425f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.233.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.621: INFO: Pod "webserver-deployment-55df494869-g8xh8" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-g8xh8 webserver-deployment-55df494869- deployment-2466  fcba5686-998b-4910-8f80-df11c800a489 140170 0 2022-08-30 16:12:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:7290f3942cf84b8b9bdcc704ab38f0f906dd448896c41eebacf3f3622f740f0b cni.projectcalico.org/podIP:172.30.78.54/32 cni.projectcalico.org/podIPs:172.30.78.54/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.78.54"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.78.54"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 331f760d-96ea-4684-b449-ecc20a22cdfc 0xc007e24567 0xc007e24568}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"331f760d-96ea-4684-b449-ecc20a22cdfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 16:12:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.78.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gx4p2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gx4p2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.158,PodIP:172.30.78.54,StartTime:2022-08-30 16:12:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 16:12:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://f51ed9c82320f4431924628d88c9c669ff2bd83844c35bb4aae688c81cddc76b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.78.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.622: INFO: Pod "webserver-deployment-55df494869-hvlqt" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-hvlqt webserver-deployment-55df494869- deployment-2466  687fc53d-6523-41bd-8370-391aee9d167b 140177 0 2022-08-30 16:12:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:94e081ca03aa2cdc020b5bef896d2b5ade5b3f762f0a6cccb1c96c5a48ca965a cni.projectcalico.org/podIP:172.30.78.57/32 cni.projectcalico.org/podIPs:172.30.78.57/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.78.57"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.78.57"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 331f760d-96ea-4684-b449-ecc20a22cdfc 0xc007e24807 0xc007e24808}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"331f760d-96ea-4684-b449-ecc20a22cdfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 16:12:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.78.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g7lp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g7lp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.158,PodIP:172.30.78.57,StartTime:2022-08-30 16:12:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 16:12:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b29c98fa62a28b12dd7f236a336379ab44262626858de1a51c55a073458d6995,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.78.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.622: INFO: Pod "webserver-deployment-55df494869-kr256" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-kr256 webserver-deployment-55df494869- deployment-2466  d023ab64-2dfd-4027-847d-23b44be5be0c 140167 0 2022-08-30 16:12:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:ec68caf3bfabdf00253522e7460b1af3570d055d5091e21b7d85819441e69231 cni.projectcalico.org/podIP:172.30.233.210/32 cni.projectcalico.org/podIPs:172.30.233.210/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.210"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.210"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 331f760d-96ea-4684-b449-ecc20a22cdfc 0xc007e24a87 0xc007e24a88}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"331f760d-96ea-4684-b449-ecc20a22cdfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-30 16:12:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.233.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-08-30 16:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kbnnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kbnnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.189,PodIP:172.30.233.210,StartTime:2022-08-30 16:12:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 16:12:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://11b44f28f920d478cbf5143f66932d439eee00ad8d8916a0710f6a04dc770bf7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.233.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.623: INFO: Pod "webserver-deployment-55df494869-mcc2d" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-mcc2d webserver-deployment-55df494869- deployment-2466  43a94f1c-472c-4bc5-8110-d02b271aa54f 140174 0 2022-08-30 16:12:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:94b7e6c66a6a79734a4087d6f1123f29d6a231a33e1f255a2f1a142bcb7fb0d8 cni.projectcalico.org/podIP:172.30.78.33/32 cni.projectcalico.org/podIPs:172.30.78.33/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.78.33"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.78.33"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 331f760d-96ea-4684-b449-ecc20a22cdfc 0xc007e24d37 0xc007e24d38}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"331f760d-96ea-4684-b449-ecc20a22cdfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 16:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 16:12:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.78.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lsm7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lsm7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.158,PodIP:172.30.78.33,StartTime:2022-08-30 16:12:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 16:12:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://fcd14df71cd0898056e8e33d7bbfd7a19c23345ae9e30195d77e513c66d728d7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.78.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.623: INFO: Pod "webserver-deployment-55df494869-nhmn8" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-nhmn8 webserver-deployment-55df494869- deployment-2466  e13153d5-cc52-4ee2-9858-6c933862ef3f 140133 0 2022-08-30 16:12:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:b5bb66dd52a001129b0592c5f39f4a0a993c0949678aeb932a5bf9ea957a9f2a cni.projectcalico.org/podIP:172.30.38.245/32 cni.projectcalico.org/podIPs:172.30.38.245/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.245"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.245"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 331f760d-96ea-4684-b449-ecc20a22cdfc 0xc007e24fd7 0xc007e24fd8}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"331f760d-96ea-4684-b449-ecc20a22cdfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 16:12:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j76rc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j76rc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.187,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.187,PodIP:172.30.38.245,StartTime:2022-08-30 16:12:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 16:12:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://bcc1e4c08b4330014709ab2e52546c80d161039501bc3f3d9df3d75b68645bc5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.623: INFO: Pod "webserver-deployment-55df494869-spjmt" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-spjmt webserver-deployment-55df494869- deployment-2466  72d7ec7e-d675-48ae-a6d1-c79a09450e31 140324 0 2022-08-30 16:12:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 331f760d-96ea-4684-b449-ecc20a22cdfc 0xc007e25257 0xc007e25258}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"331f760d-96ea-4684-b449-ecc20a22cdfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6r9p6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6r9p6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.624: INFO: Pod "webserver-deployment-55df494869-tx9m9" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-tx9m9 webserver-deployment-55df494869- deployment-2466  47c01546-1271-4b13-909d-25126b1b5449 140180 0 2022-08-30 16:12:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:9f15b9246f9d88eeaea096e00cd48e2bc4e74822043596551a1bda4c78693fd1 cni.projectcalico.org/podIP:172.30.38.231/32 cni.projectcalico.org/podIPs:172.30.38.231/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.231"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.231"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 331f760d-96ea-4684-b449-ecc20a22cdfc 0xc007e25427 0xc007e25428}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"331f760d-96ea-4684-b449-ecc20a22cdfc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 16:12:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfmpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfmpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.187,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.187,PodIP:172.30.38.231,StartTime:2022-08-30 16:12:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 16:12:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1f155db81d6a95b90f1a8d8dc30dd80509d9cc16bbc49e684a80070007d142c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.635: INFO: Pod "webserver-deployment-57ccb67bb8-4xk92" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-4xk92 webserver-deployment-57ccb67bb8- deployment-2466  9f8f9b6b-ddb9-4b3c-b3df-f2594a129af3 140281 0 2022-08-30 16:12:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:785343f5e2c355d0d9834ece48019022412889804a70530c763a428522a6f452 cni.projectcalico.org/podIP:172.30.78.59/32 cni.projectcalico.org/podIPs:172.30.78.59/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.78.59"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.78.59"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 cd0fc58c-b0fd-4953-9cab-25eb4459aaea 0xc007e256c7 0xc007e256c8}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd0fc58c-b0fd-4953-9cab-25eb4459aaea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-30 16:12:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8lfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8lfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.158,PodIP:,StartTime:2022-08-30 16:12:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.636: INFO: Pod "webserver-deployment-57ccb67bb8-6sw9b" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-6sw9b webserver-deployment-57ccb67bb8- deployment-2466  0cc274fa-b8ea-4db3-b80d-0cabe4aaa3d1 140317 0 2022-08-30 16:12:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:da638f8cfaf6c22ac648ea079c10895cb1a7793451421da47b45cbdc2673333d cni.projectcalico.org/podIP:172.30.233.209/32 cni.projectcalico.org/podIPs:172.30.233.209/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.209"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.209"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 cd0fc58c-b0fd-4953-9cab-25eb4459aaea 0xc007e25947 0xc007e25948}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd0fc58c-b0fd-4953-9cab-25eb4459aaea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-30 16:12:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkm92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkm92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.189,PodIP:,StartTime:2022-08-30 16:12:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.636: INFO: Pod "webserver-deployment-57ccb67bb8-98mt7" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-98mt7 webserver-deployment-57ccb67bb8- deployment-2466  83c12666-15f9-4cda-98f2-6955e1e0a696 140323 0 2022-08-30 16:12:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 cd0fc58c-b0fd-4953-9cab-25eb4459aaea 0xc007e25bc7 0xc007e25bc8}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd0fc58c-b0fd-4953-9cab-25eb4459aaea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ffn7n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ffn7n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.636: INFO: Pod "webserver-deployment-57ccb67bb8-bndfc" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-bndfc webserver-deployment-57ccb67bb8- deployment-2466  a9a3eca9-8575-4bef-8d6c-cc2db593c65a 140266 0 2022-08-30 16:12:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:7264288f567392881f4bfb7f3c55902ae59e3bc2685ecfb41b21f48403d20e90 cni.projectcalico.org/podIP:172.30.38.253/32 cni.projectcalico.org/podIPs:172.30.38.253/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.253"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.253"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 cd0fc58c-b0fd-4953-9cab-25eb4459aaea 0xc007e25d87 0xc007e25d88}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd0fc58c-b0fd-4953-9cab-25eb4459aaea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-30 16:12:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-825ch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-825ch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.187,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.187,PodIP:,StartTime:2022-08-30 16:12:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.637: INFO: Pod "webserver-deployment-57ccb67bb8-glprv" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-glprv webserver-deployment-57ccb67bb8- deployment-2466  74ccc555-7cc3-4b3a-abab-cf8714b08a1f 140283 0 2022-08-30 16:12:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:3dbdb2cbf1521b054d721d5f25cf4ba421e855cbf770ed6426deb72980afc5fe cni.projectcalico.org/podIP:172.30.38.240/32 cni.projectcalico.org/podIPs:172.30.38.240/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.240"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.240"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 cd0fc58c-b0fd-4953-9cab-25eb4459aaea 0xc00cee6027 0xc00cee6028}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd0fc58c-b0fd-4953-9cab-25eb4459aaea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-30 16:12:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bcdvs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bcdvs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.187,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.187,PodIP:,StartTime:2022-08-30 16:12:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 16:12:16.637: INFO: Pod "webserver-deployment-57ccb67bb8-th9tj" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-th9tj webserver-deployment-57ccb67bb8- deployment-2466  684c80b4-181a-43e8-9374-15c4ec7a0aa4 140291 0 2022-08-30 16:12:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:870ccb149669afddb2c4d6ab0c4f1710d193c2bc3a40610b21e33e215ad30c7f cni.projectcalico.org/podIP:172.30.233.198/32 cni.projectcalico.org/podIPs:172.30.233.198/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.198"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.198"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 cd0fc58c-b0fd-4953-9cab-25eb4459aaea 0xc00cee62a7 0xc00cee62a8}] []  [{kube-controller-manager Update v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd0fc58c-b0fd-4953-9cab-25eb4459aaea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-30 16:12:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-30 16:12:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:12:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bz845,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bz845,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c59,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7pkbk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:12:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.189,PodIP:,StartTime:2022-08-30 16:12:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 30 16:12:16.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2466" for this suite.

• [SLOW TEST:6.786 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":312,"skipped":5704,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:12:16.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 30 16:12:17.017: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2625  6572d516-f919-41fd-ae16-63602e1bed3b 140389 0 2022-08-30 16:12:16 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-08-30 16:12:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 16:12:17.017: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2625  6572d516-f919-41fd-ae16-63602e1bed3b 140392 0 2022-08-30 16:12:16 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-08-30 16:12:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 30 16:12:17.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2625" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":313,"skipped":5729,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:12:17.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-c20281ea-90e8-4539-9a74-f705ccf31f70
STEP: Creating a pod to test consume secrets
W0830 16:12:17.275621      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:12:17.276: INFO: Waiting up to 5m0s for pod "pod-secrets-e579213f-bf85-4d76-a4da-9f46ee70080c" in namespace "secrets-4578" to be "Succeeded or Failed"
Aug 30 16:12:17.306: INFO: Pod "pod-secrets-e579213f-bf85-4d76-a4da-9f46ee70080c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.024047ms
Aug 30 16:12:19.332: INFO: Pod "pod-secrets-e579213f-bf85-4d76-a4da-9f46ee70080c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05594475s
Aug 30 16:12:21.352: INFO: Pod "pod-secrets-e579213f-bf85-4d76-a4da-9f46ee70080c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075518717s
Aug 30 16:12:23.390: INFO: Pod "pod-secrets-e579213f-bf85-4d76-a4da-9f46ee70080c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.114079401s
STEP: Saw pod success
Aug 30 16:12:23.390: INFO: Pod "pod-secrets-e579213f-bf85-4d76-a4da-9f46ee70080c" satisfied condition "Succeeded or Failed"
Aug 30 16:12:23.408: INFO: Trying to get logs from node 10.63.224.189 pod pod-secrets-e579213f-bf85-4d76-a4da-9f46ee70080c container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 16:12:23.523: INFO: Waiting for pod pod-secrets-e579213f-bf85-4d76-a4da-9f46ee70080c to disappear
Aug 30 16:12:23.546: INFO: Pod pod-secrets-e579213f-bf85-4d76-a4da-9f46ee70080c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 30 16:12:23.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4578" for this suite.

• [SLOW TEST:6.534 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":314,"skipped":5802,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:12:23.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
W0830 16:12:23.810349      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
W0830 16:12:26.135295      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 30 16:12:28.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3089" for this suite.

• [SLOW TEST:5.158 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":315,"skipped":5837,"failed":0}
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:12:28.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 30 16:12:29.692: INFO: starting watch
STEP: patching
STEP: updating
Aug 30 16:12:29.779: INFO: waiting for watch events with expected annotations
Aug 30 16:12:29.781: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 16:12:30.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3626" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":316,"skipped":5838,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:12:30.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-pgrh
STEP: Creating a pod to test atomic-volume-subpath
W0830 16:12:30.394296      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container-subpath-secret-pgrh" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-secret-pgrh" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-secret-pgrh" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-secret-pgrh" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:12:30.395: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pgrh" in namespace "subpath-6131" to be "Succeeded or Failed"
Aug 30 16:12:30.411: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Pending", Reason="", readiness=false. Elapsed: 15.864782ms
Aug 30 16:12:32.427: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031739763s
Aug 30 16:12:34.450: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=true. Elapsed: 4.055275886s
Aug 30 16:12:36.467: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=true. Elapsed: 6.07191361s
Aug 30 16:12:38.485: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=true. Elapsed: 8.090103273s
Aug 30 16:12:40.504: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=true. Elapsed: 10.108842076s
Aug 30 16:12:42.519: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=true. Elapsed: 12.124454161s
Aug 30 16:12:44.533: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=true. Elapsed: 14.138671881s
Aug 30 16:12:46.555: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=true. Elapsed: 16.160069632s
Aug 30 16:12:48.572: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=true. Elapsed: 18.177573925s
Aug 30 16:12:50.587: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=true. Elapsed: 20.192362363s
Aug 30 16:12:52.604: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=true. Elapsed: 22.208767634s
Aug 30 16:12:54.624: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Running", Reason="", readiness=false. Elapsed: 24.229316131s
Aug 30 16:12:56.673: INFO: Pod "pod-subpath-test-secret-pgrh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.278273205s
STEP: Saw pod success
Aug 30 16:12:56.673: INFO: Pod "pod-subpath-test-secret-pgrh" satisfied condition "Succeeded or Failed"
Aug 30 16:12:56.685: INFO: Trying to get logs from node 10.63.224.189 pod pod-subpath-test-secret-pgrh container test-container-subpath-secret-pgrh: <nil>
STEP: delete the pod
Aug 30 16:12:57.490: INFO: Waiting for pod pod-subpath-test-secret-pgrh to disappear
Aug 30 16:12:57.505: INFO: Pod pod-subpath-test-secret-pgrh no longer exists
STEP: Deleting pod pod-subpath-test-secret-pgrh
Aug 30 16:12:57.506: INFO: Deleting pod "pod-subpath-test-secret-pgrh" in namespace "subpath-6131"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 30 16:12:57.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6131" for this suite.

• [SLOW TEST:27.421 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":317,"skipped":5871,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:12:57.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
W0830 16:12:57.845768      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "oidc-discovery-validator" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "oidc-discovery-validator" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "oidc-discovery-validator" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "oidc-discovery-validator" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:12:57.846: INFO: created pod
Aug 30 16:12:57.848: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5882" to be "Succeeded or Failed"
Aug 30 16:12:57.862: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.041877ms
Aug 30 16:12:59.893: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045251066s
Aug 30 16:13:01.924: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07579758s
Aug 30 16:13:03.946: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098067008s
STEP: Saw pod success
Aug 30 16:13:03.946: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 30 16:13:33.948: INFO: polling logs
Aug 30 16:13:33.983: INFO: Pod logs: 
I0830 16:12:59.643570       1 log.go:195] OK: Got token
I0830 16:12:59.643653       1 log.go:195] validating with in-cluster discovery
I0830 16:12:59.644297       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0830 16:12:59.644326       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-5882:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1661876578, NotBefore:1661875978, IssuedAt:1661875978, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5882", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f821646c-8c32-4ff6-a4fe-c1993aaddce2"}}}
I0830 16:12:59.683502       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0830 16:12:59.713171       1 log.go:195] OK: Validated signature on JWT
I0830 16:12:59.713481       1 log.go:195] OK: Got valid claims from token!
I0830 16:12:59.713545       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-5882:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1661876578, NotBefore:1661875978, IssuedAt:1661875978, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5882", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f821646c-8c32-4ff6-a4fe-c1993aaddce2"}}}

Aug 30 16:13:33.983: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 30 16:13:34.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5882" for this suite.

• [SLOW TEST:36.489 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":318,"skipped":5881,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:13:34.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
W0830 16:13:36.269739      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:13:36.283: INFO: running pods: 0 < 1
Aug 30 16:13:38.302: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 30 16:13:40.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2216" for this suite.

• [SLOW TEST:6.405 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":319,"skipped":5889,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:13:40.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0830 16:13:40.646089      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:13:40.646: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c0b7d8fb-0311-4ec9-9a1d-f8da5103f9e0" in namespace "projected-4" to be "Succeeded or Failed"
Aug 30 16:13:40.660: INFO: Pod "downwardapi-volume-c0b7d8fb-0311-4ec9-9a1d-f8da5103f9e0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.296786ms
Aug 30 16:13:42.681: INFO: Pod "downwardapi-volume-c0b7d8fb-0311-4ec9-9a1d-f8da5103f9e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035203579s
Aug 30 16:13:44.696: INFO: Pod "downwardapi-volume-c0b7d8fb-0311-4ec9-9a1d-f8da5103f9e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04986186s
Aug 30 16:13:46.711: INFO: Pod "downwardapi-volume-c0b7d8fb-0311-4ec9-9a1d-f8da5103f9e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064686888s
STEP: Saw pod success
Aug 30 16:13:46.711: INFO: Pod "downwardapi-volume-c0b7d8fb-0311-4ec9-9a1d-f8da5103f9e0" satisfied condition "Succeeded or Failed"
Aug 30 16:13:46.725: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-c0b7d8fb-0311-4ec9-9a1d-f8da5103f9e0 container client-container: <nil>
STEP: delete the pod
Aug 30 16:13:46.802: INFO: Waiting for pod downwardapi-volume-c0b7d8fb-0311-4ec9-9a1d-f8da5103f9e0 to disappear
Aug 30 16:13:46.815: INFO: Pod downwardapi-volume-c0b7d8fb-0311-4ec9-9a1d-f8da5103f9e0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 16:13:46.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4" for this suite.

• [SLOW TEST:6.377 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":320,"skipped":5891,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:13:46.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 16:13:47.101: INFO: created pod pod-service-account-defaultsa
W0830 16:13:47.101389      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:13:47.101: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 30 16:13:47.160: INFO: created pod pod-service-account-mountsa
Aug 30 16:13:47.160: INFO: pod pod-service-account-mountsa service account token volume mount: true
W0830 16:13:47.160282      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 16:13:47.209121      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:13:47.211: INFO: created pod pod-service-account-nomountsa
Aug 30 16:13:47.211: INFO: pod pod-service-account-nomountsa service account token volume mount: false
W0830 16:13:47.256567      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:13:47.256: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 30 16:13:47.256: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 30 16:13:47.313: INFO: created pod pod-service-account-mountsa-mountspec
Aug 30 16:13:47.313: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
W0830 16:13:47.313281      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 16:13:47.393819      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:13:47.394: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 30 16:13:47.394: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
W0830 16:13:47.449453      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:13:47.449: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 30 16:13:47.449: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
W0830 16:13:47.592363      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:13:47.592: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 30 16:13:47.593: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
W0830 16:13:47.646687      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:13:47.647: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 30 16:13:47.647: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 30 16:13:47.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1558" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":321,"skipped":5901,"failed":0}
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:13:47.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Aug 30 16:13:47.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
W0830 16:13:48.535641      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "sample-apiserver", "etcd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "sample-apiserver", "etcd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "sample-apiserver", "etcd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "sample-apiserver", "etcd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:13:48.567: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 30 16:13:50.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:13:52.786: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:13:54.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:13:56.781: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:13:58.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:14:00.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:14:02.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:14:04.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:14:06.785: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:14:08.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:14:10.798: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 13, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 16:14:15.868: INFO: Waited 3.062644172s for the sample-apiserver to be ready to handle requests.
I0830 16:14:17.048791      21 request.go:601] Waited for 1.043502516s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/performance.openshift.io/v2
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Aug 30 16:14:17.857: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Aug 30 16:14:18.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7786" for this suite.

• [SLOW TEST:30.935 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":322,"skipped":5904,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:14:18.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-6820
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 30 16:14:18.768: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
W0830 16:14:18.862491      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 16:14:18.958027      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 16:14:19.006297      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:14:19.023: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:14:21.055: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:14:23.038: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:14:25.037: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:14:27.040: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:14:29.041: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:14:31.038: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:14:33.038: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:14:35.041: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:14:37.044: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:14:39.040: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 30 16:14:41.042: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 30 16:14:41.073: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 30 16:14:41.106: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
W0830 16:14:41.176246      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:14:43.216: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 30 16:14:43.216: INFO: Breadth first check of 172.30.78.41 on host 10.63.224.158...
Aug 30 16:14:43.231: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.233.230:9080/dial?request=hostname&protocol=http&host=172.30.78.41&port=8083&tries=1'] Namespace:pod-network-test-6820 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 16:14:43.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 16:14:43.233: INFO: ExecWithOptions: Clientset creation
Aug 30 16:14:43.233: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6820/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.233.230%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.78.41%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 16:14:43.698: INFO: Waiting for responses: map[]
Aug 30 16:14:43.698: INFO: reached 172.30.78.41 after 0/1 tries
Aug 30 16:14:43.698: INFO: Breadth first check of 172.30.38.217 on host 10.63.224.187...
Aug 30 16:14:43.729: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.233.230:9080/dial?request=hostname&protocol=http&host=172.30.38.217&port=8083&tries=1'] Namespace:pod-network-test-6820 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 16:14:43.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 16:14:43.732: INFO: ExecWithOptions: Clientset creation
Aug 30 16:14:43.733: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6820/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.233.230%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.38.217%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 16:14:44.298: INFO: Waiting for responses: map[]
Aug 30 16:14:44.298: INFO: reached 172.30.38.217 after 0/1 tries
Aug 30 16:14:44.298: INFO: Breadth first check of 172.30.233.205 on host 10.63.224.189...
Aug 30 16:14:44.315: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.233.230:9080/dial?request=hostname&protocol=http&host=172.30.233.205&port=8083&tries=1'] Namespace:pod-network-test-6820 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 16:14:44.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 16:14:44.316: INFO: ExecWithOptions: Clientset creation
Aug 30 16:14:44.316: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6820/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.233.230%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.233.205%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 16:14:44.568: INFO: Waiting for responses: map[]
Aug 30 16:14:44.568: INFO: reached 172.30.233.205 after 0/1 tries
Aug 30 16:14:44.568: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 30 16:14:44.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6820" for this suite.

• [SLOW TEST:25.954 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":323,"skipped":5909,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:14:44.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-2878/configmap-test-623c2d1e-9a8e-41fa-b1a4-af35663799f3
STEP: Creating a pod to test consume configMaps
Aug 30 16:14:44.824: INFO: Waiting up to 5m0s for pod "pod-configmaps-e44f9ba9-94f6-47ff-acbf-e39fb16c735f" in namespace "configmap-2878" to be "Succeeded or Failed"
W0830 16:14:44.824104      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:14:44.838: INFO: Pod "pod-configmaps-e44f9ba9-94f6-47ff-acbf-e39fb16c735f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.110791ms
Aug 30 16:14:46.855: INFO: Pod "pod-configmaps-e44f9ba9-94f6-47ff-acbf-e39fb16c735f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031409454s
Aug 30 16:14:48.871: INFO: Pod "pod-configmaps-e44f9ba9-94f6-47ff-acbf-e39fb16c735f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046754518s
Aug 30 16:14:50.884: INFO: Pod "pod-configmaps-e44f9ba9-94f6-47ff-acbf-e39fb16c735f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059757112s
STEP: Saw pod success
Aug 30 16:14:50.884: INFO: Pod "pod-configmaps-e44f9ba9-94f6-47ff-acbf-e39fb16c735f" satisfied condition "Succeeded or Failed"
Aug 30 16:14:50.894: INFO: Trying to get logs from node 10.63.224.189 pod pod-configmaps-e44f9ba9-94f6-47ff-acbf-e39fb16c735f container env-test: <nil>
STEP: delete the pod
Aug 30 16:14:50.965: INFO: Waiting for pod pod-configmaps-e44f9ba9-94f6-47ff-acbf-e39fb16c735f to disappear
Aug 30 16:14:50.976: INFO: Pod pod-configmaps-e44f9ba9-94f6-47ff-acbf-e39fb16c735f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 30 16:14:50.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2878" for this suite.

• [SLOW TEST:6.412 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":324,"skipped":5928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:14:51.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-200a5e2b-a860-4da9-97f4-106c78edb3c6
STEP: Creating a pod to test consume configMaps
W0830 16:14:51.292184      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:14:51.292: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b17ff2dc-76a2-42ce-9529-af23d2f220d6" in namespace "projected-5260" to be "Succeeded or Failed"
Aug 30 16:14:51.303: INFO: Pod "pod-projected-configmaps-b17ff2dc-76a2-42ce-9529-af23d2f220d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.717296ms
Aug 30 16:14:53.322: INFO: Pod "pod-projected-configmaps-b17ff2dc-76a2-42ce-9529-af23d2f220d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030012946s
Aug 30 16:14:55.341: INFO: Pod "pod-projected-configmaps-b17ff2dc-76a2-42ce-9529-af23d2f220d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048967711s
Aug 30 16:14:57.355: INFO: Pod "pod-projected-configmaps-b17ff2dc-76a2-42ce-9529-af23d2f220d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063141528s
STEP: Saw pod success
Aug 30 16:14:57.355: INFO: Pod "pod-projected-configmaps-b17ff2dc-76a2-42ce-9529-af23d2f220d6" satisfied condition "Succeeded or Failed"
Aug 30 16:14:57.366: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-configmaps-b17ff2dc-76a2-42ce-9529-af23d2f220d6 container agnhost-container: <nil>
STEP: delete the pod
Aug 30 16:14:57.515: INFO: Waiting for pod pod-projected-configmaps-b17ff2dc-76a2-42ce-9529-af23d2f220d6 to disappear
Aug 30 16:14:57.527: INFO: Pod pod-projected-configmaps-b17ff2dc-76a2-42ce-9529-af23d2f220d6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 30 16:14:57.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5260" for this suite.

• [SLOW TEST:6.549 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":325,"skipped":5964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:14:57.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
W0830 16:14:57.743302      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:14:57.743: INFO: Waiting up to 5m0s for pod "pod-b317e34d-55a4-4dd6-8f98-de080e1b04dd" in namespace "emptydir-2475" to be "Succeeded or Failed"
Aug 30 16:14:57.756: INFO: Pod "pod-b317e34d-55a4-4dd6-8f98-de080e1b04dd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.168639ms
Aug 30 16:14:59.771: INFO: Pod "pod-b317e34d-55a4-4dd6-8f98-de080e1b04dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028012014s
Aug 30 16:15:01.799: INFO: Pod "pod-b317e34d-55a4-4dd6-8f98-de080e1b04dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056302776s
Aug 30 16:15:03.816: INFO: Pod "pod-b317e34d-55a4-4dd6-8f98-de080e1b04dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073109437s
STEP: Saw pod success
Aug 30 16:15:03.816: INFO: Pod "pod-b317e34d-55a4-4dd6-8f98-de080e1b04dd" satisfied condition "Succeeded or Failed"
Aug 30 16:15:03.833: INFO: Trying to get logs from node 10.63.224.189 pod pod-b317e34d-55a4-4dd6-8f98-de080e1b04dd container test-container: <nil>
STEP: delete the pod
Aug 30 16:15:03.918: INFO: Waiting for pod pod-b317e34d-55a4-4dd6-8f98-de080e1b04dd to disappear
Aug 30 16:15:03.932: INFO: Pod pod-b317e34d-55a4-4dd6-8f98-de080e1b04dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 30 16:15:03.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2475" for this suite.

• [SLOW TEST:6.386 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":326,"skipped":6048,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:15:03.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 16:15:04.875379      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 16:15:04.934: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 16:15:06.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 15, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 15, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 15, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 15, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:15:10.054: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 16:15:10.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6288-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 16:15:13.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5380" for this suite.
STEP: Destroying namespace "webhook-5380-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:10.126 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":327,"skipped":6077,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:15:14.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-6b56ccef-f1dd-442a-8982-be3dd20d4480
STEP: Creating a pod to test consume configMaps
W0830 16:15:14.475297      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:15:14.475: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7bb212f8-761b-43e4-8b13-4e56596c057d" in namespace "projected-6087" to be "Succeeded or Failed"
Aug 30 16:15:14.488: INFO: Pod "pod-projected-configmaps-7bb212f8-761b-43e4-8b13-4e56596c057d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.058388ms
Aug 30 16:15:16.508: INFO: Pod "pod-projected-configmaps-7bb212f8-761b-43e4-8b13-4e56596c057d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032399629s
Aug 30 16:15:18.525: INFO: Pod "pod-projected-configmaps-7bb212f8-761b-43e4-8b13-4e56596c057d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049245416s
Aug 30 16:15:20.579: INFO: Pod "pod-projected-configmaps-7bb212f8-761b-43e4-8b13-4e56596c057d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.103590529s
STEP: Saw pod success
Aug 30 16:15:20.580: INFO: Pod "pod-projected-configmaps-7bb212f8-761b-43e4-8b13-4e56596c057d" satisfied condition "Succeeded or Failed"
Aug 30 16:15:20.613: INFO: Trying to get logs from node 10.63.224.189 pod pod-projected-configmaps-7bb212f8-761b-43e4-8b13-4e56596c057d container agnhost-container: <nil>
STEP: delete the pod
Aug 30 16:15:20.699: INFO: Waiting for pod pod-projected-configmaps-7bb212f8-761b-43e4-8b13-4e56596c057d to disappear
Aug 30 16:15:20.713: INFO: Pod pod-projected-configmaps-7bb212f8-761b-43e4-8b13-4e56596c057d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 30 16:15:20.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6087" for this suite.

• [SLOW TEST:6.705 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":328,"skipped":6097,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:15:20.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
W0830 16:15:20.946829      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: When the matched label of one of its pods change
Aug 30 16:15:20.959: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 30 16:15:25.971: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 30 16:15:27.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5362" for this suite.

• [SLOW TEST:6.302 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":329,"skipped":6123,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:15:27.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 16:15:27.217: INFO: Creating pod...
W0830 16:15:27.276480      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:15:31.307: INFO: Creating service...
Aug 30 16:15:31.359: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/pods/agnhost/proxy/some/path/with/DELETE
Aug 30 16:15:31.390: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 30 16:15:31.390: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/pods/agnhost/proxy/some/path/with/GET
Aug 30 16:15:31.410: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 30 16:15:31.410: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/pods/agnhost/proxy/some/path/with/HEAD
Aug 30 16:15:31.433: INFO: http.Client request:HEAD | StatusCode:200
Aug 30 16:15:31.433: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 30 16:15:31.454: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 30 16:15:31.454: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/pods/agnhost/proxy/some/path/with/PATCH
Aug 30 16:15:31.480: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 30 16:15:31.480: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/pods/agnhost/proxy/some/path/with/POST
Aug 30 16:15:31.500: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 30 16:15:31.500: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/pods/agnhost/proxy/some/path/with/PUT
Aug 30 16:15:31.528: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 30 16:15:31.528: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/services/test-service/proxy/some/path/with/DELETE
Aug 30 16:15:31.584: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 30 16:15:31.584: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/services/test-service/proxy/some/path/with/GET
Aug 30 16:15:31.619: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 30 16:15:31.619: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/services/test-service/proxy/some/path/with/HEAD
Aug 30 16:15:31.645: INFO: http.Client request:HEAD | StatusCode:200
Aug 30 16:15:31.645: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/services/test-service/proxy/some/path/with/OPTIONS
Aug 30 16:15:31.676: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 30 16:15:31.676: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/services/test-service/proxy/some/path/with/PATCH
Aug 30 16:15:31.714: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 30 16:15:31.714: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/services/test-service/proxy/some/path/with/POST
Aug 30 16:15:31.742: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 30 16:15:31.742: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8053/services/test-service/proxy/some/path/with/PUT
Aug 30 16:15:31.788: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Aug 30 16:15:31.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8053" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":330,"skipped":6136,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:15:31.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 30 16:15:31.986: INFO: PodSpec: initContainers in spec.initContainers
W0830 16:15:32.062459      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 30 16:15:39.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3334" for this suite.

• [SLOW TEST:7.203 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":331,"skipped":6152,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:15:39.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
W0830 16:15:39.221642      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 30 16:15:40.121: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0830 16:15:40.121267      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 30 16:15:40.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4399" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":332,"skipped":6169,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:15:40.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-hfwm
STEP: Creating a pod to test atomic-volume-subpath
Aug 30 16:15:40.436: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hfwm" in namespace "subpath-8338" to be "Succeeded or Failed"
W0830 16:15:40.436212      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container-subpath-configmap-hfwm" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-configmap-hfwm" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-configmap-hfwm" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-configmap-hfwm" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:15:40.446: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Pending", Reason="", readiness=false. Elapsed: 9.852432ms
Aug 30 16:15:42.461: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024514422s
Aug 30 16:15:44.479: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=true. Elapsed: 4.043259583s
Aug 30 16:15:46.495: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=true. Elapsed: 6.058647962s
Aug 30 16:15:48.509: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=true. Elapsed: 8.073219742s
Aug 30 16:15:50.525: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=true. Elapsed: 10.088799161s
Aug 30 16:15:52.540: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=true. Elapsed: 12.103723962s
Aug 30 16:15:54.565: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=true. Elapsed: 14.129017279s
Aug 30 16:15:56.576: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=true. Elapsed: 16.139753992s
Aug 30 16:15:58.595: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=true. Elapsed: 18.159343292s
Aug 30 16:16:00.612: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=true. Elapsed: 20.176254517s
Aug 30 16:16:02.624: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=true. Elapsed: 22.188023874s
Aug 30 16:16:04.640: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Running", Reason="", readiness=false. Elapsed: 24.204085657s
Aug 30 16:16:06.661: INFO: Pod "pod-subpath-test-configmap-hfwm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.225310767s
STEP: Saw pod success
Aug 30 16:16:06.662: INFO: Pod "pod-subpath-test-configmap-hfwm" satisfied condition "Succeeded or Failed"
Aug 30 16:16:06.677: INFO: Trying to get logs from node 10.63.224.189 pod pod-subpath-test-configmap-hfwm container test-container-subpath-configmap-hfwm: <nil>
STEP: delete the pod
Aug 30 16:16:06.798: INFO: Waiting for pod pod-subpath-test-configmap-hfwm to disappear
Aug 30 16:16:06.811: INFO: Pod pod-subpath-test-configmap-hfwm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hfwm
Aug 30 16:16:06.811: INFO: Deleting pod "pod-subpath-test-configmap-hfwm" in namespace "subpath-8338"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 30 16:16:06.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8338" for this suite.

• [SLOW TEST:26.688 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":333,"skipped":6170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:16:06.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 30 16:16:06.986: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 16:16:07.034: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 16:16:07.074: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.158 before test
Aug 30 16:16:07.187: INFO: calico-kube-controllers-867bb5b44d-wqzpr from calico-system started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 30 16:16:07.187: INFO: calico-node-rfpmd from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:16:07.187: INFO: ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-q4rqr from ibm-system started at 2022-08-30 12:17:08 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container ibm-cloud-provider-ip-130-198-93-237 ready: true, restart count 0
Aug 30 16:16:07.187: INFO: ibm-file-plugin-ccc878f48-frwb7 from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 30 16:16:07.187: INFO: ibm-keepalived-watcher-c2b7b from kube-system started at 2022-08-30 12:07:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 16:16:07.187: INFO: ibm-master-proxy-static-10.63.224.158 from kube-system started at 2022-08-30 12:07:30 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 16:16:07.187: INFO: 	Container pause ready: true, restart count 0
Aug 30 16:16:07.187: INFO: ibm-storage-watcher-6fcfdb7ccc-5s2xq from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 30 16:16:07.187: INFO: ibmcloud-block-storage-driver-kx4tx from kube-system started at 2022-08-30 12:07:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 16:16:07.187: INFO: ibmcloud-block-storage-plugin-7495f48b76-lht72 from kube-system started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 30 16:16:07.187: INFO: vpn-59795d4f7c-rf6fw from kube-system started at 2022-08-30 12:18:35 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container vpn ready: true, restart count 0
Aug 30 16:16:07.187: INFO: cluster-node-tuning-operator-6c4d4b46dd-dsdts from openshift-cluster-node-tuning-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 30 16:16:07.187: INFO: tuned-mpvrs from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container tuned ready: true, restart count 0
Aug 30 16:16:07.187: INFO: cluster-samples-operator-79cb65b9b5-jbclm from openshift-cluster-samples-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 30 16:16:07.187: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 30 16:16:07.187: INFO: cluster-storage-operator-97dcf6b44-krf6f from openshift-cluster-storage-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 30 16:16:07.187: INFO: csi-snapshot-controller-7d8bf4bb58-6lkk4 from openshift-cluster-storage-operator started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 16:16:07.187: INFO: csi-snapshot-controller-operator-7b97dc6dfd-xjp62 from openshift-cluster-storage-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 30 16:16:07.187: INFO: csi-snapshot-webhook-8f4fd6cc6-48czg from openshift-cluster-storage-operator started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container webhook ready: true, restart count 0
Aug 30 16:16:07.187: INFO: console-operator-55bcdb7b48-tgxd4 from openshift-console-operator started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container console-operator ready: true, restart count 1
Aug 30 16:16:07.187: INFO: console-5dd7474d84-js5g9 from openshift-console started at 2022-08-30 12:14:31 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container console ready: true, restart count 0
Aug 30 16:16:07.187: INFO: downloads-6f74f6fcbf-lg88b from openshift-console started at 2022-08-30 16:04:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.187: INFO: 	Container download-server ready: true, restart count 0
Aug 30 16:16:07.188: INFO: dns-operator-5549dbd7c9-c4h6t from openshift-dns-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container dns-operator ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.188: INFO: dns-default-v6jl9 from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container dns ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.188: INFO: node-resolver-ppzz9 from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 16:16:07.188: INFO: cluster-image-registry-operator-cb6448756-qh2wd from openshift-image-registry started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 30 16:16:07.188: INFO: image-registry-7fb4f45578-scb5q from openshift-image-registry started at 2022-08-30 12:18:40 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container registry ready: true, restart count 0
Aug 30 16:16:07.188: INFO: node-ca-4pbx7 from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 16:16:07.188: INFO: ingress-canary-vwqf6 from openshift-ingress-canary started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 16:16:07.188: INFO: ingress-operator-57457886bd-v8vkg from openshift-ingress-operator started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.188: INFO: router-default-bdd65b74f-jwc99 from openshift-ingress started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container router ready: true, restart count 0
Aug 30 16:16:07.188: INFO: insights-operator-6d9b46b7c5-6q6pl from openshift-insights started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container insights-operator ready: true, restart count 1
Aug 30 16:16:07.188: INFO: openshift-kube-proxy-ms7hb from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.188: INFO: kube-storage-version-migrator-operator-fb46b8c8d-n5q2j from openshift-kube-storage-version-migrator-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Aug 30 16:16:07.188: INFO: marketplace-operator-84ff65f9c9-mcwn4 from openshift-marketplace started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 30 16:16:07.188: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-08-30 16:04:13 +0000 UTC (6 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container alertmanager ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 16:16:07.188: INFO: cluster-monitoring-operator-677fb4cf4b-wfrk9 from openshift-monitoring started at 2022-08-30 12:09:18 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.188: INFO: kube-state-metrics-b6455c4dc-pknn6 from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (3 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 30 16:16:07.188: INFO: node-exporter-ct2nk from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.188: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 16:16:07.188: INFO: prometheus-adapter-67874b74f7-2kkcq from openshift-monitoring started at 2022-08-30 12:14:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.188: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 16:16:07.188: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-08-30 16:04:13 +0000 UTC (6 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 16:16:07.189: INFO: prometheus-operator-admission-webhook-f5f88b968-wkmnq from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 16:16:07.189: INFO: thanos-querier-75bdcf8599-tdj9h from openshift-monitoring started at 2022-08-30 14:52:49 +0000 UTC (6 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 16:16:07.189: INFO: multus-additional-cni-plugins-fwxxj from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 16:16:07.189: INFO: multus-admission-controller-gvnw7 from openshift-multus started at 2022-08-30 12:09:19 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 16:16:07.189: INFO: multus-n2s54 from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 16:16:07.189: INFO: network-metrics-daemon-ncgdn from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 16:16:07.189: INFO: network-check-source-5cb989cf6f-8pbqc from openshift-network-diagnostics started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 30 16:16:07.189: INFO: network-check-target-tqzgf from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 16:16:07.189: INFO: catalog-operator-5d9dd4bb98-hwstf from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 30 16:16:07.189: INFO: olm-operator-757497677b-z5lbc from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container olm-operator ready: true, restart count 0
Aug 30 16:16:07.189: INFO: package-server-manager-784548687f-9vg2b from openshift-operator-lifecycle-manager started at 2022-08-30 12:09:19 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container package-server-manager ready: true, restart count 0
Aug 30 16:16:07.189: INFO: packageserver-b5cbb5ff5-2vxb9 from openshift-operator-lifecycle-manager started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 16:16:07.189: INFO: metrics-67dbdb4ffd-gj9lp from openshift-roks-metrics started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container metrics ready: true, restart count 3
Aug 30 16:16:07.189: INFO: push-gateway-58dc4cbdf8-kl5x7 from openshift-roks-metrics started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container push-gateway ready: true, restart count 0
Aug 30 16:16:07.189: INFO: service-ca-operator-5df8fbb45b-fz8bw from openshift-service-ca-operator started at 2022-08-30 12:09:18 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container service-ca-operator ready: true, restart count 1
Aug 30 16:16:07.189: INFO: service-ca-5f4d84b84b-jwhj4 from openshift-service-ca started at 2022-08-30 16:04:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container service-ca-controller ready: false, restart count 0
Aug 30 16:16:07.189: INFO: sonobuoy-e2e-job-a8c1c5c47c2346af from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container e2e ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:16:07.189: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-fngwd from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:16:07.189: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 16:16:07.189: INFO: tigera-operator-7b76886f74-5rs97 from tigera-operator started at 2022-08-30 12:07:46 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.189: INFO: 	Container tigera-operator ready: true, restart count 2
Aug 30 16:16:07.189: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.187 before test
Aug 30 16:16:07.304: INFO: calico-node-bv858 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.304: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:16:07.305: INFO: calico-typha-66986f6f4d-mg7gw from calico-system started at 2022-08-30 12:08:59 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.305: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 16:16:07.305: INFO: ibm-cloud-provider-ip-130-198-93-237-6fc97678b5-gq5c7 from ibm-system started at 2022-08-30 12:17:08 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.305: INFO: 	Container ibm-cloud-provider-ip-130-198-93-237 ready: true, restart count 0
Aug 30 16:16:07.305: INFO: ibm-keepalived-watcher-7czrz from kube-system started at 2022-08-30 12:07:36 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.305: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 16:16:07.305: INFO: ibm-master-proxy-static-10.63.224.187 from kube-system started at 2022-08-30 12:07:34 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.305: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 16:16:07.306: INFO: 	Container pause ready: true, restart count 0
Aug 30 16:16:07.306: INFO: ibmcloud-block-storage-driver-m5jbh from kube-system started at 2022-08-30 12:07:42 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.306: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 16:16:07.306: INFO: tuned-c2ph5 from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.306: INFO: 	Container tuned ready: true, restart count 0
Aug 30 16:16:07.306: INFO: csi-snapshot-controller-7d8bf4bb58-gv7xl from openshift-cluster-storage-operator started at 2022-08-30 12:10:04 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.306: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 16:16:07.306: INFO: csi-snapshot-webhook-8f4fd6cc6-pnk9x from openshift-cluster-storage-operator started at 2022-08-30 12:10:00 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.306: INFO: 	Container webhook ready: true, restart count 0
Aug 30 16:16:07.306: INFO: console-5dd7474d84-t6lg4 from openshift-console started at 2022-08-30 14:52:49 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.307: INFO: 	Container console ready: true, restart count 0
Aug 30 16:16:07.307: INFO: downloads-6f74f6fcbf-sbj8k from openshift-console started at 2022-08-30 12:10:07 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.307: INFO: 	Container download-server ready: true, restart count 0
Aug 30 16:16:07.307: INFO: dns-default-dltcr from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.307: INFO: 	Container dns ready: true, restart count 0
Aug 30 16:16:07.307: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.307: INFO: node-resolver-n66mp from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.307: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 16:16:07.307: INFO: node-ca-9bj8c from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.307: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 16:16:07.308: INFO: registry-pvc-permissions-2pnhd from openshift-image-registry started at 2022-08-30 12:18:48 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.308: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 30 16:16:07.308: INFO: ingress-canary-zjwth from openshift-ingress-canary started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.308: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 16:16:07.308: INFO: router-default-bdd65b74f-9xp2d from openshift-ingress started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.308: INFO: 	Container router ready: true, restart count 0
Aug 30 16:16:07.308: INFO: openshift-kube-proxy-whcnq from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.308: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 16:16:07.308: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.309: INFO: migrator-59f7fcfc8f-l77kl from openshift-kube-storage-version-migrator started at 2022-08-30 12:10:00 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.309: INFO: 	Container migrator ready: true, restart count 0
Aug 30 16:16:07.309: INFO: certified-operators-t6598 from openshift-marketplace started at 2022-08-30 13:47:48 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.309: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 16:16:07.309: INFO: community-operators-7q78j from openshift-marketplace started at 2022-08-30 16:15:38 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.309: INFO: 	Container registry-server ready: false, restart count 0
Aug 30 16:16:07.309: INFO: community-operators-btnw8 from openshift-marketplace started at 2022-08-30 12:11:53 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.310: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 16:16:07.310: INFO: redhat-marketplace-9rltc from openshift-marketplace started at 2022-08-30 12:11:54 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.310: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 16:16:07.310: INFO: redhat-operators-cnsks from openshift-marketplace started at 2022-08-30 12:11:53 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.310: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 16:16:07.310: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-08-30 12:14:46 +0000 UTC (6 container statuses recorded)
Aug 30 16:16:07.310: INFO: 	Container alertmanager ready: true, restart count 0
Aug 30 16:16:07.310: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 16:16:07.310: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 16:16:07.311: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.311: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 16:16:07.311: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 16:16:07.311: INFO: node-exporter-gls7c from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.311: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.311: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 16:16:07.311: INFO: openshift-state-metrics-7984888fbd-dgxfd from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (3 container statuses recorded)
Aug 30 16:16:07.311: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 16:16:07.312: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 16:16:07.312: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 30 16:16:07.312: INFO: prometheus-adapter-67874b74f7-xwhkv from openshift-monitoring started at 2022-08-30 12:14:39 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.312: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 16:16:07.312: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-08-30 12:15:07 +0000 UTC (6 container statuses recorded)
Aug 30 16:16:07.312: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 16:16:07.312: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.312: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 16:16:07.312: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 16:16:07.312: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 16:16:07.312: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 16:16:07.312: INFO: prometheus-operator-admission-webhook-f5f88b968-wjzg5 from openshift-monitoring started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.313: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 16:16:07.313: INFO: prometheus-operator-dd6c54897-9z7dk from openshift-monitoring started at 2022-08-30 16:04:09 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.313: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.313: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 30 16:16:07.313: INFO: telemeter-client-dd46ccd5b-9t976 from openshift-monitoring started at 2022-08-30 16:04:09 +0000 UTC (3 container statuses recorded)
Aug 30 16:16:07.313: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.313: INFO: 	Container reload ready: true, restart count 0
Aug 30 16:16:07.313: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 30 16:16:07.313: INFO: thanos-querier-75bdcf8599-tb7gh from openshift-monitoring started at 2022-08-30 12:14:46 +0000 UTC (6 container statuses recorded)
Aug 30 16:16:07.313: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.313: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 16:16:07.313: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 16:16:07.313: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 16:16:07.313: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 16:16:07.313: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 16:16:07.313: INFO: multus-additional-cni-plugins-8j4kx from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.313: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 16:16:07.314: INFO: multus-admission-controller-xzspk from openshift-multus started at 2022-08-30 12:09:28 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.314: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.314: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 16:16:07.314: INFO: multus-qpmzg from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.314: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 16:16:07.314: INFO: network-metrics-daemon-nlcxx from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.314: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.314: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 16:16:07.314: INFO: network-check-target-wrm4t from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.314: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 16:16:07.314: INFO: network-operator-55b69485bb-qth5v from openshift-network-operator started at 2022-08-30 12:07:47 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.314: INFO: 	Container network-operator ready: true, restart count 1
Aug 30 16:16:07.314: INFO: collect-profiles-27697905-dflw2 from openshift-operator-lifecycle-manager started at 2022-08-30 15:45:00 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.315: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 16:16:07.315: INFO: collect-profiles-27697920-qbx4p from openshift-operator-lifecycle-manager started at 2022-08-30 16:00:00 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.315: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 16:16:07.315: INFO: collect-profiles-27697935-pdt4x from openshift-operator-lifecycle-manager started at 2022-08-30 16:15:00 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.315: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 16:16:07.315: INFO: packageserver-b5cbb5ff5-nc6jk from openshift-operator-lifecycle-manager started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.315: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 16:16:07.315: INFO: sonobuoy from sonobuoy started at 2022-08-30 14:26:02 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.315: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 16:16:07.316: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-7b4kf from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.316: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:16:07.316: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 16:16:07.316: INFO: 
Logging pods the apiserver thinks is on node 10.63.224.189 before test
Aug 30 16:16:07.443: INFO: calico-node-g5sh7 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 16:16:07.444: INFO: calico-typha-66986f6f4d-rd6m6 from calico-system started at 2022-08-30 12:08:51 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 16:16:07.444: INFO: ibm-keepalived-watcher-ctfdm from kube-system started at 2022-08-30 12:08:00 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 16:16:07.444: INFO: ibm-master-proxy-static-10.63.224.189 from kube-system started at 2022-08-30 12:07:47 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 16:16:07.444: INFO: 	Container pause ready: true, restart count 0
Aug 30 16:16:07.444: INFO: ibmcloud-block-storage-driver-xl7s6 from kube-system started at 2022-08-30 12:08:07 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 16:16:07.444: INFO: tuned-p2mrf from openshift-cluster-node-tuning-operator started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container tuned ready: true, restart count 0
Aug 30 16:16:07.444: INFO: dns-default-jkgss from openshift-dns started at 2022-08-30 16:04:36 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container dns ready: true, restart count 0
Aug 30 16:16:07.444: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.444: INFO: node-resolver-9tslx from openshift-dns started at 2022-08-30 12:13:15 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 16:16:07.444: INFO: node-ca-mq7hh from openshift-image-registry started at 2022-08-30 12:13:16 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 16:16:07.444: INFO: ingress-canary-mzhjj from openshift-ingress-canary started at 2022-08-30 16:04:36 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 16:16:07.444: INFO: openshift-kube-proxy-5zfxw from openshift-kube-proxy started at 2022-08-30 12:08:14 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 16:16:07.444: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.444: INFO: node-exporter-9w9vp from openshift-monitoring started at 2022-08-30 12:13:42 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.444: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 16:16:07.444: INFO: multus-additional-cni-plugins-gnz55 from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 16:16:07.444: INFO: multus-admission-controller-b9vjn from openshift-multus started at 2022-08-30 16:04:40 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.444: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 16:16:07.444: INFO: multus-nzfsd from openshift-multus started at 2022-08-30 12:08:09 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 16:16:07.444: INFO: network-metrics-daemon-hzwd2 from openshift-multus started at 2022-08-30 12:08:10 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 16:16:07.444: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 16:16:07.444: INFO: network-check-target-whfqq from openshift-network-diagnostics started at 2022-08-30 12:08:17 +0000 UTC (1 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 16:16:07.444: INFO: sonobuoy-systemd-logs-daemon-set-0fd8f604687d4ecc-x4jz5 from sonobuoy started at 2022-08-30 14:26:15 +0000 UTC (2 container statuses recorded)
Aug 30 16:16:07.444: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 16:16:07.444: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
W0830 16:16:07.567749      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "without-label" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "without-label" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "without-label" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "without-label" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0bda3394-f201-4e31-8277-b83e8835325c 42
STEP: Trying to relaunch the pod, now with labels.
W0830 16:16:11.776902      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "with-labels" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "with-labels" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "with-labels" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "with-labels" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: removing the label kubernetes.io/e2e-0bda3394-f201-4e31-8277-b83e8835325c off the node 10.63.224.189
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0bda3394-f201-4e31-8277-b83e8835325c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 30 16:16:15.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5717" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:9.075 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":334,"skipped":6196,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:16:15.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 16:16:16.124: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f2be556f-bab4-4b47-a812-88e8a752ff70" in namespace "security-context-test-517" to be "Succeeded or Failed"
W0830 16:16:16.124126      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-readonly-false-f2be556f-bab4-4b47-a812-88e8a752ff70" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-false-f2be556f-bab4-4b47-a812-88e8a752ff70" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-false-f2be556f-bab4-4b47-a812-88e8a752ff70" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-false-f2be556f-bab4-4b47-a812-88e8a752ff70" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:16:16.148: INFO: Pod "busybox-readonly-false-f2be556f-bab4-4b47-a812-88e8a752ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 23.815684ms
Aug 30 16:16:18.163: INFO: Pod "busybox-readonly-false-f2be556f-bab4-4b47-a812-88e8a752ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039111147s
Aug 30 16:16:20.183: INFO: Pod "busybox-readonly-false-f2be556f-bab4-4b47-a812-88e8a752ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059077622s
Aug 30 16:16:22.199: INFO: Pod "busybox-readonly-false-f2be556f-bab4-4b47-a812-88e8a752ff70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074442042s
Aug 30 16:16:22.199: INFO: Pod "busybox-readonly-false-f2be556f-bab4-4b47-a812-88e8a752ff70" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 30 16:16:22.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-517" for this suite.

• [SLOW TEST:6.302 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:173
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":335,"skipped":6201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:16:22.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Aug 30 16:16:22.436: INFO: Waiting up to 5m0s for pod "security-context-47688b48-4f34-41ad-8eb9-4dda3ca6d29c" in namespace "security-context-2047" to be "Succeeded or Failed"
W0830 16:16:22.436046      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:16:22.450: INFO: Pod "security-context-47688b48-4f34-41ad-8eb9-4dda3ca6d29c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.113009ms
Aug 30 16:16:24.468: INFO: Pod "security-context-47688b48-4f34-41ad-8eb9-4dda3ca6d29c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032335327s
Aug 30 16:16:26.487: INFO: Pod "security-context-47688b48-4f34-41ad-8eb9-4dda3ca6d29c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050958038s
Aug 30 16:16:28.503: INFO: Pod "security-context-47688b48-4f34-41ad-8eb9-4dda3ca6d29c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067161345s
STEP: Saw pod success
Aug 30 16:16:28.503: INFO: Pod "security-context-47688b48-4f34-41ad-8eb9-4dda3ca6d29c" satisfied condition "Succeeded or Failed"
Aug 30 16:16:28.517: INFO: Trying to get logs from node 10.63.224.189 pod security-context-47688b48-4f34-41ad-8eb9-4dda3ca6d29c container test-container: <nil>
STEP: delete the pod
Aug 30 16:16:28.596: INFO: Waiting for pod security-context-47688b48-4f34-41ad-8eb9-4dda3ca6d29c to disappear
Aug 30 16:16:28.607: INFO: Pod security-context-47688b48-4f34-41ad-8eb9-4dda3ca6d29c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 30 16:16:28.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2047" for this suite.

• [SLOW TEST:6.410 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":336,"skipped":6283,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:16:28.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0830 16:16:28.957995      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 30 16:17:28.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3934" for this suite.

• [SLOW TEST:60.367 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":337,"skipped":6292,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:17:29.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 30 16:17:29.203: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c762777f-438b-40b0-9dc6-39db440b3bda" in namespace "projected-2592" to be "Succeeded or Failed"
W0830 16:17:29.202379      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:17:29.215: INFO: Pod "downwardapi-volume-c762777f-438b-40b0-9dc6-39db440b3bda": Phase="Pending", Reason="", readiness=false. Elapsed: 12.773989ms
Aug 30 16:17:31.233: INFO: Pod "downwardapi-volume-c762777f-438b-40b0-9dc6-39db440b3bda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030130132s
Aug 30 16:17:33.246: INFO: Pod "downwardapi-volume-c762777f-438b-40b0-9dc6-39db440b3bda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042931875s
Aug 30 16:17:35.260: INFO: Pod "downwardapi-volume-c762777f-438b-40b0-9dc6-39db440b3bda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056941933s
STEP: Saw pod success
Aug 30 16:17:35.260: INFO: Pod "downwardapi-volume-c762777f-438b-40b0-9dc6-39db440b3bda" satisfied condition "Succeeded or Failed"
Aug 30 16:17:35.272: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-c762777f-438b-40b0-9dc6-39db440b3bda container client-container: <nil>
STEP: delete the pod
Aug 30 16:17:35.344: INFO: Waiting for pod downwardapi-volume-c762777f-438b-40b0-9dc6-39db440b3bda to disappear
Aug 30 16:17:35.366: INFO: Pod downwardapi-volume-c762777f-438b-40b0-9dc6-39db440b3bda no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 30 16:17:35.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2592" for this suite.

• [SLOW TEST:6.388 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":338,"skipped":6300,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:17:35.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
W0830 16:17:35.578049      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:17:35.578: INFO: Waiting up to 5m0s for pod "security-context-e9413326-0b1c-46e5-b101-6a431e3475d5" in namespace "security-context-1112" to be "Succeeded or Failed"
Aug 30 16:17:35.597: INFO: Pod "security-context-e9413326-0b1c-46e5-b101-6a431e3475d5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.806051ms
Aug 30 16:17:37.615: INFO: Pod "security-context-e9413326-0b1c-46e5-b101-6a431e3475d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036802432s
Aug 30 16:17:39.631: INFO: Pod "security-context-e9413326-0b1c-46e5-b101-6a431e3475d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053301958s
STEP: Saw pod success
Aug 30 16:17:39.632: INFO: Pod "security-context-e9413326-0b1c-46e5-b101-6a431e3475d5" satisfied condition "Succeeded or Failed"
Aug 30 16:17:39.646: INFO: Trying to get logs from node 10.63.224.189 pod security-context-e9413326-0b1c-46e5-b101-6a431e3475d5 container test-container: <nil>
STEP: delete the pod
Aug 30 16:17:39.714: INFO: Waiting for pod security-context-e9413326-0b1c-46e5-b101-6a431e3475d5 to disappear
Aug 30 16:17:39.732: INFO: Pod security-context-e9413326-0b1c-46e5-b101-6a431e3475d5 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 30 16:17:39.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1112" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":339,"skipped":6317,"failed":0}
SSSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:17:39.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
W0830 16:17:39.921569      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template
W0830 16:17:39.962010      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:17:39.962: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Aug 30 16:17:39.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5067" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":340,"skipped":6321,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:17:40.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-8348d698-aba0-42c8-915c-15d24e336570
STEP: Creating a pod to test consume secrets
W0830 16:17:40.295970      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:17:40.296: INFO: Waiting up to 5m0s for pod "pod-secrets-1f8abe32-333b-4d9f-a56d-416712ced7da" in namespace "secrets-8715" to be "Succeeded or Failed"
Aug 30 16:17:40.308: INFO: Pod "pod-secrets-1f8abe32-333b-4d9f-a56d-416712ced7da": Phase="Pending", Reason="", readiness=false. Elapsed: 12.267489ms
Aug 30 16:17:42.325: INFO: Pod "pod-secrets-1f8abe32-333b-4d9f-a56d-416712ced7da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02874068s
Aug 30 16:17:44.345: INFO: Pod "pod-secrets-1f8abe32-333b-4d9f-a56d-416712ced7da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048979273s
Aug 30 16:17:46.363: INFO: Pod "pod-secrets-1f8abe32-333b-4d9f-a56d-416712ced7da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066689831s
STEP: Saw pod success
Aug 30 16:17:46.363: INFO: Pod "pod-secrets-1f8abe32-333b-4d9f-a56d-416712ced7da" satisfied condition "Succeeded or Failed"
Aug 30 16:17:46.379: INFO: Trying to get logs from node 10.63.224.189 pod pod-secrets-1f8abe32-333b-4d9f-a56d-416712ced7da container secret-volume-test: <nil>
STEP: delete the pod
Aug 30 16:17:46.458: INFO: Waiting for pod pod-secrets-1f8abe32-333b-4d9f-a56d-416712ced7da to disappear
Aug 30 16:17:46.469: INFO: Pod pod-secrets-1f8abe32-333b-4d9f-a56d-416712ced7da no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 30 16:17:46.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8715" for this suite.
STEP: Destroying namespace "secret-namespace-7218" for this suite.

• [SLOW TEST:6.532 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":341,"skipped":6337,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:17:46.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
W0830 16:17:46.739651      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:17:46.739: INFO: Waiting up to 5m0s for pod "test-pod-f1e6a135-c0dd-4a6c-88fe-07b759dc7d4b" in namespace "svcaccounts-9623" to be "Succeeded or Failed"
Aug 30 16:17:46.750: INFO: Pod "test-pod-f1e6a135-c0dd-4a6c-88fe-07b759dc7d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.366761ms
Aug 30 16:17:48.770: INFO: Pod "test-pod-f1e6a135-c0dd-4a6c-88fe-07b759dc7d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030890223s
Aug 30 16:17:50.786: INFO: Pod "test-pod-f1e6a135-c0dd-4a6c-88fe-07b759dc7d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046273917s
Aug 30 16:17:52.814: INFO: Pod "test-pod-f1e6a135-c0dd-4a6c-88fe-07b759dc7d4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074086895s
STEP: Saw pod success
Aug 30 16:17:52.814: INFO: Pod "test-pod-f1e6a135-c0dd-4a6c-88fe-07b759dc7d4b" satisfied condition "Succeeded or Failed"
Aug 30 16:17:52.846: INFO: Trying to get logs from node 10.63.224.189 pod test-pod-f1e6a135-c0dd-4a6c-88fe-07b759dc7d4b container agnhost-container: <nil>
STEP: delete the pod
Aug 30 16:17:52.998: INFO: Waiting for pod test-pod-f1e6a135-c0dd-4a6c-88fe-07b759dc7d4b to disappear
Aug 30 16:17:53.010: INFO: Pod test-pod-f1e6a135-c0dd-4a6c-88fe-07b759dc7d4b no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 30 16:17:53.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9623" for this suite.

• [SLOW TEST:6.558 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":342,"skipped":6350,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:17:53.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
W0830 16:17:53.214438      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5116, will wait for the garbage collector to delete the pods
Aug 30 16:17:57.327: INFO: Deleting Job.batch foo took: 28.692022ms
Aug 30 16:17:57.428: INFO: Terminating Job.batch foo pods took: 100.878195ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 30 16:18:29.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5116" for this suite.

• [SLOW TEST:36.919 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":343,"skipped":6352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:18:30.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
W0830 16:18:30.808633      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready
Aug 30 16:18:30.834: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 16:18:32.887: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 18, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 18, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 18, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 18, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 30 16:18:35.984: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 16:18:36.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8347-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 16:18:39.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8755" for this suite.
STEP: Destroying namespace "webhook-8755-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:9.707 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":344,"skipped":6380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:18:39.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-5329
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5329 to expose endpoints map[]
Aug 30 16:18:39.955: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Aug 30 16:18:41.002: INFO: successfully validated that service endpoint-test2 in namespace services-5329 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5329
W0830 16:18:41.061147      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:18:41.075: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:18:43.095: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5329 to expose endpoints map[pod1:[80]]
Aug 30 16:18:43.150: INFO: successfully validated that service endpoint-test2 in namespace services-5329 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Aug 30 16:18:43.150: INFO: Creating new exec pod
W0830 16:18:43.203937      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:18:48.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-5329 exec execpod7fwgt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 30 16:18:48.652: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 30 16:18:48.652: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 16:18:48.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-5329 exec execpod7fwgt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.88.60 80'
Aug 30 16:18:49.052: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 172.21.88.60 80\nConnection to 172.21.88.60 80 port [tcp/http] succeeded!\n"
Aug 30 16:18:49.052: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-5329
W0830 16:18:49.122755      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:18:49.141: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:18:51.162: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5329 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 30 16:18:51.243: INFO: successfully validated that service endpoint-test2 in namespace services-5329 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Aug 30 16:18:52.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-5329 exec execpod7fwgt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 30 16:18:52.671: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 30 16:18:52.671: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 16:18:52.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-5329 exec execpod7fwgt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.88.60 80'
Aug 30 16:18:53.043: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.88.60 80\nConnection to 172.21.88.60 80 port [tcp/http] succeeded!\n"
Aug 30 16:18:53.043: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5329
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5329 to expose endpoints map[pod2:[80]]
Aug 30 16:18:53.178: INFO: successfully validated that service endpoint-test2 in namespace services-5329 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Aug 30 16:18:54.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-5329 exec execpod7fwgt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 30 16:18:54.640: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 30 16:18:54.640: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 16:18:54.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-5329 exec execpod7fwgt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.88.60 80'
Aug 30 16:18:55.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.88.60 80\nConnection to 172.21.88.60 80 port [tcp/http] succeeded!\n"
Aug 30 16:18:55.679: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-5329
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5329 to expose endpoints map[]
Aug 30 16:18:55.811: INFO: successfully validated that service endpoint-test2 in namespace services-5329 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 16:18:55.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5329" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:16.197 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":345,"skipped":6435,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:18:55.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 30 16:18:56.142: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32378900-bb39-42cf-87c2-8dab411e2504" in namespace "downward-api-4345" to be "Succeeded or Failed"
W0830 16:18:56.142351      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:18:56.155: INFO: Pod "downwardapi-volume-32378900-bb39-42cf-87c2-8dab411e2504": Phase="Pending", Reason="", readiness=false. Elapsed: 13.33168ms
Aug 30 16:18:58.176: INFO: Pod "downwardapi-volume-32378900-bb39-42cf-87c2-8dab411e2504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033573304s
Aug 30 16:19:00.192: INFO: Pod "downwardapi-volume-32378900-bb39-42cf-87c2-8dab411e2504": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050337373s
Aug 30 16:19:02.214: INFO: Pod "downwardapi-volume-32378900-bb39-42cf-87c2-8dab411e2504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071728709s
STEP: Saw pod success
Aug 30 16:19:02.214: INFO: Pod "downwardapi-volume-32378900-bb39-42cf-87c2-8dab411e2504" satisfied condition "Succeeded or Failed"
Aug 30 16:19:02.224: INFO: Trying to get logs from node 10.63.224.189 pod downwardapi-volume-32378900-bb39-42cf-87c2-8dab411e2504 container client-container: <nil>
STEP: delete the pod
Aug 30 16:19:02.338: INFO: Waiting for pod downwardapi-volume-32378900-bb39-42cf-87c2-8dab411e2504 to disappear
Aug 30 16:19:02.374: INFO: Pod downwardapi-volume-32378900-bb39-42cf-87c2-8dab411e2504 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 30 16:19:02.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4345" for this suite.

• [SLOW TEST:6.577 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":346,"skipped":6450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:19:02.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
W0830 16:19:02.757191      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 16:19:02.871972      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Aug 30 16:19:23.384: INFO: EndpointSlice for Service endpointslice-2217/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 30 16:19:33.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2217" for this suite.

• [SLOW TEST:30.952 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":347,"skipped":6479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:19:33.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 30 16:19:33.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
Aug 30 16:19:43.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 30 16:20:21.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-880" for this suite.

• [SLOW TEST:47.683 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":348,"skipped":6524,"failed":0}
S
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:20:21.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
W0830 16:20:21.478765      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:20:21.490: INFO: The status of Pod pod-update-activedeadlineseconds-01a72998-dbd7-4922-9b3e-3772141419d4 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:20:23.504: INFO: The status of Pod pod-update-activedeadlineseconds-01a72998-dbd7-4922-9b3e-3772141419d4 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:20:25.508: INFO: The status of Pod pod-update-activedeadlineseconds-01a72998-dbd7-4922-9b3e-3772141419d4 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 30 16:20:26.087: INFO: Successfully updated pod "pod-update-activedeadlineseconds-01a72998-dbd7-4922-9b3e-3772141419d4"
Aug 30 16:20:26.087: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-01a72998-dbd7-4922-9b3e-3772141419d4" in namespace "pods-6131" to be "terminated due to deadline exceeded"
Aug 30 16:20:26.097: INFO: Pod "pod-update-activedeadlineseconds-01a72998-dbd7-4922-9b3e-3772141419d4": Phase="Running", Reason="", readiness=true. Elapsed: 10.242938ms
Aug 30 16:20:28.109: INFO: Pod "pod-update-activedeadlineseconds-01a72998-dbd7-4922-9b3e-3772141419d4": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.022586484s
Aug 30 16:20:28.109: INFO: Pod "pod-update-activedeadlineseconds-01a72998-dbd7-4922-9b3e-3772141419d4" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 30 16:20:28.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6131" for this suite.

• [SLOW TEST:6.999 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":349,"skipped":6525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:20:28.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-w44w
STEP: Creating a pod to test atomic-volume-subpath
W0830 16:20:28.447246      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container-subpath-configmap-w44w" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-configmap-w44w" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-configmap-w44w" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-configmap-w44w" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:20:28.447: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w44w" in namespace "subpath-5572" to be "Succeeded or Failed"
Aug 30 16:20:28.465: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Pending", Reason="", readiness=false. Elapsed: 17.673332ms
Aug 30 16:20:30.479: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031443768s
Aug 30 16:20:32.496: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=true. Elapsed: 4.048903019s
Aug 30 16:20:34.510: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=true. Elapsed: 6.063181042s
Aug 30 16:20:36.528: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=true. Elapsed: 8.081050309s
Aug 30 16:20:38.559: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=true. Elapsed: 10.111668472s
Aug 30 16:20:40.575: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=true. Elapsed: 12.127624994s
Aug 30 16:20:42.591: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=true. Elapsed: 14.143718777s
Aug 30 16:20:44.610: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=true. Elapsed: 16.162653078s
Aug 30 16:20:46.627: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=true. Elapsed: 18.180015961s
Aug 30 16:20:48.644: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=true. Elapsed: 20.197330447s
Aug 30 16:20:50.657: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=true. Elapsed: 22.209910637s
Aug 30 16:20:52.672: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Running", Reason="", readiness=false. Elapsed: 24.22529941s
Aug 30 16:20:54.689: INFO: Pod "pod-subpath-test-configmap-w44w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.241597712s
STEP: Saw pod success
Aug 30 16:20:54.689: INFO: Pod "pod-subpath-test-configmap-w44w" satisfied condition "Succeeded or Failed"
Aug 30 16:20:54.699: INFO: Trying to get logs from node 10.63.224.189 pod pod-subpath-test-configmap-w44w container test-container-subpath-configmap-w44w: <nil>
STEP: delete the pod
Aug 30 16:20:54.792: INFO: Waiting for pod pod-subpath-test-configmap-w44w to disappear
Aug 30 16:20:54.802: INFO: Pod pod-subpath-test-configmap-w44w no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w44w
Aug 30 16:20:54.802: INFO: Deleting pod "pod-subpath-test-configmap-w44w" in namespace "subpath-5572"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 30 16:20:54.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5572" for this suite.

• [SLOW TEST:26.701 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":350,"skipped":6552,"failed":0}
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:20:54.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-4786
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4786 to expose endpoints map[]
Aug 30 16:20:55.059: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Aug 30 16:20:56.122: INFO: successfully validated that service multi-endpoint-test in namespace services-4786 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4786
W0830 16:20:56.177349      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:20:56.200: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:20:58.210: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4786 to expose endpoints map[pod1:[100]]
Aug 30 16:20:58.256: INFO: successfully validated that service multi-endpoint-test in namespace services-4786 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4786
W0830 16:20:58.298117      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:20:58.307: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:21:00.320: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:21:02.330: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4786 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 30 16:21:02.450: INFO: successfully validated that service multi-endpoint-test in namespace services-4786 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Aug 30 16:21:02.450: INFO: Creating new exec pod
W0830 16:21:02.492602      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:21:07.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4786 exec execpodjwsbp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Aug 30 16:21:07.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 30 16:21:07.942: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 16:21:07.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4786 exec execpodjwsbp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.27.82 80'
Aug 30 16:21:08.392: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.27.82 80\nConnection to 172.21.27.82 80 port [tcp/http] succeeded!\n"
Aug 30 16:21:08.392: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 16:21:08.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4786 exec execpodjwsbp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Aug 30 16:21:08.774: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 30 16:21:08.774: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 16:21:08.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-4786 exec execpodjwsbp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.27.82 81'
Aug 30 16:21:09.293: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.27.82 81\nConnection to 172.21.27.82 81 port [tcp/*] succeeded!\n"
Aug 30 16:21:09.294: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4786
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4786 to expose endpoints map[pod2:[101]]
Aug 30 16:21:10.380: INFO: successfully validated that service multi-endpoint-test in namespace services-4786 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4786
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4786 to expose endpoints map[]
Aug 30 16:21:11.442: INFO: successfully validated that service multi-endpoint-test in namespace services-4786 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 16:21:11.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4786" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:16.716 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":351,"skipped":6554,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:21:11.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
W0830 16:21:11.835034      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:21:11.843: INFO: The status of Pod pod-update-c0687d09-984e-4a54-95d5-47ced4e81701 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:21:13.860: INFO: The status of Pod pod-update-c0687d09-984e-4a54-95d5-47ced4e81701 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:21:15.855: INFO: The status of Pod pod-update-c0687d09-984e-4a54-95d5-47ced4e81701 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 30 16:21:16.436: INFO: Successfully updated pod "pod-update-c0687d09-984e-4a54-95d5-47ced4e81701"
STEP: verifying the updated pod is in kubernetes
Aug 30 16:21:16.455: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 30 16:21:16.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9013" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":352,"skipped":6561,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:21:16.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Aug 30 16:21:16.599: INFO: Creating simple deployment test-new-deployment
W0830 16:21:16.615562      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:21:16.635: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Aug 30 16:21:18.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 30, 16, 21, 16, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 21, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 30, 16, 21, 16, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 30, 16, 21, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-55df494869\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 16:21:20.764: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-7551  affef5aa-5232-4cfc-81ab-7bde3a134a29 147406 3 2022-08-30 16:21:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-08-30 16:21:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:21:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008277c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-30 16:21:19 +0000 UTC,LastTransitionTime:2022-08-30 16:21:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2022-08-30 16:21:19 +0000 UTC,LastTransitionTime:2022-08-30 16:21:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 30 16:21:20.773: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-7551  81a572ee-2bbe-4ace-a6d4-59b78214815f 147405 2 2022-08-30 16:21:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment affef5aa-5232-4cfc-81ab-7bde3a134a29 0xc008340137 0xc008340138}] []  [{kube-controller-manager Update apps/v1 2022-08-30 16:21:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"affef5aa-5232-4cfc-81ab-7bde3a134a29\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-30 16:21:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083401c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 16:21:20.784: INFO: Pod "test-new-deployment-55df494869-m75jz" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-m75jz test-new-deployment-55df494869- deployment-7551  9a2f4f7d-f1e4-40c8-9284-5c013b775308 147392 0 2022-08-30 16:21:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:539c23d2c26414323a3f182b863fba4289c28c053dd65ea7d89da010da5ba8bf cni.projectcalico.org/podIP:172.30.233.228/32 cni.projectcalico.org/podIPs:172.30.233.228/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.228"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.233.228"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-55df494869 81a572ee-2bbe-4ace-a6d4-59b78214815f 0xc008340647 0xc008340648}] []  [{kube-controller-manager Update v1 2022-08-30 16:21:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81a572ee-2bbe-4ace-a6d4-59b78214815f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-30 16:21:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-08-30 16:21:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-08-30 16:21:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.233.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qkfpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qkfpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.63.224.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c68,c17,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:21:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:21:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:21:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-30 16:21:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.63.224.189,PodIP:172.30.233.228,StartTime:2022-08-30 16:21:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-30 16:21:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b1582d4ad694807031f9ef9ab058ee5426545da958a968f499ebab2c132cf646,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.233.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 30 16:21:20.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7551" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":353,"skipped":6565,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:21:20.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8331
W0830 16:21:20.978628      21 warnings.go:70] would violate PodSecurity "restricted:latest": host namespaces (hostNetwork=true)
Aug 30 16:21:20.987: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 30 16:21:22.998: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 30 16:21:23.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8331 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 30 16:21:23.491: INFO: rc: 7
Aug 30 16:21:23.522: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 30 16:21:23.533: INFO: Pod kube-proxy-mode-detector no longer exists
Aug 30 16:21:23.533: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8331 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-8331
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8331
W0830 16:21:23.603984      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-clusterip-timeout" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-clusterip-timeout" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-clusterip-timeout" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-clusterip-timeout" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 16:21:23.604145      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8331, replica count: 3
I0830 16:21:26.655794      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 16:21:26.689: INFO: Creating new exec pod
W0830 16:21:26.735799      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 16:21:31.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8331 exec execpod-affinity8qlpn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Aug 30 16:21:32.213: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Aug 30 16:21:32.213: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 16:21:32.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8331 exec execpod-affinity8qlpn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.177.9 80'
Aug 30 16:21:32.607: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.177.9 80\nConnection to 172.21.177.9 80 port [tcp/http] succeeded!\n"
Aug 30 16:21:32.607: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 30 16:21:32.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8331 exec execpod-affinity8qlpn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.177.9:80/ ; done'
Aug 30 16:21:33.199: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n"
Aug 30 16:21:33.199: INFO: stdout: "\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9\naffinity-clusterip-timeout-6n4l9"
Aug 30 16:21:33.199: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.199: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.199: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.199: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.199: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.199: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.199: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.199: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.199: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.200: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.200: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.200: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.200: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.200: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.200: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.200: INFO: Received response from host: affinity-clusterip-timeout-6n4l9
Aug 30 16:21:33.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8331 exec execpod-affinity8qlpn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.177.9:80/'
Aug 30 16:21:33.574: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n"
Aug 30 16:21:33.574: INFO: stdout: "affinity-clusterip-timeout-6n4l9"
Aug 30 16:21:53.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1611282519 --namespace=services-8331 exec execpod-affinity8qlpn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.177.9:80/'
Aug 30 16:21:54.035: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.177.9:80/\n"
Aug 30 16:21:54.035: INFO: stdout: "affinity-clusterip-timeout-fndvc"
Aug 30 16:21:54.035: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8331, will wait for the garbage collector to delete the pods
Aug 30 16:21:54.182: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 20.290146ms
Aug 30 16:21:54.383: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 200.815863ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 30 16:21:57.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8331" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

• [SLOW TEST:36.432 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":354,"skipped":6573,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:21:57.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
W0830 16:21:57.399781      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 30 16:22:01.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2469" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":355,"skipped":6587,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 30 16:22:01.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1611282519
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 30 16:22:01.788: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4475  f67aa014-fc60-4408-987a-570bc4b64a86 147907 0 2022-08-30 16:22:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-30 16:22:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 16:22:01.789: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4475  f67aa014-fc60-4408-987a-570bc4b64a86 147914 0 2022-08-30 16:22:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-30 16:22:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 30 16:22:01.887: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4475  f67aa014-fc60-4408-987a-570bc4b64a86 147918 0 2022-08-30 16:22:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-30 16:22:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 16:22:01.887: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4475  f67aa014-fc60-4408-987a-570bc4b64a86 147919 0 2022-08-30 16:22:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-30 16:22:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 30 16:22:01.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4475" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":356,"skipped":6603,"failed":0}
SSSSSSAug 30 16:22:01.944: INFO: Running AfterSuite actions on all nodes
Aug 30 16:22:01.944: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Aug 30 16:22:01.944: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Aug 30 16:22:01.944: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Aug 30 16:22:01.944: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Aug 30 16:22:01.944: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Aug 30 16:22:01.944: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Aug 30 16:22:01.944: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Aug 30 16:22:01.945: INFO: Running AfterSuite actions on node 1
Aug 30 16:22:01.949: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6609,"failed":0}

Ran 356 of 6965 Specs in 6923.214 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6609 Skipped
PASS

Ginkgo ran 1 suite in 1h55m25.381355628s
Test Suite Passed

I0829 15:07:18.673232      20 e2e.go:129] Starting e2e run "a3892898-a766-488a-a3ff-19fe219e0304" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1661785638 - Will randomize all specs
Will run 356 of 6971 specs

Aug 29 15:07:21.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:07:21.340: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 29 15:07:21.359: INFO: Condition Ready of node ip-172-31-25-142.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}]. Failure
Aug 29 15:07:21.359: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:07:21.359: INFO: 	-> Node ip-172-31-25-142.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:07:21.360: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:07:51.368: INFO: Condition Ready of node ip-172-31-25-142.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}]. Failure
Aug 29 15:07:51.368: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:07:51.368: INFO: 	-> Node ip-172-31-25-142.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:07:51.368: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:08:21.376: INFO: Condition Ready of node ip-172-31-25-142.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}]. Failure
Aug 29 15:08:21.376: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:08:21.377: INFO: 	-> Node ip-172-31-25-142.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:08:21.377: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:08:51.375: INFO: Condition Ready of node ip-172-31-25-142.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}]. Failure
Aug 29 15:08:51.375: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:08:51.375: INFO: 	-> Node ip-172-31-25-142.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:08:51.375: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:09:21.377: INFO: Condition Ready of node ip-172-31-25-142.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}]. Failure
Aug 29 15:09:21.377: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:09:21.378: INFO: 	-> Node ip-172-31-25-142.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:09:21.378: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:09:51.378: INFO: Condition Ready of node ip-172-31-25-142.eu-central-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}]. Failure
Aug 29 15:09:51.378: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Aug 29 15:09:51.378: INFO: 	-> Node ip-172-31-25-142.eu-central-1.compute.internal [[[ Ready=false, Network(available)=false, Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2022-08-29 15:06:59 +0000 UTC}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Aug 29 15:09:51.378: INFO: ==== node wait: 2 out of 3 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Aug 29 15:10:21.385: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 29 15:10:21.444: INFO: The status of Pod canal-xl6zl is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 15:10:21.444: INFO: 18 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 29 15:10:21.444: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Aug 29 15:10:21.444: INFO: POD          NODE                                            PHASE    GRACE  CONDITIONS
Aug 29 15:10:21.444: INFO: canal-xl6zl  ip-172-31-25-142.eu-central-1.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:10:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:07:00 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:07:00 +0000 UTC ContainersNotReady containers with unready status: [calico-node kube-flannel]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:06:54 +0000 UTC  }]
Aug 29 15:10:21.445: INFO: 
Aug 29 15:10:23.500: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
Aug 29 15:10:23.500: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Aug 29 15:10:23.500: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 29 15:10:23.514: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'aws-node-termination-handler' (0 seconds elapsed)
Aug 29 15:10:23.514: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Aug 29 15:10:23.514: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'envoy-agent' (0 seconds elapsed)
Aug 29 15:10:23.514: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 29 15:10:23.514: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Aug 29 15:10:23.514: INFO: e2e test version: v1.24.3
Aug 29 15:10:23.521: INFO: kube-apiserver version: v1.24.3
Aug 29 15:10:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:10:23.529: INFO: Cluster IP family: ipv4
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:10:23.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
W0829 15:10:23.563625      20 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Aug 29 15:10:23.563: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Aug 29 15:10:23.592: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-006e2b8c-1a12-4b4b-b2ef-8dd0b2999c0b
STEP: Creating a pod to test consume secrets
Aug 29 15:10:23.633: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520" in namespace "projected-5631" to be "Succeeded or Failed"
Aug 29 15:10:23.676: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Pending", Reason="", readiness=false. Elapsed: 42.999581ms
Aug 29 15:10:25.723: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090364197s
Aug 29 15:10:27.737: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Pending", Reason="", readiness=false. Elapsed: 4.104189703s
Aug 29 15:10:29.747: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Pending", Reason="", readiness=false. Elapsed: 6.114402999s
Aug 29 15:10:31.767: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Pending", Reason="", readiness=false. Elapsed: 8.133757742s
Aug 29 15:10:34.010: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Pending", Reason="", readiness=false. Elapsed: 10.377512641s
Aug 29 15:10:36.141: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Running", Reason="", readiness=true. Elapsed: 12.508481335s
Aug 29 15:10:38.153: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Running", Reason="", readiness=true. Elapsed: 14.520391849s
Aug 29 15:10:40.167: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Running", Reason="", readiness=true. Elapsed: 16.534283447s
Aug 29 15:10:42.176: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Running", Reason="", readiness=false. Elapsed: 18.542883137s
Aug 29 15:10:44.190: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Running", Reason="", readiness=false. Elapsed: 20.557550427s
Aug 29 15:10:46.198: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.564950417s
STEP: Saw pod success
Aug 29 15:10:46.198: INFO: Pod "pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520" satisfied condition "Succeeded or Failed"
Aug 29 15:10:46.215: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:10:46.277: INFO: Waiting for pod pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520 to disappear
Aug 29 15:10:46.284: INFO: Pod pod-projected-secrets-8d652129-e91f-4b62-8a01-c20b80326520 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 29 15:10:46.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5631" for this suite.

• [SLOW TEST:22.778 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":1,"skipped":11,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:10:46.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 29 15:10:48.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5809" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":2,"skipped":35,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:10:48.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Aug 29 15:10:48.638: INFO: The status of Pod annotationupdate66598d8f-4065-470b-b757-d3cf23ac317d is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:10:50.654: INFO: The status of Pod annotationupdate66598d8f-4065-470b-b757-d3cf23ac317d is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:10:52.645: INFO: The status of Pod annotationupdate66598d8f-4065-470b-b757-d3cf23ac317d is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:10:54.645: INFO: The status of Pod annotationupdate66598d8f-4065-470b-b757-d3cf23ac317d is Running (Ready = true)
Aug 29 15:10:55.620: INFO: Successfully updated pod "annotationupdate66598d8f-4065-470b-b757-d3cf23ac317d"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 15:10:57.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4116" for this suite.

• [SLOW TEST:9.123 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":3,"skipped":47,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:10:57.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 29 15:10:58.106: INFO: Waiting up to 5m0s for pod "pod-0ef0b65e-f3cd-4414-aa75-2299db4dbd28" in namespace "emptydir-7176" to be "Succeeded or Failed"
Aug 29 15:10:58.131: INFO: Pod "pod-0ef0b65e-f3cd-4414-aa75-2299db4dbd28": Phase="Pending", Reason="", readiness=false. Elapsed: 23.916617ms
Aug 29 15:11:00.140: INFO: Pod "pod-0ef0b65e-f3cd-4414-aa75-2299db4dbd28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033046061s
Aug 29 15:11:02.152: INFO: Pod "pod-0ef0b65e-f3cd-4414-aa75-2299db4dbd28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045055235s
STEP: Saw pod success
Aug 29 15:11:02.152: INFO: Pod "pod-0ef0b65e-f3cd-4414-aa75-2299db4dbd28" satisfied condition "Succeeded or Failed"
Aug 29 15:11:02.166: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-0ef0b65e-f3cd-4414-aa75-2299db4dbd28 container test-container: <nil>
STEP: delete the pod
Aug 29 15:11:02.230: INFO: Waiting for pod pod-0ef0b65e-f3cd-4414-aa75-2299db4dbd28 to disappear
Aug 29 15:11:02.245: INFO: Pod pod-0ef0b65e-f3cd-4414-aa75-2299db4dbd28 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 15:11:02.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7176" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":4,"skipped":117,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:11:02.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-hkzd
STEP: Creating a pod to test atomic-volume-subpath
Aug 29 15:11:02.414: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hkzd" in namespace "subpath-5531" to be "Succeeded or Failed"
Aug 29 15:11:02.420: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.337738ms
Aug 29 15:11:04.433: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=true. Elapsed: 2.019016025s
Aug 29 15:11:06.442: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=true. Elapsed: 4.027985955s
Aug 29 15:11:08.456: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=true. Elapsed: 6.041956496s
Aug 29 15:11:10.496: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=true. Elapsed: 8.081260347s
Aug 29 15:11:12.506: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=true. Elapsed: 10.091770804s
Aug 29 15:11:14.517: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=true. Elapsed: 12.103066775s
Aug 29 15:11:16.533: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=true. Elapsed: 14.118704192s
Aug 29 15:11:18.550: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=true. Elapsed: 16.136081824s
Aug 29 15:11:20.581: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=true. Elapsed: 18.166581802s
Aug 29 15:11:22.596: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=true. Elapsed: 20.181267722s
Aug 29 15:11:24.605: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Running", Reason="", readiness=false. Elapsed: 22.190812257s
Aug 29 15:11:27.176: INFO: Pod "pod-subpath-test-configmap-hkzd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.761999422s
STEP: Saw pod success
Aug 29 15:11:27.176: INFO: Pod "pod-subpath-test-configmap-hkzd" satisfied condition "Succeeded or Failed"
Aug 29 15:11:27.193: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-subpath-test-configmap-hkzd container test-container-subpath-configmap-hkzd: <nil>
STEP: delete the pod
Aug 29 15:11:27.248: INFO: Waiting for pod pod-subpath-test-configmap-hkzd to disappear
Aug 29 15:11:27.255: INFO: Pod pod-subpath-test-configmap-hkzd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hkzd
Aug 29 15:11:27.255: INFO: Deleting pod "pod-subpath-test-configmap-hkzd" in namespace "subpath-5531"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 29 15:11:27.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5531" for this suite.

• [SLOW TEST:24.974 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":5,"skipped":124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:11:27.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Aug 29 15:11:31.453: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9820 PodName:pod-sharedvolume-2bdbaf2c-d80e-4a2f-95a5-3bdacfbabe23 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:11:31.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:11:31.454: INFO: ExecWithOptions: Clientset creation
Aug 29 15:11:31.454: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/emptydir-9820/pods/pod-sharedvolume-2bdbaf2c-d80e-4a2f-95a5-3bdacfbabe23/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 29 15:11:31.601: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 15:11:31.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9820" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":6,"skipped":164,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:11:31.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 29 15:11:31.721: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3557  03885785-231a-4d80-86b4-c8321ac10d15 2976 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:11:31.722: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3557  03885785-231a-4d80-86b4-c8321ac10d15 2976 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 29 15:11:31.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3557  03885785-231a-4d80-86b4-c8321ac10d15 2977 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:11:31.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3557  03885785-231a-4d80-86b4-c8321ac10d15 2977 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 29 15:11:31.755: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3557  03885785-231a-4d80-86b4-c8321ac10d15 2978 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:11:31.755: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3557  03885785-231a-4d80-86b4-c8321ac10d15 2978 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 29 15:11:31.766: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3557  03885785-231a-4d80-86b4-c8321ac10d15 2979 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:11:31.766: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3557  03885785-231a-4d80-86b4-c8321ac10d15 2979 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 29 15:11:31.773: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3557  954d97b0-f28f-4837-a9a7-3c049091f1e5 2980 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:11:31.774: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3557  954d97b0-f28f-4837-a9a7-3c049091f1e5 2980 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 29 15:11:41.790: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3557  954d97b0-f28f-4837-a9a7-3c049091f1e5 3038 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:11:41.790: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3557  954d97b0-f28f-4837-a9a7-3c049091f1e5 3038 0 2022-08-29 15:11:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-08-29 15:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 29 15:11:51.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3557" for this suite.

• [SLOW TEST:20.201 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":7,"skipped":181,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:11:51.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Aug 29 15:11:51.887: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 15:12:51.949: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:12:51.954: INFO: Starting informer...
STEP: Starting pods...
Aug 29 15:12:52.194: INFO: Pod1 is running on ip-172-31-25-142.eu-central-1.compute.internal. Tainting Node
Aug 29 15:12:56.454: INFO: Pod2 is running on ip-172-31-25-142.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 29 15:13:02.407: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 29 15:13:22.453: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Aug 29 15:13:22.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-214" for this suite.

• [SLOW TEST:90.693 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":8,"skipped":192,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:13:22.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-9496
Aug 29 15:13:22.605: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:13:24.613: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 29 15:13:24.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-9496 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 29 15:13:25.188: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Aug 29 15:13:25.188: INFO: stdout: "ipvs"
Aug 29 15:13:25.188: INFO: proxyMode: ipvs
Aug 29 15:13:25.208: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 29 15:13:25.214: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-9496
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9496
I0829 15:13:25.249197      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9496, replica count: 3
I0829 15:13:28.303087      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:13:28.354: INFO: Creating new exec pod
Aug 29 15:13:31.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-9496 exec execpod-affinity28hfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Aug 29 15:13:32.189: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Aug 29 15:13:32.189: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:13:32.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-9496 exec execpod-affinity28hfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.249 80'
Aug 29 15:13:32.399: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.23.249 80\nConnection to 10.240.23.249 80 port [tcp/http] succeeded!\n"
Aug 29 15:13:32.399: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:13:32.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-9496 exec execpod-affinity28hfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.26.197 31254'
Aug 29 15:13:32.625: INFO: stderr: "+ nc -v -t -w 2 172.31.26.197 31254\n+ echo hostName\nConnection to 172.31.26.197 31254 port [tcp/*] succeeded!\n"
Aug 29 15:13:32.625: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:13:32.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-9496 exec execpod-affinity28hfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.214 31254'
Aug 29 15:13:32.839: INFO: stderr: "+ nc -v -t -w 2 172.31.16.214 31254\n+ echo hostName\nConnection to 172.31.16.214 31254 port [tcp/*] succeeded!\n"
Aug 29 15:13:32.839: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:13:32.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-9496 exec execpod-affinity28hfm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.214:31254/ ; done'
Aug 29 15:13:33.158: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n"
Aug 29 15:13:33.158: INFO: stdout: "\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp\naffinity-nodeport-timeout-6zkxp"
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Received response from host: affinity-nodeport-timeout-6zkxp
Aug 29 15:13:33.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-9496 exec execpod-affinity28hfm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.16.214:31254/'
Aug 29 15:13:33.364: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n"
Aug 29 15:13:33.364: INFO: stdout: "affinity-nodeport-timeout-6zkxp"
Aug 29 15:15:43.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-9496 exec execpod-affinity28hfm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.16.214:31254/'
Aug 29 15:15:43.636: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.16.214:31254/\n"
Aug 29 15:15:43.636: INFO: stdout: "affinity-nodeport-timeout-jzhdf"
Aug 29 15:15:43.636: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9496, will wait for the garbage collector to delete the pods
Aug 29 15:15:44.307: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 36.829946ms
Aug 29 15:15:44.407: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.640736ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 15:15:47.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9496" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:144.933 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":9,"skipped":201,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:15:47.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:15:47.530: INFO: Creating pod...
Aug 29 15:15:49.564: INFO: Creating service...
Aug 29 15:15:49.580: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/pods/agnhost/proxy?method=DELETE
Aug 29 15:15:49.606: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 29 15:15:49.606: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/pods/agnhost/proxy?method=OPTIONS
Aug 29 15:15:49.615: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 29 15:15:49.615: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/pods/agnhost/proxy?method=PATCH
Aug 29 15:15:49.630: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 29 15:15:49.631: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/pods/agnhost/proxy?method=POST
Aug 29 15:15:49.648: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 29 15:15:49.649: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/pods/agnhost/proxy?method=PUT
Aug 29 15:15:49.660: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 29 15:15:49.660: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 29 15:15:49.676: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 29 15:15:49.676: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 29 15:15:49.692: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 29 15:15:49.692: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 29 15:15:49.703: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 29 15:15:49.703: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/services/e2e-proxy-test-service/proxy?method=POST
Aug 29 15:15:49.719: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 29 15:15:49.719: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/services/e2e-proxy-test-service/proxy?method=PUT
Aug 29 15:15:49.730: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 29 15:15:49.731: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/pods/agnhost/proxy?method=GET
Aug 29 15:15:49.738: INFO: http.Client request:GET StatusCode:301
Aug 29 15:15:49.738: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/services/e2e-proxy-test-service/proxy?method=GET
Aug 29 15:15:49.757: INFO: http.Client request:GET StatusCode:301
Aug 29 15:15:49.757: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/pods/agnhost/proxy?method=HEAD
Aug 29 15:15:49.762: INFO: http.Client request:HEAD StatusCode:301
Aug 29 15:15:49.763: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-3614/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 29 15:15:49.775: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Aug 29 15:15:49.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3614" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":10,"skipped":213,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:15:49.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 29 15:15:53.945: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 29 15:15:54.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3026" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":11,"skipped":221,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:15:54.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Aug 29 15:16:00.517: INFO: 91 pods remaining
Aug 29 15:16:00.521: INFO: 84 pods has nil DeletionTimestamp
Aug 29 15:16:00.522: INFO: 
Aug 29 15:16:02.433: INFO: 59 pods remaining
Aug 29 15:16:02.440: INFO: 56 pods has nil DeletionTimestamp
Aug 29 15:16:02.440: INFO: 
Aug 29 15:16:02.556: INFO: 53 pods remaining
Aug 29 15:16:02.556: INFO: 53 pods has nil DeletionTimestamp
Aug 29 15:16:02.556: INFO: 
Aug 29 15:16:03.745: INFO: 49 pods remaining
Aug 29 15:16:03.745: INFO: 47 pods has nil DeletionTimestamp
Aug 29 15:16:03.745: INFO: 
Aug 29 15:16:04.564: INFO: 31 pods remaining
Aug 29 15:16:04.564: INFO: 31 pods has nil DeletionTimestamp
Aug 29 15:16:04.564: INFO: 
Aug 29 15:16:05.548: INFO: 25 pods remaining
Aug 29 15:16:05.549: INFO: 17 pods has nil DeletionTimestamp
Aug 29 15:16:05.549: INFO: 
STEP: Gathering metrics
W0829 15:16:06.520942      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 15:16:06.521: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 29 15:16:06.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3183" for this suite.

• [SLOW TEST:12.405 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":12,"skipped":223,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:16:06.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 29 15:16:06.611: INFO: Waiting up to 5m0s for pod "pod-0fafc41c-3710-4ec2-bc9d-32487deef30d" in namespace "emptydir-3244" to be "Succeeded or Failed"
Aug 29 15:16:06.619: INFO: Pod "pod-0fafc41c-3710-4ec2-bc9d-32487deef30d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.764132ms
Aug 29 15:16:08.630: INFO: Pod "pod-0fafc41c-3710-4ec2-bc9d-32487deef30d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018591797s
Aug 29 15:16:10.662: INFO: Pod "pod-0fafc41c-3710-4ec2-bc9d-32487deef30d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05052408s
Aug 29 15:16:12.703: INFO: Pod "pod-0fafc41c-3710-4ec2-bc9d-32487deef30d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091778705s
Aug 29 15:16:14.715: INFO: Pod "pod-0fafc41c-3710-4ec2-bc9d-32487deef30d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.103389442s
Aug 29 15:16:16.728: INFO: Pod "pod-0fafc41c-3710-4ec2-bc9d-32487deef30d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.115973105s
STEP: Saw pod success
Aug 29 15:16:16.728: INFO: Pod "pod-0fafc41c-3710-4ec2-bc9d-32487deef30d" satisfied condition "Succeeded or Failed"
Aug 29 15:16:16.733: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-0fafc41c-3710-4ec2-bc9d-32487deef30d container test-container: <nil>
STEP: delete the pod
Aug 29 15:16:17.452: INFO: Waiting for pod pod-0fafc41c-3710-4ec2-bc9d-32487deef30d to disappear
Aug 29 15:16:17.459: INFO: Pod pod-0fafc41c-3710-4ec2-bc9d-32487deef30d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 15:16:17.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3244" for this suite.

• [SLOW TEST:10.939 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":13,"skipped":235,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:16:17.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Aug 29 15:16:17.604: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Aug 29 15:16:17.666: INFO: waiting for watch events with expected annotations in namespace
Aug 29 15:16:17.667: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Aug 29 15:16:17.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-9723" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":14,"skipped":253,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:16:17.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 29 15:16:31.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-452" for this suite.

• [SLOW TEST:14.180 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":15,"skipped":255,"failed":0}
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:16:31.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:16:34.308: INFO: Deleting pod "var-expansion-ebad3a49-35f7-4be8-ae44-29387232ecf4" in namespace "var-expansion-2769"
Aug 29 15:16:34.320: INFO: Wait up to 5m0s for pod "var-expansion-ebad3a49-35f7-4be8-ae44-29387232ecf4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 29 15:16:38.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2769" for this suite.

• [SLOW TEST:6.415 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":16,"skipped":255,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:16:38.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4476
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4476
I0829 15:16:38.456014      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4476, replica count: 2
I0829 15:16:41.508899      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:16:41.509: INFO: Creating new exec pod
Aug 29 15:16:45.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-4476 exec execpodt7v5h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 15:16:45.882: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 15:16:45.882: INFO: stdout: ""
Aug 29 15:16:46.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-4476 exec execpodt7v5h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 15:16:47.120: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 15:16:47.120: INFO: stdout: "externalname-service-5nkfd"
Aug 29 15:16:47.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-4476 exec execpodt7v5h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.31.193 80'
Aug 29 15:16:47.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.31.193 80\nConnection to 10.240.31.193 80 port [tcp/http] succeeded!\n"
Aug 29 15:16:47.329: INFO: stdout: ""
Aug 29 15:16:48.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-4476 exec execpodt7v5h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.31.193 80'
Aug 29 15:16:48.548: INFO: stderr: "+ nc -v -t -w 2 10.240.31.193 80\nConnection to 10.240.31.193 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Aug 29 15:16:48.548: INFO: stdout: "externalname-service-5nkfd"
Aug 29 15:16:48.548: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 15:16:48.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4476" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.240 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":17,"skipped":270,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:16:48.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:16:50.088: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 29 15:16:50.091: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 29 15:16:50.091: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 29 15:16:50.091: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 29 15:16:50.091: INFO: Checking APIGroup: apps
Aug 29 15:16:50.093: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 29 15:16:50.093: INFO: Versions found [{apps/v1 v1}]
Aug 29 15:16:50.093: INFO: apps/v1 matches apps/v1
Aug 29 15:16:50.093: INFO: Checking APIGroup: events.k8s.io
Aug 29 15:16:50.100: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 29 15:16:50.101: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Aug 29 15:16:50.101: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 29 15:16:50.101: INFO: Checking APIGroup: authentication.k8s.io
Aug 29 15:16:50.104: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 29 15:16:50.104: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 29 15:16:50.104: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 29 15:16:50.104: INFO: Checking APIGroup: authorization.k8s.io
Aug 29 15:16:50.112: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 29 15:16:50.112: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 29 15:16:50.112: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 29 15:16:50.112: INFO: Checking APIGroup: autoscaling
Aug 29 15:16:50.125: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 29 15:16:50.125: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Aug 29 15:16:50.125: INFO: autoscaling/v2 matches autoscaling/v2
Aug 29 15:16:50.125: INFO: Checking APIGroup: batch
Aug 29 15:16:50.128: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 29 15:16:50.128: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Aug 29 15:16:50.128: INFO: batch/v1 matches batch/v1
Aug 29 15:16:50.128: INFO: Checking APIGroup: certificates.k8s.io
Aug 29 15:16:50.131: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 29 15:16:50.131: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 29 15:16:50.131: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 29 15:16:50.131: INFO: Checking APIGroup: networking.k8s.io
Aug 29 15:16:50.134: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 29 15:16:50.134: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 29 15:16:50.134: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 29 15:16:50.134: INFO: Checking APIGroup: policy
Aug 29 15:16:50.138: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 29 15:16:50.138: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Aug 29 15:16:50.138: INFO: policy/v1 matches policy/v1
Aug 29 15:16:50.138: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 29 15:16:50.140: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 29 15:16:50.140: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 29 15:16:50.140: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 29 15:16:50.140: INFO: Checking APIGroup: storage.k8s.io
Aug 29 15:16:50.146: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 29 15:16:50.146: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 29 15:16:50.146: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 29 15:16:50.146: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 29 15:16:50.150: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 29 15:16:50.150: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 29 15:16:50.150: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 29 15:16:50.150: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 29 15:16:50.154: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 29 15:16:50.154: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 29 15:16:50.154: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 29 15:16:50.154: INFO: Checking APIGroup: scheduling.k8s.io
Aug 29 15:16:50.162: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 29 15:16:50.162: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 29 15:16:50.162: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 29 15:16:50.162: INFO: Checking APIGroup: coordination.k8s.io
Aug 29 15:16:50.165: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 29 15:16:50.165: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 29 15:16:50.165: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 29 15:16:50.165: INFO: Checking APIGroup: node.k8s.io
Aug 29 15:16:50.167: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 29 15:16:50.167: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Aug 29 15:16:50.167: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 29 15:16:50.167: INFO: Checking APIGroup: discovery.k8s.io
Aug 29 15:16:50.170: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 29 15:16:50.170: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Aug 29 15:16:50.170: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 29 15:16:50.171: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 29 15:16:50.175: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Aug 29 15:16:50.175: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Aug 29 15:16:50.175: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Aug 29 15:16:50.175: INFO: Checking APIGroup: apps.kubermatic.k8c.io
Aug 29 15:16:50.178: INFO: PreferredVersion.GroupVersion: apps.kubermatic.k8c.io/v1
Aug 29 15:16:50.178: INFO: Versions found [{apps.kubermatic.k8c.io/v1 v1}]
Aug 29 15:16:50.178: INFO: apps.kubermatic.k8c.io/v1 matches apps.kubermatic.k8c.io/v1
Aug 29 15:16:50.178: INFO: Checking APIGroup: crd.projectcalico.org
Aug 29 15:16:50.181: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 29 15:16:50.181: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 29 15:16:50.181: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 29 15:16:50.181: INFO: Checking APIGroup: cluster.k8s.io
Aug 29 15:16:50.184: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
Aug 29 15:16:50.184: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
Aug 29 15:16:50.184: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
Aug 29 15:16:50.184: INFO: Checking APIGroup: metrics.k8s.io
Aug 29 15:16:50.186: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 29 15:16:50.186: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 29 15:16:50.186: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Aug 29 15:16:50.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2623" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":18,"skipped":291,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:16:50.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2805
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-2805
Aug 29 15:16:50.298: INFO: Found 0 stateful pods, waiting for 1
Aug 29 15:17:00.311: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 29 15:17:00.509: INFO: Deleting all statefulset in ns statefulset-2805
Aug 29 15:17:00.514: INFO: Scaling statefulset ss to 0
Aug 29 15:17:10.545: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 15:17:10.554: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 29 15:17:10.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2805" for this suite.

• [SLOW TEST:20.383 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":19,"skipped":299,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:17:10.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-692
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 29 15:17:10.641: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 15:17:10.713: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:17:12.725: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:17:14.722: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:17:16.723: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:17:18.720: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:17:20.724: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 15:17:23.061: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 29 15:17:23.072: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 29 15:17:23.084: INFO: The status of Pod netserver-2 is Running (Ready = false)
Aug 29 15:17:25.094: INFO: The status of Pod netserver-2 is Running (Ready = false)
Aug 29 15:17:27.096: INFO: The status of Pod netserver-2 is Running (Ready = false)
Aug 29 15:17:29.092: INFO: The status of Pod netserver-2 is Running (Ready = false)
Aug 29 15:17:31.096: INFO: The status of Pod netserver-2 is Running (Ready = false)
Aug 29 15:17:33.096: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 29 15:17:35.387: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 29 15:17:35.387: INFO: Breadth first check of 172.25.1.40 on host 172.31.16.214...
Aug 29 15:17:35.392: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.41:9080/dial?request=hostname&protocol=udp&host=172.25.1.40&port=8081&tries=1'] Namespace:pod-network-test-692 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:17:35.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:17:35.393: INFO: ExecWithOptions: Clientset creation
Aug 29 15:17:35.393: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-692/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.1.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 15:17:35.504: INFO: Waiting for responses: map[]
Aug 29 15:17:35.504: INFO: reached 172.25.1.40 after 0/1 tries
Aug 29 15:17:35.504: INFO: Breadth first check of 172.25.2.46 on host 172.31.25.142...
Aug 29 15:17:35.518: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.41:9080/dial?request=hostname&protocol=udp&host=172.25.2.46&port=8081&tries=1'] Namespace:pod-network-test-692 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:17:35.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:17:35.519: INFO: ExecWithOptions: Clientset creation
Aug 29 15:17:35.519: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-692/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.2.46%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 15:17:35.628: INFO: Waiting for responses: map[]
Aug 29 15:17:35.628: INFO: reached 172.25.2.46 after 0/1 tries
Aug 29 15:17:35.628: INFO: Breadth first check of 172.25.0.40 on host 172.31.26.197...
Aug 29 15:17:35.639: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.41:9080/dial?request=hostname&protocol=udp&host=172.25.0.40&port=8081&tries=1'] Namespace:pod-network-test-692 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:17:35.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:17:35.640: INFO: ExecWithOptions: Clientset creation
Aug 29 15:17:35.640: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-692/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.0.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 15:17:35.752: INFO: Waiting for responses: map[]
Aug 29 15:17:35.753: INFO: reached 172.25.0.40 after 0/1 tries
Aug 29 15:17:35.753: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 29 15:17:35.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-692" for this suite.

• [SLOW TEST:25.175 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":20,"skipped":301,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:17:35.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:17:36.234: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:17:39.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 29 15:17:39.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 15:17:39.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9524" for this suite.
STEP: Destroying namespace "webhook-9524-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":21,"skipped":306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:17:39.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Aug 29 15:17:40.181: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Aug 29 15:17:40.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2250" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":22,"skipped":339,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:17:40.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 15:17:40.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3939" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":23,"skipped":372,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:17:40.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-52593b2a-349d-49f7-9afa-70ffce06f7d3 in namespace container-probe-3387
Aug 29 15:17:42.461: INFO: Started pod liveness-52593b2a-349d-49f7-9afa-70ffce06f7d3 in namespace container-probe-3387
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 15:17:42.477: INFO: Initial restart count of pod liveness-52593b2a-349d-49f7-9afa-70ffce06f7d3 is 0
Aug 29 15:18:03.083: INFO: Restart count of pod container-probe-3387/liveness-52593b2a-349d-49f7-9afa-70ffce06f7d3 is now 1 (20.606303702s elapsed)
Aug 29 15:18:23.214: INFO: Restart count of pod container-probe-3387/liveness-52593b2a-349d-49f7-9afa-70ffce06f7d3 is now 2 (40.737748913s elapsed)
Aug 29 15:18:43.331: INFO: Restart count of pod container-probe-3387/liveness-52593b2a-349d-49f7-9afa-70ffce06f7d3 is now 3 (1m0.854840929s elapsed)
Aug 29 15:19:03.427: INFO: Restart count of pod container-probe-3387/liveness-52593b2a-349d-49f7-9afa-70ffce06f7d3 is now 4 (1m20.950773297s elapsed)
Aug 29 15:20:06.091: INFO: Restart count of pod container-probe-3387/liveness-52593b2a-349d-49f7-9afa-70ffce06f7d3 is now 5 (2m23.614182838s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 29 15:20:06.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3387" for this suite.

• [SLOW TEST:145.825 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":24,"skipped":380,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:20:06.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 29 15:20:06.289: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 15:21:06.367: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:21:06.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Aug 29 15:21:11.090: INFO: found a healthy node: ip-172-31-25-142.eu-central-1.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:21:25.139: INFO: pods created so far: [1 1 1]
Aug 29 15:21:25.139: INFO: length of pods created so far: 3
Aug 29 15:21:29.158: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Aug 29 15:21:36.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3967" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 29 15:21:36.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5713" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:90.139 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":25,"skipped":387,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:21:36.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0829 15:21:46.501180      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 15:21:46.501: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 29 15:21:46.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8210" for this suite.

• [SLOW TEST:10.178 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":26,"skipped":389,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:21:46.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 15:21:46.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8778825d-2957-46c2-ba1e-d5e82b894559" in namespace "projected-9113" to be "Succeeded or Failed"
Aug 29 15:21:46.948: INFO: Pod "downwardapi-volume-8778825d-2957-46c2-ba1e-d5e82b894559": Phase="Pending", Reason="", readiness=false. Elapsed: 14.36037ms
Aug 29 15:21:48.957: INFO: Pod "downwardapi-volume-8778825d-2957-46c2-ba1e-d5e82b894559": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023237591s
Aug 29 15:21:50.978: INFO: Pod "downwardapi-volume-8778825d-2957-46c2-ba1e-d5e82b894559": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044388838s
STEP: Saw pod success
Aug 29 15:21:50.978: INFO: Pod "downwardapi-volume-8778825d-2957-46c2-ba1e-d5e82b894559" satisfied condition "Succeeded or Failed"
Aug 29 15:21:50.988: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-8778825d-2957-46c2-ba1e-d5e82b894559 container client-container: <nil>
STEP: delete the pod
Aug 29 15:21:51.040: INFO: Waiting for pod downwardapi-volume-8778825d-2957-46c2-ba1e-d5e82b894559 to disappear
Aug 29 15:21:51.046: INFO: Pod downwardapi-volume-8778825d-2957-46c2-ba1e-d5e82b894559 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 15:21:51.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9113" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":27,"skipped":407,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:21:51.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 29 15:21:51.167: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 29 15:21:51.179: INFO: starting watch
STEP: patching
STEP: updating
Aug 29 15:21:51.213: INFO: waiting for watch events with expected annotations
Aug 29 15:21:51.214: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 29 15:21:51.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-339" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":28,"skipped":424,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:21:51.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
Aug 29 15:21:51.395: INFO: The status of Pod pod-hostip-cc933953-0983-4719-acde-d4bcb5c87349 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:21:53.408: INFO: The status of Pod pod-hostip-cc933953-0983-4719-acde-d4bcb5c87349 is Running (Ready = true)
Aug 29 15:21:53.420: INFO: Pod pod-hostip-cc933953-0983-4719-acde-d4bcb5c87349 has hostIP: 172.31.25.142
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 29 15:21:53.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4937" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":29,"skipped":434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:21:53.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 29 15:21:57.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7881" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":30,"skipped":487,"failed":0}
S
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:21:57.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 15:21:57.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7499" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":31,"skipped":488,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:21:57.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 29 15:21:57.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-197 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 29 15:21:57.970: INFO: stderr: ""
Aug 29 15:21:57.970: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Aug 29 15:21:57.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-197 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Aug 29 15:21:58.688: INFO: stderr: ""
Aug 29 15:21:58.688: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 29 15:21:58.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-197 delete pods e2e-test-httpd-pod'
Aug 29 15:22:01.243: INFO: stderr: ""
Aug 29 15:22:01.244: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 15:22:01.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-197" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":32,"skipped":501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:22:01.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-f1bbf487-0a32-4c5d-9c45-3b1a49dfeb2f
STEP: Creating a pod to test consume configMaps
Aug 29 15:22:01.366: INFO: Waiting up to 5m0s for pod "pod-configmaps-916e2338-7393-4381-aeed-7ef9991fcad1" in namespace "configmap-7908" to be "Succeeded or Failed"
Aug 29 15:22:01.380: INFO: Pod "pod-configmaps-916e2338-7393-4381-aeed-7ef9991fcad1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.703544ms
Aug 29 15:22:03.386: INFO: Pod "pod-configmaps-916e2338-7393-4381-aeed-7ef9991fcad1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01983005s
Aug 29 15:22:05.394: INFO: Pod "pod-configmaps-916e2338-7393-4381-aeed-7ef9991fcad1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028349697s
Aug 29 15:22:07.402: INFO: Pod "pod-configmaps-916e2338-7393-4381-aeed-7ef9991fcad1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03550526s
Aug 29 15:22:09.538: INFO: Pod "pod-configmaps-916e2338-7393-4381-aeed-7ef9991fcad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.171552518s
STEP: Saw pod success
Aug 29 15:22:09.538: INFO: Pod "pod-configmaps-916e2338-7393-4381-aeed-7ef9991fcad1" satisfied condition "Succeeded or Failed"
Aug 29 15:22:09.544: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-configmaps-916e2338-7393-4381-aeed-7ef9991fcad1 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:22:09.579: INFO: Waiting for pod pod-configmaps-916e2338-7393-4381-aeed-7ef9991fcad1 to disappear
Aug 29 15:22:09.586: INFO: Pod pod-configmaps-916e2338-7393-4381-aeed-7ef9991fcad1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 15:22:09.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7908" for this suite.

• [SLOW TEST:8.322 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":33,"skipped":558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:22:09.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:22:10.789: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:22:14.100: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 15:22:14.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9411" for this suite.
STEP: Destroying namespace "webhook-9411-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.549 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":34,"skipped":629,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:22:15.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 29 15:22:16.373: INFO: Pod name wrapped-volume-race-c34e62bc-4fe1-47fc-900b-e891bc65d581: Found 0 pods out of 5
Aug 29 15:22:21.390: INFO: Pod name wrapped-volume-race-c34e62bc-4fe1-47fc-900b-e891bc65d581: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c34e62bc-4fe1-47fc-900b-e891bc65d581 in namespace emptydir-wrapper-4195, will wait for the garbage collector to delete the pods
Aug 29 15:22:33.606: INFO: Deleting ReplicationController wrapped-volume-race-c34e62bc-4fe1-47fc-900b-e891bc65d581 took: 41.894794ms
Aug 29 15:22:33.708: INFO: Terminating ReplicationController wrapped-volume-race-c34e62bc-4fe1-47fc-900b-e891bc65d581 pods took: 102.55573ms
STEP: Creating RC which spawns configmap-volume pods
Aug 29 15:22:37.544: INFO: Pod name wrapped-volume-race-ebb1c3db-81fc-4502-8afa-a6ccd33d503b: Found 0 pods out of 5
Aug 29 15:22:42.575: INFO: Pod name wrapped-volume-race-ebb1c3db-81fc-4502-8afa-a6ccd33d503b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ebb1c3db-81fc-4502-8afa-a6ccd33d503b in namespace emptydir-wrapper-4195, will wait for the garbage collector to delete the pods
Aug 29 15:22:54.848: INFO: Deleting ReplicationController wrapped-volume-race-ebb1c3db-81fc-4502-8afa-a6ccd33d503b took: 10.426776ms
Aug 29 15:22:55.053: INFO: Terminating ReplicationController wrapped-volume-race-ebb1c3db-81fc-4502-8afa-a6ccd33d503b pods took: 204.407144ms
STEP: Creating RC which spawns configmap-volume pods
Aug 29 15:22:57.889: INFO: Pod name wrapped-volume-race-8172f899-8600-4394-9aca-32a7399ddcad: Found 0 pods out of 5
Aug 29 15:23:02.917: INFO: Pod name wrapped-volume-race-8172f899-8600-4394-9aca-32a7399ddcad: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8172f899-8600-4394-9aca-32a7399ddcad in namespace emptydir-wrapper-4195, will wait for the garbage collector to delete the pods
Aug 29 15:23:15.100: INFO: Deleting ReplicationController wrapped-volume-race-8172f899-8600-4394-9aca-32a7399ddcad took: 17.082906ms
Aug 29 15:23:15.201: INFO: Terminating ReplicationController wrapped-volume-race-8172f899-8600-4394-9aca-32a7399ddcad pods took: 100.490361ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Aug 29 15:23:19.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4195" for this suite.

• [SLOW TEST:63.956 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":35,"skipped":652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:23:19.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2594.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2594.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2594.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2594.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2594.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2594.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2594.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2594.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2594.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2594.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 105.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.105_udp@PTR;check="$$(dig +tcp +noall +answer +search 105.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.105_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2594.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2594.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2594.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2594.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2594.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2594.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2594.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2594.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2594.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2594.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 105.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.105_udp@PTR;check="$$(dig +tcp +noall +answer +search 105.20.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.20.105_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 15:23:29.264: INFO: Unable to read wheezy_udp@dns-test-service.dns-2594.svc.cluster.local from pod dns-2594/dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362: the server could not find the requested resource (get pods dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362)
Aug 29 15:23:29.272: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2594.svc.cluster.local from pod dns-2594/dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362: the server could not find the requested resource (get pods dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362)
Aug 29 15:23:29.282: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local from pod dns-2594/dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362: the server could not find the requested resource (get pods dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362)
Aug 29 15:23:29.291: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local from pod dns-2594/dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362: the server could not find the requested resource (get pods dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362)
Aug 29 15:23:29.357: INFO: Unable to read jessie_udp@dns-test-service.dns-2594.svc.cluster.local from pod dns-2594/dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362: the server could not find the requested resource (get pods dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362)
Aug 29 15:23:29.374: INFO: Unable to read jessie_tcp@dns-test-service.dns-2594.svc.cluster.local from pod dns-2594/dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362: the server could not find the requested resource (get pods dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362)
Aug 29 15:23:29.384: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local from pod dns-2594/dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362: the server could not find the requested resource (get pods dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362)
Aug 29 15:23:29.392: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local from pod dns-2594/dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362: the server could not find the requested resource (get pods dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362)
Aug 29 15:23:29.461: INFO: Lookups using dns-2594/dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362 failed for: [wheezy_udp@dns-test-service.dns-2594.svc.cluster.local wheezy_tcp@dns-test-service.dns-2594.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local jessie_udp@dns-test-service.dns-2594.svc.cluster.local jessie_tcp@dns-test-service.dns-2594.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2594.svc.cluster.local]

Aug 29 15:23:34.621: INFO: DNS probes using dns-2594/dns-test-b452ad0d-253d-4bcc-bdf0-c34f42252362 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 29 15:23:35.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2594" for this suite.

• [SLOW TEST:16.874 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":36,"skipped":679,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:23:36.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-84e7d2af-dd57-4e52-a51a-d43a0b7a3b38
STEP: Creating a pod to test consume configMaps
Aug 29 15:23:36.076: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce7ec18f-ed47-4fc6-b77a-2dbddd8bc43b" in namespace "configmap-1032" to be "Succeeded or Failed"
Aug 29 15:23:36.087: INFO: Pod "pod-configmaps-ce7ec18f-ed47-4fc6-b77a-2dbddd8bc43b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.727012ms
Aug 29 15:23:38.096: INFO: Pod "pod-configmaps-ce7ec18f-ed47-4fc6-b77a-2dbddd8bc43b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019712092s
Aug 29 15:23:40.107: INFO: Pod "pod-configmaps-ce7ec18f-ed47-4fc6-b77a-2dbddd8bc43b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030210134s
STEP: Saw pod success
Aug 29 15:23:40.107: INFO: Pod "pod-configmaps-ce7ec18f-ed47-4fc6-b77a-2dbddd8bc43b" satisfied condition "Succeeded or Failed"
Aug 29 15:23:40.113: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-configmaps-ce7ec18f-ed47-4fc6-b77a-2dbddd8bc43b container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:23:40.629: INFO: Waiting for pod pod-configmaps-ce7ec18f-ed47-4fc6-b77a-2dbddd8bc43b to disappear
Aug 29 15:23:40.637: INFO: Pod pod-configmaps-ce7ec18f-ed47-4fc6-b77a-2dbddd8bc43b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 15:23:40.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1032" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":37,"skipped":684,"failed":0}
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:23:40.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Aug 29 15:23:40.742: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 15:24:40.799: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:24:40.807: INFO: Starting informer...
STEP: Starting pod...
Aug 29 15:24:41.028: INFO: Pod is running on ip-172-31-16-214.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 29 15:24:41.065: INFO: Pod wasn't evicted. Proceeding
Aug 29 15:24:41.065: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 29 15:25:56.104: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Aug 29 15:25:56.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5149" for this suite.

• [SLOW TEST:135.458 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":38,"skipped":685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:25:56.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 29 15:26:01.299: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 29 15:26:01.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6935" for this suite.

• [SLOW TEST:5.256 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":39,"skipped":723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:01.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-33d65570-cec3-4d7d-aa9d-376f94a71b73
STEP: Creating a pod to test consume secrets
Aug 29 15:26:01.764: INFO: Waiting up to 5m0s for pod "pod-secrets-c9cf0e94-604e-4eab-8a17-4cb37e37893e" in namespace "secrets-9349" to be "Succeeded or Failed"
Aug 29 15:26:01.776: INFO: Pod "pod-secrets-c9cf0e94-604e-4eab-8a17-4cb37e37893e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.255597ms
Aug 29 15:26:03.784: INFO: Pod "pod-secrets-c9cf0e94-604e-4eab-8a17-4cb37e37893e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019244649s
Aug 29 15:26:05.795: INFO: Pod "pod-secrets-c9cf0e94-604e-4eab-8a17-4cb37e37893e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030665735s
Aug 29 15:26:07.808: INFO: Pod "pod-secrets-c9cf0e94-604e-4eab-8a17-4cb37e37893e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043138957s
STEP: Saw pod success
Aug 29 15:26:07.808: INFO: Pod "pod-secrets-c9cf0e94-604e-4eab-8a17-4cb37e37893e" satisfied condition "Succeeded or Failed"
Aug 29 15:26:07.814: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-secrets-c9cf0e94-604e-4eab-8a17-4cb37e37893e container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:26:08.065: INFO: Waiting for pod pod-secrets-c9cf0e94-604e-4eab-8a17-4cb37e37893e to disappear
Aug 29 15:26:08.072: INFO: Pod pod-secrets-c9cf0e94-604e-4eab-8a17-4cb37e37893e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 29 15:26:08.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9349" for this suite.
STEP: Destroying namespace "secret-namespace-3162" for this suite.

• [SLOW TEST:6.707 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":40,"skipped":745,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:08.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-1508/secret-test-689e0ccb-7b1a-417a-bca8-5e6029f115ef
STEP: Creating a pod to test consume secrets
Aug 29 15:26:08.235: INFO: Waiting up to 5m0s for pod "pod-configmaps-d53dbb9c-c9df-4a76-85d3-dcc2fd811f94" in namespace "secrets-1508" to be "Succeeded or Failed"
Aug 29 15:26:08.246: INFO: Pod "pod-configmaps-d53dbb9c-c9df-4a76-85d3-dcc2fd811f94": Phase="Pending", Reason="", readiness=false. Elapsed: 11.639351ms
Aug 29 15:26:10.256: INFO: Pod "pod-configmaps-d53dbb9c-c9df-4a76-85d3-dcc2fd811f94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021565263s
Aug 29 15:26:12.266: INFO: Pod "pod-configmaps-d53dbb9c-c9df-4a76-85d3-dcc2fd811f94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03167821s
STEP: Saw pod success
Aug 29 15:26:12.267: INFO: Pod "pod-configmaps-d53dbb9c-c9df-4a76-85d3-dcc2fd811f94" satisfied condition "Succeeded or Failed"
Aug 29 15:26:12.273: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-configmaps-d53dbb9c-c9df-4a76-85d3-dcc2fd811f94 container env-test: <nil>
STEP: delete the pod
Aug 29 15:26:12.304: INFO: Waiting for pod pod-configmaps-d53dbb9c-c9df-4a76-85d3-dcc2fd811f94 to disappear
Aug 29 15:26:12.309: INFO: Pod pod-configmaps-d53dbb9c-c9df-4a76-85d3-dcc2fd811f94 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 29 15:26:12.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1508" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":41,"skipped":751,"failed":0}

------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:12.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 29 15:26:12.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6069" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":42,"skipped":751,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:12.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:26:12.526: INFO: Creating ReplicaSet my-hostname-basic-ba67895a-6380-4e95-b09b-9475299bfe69
Aug 29 15:26:12.563: INFO: Pod name my-hostname-basic-ba67895a-6380-4e95-b09b-9475299bfe69: Found 0 pods out of 1
Aug 29 15:26:17.727: INFO: Pod name my-hostname-basic-ba67895a-6380-4e95-b09b-9475299bfe69: Found 1 pods out of 1
Aug 29 15:26:17.727: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ba67895a-6380-4e95-b09b-9475299bfe69" is running
Aug 29 15:26:17.737: INFO: Pod "my-hostname-basic-ba67895a-6380-4e95-b09b-9475299bfe69-pg2pc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:26:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:26:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:26:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 15:26:12 +0000 UTC Reason: Message:}])
Aug 29 15:26:17.737: INFO: Trying to dial the pod
Aug 29 15:26:22.766: INFO: Controller my-hostname-basic-ba67895a-6380-4e95-b09b-9475299bfe69: Got expected result from replica 1 [my-hostname-basic-ba67895a-6380-4e95-b09b-9475299bfe69-pg2pc]: "my-hostname-basic-ba67895a-6380-4e95-b09b-9475299bfe69-pg2pc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 29 15:26:22.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2187" for this suite.

• [SLOW TEST:10.339 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":43,"skipped":751,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:22.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:22.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-8591
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Aug 29 15:26:23.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-6183" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 29 15:26:23.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8591" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":44,"skipped":776,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:23.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 29 15:26:28.900: INFO: Successfully updated pod "adopt-release-7672z"
STEP: Checking that the Job readopts the Pod
Aug 29 15:26:28.900: INFO: Waiting up to 15m0s for pod "adopt-release-7672z" in namespace "job-3035" to be "adopted"
Aug 29 15:26:28.923: INFO: Pod "adopt-release-7672z": Phase="Running", Reason="", readiness=true. Elapsed: 23.172827ms
Aug 29 15:26:30.934: INFO: Pod "adopt-release-7672z": Phase="Running", Reason="", readiness=true. Elapsed: 2.033621998s
Aug 29 15:26:30.934: INFO: Pod "adopt-release-7672z" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 29 15:26:31.454: INFO: Successfully updated pod "adopt-release-7672z"
STEP: Checking that the Job releases the Pod
Aug 29 15:26:31.455: INFO: Waiting up to 15m0s for pod "adopt-release-7672z" in namespace "job-3035" to be "released"
Aug 29 15:26:32.064: INFO: Pod "adopt-release-7672z": Phase="Running", Reason="", readiness=true. Elapsed: 609.599415ms
Aug 29 15:26:34.073: INFO: Pod "adopt-release-7672z": Phase="Running", Reason="", readiness=true. Elapsed: 2.617865362s
Aug 29 15:26:34.073: INFO: Pod "adopt-release-7672z" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 29 15:26:34.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3035" for this suite.

• [SLOW TEST:10.280 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":45,"skipped":796,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:34.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-18e609d6-0f3f-4aaa-817d-0cf2ebabb1b2
STEP: Creating a pod to test consume secrets
Aug 29 15:26:34.255: INFO: Waiting up to 5m0s for pod "pod-secrets-bebdcae9-283a-4673-a742-018004d0b5d4" in namespace "secrets-2908" to be "Succeeded or Failed"
Aug 29 15:26:34.264: INFO: Pod "pod-secrets-bebdcae9-283a-4673-a742-018004d0b5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.437936ms
Aug 29 15:26:36.274: INFO: Pod "pod-secrets-bebdcae9-283a-4673-a742-018004d0b5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018500486s
Aug 29 15:26:38.283: INFO: Pod "pod-secrets-bebdcae9-283a-4673-a742-018004d0b5d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027660425s
STEP: Saw pod success
Aug 29 15:26:38.284: INFO: Pod "pod-secrets-bebdcae9-283a-4673-a742-018004d0b5d4" satisfied condition "Succeeded or Failed"
Aug 29 15:26:38.290: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-secrets-bebdcae9-283a-4673-a742-018004d0b5d4 container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:26:38.326: INFO: Waiting for pod pod-secrets-bebdcae9-283a-4673-a742-018004d0b5d4 to disappear
Aug 29 15:26:38.336: INFO: Pod pod-secrets-bebdcae9-283a-4673-a742-018004d0b5d4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 29 15:26:38.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2908" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":46,"skipped":798,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:38.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Aug 29 15:26:38.408: INFO: namespace kubectl-5905
Aug 29 15:26:38.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-5905 create -f -'
Aug 29 15:26:38.740: INFO: stderr: ""
Aug 29 15:26:38.740: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 29 15:26:39.748: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:26:39.749: INFO: Found 0 / 1
Aug 29 15:26:40.750: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:26:40.750: INFO: Found 0 / 1
Aug 29 15:26:41.748: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:26:41.749: INFO: Found 1 / 1
Aug 29 15:26:41.749: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 29 15:26:41.755: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 15:26:41.755: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 29 15:26:41.755: INFO: wait on agnhost-primary startup in kubectl-5905 
Aug 29 15:26:41.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-5905 logs agnhost-primary-2fxqk agnhost-primary'
Aug 29 15:26:41.852: INFO: stderr: ""
Aug 29 15:26:41.852: INFO: stdout: "Paused\n"
STEP: exposing RC
Aug 29 15:26:41.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-5905 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 29 15:26:41.976: INFO: stderr: ""
Aug 29 15:26:41.976: INFO: stdout: "service/rm2 exposed\n"
Aug 29 15:26:41.994: INFO: Service rm2 in namespace kubectl-5905 found.
STEP: exposing service
Aug 29 15:26:44.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-5905 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 29 15:26:44.165: INFO: stderr: ""
Aug 29 15:26:44.165: INFO: stdout: "service/rm3 exposed\n"
Aug 29 15:26:44.176: INFO: Service rm3 in namespace kubectl-5905 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 15:26:46.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5905" for this suite.

• [SLOW TEST:7.860 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":47,"skipped":799,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:46.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:26:46.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 29 15:26:46.298: INFO: The status of Pod pod-exec-websocket-3feebe43-9e7a-44ac-8937-64bdd38879ce is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:26:48.305: INFO: The status of Pod pod-exec-websocket-3feebe43-9e7a-44ac-8937-64bdd38879ce is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 29 15:26:48.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-182" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":48,"skipped":815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:48.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 15:26:48.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea6422a6-a5f6-4934-881d-fd7ee8b164c7" in namespace "projected-2266" to be "Succeeded or Failed"
Aug 29 15:26:48.580: INFO: Pod "downwardapi-volume-ea6422a6-a5f6-4934-881d-fd7ee8b164c7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.373305ms
Aug 29 15:26:50.594: INFO: Pod "downwardapi-volume-ea6422a6-a5f6-4934-881d-fd7ee8b164c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037686907s
Aug 29 15:26:52.612: INFO: Pod "downwardapi-volume-ea6422a6-a5f6-4934-881d-fd7ee8b164c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055899546s
STEP: Saw pod success
Aug 29 15:26:52.612: INFO: Pod "downwardapi-volume-ea6422a6-a5f6-4934-881d-fd7ee8b164c7" satisfied condition "Succeeded or Failed"
Aug 29 15:26:52.618: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-ea6422a6-a5f6-4934-881d-fd7ee8b164c7 container client-container: <nil>
STEP: delete the pod
Aug 29 15:26:52.651: INFO: Waiting for pod downwardapi-volume-ea6422a6-a5f6-4934-881d-fd7ee8b164c7 to disappear
Aug 29 15:26:52.658: INFO: Pod downwardapi-volume-ea6422a6-a5f6-4934-881d-fd7ee8b164c7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 15:26:52.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2266" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":49,"skipped":845,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:26:52.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-60c6c5aa-d6c8-416c-b979-0cc339b48297 in namespace container-probe-2150
Aug 29 15:26:54.783: INFO: Started pod liveness-60c6c5aa-d6c8-416c-b979-0cc339b48297 in namespace container-probe-2150
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 15:26:54.791: INFO: Initial restart count of pod liveness-60c6c5aa-d6c8-416c-b979-0cc339b48297 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 29 15:30:56.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2150" for this suite.

• [SLOW TEST:243.957 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":50,"skipped":864,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:30:56.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 29 15:30:56.815: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 15:31:56.892: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:31:56.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:31:57.002: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Aug 29 15:31:57.009: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Aug 29 15:31:57.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9420" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 29 15:31:57.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1089" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:60.651 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":51,"skipped":865,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:31:57.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 29 15:31:57.409: INFO: Waiting up to 5m0s for pod "pod-6015a0fd-2acb-4c53-aed6-1d22c13361bd" in namespace "emptydir-9822" to be "Succeeded or Failed"
Aug 29 15:31:57.415: INFO: Pod "pod-6015a0fd-2acb-4c53-aed6-1d22c13361bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.480006ms
Aug 29 15:31:59.423: INFO: Pod "pod-6015a0fd-2acb-4c53-aed6-1d22c13361bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014533803s
Aug 29 15:32:01.438: INFO: Pod "pod-6015a0fd-2acb-4c53-aed6-1d22c13361bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029770584s
STEP: Saw pod success
Aug 29 15:32:01.439: INFO: Pod "pod-6015a0fd-2acb-4c53-aed6-1d22c13361bd" satisfied condition "Succeeded or Failed"
Aug 29 15:32:01.445: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-6015a0fd-2acb-4c53-aed6-1d22c13361bd container test-container: <nil>
STEP: delete the pod
Aug 29 15:32:01.495: INFO: Waiting for pod pod-6015a0fd-2acb-4c53-aed6-1d22c13361bd to disappear
Aug 29 15:32:01.502: INFO: Pod pod-6015a0fd-2acb-4c53-aed6-1d22c13361bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 15:32:01.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9822" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":52,"skipped":872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:32:01.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 29 15:32:01.646: INFO: Waiting up to 5m0s for pod "pod-92fed320-531a-4bac-9b42-aedd8fde22f5" in namespace "emptydir-6869" to be "Succeeded or Failed"
Aug 29 15:32:01.658: INFO: Pod "pod-92fed320-531a-4bac-9b42-aedd8fde22f5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.674982ms
Aug 29 15:32:03.678: INFO: Pod "pod-92fed320-531a-4bac-9b42-aedd8fde22f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032060985s
Aug 29 15:32:05.691: INFO: Pod "pod-92fed320-531a-4bac-9b42-aedd8fde22f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045203938s
Aug 29 15:32:07.699: INFO: Pod "pod-92fed320-531a-4bac-9b42-aedd8fde22f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053016905s
STEP: Saw pod success
Aug 29 15:32:07.699: INFO: Pod "pod-92fed320-531a-4bac-9b42-aedd8fde22f5" satisfied condition "Succeeded or Failed"
Aug 29 15:32:07.711: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-92fed320-531a-4bac-9b42-aedd8fde22f5 container test-container: <nil>
STEP: delete the pod
Aug 29 15:32:07.754: INFO: Waiting for pod pod-92fed320-531a-4bac-9b42-aedd8fde22f5 to disappear
Aug 29 15:32:07.770: INFO: Pod pod-92fed320-531a-4bac-9b42-aedd8fde22f5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 15:32:07.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6869" for this suite.

• [SLOW TEST:6.274 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":53,"skipped":928,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:32:07.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Aug 29 15:32:07.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9203 api-versions'
Aug 29 15:32:08.002: INFO: stderr: ""
Aug 29 15:32:08.002: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps.kubermatic.k8c.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 15:32:08.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9203" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":54,"skipped":933,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:32:08.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-f8c783d2-c9c0-4786-aac8-ce1f6ff9c642 in namespace container-probe-5657
Aug 29 15:32:10.129: INFO: Started pod busybox-f8c783d2-c9c0-4786-aac8-ce1f6ff9c642 in namespace container-probe-5657
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 15:32:10.135: INFO: Initial restart count of pod busybox-f8c783d2-c9c0-4786-aac8-ce1f6ff9c642 is 0
Aug 29 15:33:00.408: INFO: Restart count of pod container-probe-5657/busybox-f8c783d2-c9c0-4786-aac8-ce1f6ff9c642 is now 1 (50.273177113s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 29 15:33:00.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5657" for this suite.

• [SLOW TEST:52.415 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":55,"skipped":950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:33:00.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:33:01.203: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 29 15:33:01.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:33:01.234: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:33:02.257: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:33:02.257: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:33:03.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:33:03.350: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:33:04.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:33:04.255: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:33:05.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:33:05.267: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:33:07.189: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 15:33:07.190: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 29 15:33:07.245: INFO: Wrong image for pod: daemon-set-4ddw2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:07.245: INFO: Wrong image for pod: daemon-set-jf4mq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:07.246: INFO: Wrong image for pod: daemon-set-rf4xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:08.292: INFO: Wrong image for pod: daemon-set-jf4mq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:08.292: INFO: Wrong image for pod: daemon-set-rf4xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:09.284: INFO: Wrong image for pod: daemon-set-jf4mq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:09.284: INFO: Wrong image for pod: daemon-set-rf4xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:10.287: INFO: Wrong image for pod: daemon-set-jf4mq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:10.287: INFO: Wrong image for pod: daemon-set-rf4xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:11.287: INFO: Wrong image for pod: daemon-set-jf4mq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:11.287: INFO: Wrong image for pod: daemon-set-rf4xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:11.287: INFO: Pod daemon-set-wx4hh is not available
Aug 29 15:33:12.286: INFO: Wrong image for pod: daemon-set-jf4mq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:13.290: INFO: Wrong image for pod: daemon-set-jf4mq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Aug 29 15:33:13.290: INFO: Pod daemon-set-zgf7b is not available
Aug 29 15:33:16.286: INFO: Pod daemon-set-qrv6l is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 29 15:33:16.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:33:16.313: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:33:17.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 15:33:17.343: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5179, will wait for the garbage collector to delete the pods
Aug 29 15:33:17.453: INFO: Deleting DaemonSet.extensions daemon-set took: 13.662566ms
Aug 29 15:33:17.559: INFO: Terminating DaemonSet.extensions daemon-set pods took: 106.108251ms
Aug 29 15:33:20.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:33:20.173: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 15:33:20.189: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10969"},"items":null}

Aug 29 15:33:20.198: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10969"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 29 15:33:20.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5179" for this suite.

• [SLOW TEST:19.807 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":56,"skipped":979,"failed":0}
S
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:33:20.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Aug 29 15:33:20.419: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Aug 29 15:33:22.582: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 29 15:33:24.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-829" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":57,"skipped":980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:33:24.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-9cdd8a8f-c925-4f83-84a6-44dd5104414f
STEP: Creating a pod to test consume secrets
Aug 29 15:33:24.747: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5f52f57-27e1-4485-8c44-9272443e1a1c" in namespace "projected-8094" to be "Succeeded or Failed"
Aug 29 15:33:24.763: INFO: Pod "pod-projected-secrets-d5f52f57-27e1-4485-8c44-9272443e1a1c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.051348ms
Aug 29 15:33:26.783: INFO: Pod "pod-projected-secrets-d5f52f57-27e1-4485-8c44-9272443e1a1c": Phase="Running", Reason="", readiness=false. Elapsed: 2.035856707s
Aug 29 15:33:28.793: INFO: Pod "pod-projected-secrets-d5f52f57-27e1-4485-8c44-9272443e1a1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045456743s
STEP: Saw pod success
Aug 29 15:33:28.794: INFO: Pod "pod-projected-secrets-d5f52f57-27e1-4485-8c44-9272443e1a1c" satisfied condition "Succeeded or Failed"
Aug 29 15:33:28.801: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-projected-secrets-d5f52f57-27e1-4485-8c44-9272443e1a1c container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:33:28.838: INFO: Waiting for pod pod-projected-secrets-d5f52f57-27e1-4485-8c44-9272443e1a1c to disappear
Aug 29 15:33:28.843: INFO: Pod pod-projected-secrets-d5f52f57-27e1-4485-8c44-9272443e1a1c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 29 15:33:28.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8094" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":58,"skipped":1004,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:33:28.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:33:29.551: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 15:33:31.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 33, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:33:34.616: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 29 15:33:36.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=webhook-5319 attach --namespace=webhook-5319 to-be-attached-pod -i -c=container1'
Aug 29 15:33:37.165: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 15:33:37.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5319" for this suite.
STEP: Destroying namespace "webhook-5319-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.435 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":59,"skipped":1005,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:33:37.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-5fb6464e-46b1-4754-830b-0def58ef87ec
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 15:33:39.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3373" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":60,"skipped":1010,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:33:39.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 29 15:33:39.607: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 29 15:33:43.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-601" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":61,"skipped":1027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:33:43.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 29 15:33:43.874: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 15:34:43.934: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Aug 29 15:34:43.977: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 29 15:34:43.993: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 29 15:34:44.041: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 29 15:34:44.052: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 29 15:34:44.084: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 29 15:34:44.102: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 29 15:35:00.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1040" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:77.009 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":62,"skipped":1061,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:35:00.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:35:00.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Creating first CR 
Aug 29 15:35:03.519: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T15:35:03Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T15:35:03Z]] name:name1 resourceVersion:11771 uid:86f15e8e-9b44-4ff0-9314-1468fa97e940] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 29 15:35:13.548: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T15:35:13Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T15:35:13Z]] name:name2 resourceVersion:11877 uid:2a12b43d-ba0d-45a2-b954-abbbe57aefe0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 29 15:35:23.570: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T15:35:03Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T15:35:23Z]] name:name1 resourceVersion:11904 uid:86f15e8e-9b44-4ff0-9314-1468fa97e940] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 29 15:35:33.603: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T15:35:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T15:35:33Z]] name:name2 resourceVersion:11935 uid:2a12b43d-ba0d-45a2-b954-abbbe57aefe0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 29 15:35:43.631: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T15:35:03Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T15:35:23Z]] name:name1 resourceVersion:11964 uid:86f15e8e-9b44-4ff0-9314-1468fa97e940] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 29 15:35:53.662: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-08-29T15:35:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-08-29T15:35:33Z]] name:name2 resourceVersion:11994 uid:2a12b43d-ba0d-45a2-b954-abbbe57aefe0] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 15:36:04.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-4993" for this suite.

• [SLOW TEST:64.049 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":63,"skipped":1078,"failed":0}
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:36:04.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:36:05.349: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5c619b9d-020a-42e4-91cf-3c94bc9e6d7e" in namespace "security-context-test-7877" to be "Succeeded or Failed"
Aug 29 15:36:05.373: INFO: Pod "alpine-nnp-false-5c619b9d-020a-42e4-91cf-3c94bc9e6d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.253521ms
Aug 29 15:36:07.383: INFO: Pod "alpine-nnp-false-5c619b9d-020a-42e4-91cf-3c94bc9e6d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034233165s
Aug 29 15:36:09.391: INFO: Pod "alpine-nnp-false-5c619b9d-020a-42e4-91cf-3c94bc9e6d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042082285s
Aug 29 15:36:11.404: INFO: Pod "alpine-nnp-false-5c619b9d-020a-42e4-91cf-3c94bc9e6d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054737361s
Aug 29 15:36:11.404: INFO: Pod "alpine-nnp-false-5c619b9d-020a-42e4-91cf-3c94bc9e6d7e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 29 15:36:11.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7877" for this suite.

• [SLOW TEST:7.028 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":64,"skipped":1079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:36:11.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 29 15:36:11.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:36:11.987: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:36:13.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:36:13.030: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:36:14.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 15:36:14.004: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Aug 29 15:36:14.017: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Aug 29 15:36:14.033: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Aug 29 15:36:14.039: INFO: Observed &DaemonSet event: ADDED
Aug 29 15:36:14.039: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.040: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.040: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.040: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.041: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.041: INFO: Found daemon set daemon-set in namespace daemonsets-7082 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 15:36:14.041: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Aug 29 15:36:14.060: INFO: Observed &DaemonSet event: ADDED
Aug 29 15:36:14.060: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.060: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.060: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.061: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.061: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.061: INFO: Observed daemon set daemon-set in namespace daemonsets-7082 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 15:36:14.061: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 15:36:14.061: INFO: Found daemon set daemon-set in namespace daemonsets-7082 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 29 15:36:14.061: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7082, will wait for the garbage collector to delete the pods
Aug 29 15:36:14.135: INFO: Deleting DaemonSet.extensions daemon-set took: 10.768989ms
Aug 29 15:36:14.236: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.813353ms
Aug 29 15:36:16.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:36:16.849: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 15:36:16.856: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12157"},"items":null}

Aug 29 15:36:16.861: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12157"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 29 15:36:17.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7082" for this suite.

• [SLOW TEST:5.305 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":65,"skipped":1114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:36:17.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:36:18.012: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:36:21.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 15:36:31.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-375" for this suite.
STEP: Destroying namespace "webhook-375-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:14.312 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":66,"skipped":1178,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:36:31.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Aug 29 15:36:31.588: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:36:34.081: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Aug 29 15:36:34.139: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:36:36.151: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Aug 29 15:36:36.209: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 29 15:36:36.454: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 29 15:36:38.455: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 29 15:36:38.465: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 29 15:36:40.456: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 29 15:36:40.471: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 29 15:36:40.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8293" for this suite.

• [SLOW TEST:9.056 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":67,"skipped":1197,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:36:40.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 29 15:37:08.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6310" for this suite.

• [SLOW TEST:27.504 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":68,"skipped":1203,"failed":0}
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:37:08.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Aug 29 15:37:08.180: INFO: The status of Pod labelsupdate9cc0f124-1b25-4cd9-a0cb-c3b8c8ba29a1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:37:10.398: INFO: The status of Pod labelsupdate9cc0f124-1b25-4cd9-a0cb-c3b8c8ba29a1 is Running (Ready = true)
Aug 29 15:37:10.951: INFO: Successfully updated pod "labelsupdate9cc0f124-1b25-4cd9-a0cb-c3b8c8ba29a1"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 15:37:15.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3698" for this suite.

• [SLOW TEST:6.988 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":69,"skipped":1203,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:37:15.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:37:15.091: INFO: Creating deployment "test-recreate-deployment"
Aug 29 15:37:15.106: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 29 15:37:15.124: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 29 15:37:17.186: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 29 15:37:17.198: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 29 15:37:17.218: INFO: Updating deployment test-recreate-deployment
Aug 29 15:37:17.218: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 15:37:17.347: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1766  6e21a42f-c556-4e80-9fa6-890b06732bc3 12655 2 2022-08-29 15:37:15 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-29 15:37:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 15:37:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00158b578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-29 15:37:17 +0000 UTC,LastTransitionTime:2022-08-29 15:37:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2022-08-29 15:37:17 +0000 UTC,LastTransitionTime:2022-08-29 15:37:15 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 29 15:37:17.357: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-1766  c62ea5cb-a4e1-454e-bc6e-c0ea992eebe1 12652 1 2022-08-29 15:37:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6e21a42f-c556-4e80-9fa6-890b06732bc3 0xc000cf75f0 0xc000cf75f1}] []  [{kube-controller-manager Update apps/v1 2022-08-29 15:37:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e21a42f-c556-4e80-9fa6-890b06732bc3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 15:37:17 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000cf7688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 15:37:17.357: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 29 15:37:17.357: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-845d658455  deployment-1766  17576989-c7bd-49b4-af56-eddb2d1492c1 12643 2 2022-08-29 15:37:15 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6e21a42f-c556-4e80-9fa6-890b06732bc3 0xc000cf74c7 0xc000cf74c8}] []  [{kube-controller-manager Update apps/v1 2022-08-29 15:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e21a42f-c556-4e80-9fa6-890b06732bc3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 15:37:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 845d658455,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000cf7588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 15:37:17.367: INFO: Pod "test-recreate-deployment-cd8586fc7-9fn8d" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-9fn8d test-recreate-deployment-cd8586fc7- deployment-1766  6b95d374-1c67-4277-81c0-6cb4e81ddc84 12654 0 2022-08-29 15:37:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 c62ea5cb-a4e1-454e-bc6e-c0ea992eebe1 0xc00158b8f0 0xc00158b8f1}] []  [{kube-controller-manager Update v1 2022-08-29 15:37:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c62ea5cb-a4e1-454e-bc6e-c0ea992eebe1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 15:37:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n25dn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n25dn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:37:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:37:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:37:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 15:37:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:,StartTime:2022-08-29 15:37:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 29 15:37:17.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1766" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":70,"skipped":1205,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:37:17.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Aug 29 15:37:17.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Aug 29 15:37:18.107: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Aug 29 15:37:20.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:37:22.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:37:24.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:37:26.419: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:37:28.411: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:37:30.413: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 37, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 15:37:32.771: INFO: Waited 340.539666ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Aug 29 15:37:33.023: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Aug 29 15:37:34.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6414" for this suite.

• [SLOW TEST:16.710 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":71,"skipped":1222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:37:34.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:37:34.206: INFO: Endpoints addresses: [192.168.30.10] , ports: [6443]
Aug 29 15:37:34.206: INFO: EndpointSlices addresses: [192.168.30.10] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 29 15:37:34.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7638" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":72,"skipped":1262,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:37:34.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Aug 29 15:37:34.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 create -f -'
Aug 29 15:37:35.287: INFO: stderr: ""
Aug 29 15:37:35.287: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 29 15:37:35.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 15:37:35.423: INFO: stderr: ""
Aug 29 15:37:35.423: INFO: stdout: "update-demo-nautilus-qfpbl update-demo-nautilus-qmqx9 "
Aug 29 15:37:35.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 get pods update-demo-nautilus-qfpbl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 15:37:35.528: INFO: stderr: ""
Aug 29 15:37:35.528: INFO: stdout: ""
Aug 29 15:37:35.528: INFO: update-demo-nautilus-qfpbl is created but not running
Aug 29 15:37:40.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 15:37:40.758: INFO: stderr: ""
Aug 29 15:37:40.758: INFO: stdout: "update-demo-nautilus-qfpbl update-demo-nautilus-qmqx9 "
Aug 29 15:37:40.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 get pods update-demo-nautilus-qfpbl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 15:37:40.872: INFO: stderr: ""
Aug 29 15:37:40.873: INFO: stdout: "true"
Aug 29 15:37:40.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 get pods update-demo-nautilus-qfpbl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 15:37:41.048: INFO: stderr: ""
Aug 29 15:37:41.048: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 15:37:41.048: INFO: validating pod update-demo-nautilus-qfpbl
Aug 29 15:37:41.059: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 15:37:41.059: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 15:37:41.059: INFO: update-demo-nautilus-qfpbl is verified up and running
Aug 29 15:37:41.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 get pods update-demo-nautilus-qmqx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 15:37:41.216: INFO: stderr: ""
Aug 29 15:37:41.216: INFO: stdout: "true"
Aug 29 15:37:41.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 get pods update-demo-nautilus-qmqx9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 15:37:41.411: INFO: stderr: ""
Aug 29 15:37:41.411: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 15:37:41.411: INFO: validating pod update-demo-nautilus-qmqx9
Aug 29 15:37:41.429: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 15:37:41.429: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 15:37:41.429: INFO: update-demo-nautilus-qmqx9 is verified up and running
STEP: using delete to clean up resources
Aug 29 15:37:41.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 delete --grace-period=0 --force -f -'
Aug 29 15:37:41.596: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 15:37:41.596: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 29 15:37:41.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 get rc,svc -l name=update-demo --no-headers'
Aug 29 15:37:41.694: INFO: stderr: "No resources found in kubectl-6450 namespace.\n"
Aug 29 15:37:41.694: INFO: stdout: ""
Aug 29 15:37:41.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6450 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 29 15:37:41.794: INFO: stderr: ""
Aug 29 15:37:41.794: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 15:37:41.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6450" for this suite.

• [SLOW TEST:7.583 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":73,"skipped":1271,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:37:41.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 29 15:37:56.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7496" for this suite.

• [SLOW TEST:14.289 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":74,"skipped":1280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:37:56.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:37:58.219: INFO: Deleting pod "var-expansion-2fea4489-1b19-40e5-99bd-fa0d28ee4447" in namespace "var-expansion-3501"
Aug 29 15:37:58.234: INFO: Wait up to 5m0s for pod "var-expansion-2fea4489-1b19-40e5-99bd-fa0d28ee4447" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 29 15:38:00.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3501" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":75,"skipped":1313,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:38:00.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-79cdb6b5-5b65-49ce-a0b6-49578c085d0f
STEP: Creating secret with name secret-projected-all-test-volume-ca18a106-36ca-4568-b83c-25122bfd1ed3
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 29 15:38:00.393: INFO: Waiting up to 5m0s for pod "projected-volume-68d0dc88-83ed-4f81-b6d3-90b85f251a45" in namespace "projected-4647" to be "Succeeded or Failed"
Aug 29 15:38:00.409: INFO: Pod "projected-volume-68d0dc88-83ed-4f81-b6d3-90b85f251a45": Phase="Pending", Reason="", readiness=false. Elapsed: 16.201131ms
Aug 29 15:38:02.974: INFO: Pod "projected-volume-68d0dc88-83ed-4f81-b6d3-90b85f251a45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580338476s
Aug 29 15:38:04.985: INFO: Pod "projected-volume-68d0dc88-83ed-4f81-b6d3-90b85f251a45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.591721981s
STEP: Saw pod success
Aug 29 15:38:04.985: INFO: Pod "projected-volume-68d0dc88-83ed-4f81-b6d3-90b85f251a45" satisfied condition "Succeeded or Failed"
Aug 29 15:38:04.990: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod projected-volume-68d0dc88-83ed-4f81-b6d3-90b85f251a45 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 29 15:38:05.051: INFO: Waiting for pod projected-volume-68d0dc88-83ed-4f81-b6d3-90b85f251a45 to disappear
Aug 29 15:38:05.057: INFO: Pod projected-volume-68d0dc88-83ed-4f81-b6d3-90b85f251a45 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Aug 29 15:38:05.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4647" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":76,"skipped":1319,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:38:05.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-f523e5d1-51a3-4c25-be34-b3e4589bbbb3 in namespace container-probe-9002
Aug 29 15:38:08.445: INFO: Started pod busybox-f523e5d1-51a3-4c25-be34-b3e4589bbbb3 in namespace container-probe-9002
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 15:38:08.453: INFO: Initial restart count of pod busybox-f523e5d1-51a3-4c25-be34-b3e4589bbbb3 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 29 15:42:09.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9002" for this suite.

• [SLOW TEST:244.182 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":77,"skipped":1342,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:42:09.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-9nqr
STEP: Creating a pod to test atomic-volume-subpath
Aug 29 15:42:09.399: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9nqr" in namespace "subpath-3218" to be "Succeeded or Failed"
Aug 29 15:42:09.413: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Pending", Reason="", readiness=false. Elapsed: 14.079708ms
Aug 29 15:42:11.425: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 2.026272503s
Aug 29 15:42:13.436: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 4.037061426s
Aug 29 15:42:15.447: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 6.048095296s
Aug 29 15:42:17.460: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 8.060997441s
Aug 29 15:42:19.469: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 10.070634974s
Aug 29 15:42:21.480: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 12.081499742s
Aug 29 15:42:23.491: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 14.092851914s
Aug 29 15:42:25.504: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 16.105282878s
Aug 29 15:42:27.519: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 18.120587796s
Aug 29 15:42:29.529: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 20.130044785s
Aug 29 15:42:31.540: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Running", Reason="", readiness=true. Elapsed: 22.141491229s
Aug 29 15:42:33.558: INFO: Pod "pod-subpath-test-secret-9nqr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.15926041s
STEP: Saw pod success
Aug 29 15:42:33.558: INFO: Pod "pod-subpath-test-secret-9nqr" satisfied condition "Succeeded or Failed"
Aug 29 15:42:33.566: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-subpath-test-secret-9nqr container test-container-subpath-secret-9nqr: <nil>
STEP: delete the pod
Aug 29 15:42:33.614: INFO: Waiting for pod pod-subpath-test-secret-9nqr to disappear
Aug 29 15:42:33.622: INFO: Pod pod-subpath-test-secret-9nqr no longer exists
STEP: Deleting pod pod-subpath-test-secret-9nqr
Aug 29 15:42:33.622: INFO: Deleting pod "pod-subpath-test-secret-9nqr" in namespace "subpath-3218"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 29 15:42:33.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3218" for this suite.

• [SLOW TEST:24.378 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":78,"skipped":1355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:42:33.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4745
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4745
STEP: Waiting until pod test-pod will start running in namespace statefulset-4745
STEP: Creating statefulset with conflicting port in namespace statefulset-4745
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4745
Aug 29 15:42:36.774: INFO: Observed stateful pod in namespace: statefulset-4745, name: ss-0, uid: b2b475e0-ac93-4a58-aef7-7ebb7fbedf6b, status phase: Pending. Waiting for statefulset controller to delete.
Aug 29 15:42:36.806: INFO: Observed stateful pod in namespace: statefulset-4745, name: ss-0, uid: b2b475e0-ac93-4a58-aef7-7ebb7fbedf6b, status phase: Failed. Waiting for statefulset controller to delete.
Aug 29 15:42:36.820: INFO: Observed stateful pod in namespace: statefulset-4745, name: ss-0, uid: b2b475e0-ac93-4a58-aef7-7ebb7fbedf6b, status phase: Failed. Waiting for statefulset controller to delete.
Aug 29 15:42:36.824: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4745
STEP: Removing pod with conflicting port in namespace statefulset-4745
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4745 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 29 15:42:38.880: INFO: Deleting all statefulset in ns statefulset-4745
Aug 29 15:42:38.892: INFO: Scaling statefulset ss to 0
Aug 29 15:42:48.927: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 15:42:48.950: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 29 15:42:49.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4745" for this suite.

• [SLOW TEST:15.399 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":79,"skipped":1384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:42:49.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-6e98f9ff-e12b-4b14-b519-8b28238a71ab
STEP: Creating a pod to test consume secrets
Aug 29 15:42:49.144: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-420c462a-1560-4b63-b8b1-580c4482024b" in namespace "projected-4761" to be "Succeeded or Failed"
Aug 29 15:42:49.164: INFO: Pod "pod-projected-secrets-420c462a-1560-4b63-b8b1-580c4482024b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.89066ms
Aug 29 15:42:51.173: INFO: Pod "pod-projected-secrets-420c462a-1560-4b63-b8b1-580c4482024b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028990301s
Aug 29 15:42:53.227: INFO: Pod "pod-projected-secrets-420c462a-1560-4b63-b8b1-580c4482024b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082551515s
STEP: Saw pod success
Aug 29 15:42:53.227: INFO: Pod "pod-projected-secrets-420c462a-1560-4b63-b8b1-580c4482024b" satisfied condition "Succeeded or Failed"
Aug 29 15:42:53.241: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-projected-secrets-420c462a-1560-4b63-b8b1-580c4482024b container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:42:53.301: INFO: Waiting for pod pod-projected-secrets-420c462a-1560-4b63-b8b1-580c4482024b to disappear
Aug 29 15:42:53.308: INFO: Pod pod-projected-secrets-420c462a-1560-4b63-b8b1-580c4482024b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 29 15:42:53.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4761" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":80,"skipped":1440,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:42:53.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Aug 29 15:42:53.441: INFO: Waiting up to 5m0s for pod "client-containers-caa1f6b9-1204-4c6d-a469-0fd65f46ec86" in namespace "containers-8100" to be "Succeeded or Failed"
Aug 29 15:42:53.452: INFO: Pod "client-containers-caa1f6b9-1204-4c6d-a469-0fd65f46ec86": Phase="Pending", Reason="", readiness=false. Elapsed: 10.74356ms
Aug 29 15:42:55.463: INFO: Pod "client-containers-caa1f6b9-1204-4c6d-a469-0fd65f46ec86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02170077s
Aug 29 15:42:57.472: INFO: Pod "client-containers-caa1f6b9-1204-4c6d-a469-0fd65f46ec86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030639647s
STEP: Saw pod success
Aug 29 15:42:57.473: INFO: Pod "client-containers-caa1f6b9-1204-4c6d-a469-0fd65f46ec86" satisfied condition "Succeeded or Failed"
Aug 29 15:42:57.479: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod client-containers-caa1f6b9-1204-4c6d-a469-0fd65f46ec86 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:42:57.515: INFO: Waiting for pod client-containers-caa1f6b9-1204-4c6d-a469-0fd65f46ec86 to disappear
Aug 29 15:42:57.521: INFO: Pod client-containers-caa1f6b9-1204-4c6d-a469-0fd65f46ec86 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 29 15:42:57.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8100" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":81,"skipped":1448,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:42:57.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 29 15:42:57.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7578" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":82,"skipped":1461,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:42:57.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0829 15:42:58.408124      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 15:42:58.408: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 29 15:42:58.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5431" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":83,"skipped":1462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:42:58.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 29 15:42:59.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:42:59.398: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:43:00.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 15:43:00.417: INFO: Node ip-172-31-25-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:43:01.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 15:43:01.422: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Aug 29 15:43:01.502: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14215"},"items":null}

Aug 29 15:43:01.511: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14216"},"items":[{"metadata":{"name":"daemon-set-m9jth","generateName":"daemon-set-","namespace":"daemonsets-7372","uid":"808aafd7-d868-4d74-b93e-6cb9dd99503c","resourceVersion":"14216","creationTimestamp":"2022-08-29T15:42:59Z","deletionTimestamp":"2022-08-29T15:43:31Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"664c730af8d82babbb12a53181572df5c890cf2fa0a66034fdc942c140670046","cni.projectcalico.org/podIP":"172.25.2.88/32","cni.projectcalico.org/podIPs":"172.25.2.88/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a603dfc3-b415-45da-aa0a-0365fbc7f5a4","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-08-29T15:42:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-29T15:42:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a603dfc3-b415-45da-aa0a-0365fbc7f5a4\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-29T15:43:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xwtpg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xwtpg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-25-142.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-25-142.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:42:59Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:43:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:43:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:42:59Z"}],"hostIP":"172.31.25.142","podIP":"172.25.2.88","podIPs":[{"ip":"172.25.2.88"}],"startTime":"2022-08-29T15:42:59Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-29T15:43:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://1f662ca774ed312190d8f2e2d3aa9e1b46e68d4d4d10b67520f807c32a1eaf30","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-pqwb5","generateName":"daemon-set-","namespace":"daemonsets-7372","uid":"5b38330c-3d0a-4887-b4e9-e9e5b268b391","resourceVersion":"14213","creationTimestamp":"2022-08-29T15:42:59Z","deletionTimestamp":"2022-08-29T15:43:31Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4f63b7f2c33c8bc0ea28936334b6617cd1956b2bc048d7433d23cbc06f7c7266","cni.projectcalico.org/podIP":"172.25.1.83/32","cni.projectcalico.org/podIPs":"172.25.1.83/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a603dfc3-b415-45da-aa0a-0365fbc7f5a4","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-08-29T15:42:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-29T15:42:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a603dfc3-b415-45da-aa0a-0365fbc7f5a4\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-29T15:43:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-klj8q","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-klj8q","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-16-214.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-16-214.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:42:59Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:43:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:43:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:42:59Z"}],"hostIP":"172.31.16.214","podIP":"172.25.1.83","podIPs":[{"ip":"172.25.1.83"}],"startTime":"2022-08-29T15:42:59Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-29T15:43:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://bf5a7b771d7d99d5c37e4aafc6001c1eb6799464de194f185fad96a9a5d0367e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v2z46","generateName":"daemon-set-","namespace":"daemonsets-7372","uid":"bd2b3b5a-3a2e-41d0-818e-1a96b4d44303","resourceVersion":"14214","creationTimestamp":"2022-08-29T15:42:59Z","deletionTimestamp":"2022-08-29T15:43:31Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b86179d44655c6405a516ccb370deb0876cc0a41f97029f81b77b7bf8d784612","cni.projectcalico.org/podIP":"172.25.0.52/32","cni.projectcalico.org/podIPs":"172.25.0.52/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a603dfc3-b415-45da-aa0a-0365fbc7f5a4","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-08-29T15:42:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-08-29T15:42:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a603dfc3-b415-45da-aa0a-0365fbc7f5a4\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-08-29T15:43:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wtp4z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wtp4z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-26-197.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-26-197.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:42:59Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:43:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:43:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-08-29T15:42:59Z"}],"hostIP":"172.31.26.197","podIP":"172.25.0.52","podIPs":[{"ip":"172.25.0.52"}],"startTime":"2022-08-29T15:42:59Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-08-29T15:43:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://44e06f21e221fdf3b2b8e850c6d69c90a72da7ac90310efd7ca56fa28fc7cdbf","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 29 15:43:01.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7372" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":84,"skipped":1502,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:43:01.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7446 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7446;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7446 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7446;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7446.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7446.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7446.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7446.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7446.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7446.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7446.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7446.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7446.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7446.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7446.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7446.svc;check="$$(dig +notcp +noall +answer +search 157.25.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.25.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.25.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.25.157_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7446 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7446;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7446 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7446;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7446.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7446.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7446.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7446.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7446.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7446.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7446.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7446.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7446.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7446.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7446.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7446.svc;check="$$(dig +notcp +noall +answer +search 157.25.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.25.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.25.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.25.157_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 15:43:05.856: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:05.866: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:05.875: INFO: Unable to read wheezy_udp@dns-test-service.dns-7446 from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:05.884: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7446 from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:05.904: INFO: Unable to read wheezy_udp@dns-test-service.dns-7446.svc from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:05.912: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7446.svc from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:05.921: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7446.svc from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:05.936: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7446.svc from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:05.974: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:05.982: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:05.989: INFO: Unable to read jessie_udp@dns-test-service.dns-7446 from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:06.003: INFO: Unable to read jessie_tcp@dns-test-service.dns-7446 from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:06.013: INFO: Unable to read jessie_udp@dns-test-service.dns-7446.svc from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:06.026: INFO: Unable to read jessie_tcp@dns-test-service.dns-7446.svc from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:06.035: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7446.svc from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:06.046: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7446.svc from pod dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb: the server could not find the requested resource (get pods dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb)
Aug 29 15:43:06.086: INFO: Lookups using dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7446 wheezy_tcp@dns-test-service.dns-7446 wheezy_udp@dns-test-service.dns-7446.svc wheezy_tcp@dns-test-service.dns-7446.svc wheezy_udp@_http._tcp.dns-test-service.dns-7446.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7446.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7446 jessie_tcp@dns-test-service.dns-7446 jessie_udp@dns-test-service.dns-7446.svc jessie_tcp@dns-test-service.dns-7446.svc jessie_udp@_http._tcp.dns-test-service.dns-7446.svc jessie_tcp@_http._tcp.dns-test-service.dns-7446.svc]

Aug 29 15:43:11.295: INFO: DNS probes using dns-7446/dns-test-4e617cf7-b1f9-47ec-918b-d0ec6fe339cb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 29 15:43:11.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7446" for this suite.

• [SLOW TEST:10.137 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":85,"skipped":1506,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:43:11.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Aug 29 15:43:11.768: INFO: created test-podtemplate-1
Aug 29 15:43:11.777: INFO: created test-podtemplate-2
Aug 29 15:43:11.792: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Aug 29 15:43:11.802: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Aug 29 15:43:11.847: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Aug 29 15:43:11.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8551" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":86,"skipped":1519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:43:11.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 29 15:43:11.979: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4208  4ce14a86-4b69-41f2-8d87-3e2407a04e12 14389 0 2022-08-29 15:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-29 15:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:43:11.979: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4208  4ce14a86-4b69-41f2-8d87-3e2407a04e12 14390 0 2022-08-29 15:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-29 15:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 29 15:43:12.011: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4208  4ce14a86-4b69-41f2-8d87-3e2407a04e12 14391 0 2022-08-29 15:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-29 15:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 15:43:12.012: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4208  4ce14a86-4b69-41f2-8d87-3e2407a04e12 14392 0 2022-08-29 15:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-08-29 15:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 29 15:43:12.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4208" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":87,"skipped":1563,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:43:12.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:43:12.706: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:43:15.738: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 15:43:15.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2086" for this suite.
STEP: Destroying namespace "webhook-2086-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":88,"skipped":1577,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:43:15.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:43:16.052: INFO: created pod pod-service-account-defaultsa
Aug 29 15:43:16.053: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 29 15:43:16.073: INFO: created pod pod-service-account-mountsa
Aug 29 15:43:16.074: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 29 15:43:16.088: INFO: created pod pod-service-account-nomountsa
Aug 29 15:43:16.088: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 29 15:43:16.101: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 29 15:43:16.101: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 29 15:43:16.117: INFO: created pod pod-service-account-mountsa-mountspec
Aug 29 15:43:16.117: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 29 15:43:16.140: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 29 15:43:16.140: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 29 15:43:16.150: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 29 15:43:16.150: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 29 15:43:16.168: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 29 15:43:16.168: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 29 15:43:16.200: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 29 15:43:16.200: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 29 15:43:16.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7016" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":89,"skipped":1587,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:43:16.257: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-g426
STEP: Creating a pod to test atomic-volume-subpath
Aug 29 15:43:16.375: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-g426" in namespace "subpath-3715" to be "Succeeded or Failed"
Aug 29 15:43:16.386: INFO: Pod "pod-subpath-test-projected-g426": Phase="Pending", Reason="", readiness=false. Elapsed: 10.92422ms
Aug 29 15:43:18.396: INFO: Pod "pod-subpath-test-projected-g426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020725757s
Aug 29 15:43:20.403: INFO: Pod "pod-subpath-test-projected-g426": Phase="Running", Reason="", readiness=true. Elapsed: 4.028135513s
Aug 29 15:43:22.426: INFO: Pod "pod-subpath-test-projected-g426": Phase="Running", Reason="", readiness=true. Elapsed: 6.051304022s
Aug 29 15:43:24.456: INFO: Pod "pod-subpath-test-projected-g426": Phase="Running", Reason="", readiness=true. Elapsed: 8.080773253s
Aug 29 15:43:26.467: INFO: Pod "pod-subpath-test-projected-g426": Phase="Running", Reason="", readiness=true. Elapsed: 10.091759998s
Aug 29 15:43:28.479: INFO: Pod "pod-subpath-test-projected-g426": Phase="Running", Reason="", readiness=true. Elapsed: 12.10374026s
Aug 29 15:43:30.488: INFO: Pod "pod-subpath-test-projected-g426": Phase="Running", Reason="", readiness=true. Elapsed: 14.112360318s
Aug 29 15:43:32.498: INFO: Pod "pod-subpath-test-projected-g426": Phase="Running", Reason="", readiness=true. Elapsed: 16.122898418s
Aug 29 15:43:34.507: INFO: Pod "pod-subpath-test-projected-g426": Phase="Running", Reason="", readiness=true. Elapsed: 18.132186198s
Aug 29 15:43:36.519: INFO: Pod "pod-subpath-test-projected-g426": Phase="Running", Reason="", readiness=true. Elapsed: 20.144166649s
Aug 29 15:43:38.529: INFO: Pod "pod-subpath-test-projected-g426": Phase="Running", Reason="", readiness=false. Elapsed: 22.154225335s
Aug 29 15:43:40.541: INFO: Pod "pod-subpath-test-projected-g426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.165943874s
STEP: Saw pod success
Aug 29 15:43:40.541: INFO: Pod "pod-subpath-test-projected-g426" satisfied condition "Succeeded or Failed"
Aug 29 15:43:40.552: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-subpath-test-projected-g426 container test-container-subpath-projected-g426: <nil>
STEP: delete the pod
Aug 29 15:43:40.602: INFO: Waiting for pod pod-subpath-test-projected-g426 to disappear
Aug 29 15:43:40.609: INFO: Pod pod-subpath-test-projected-g426 no longer exists
STEP: Deleting pod pod-subpath-test-projected-g426
Aug 29 15:43:40.609: INFO: Deleting pod "pod-subpath-test-projected-g426" in namespace "subpath-3715"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 29 15:43:40.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3715" for this suite.

• [SLOW TEST:24.377 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":90,"skipped":1606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:43:40.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-5061c5eb-97f3-4459-934b-eb8e78116ee7
STEP: Creating a pod to test consume configMaps
Aug 29 15:43:40.775: INFO: Waiting up to 5m0s for pod "pod-configmaps-a788df2e-87ae-48ed-9c4b-46bcb0793aac" in namespace "configmap-4420" to be "Succeeded or Failed"
Aug 29 15:43:40.787: INFO: Pod "pod-configmaps-a788df2e-87ae-48ed-9c4b-46bcb0793aac": Phase="Pending", Reason="", readiness=false. Elapsed: 12.145573ms
Aug 29 15:43:42.806: INFO: Pod "pod-configmaps-a788df2e-87ae-48ed-9c4b-46bcb0793aac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030771802s
Aug 29 15:43:44.815: INFO: Pod "pod-configmaps-a788df2e-87ae-48ed-9c4b-46bcb0793aac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039452183s
STEP: Saw pod success
Aug 29 15:43:44.815: INFO: Pod "pod-configmaps-a788df2e-87ae-48ed-9c4b-46bcb0793aac" satisfied condition "Succeeded or Failed"
Aug 29 15:43:44.820: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-configmaps-a788df2e-87ae-48ed-9c4b-46bcb0793aac container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:43:44.850: INFO: Waiting for pod pod-configmaps-a788df2e-87ae-48ed-9c4b-46bcb0793aac to disappear
Aug 29 15:43:44.856: INFO: Pod pod-configmaps-a788df2e-87ae-48ed-9c4b-46bcb0793aac no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 15:43:44.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4420" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":91,"skipped":1695,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:43:44.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-634b751f-c0ba-43ff-8a6f-25b84c8f9ca4 in namespace container-probe-6074
Aug 29 15:43:46.963: INFO: Started pod test-webserver-634b751f-c0ba-43ff-8a6f-25b84c8f9ca4 in namespace container-probe-6074
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 15:43:46.971: INFO: Initial restart count of pod test-webserver-634b751f-c0ba-43ff-8a6f-25b84c8f9ca4 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 29 15:47:48.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6074" for this suite.

• [SLOW TEST:243.981 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":92,"skipped":1718,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:47:48.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-66e7adde-38a9-4f64-9220-8889a7501fc1
STEP: Creating a pod to test consume secrets
Aug 29 15:47:48.962: INFO: Waiting up to 5m0s for pod "pod-secrets-64f89b6f-15c6-4867-9e7d-f1234cf41b81" in namespace "secrets-390" to be "Succeeded or Failed"
Aug 29 15:47:48.979: INFO: Pod "pod-secrets-64f89b6f-15c6-4867-9e7d-f1234cf41b81": Phase="Pending", Reason="", readiness=false. Elapsed: 17.243814ms
Aug 29 15:47:50.996: INFO: Pod "pod-secrets-64f89b6f-15c6-4867-9e7d-f1234cf41b81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033530254s
Aug 29 15:47:53.007: INFO: Pod "pod-secrets-64f89b6f-15c6-4867-9e7d-f1234cf41b81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045355435s
STEP: Saw pod success
Aug 29 15:47:53.007: INFO: Pod "pod-secrets-64f89b6f-15c6-4867-9e7d-f1234cf41b81" satisfied condition "Succeeded or Failed"
Aug 29 15:47:53.014: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-secrets-64f89b6f-15c6-4867-9e7d-f1234cf41b81 container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 15:47:53.047: INFO: Waiting for pod pod-secrets-64f89b6f-15c6-4867-9e7d-f1234cf41b81 to disappear
Aug 29 15:47:53.054: INFO: Pod pod-secrets-64f89b6f-15c6-4867-9e7d-f1234cf41b81 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 29 15:47:53.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-390" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":93,"skipped":1729,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:47:53.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3691
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:47:53.180: INFO: Found 0 stateful pods, waiting for 1
Aug 29 15:48:03.196: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Aug 29 15:48:03.243: INFO: Found 1 stateful pods, waiting for 2
Aug 29 15:48:13.261: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 15:48:13.261: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 29 15:48:13.309: INFO: Deleting all statefulset in ns statefulset-3691
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 29 15:48:13.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3691" for this suite.

• [SLOW TEST:20.287 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":94,"skipped":1776,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:48:13.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:48:13.480: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 29 15:48:13.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:48:13.514: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Aug 29 15:48:13.568: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:48:13.568: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:48:14.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:48:14.578: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:48:15.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 29 15:48:15.577: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 29 15:48:15.661: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:48:15.665: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 29 15:48:15.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:48:15.681: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:48:16.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:48:16.690: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:48:17.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:48:17.690: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:48:18.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:48:18.690: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 15:48:19.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 29 15:48:19.689: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-652, will wait for the garbage collector to delete the pods
Aug 29 15:48:19.770: INFO: Deleting DaemonSet.extensions daemon-set took: 12.406108ms
Aug 29 15:48:19.871: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.58945ms
Aug 29 15:48:22.281: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 15:48:22.281: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 15:48:22.290: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15775"},"items":null}

Aug 29 15:48:22.296: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15775"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 29 15:48:22.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-652" for this suite.

• [SLOW TEST:8.999 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":95,"skipped":1777,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:48:22.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-9eab73d6-a938-4a2a-b372-88f191c228cc
STEP: Creating a pod to test consume configMaps
Aug 29 15:48:22.472: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-842075cd-9695-41ff-a32b-b38cb138aa26" in namespace "projected-8772" to be "Succeeded or Failed"
Aug 29 15:48:22.485: INFO: Pod "pod-projected-configmaps-842075cd-9695-41ff-a32b-b38cb138aa26": Phase="Pending", Reason="", readiness=false. Elapsed: 12.407438ms
Aug 29 15:48:24.495: INFO: Pod "pod-projected-configmaps-842075cd-9695-41ff-a32b-b38cb138aa26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022856412s
Aug 29 15:48:26.508: INFO: Pod "pod-projected-configmaps-842075cd-9695-41ff-a32b-b38cb138aa26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035631836s
STEP: Saw pod success
Aug 29 15:48:26.508: INFO: Pod "pod-projected-configmaps-842075cd-9695-41ff-a32b-b38cb138aa26" satisfied condition "Succeeded or Failed"
Aug 29 15:48:26.513: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-projected-configmaps-842075cd-9695-41ff-a32b-b38cb138aa26 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 15:48:26.600: INFO: Waiting for pod pod-projected-configmaps-842075cd-9695-41ff-a32b-b38cb138aa26 to disappear
Aug 29 15:48:26.610: INFO: Pod pod-projected-configmaps-842075cd-9695-41ff-a32b-b38cb138aa26 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 29 15:48:26.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8772" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":96,"skipped":1794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:48:26.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 15:48:27.850: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 15:48:29.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 15, 48, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 48, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 15, 48, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 15, 48, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 15:48:32.908: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 15:48:34.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9365" for this suite.
STEP: Destroying namespace "webhook-9365-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.603 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":97,"skipped":1816,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:48:34.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 29 15:48:34.333: INFO: Waiting up to 5m0s for pod "pod-6019f87d-6e7b-427a-b241-67e962585bcc" in namespace "emptydir-344" to be "Succeeded or Failed"
Aug 29 15:48:34.351: INFO: Pod "pod-6019f87d-6e7b-427a-b241-67e962585bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.834477ms
Aug 29 15:48:36.363: INFO: Pod "pod-6019f87d-6e7b-427a-b241-67e962585bcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029856796s
Aug 29 15:48:38.382: INFO: Pod "pod-6019f87d-6e7b-427a-b241-67e962585bcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048408718s
STEP: Saw pod success
Aug 29 15:48:38.382: INFO: Pod "pod-6019f87d-6e7b-427a-b241-67e962585bcc" satisfied condition "Succeeded or Failed"
Aug 29 15:48:38.393: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-6019f87d-6e7b-427a-b241-67e962585bcc container test-container: <nil>
STEP: delete the pod
Aug 29 15:48:38.446: INFO: Waiting for pod pod-6019f87d-6e7b-427a-b241-67e962585bcc to disappear
Aug 29 15:48:38.450: INFO: Pod pod-6019f87d-6e7b-427a-b241-67e962585bcc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 15:48:38.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-344" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":98,"skipped":1834,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:48:38.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename conformance-tests
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Aug 29 15:48:38.527: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Aug 29 15:48:38.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-1934" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":99,"skipped":1854,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:48:38.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Aug 29 15:48:49.699: INFO: 69 pods remaining
Aug 29 15:48:49.699: INFO: 69 pods has nil DeletionTimestamp
Aug 29 15:48:49.699: INFO: 
STEP: Gathering metrics
Aug 29 15:48:54.712: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 29 15:48:54.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-22qz7" in namespace "gc-2894"
W0829 15:48:54.712735      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 15:48:55.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cz8z" in namespace "gc-2894"
Aug 29 15:48:55.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-2g2tz" in namespace "gc-2894"
Aug 29 15:48:55.438: INFO: Deleting pod "simpletest-rc-to-be-deleted-44f9r" in namespace "gc-2894"
Aug 29 15:48:55.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-487dt" in namespace "gc-2894"
Aug 29 15:48:55.534: INFO: Deleting pod "simpletest-rc-to-be-deleted-48fmm" in namespace "gc-2894"
Aug 29 15:48:55.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fncx" in namespace "gc-2894"
Aug 29 15:48:55.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vv4q" in namespace "gc-2894"
Aug 29 15:48:55.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-56hc2" in namespace "gc-2894"
Aug 29 15:48:55.651: INFO: Deleting pod "simpletest-rc-to-be-deleted-5j8d9" in namespace "gc-2894"
Aug 29 15:48:55.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tccx" in namespace "gc-2894"
Aug 29 15:48:55.705: INFO: Deleting pod "simpletest-rc-to-be-deleted-62mcx" in namespace "gc-2894"
Aug 29 15:48:55.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dl75" in namespace "gc-2894"
Aug 29 15:48:55.831: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nhv5" in namespace "gc-2894"
Aug 29 15:48:55.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-72p4t" in namespace "gc-2894"
Aug 29 15:48:55.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fqd5" in namespace "gc-2894"
Aug 29 15:48:55.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lfqf" in namespace "gc-2894"
Aug 29 15:48:56.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-7r7p6" in namespace "gc-2894"
Aug 29 15:48:56.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-7w6rj" in namespace "gc-2894"
Aug 29 15:48:56.126: INFO: Deleting pod "simpletest-rc-to-be-deleted-82p87" in namespace "gc-2894"
Aug 29 15:48:56.161: INFO: Deleting pod "simpletest-rc-to-be-deleted-8blpj" in namespace "gc-2894"
Aug 29 15:48:56.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fj55" in namespace "gc-2894"
Aug 29 15:48:56.454: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rj4h" in namespace "gc-2894"
Aug 29 15:48:56.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-8snhs" in namespace "gc-2894"
Aug 29 15:48:56.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-92gcz" in namespace "gc-2894"
Aug 29 15:48:56.510: INFO: Deleting pod "simpletest-rc-to-be-deleted-92qxl" in namespace "gc-2894"
Aug 29 15:48:56.529: INFO: Deleting pod "simpletest-rc-to-be-deleted-9g88g" in namespace "gc-2894"
Aug 29 15:48:56.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-9np8c" in namespace "gc-2894"
Aug 29 15:48:56.897: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nzw8" in namespace "gc-2894"
Aug 29 15:48:56.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6fhp" in namespace "gc-2894"
Aug 29 15:48:57.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcncr" in namespace "gc-2894"
Aug 29 15:48:57.499: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf5r5" in namespace "gc-2894"
Aug 29 15:48:57.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfnkj" in namespace "gc-2894"
Aug 29 15:48:57.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgsvf" in namespace "gc-2894"
Aug 29 15:48:57.563: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2t86" in namespace "gc-2894"
Aug 29 15:48:57.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7ldq" in namespace "gc-2894"
Aug 29 15:48:57.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9cww" in namespace "gc-2894"
Aug 29 15:48:57.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfqnr" in namespace "gc-2894"
Aug 29 15:48:57.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-f58xh" in namespace "gc-2894"
Aug 29 15:48:57.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbhzl" in namespace "gc-2894"
Aug 29 15:48:57.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffzj6" in namespace "gc-2894"
Aug 29 15:48:57.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwprb" in namespace "gc-2894"
Aug 29 15:48:57.824: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzkhm" in namespace "gc-2894"
Aug 29 15:48:57.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9nxl" in namespace "gc-2894"
Aug 29 15:48:57.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmk7c" in namespace "gc-2894"
Aug 29 15:48:57.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-gt9jk" in namespace "gc-2894"
Aug 29 15:48:57.942: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6sfs" in namespace "gc-2894"
Aug 29 15:48:57.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9n47" in namespace "gc-2894"
Aug 29 15:48:58.021: INFO: Deleting pod "simpletest-rc-to-be-deleted-hf79q" in namespace "gc-2894"
Aug 29 15:48:58.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgbcp" in namespace "gc-2894"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 29 15:48:58.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2894" for this suite.

• [SLOW TEST:19.545 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":100,"skipped":1862,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:48:58.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-943.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-943.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-943.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-943.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 15:49:04.330: INFO: DNS probes using dns-943/dns-test-ba7212f9-50ef-421a-a201-f6908cc57783 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 29 15:49:04.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-943" for this suite.

• [SLOW TEST:6.275 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":101,"skipped":1866,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:49:04.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 29 15:49:04.496: INFO: Waiting up to 5m0s for pod "pod-8ed7ac10-074e-474c-bf7e-b175bc7248c7" in namespace "emptydir-1775" to be "Succeeded or Failed"
Aug 29 15:49:04.508: INFO: Pod "pod-8ed7ac10-074e-474c-bf7e-b175bc7248c7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.109752ms
Aug 29 15:49:06.565: INFO: Pod "pod-8ed7ac10-074e-474c-bf7e-b175bc7248c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068034737s
Aug 29 15:49:08.858: INFO: Pod "pod-8ed7ac10-074e-474c-bf7e-b175bc7248c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.360986424s
Aug 29 15:49:10.870: INFO: Pod "pod-8ed7ac10-074e-474c-bf7e-b175bc7248c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.373316828s
STEP: Saw pod success
Aug 29 15:49:10.870: INFO: Pod "pod-8ed7ac10-074e-474c-bf7e-b175bc7248c7" satisfied condition "Succeeded or Failed"
Aug 29 15:49:10.880: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-8ed7ac10-074e-474c-bf7e-b175bc7248c7 container test-container: <nil>
STEP: delete the pod
Aug 29 15:49:11.214: INFO: Waiting for pod pod-8ed7ac10-074e-474c-bf7e-b175bc7248c7 to disappear
Aug 29 15:49:11.220: INFO: Pod pod-8ed7ac10-074e-474c-bf7e-b175bc7248c7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 15:49:11.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1775" for this suite.

• [SLOW TEST:6.853 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":102,"skipped":1869,"failed":0}
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:49:11.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Aug 29 15:49:11.615: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:49:13.624: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 29 15:49:14.656: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 29 15:49:15.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-321" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":103,"skipped":1869,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:49:15.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Aug 29 15:49:15.812: INFO: observed Pod pod-test in namespace pods-4049 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 29 15:49:15.816: INFO: observed Pod pod-test in namespace pods-4049 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC  }]
Aug 29 15:49:15.836: INFO: observed Pod pod-test in namespace pods-4049 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC  }]
Aug 29 15:49:16.298: INFO: observed Pod pod-test in namespace pods-4049 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC  }]
Aug 29 15:49:16.940: INFO: Found Pod pod-test in namespace pods-4049 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 15:49:15 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Aug 29 15:49:17.021: INFO: observed event type MODIFIED
Aug 29 15:49:18.944: INFO: observed event type MODIFIED
Aug 29 15:49:19.487: INFO: observed event type MODIFIED
Aug 29 15:49:19.954: INFO: observed event type MODIFIED
Aug 29 15:49:19.969: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 29 15:49:19.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4049" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":104,"skipped":1871,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:49:20.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:49:20.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 15:49:26.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-122" for this suite.

• [SLOW TEST:6.593 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":105,"skipped":1876,"failed":0}
SS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:49:26.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 29 15:54:26.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5135" for this suite.

• [SLOW TEST:300.140 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":106,"skipped":1878,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:54:26.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 15:54:26.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Aug 29 15:54:30.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 create -f -'
Aug 29 15:54:31.630: INFO: stderr: ""
Aug 29 15:54:31.630: INFO: stdout: "e2e-test-crd-publish-openapi-8533-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 29 15:54:31.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 delete e2e-test-crd-publish-openapi-8533-crds test-foo'
Aug 29 15:54:32.037: INFO: stderr: ""
Aug 29 15:54:32.037: INFO: stdout: "e2e-test-crd-publish-openapi-8533-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 29 15:54:32.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 apply -f -'
Aug 29 15:54:33.444: INFO: stderr: ""
Aug 29 15:54:33.444: INFO: stdout: "e2e-test-crd-publish-openapi-8533-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 29 15:54:33.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 delete e2e-test-crd-publish-openapi-8533-crds test-foo'
Aug 29 15:54:33.610: INFO: stderr: ""
Aug 29 15:54:33.610: INFO: stdout: "e2e-test-crd-publish-openapi-8533-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Aug 29 15:54:33.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 create -f -'
Aug 29 15:54:33.873: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 29 15:54:33.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 create -f -'
Aug 29 15:54:34.752: INFO: rc: 1
Aug 29 15:54:34.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 apply -f -'
Aug 29 15:54:34.943: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Aug 29 15:54:34.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 create -f -'
Aug 29 15:54:35.142: INFO: rc: 1
Aug 29 15:54:35.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 apply -f -'
Aug 29 15:54:35.323: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 29 15:54:35.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 explain e2e-test-crd-publish-openapi-8533-crds'
Aug 29 15:54:35.547: INFO: stderr: ""
Aug 29 15:54:35.547: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8533-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 29 15:54:35.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 explain e2e-test-crd-publish-openapi-8533-crds.metadata'
Aug 29 15:54:35.746: INFO: stderr: ""
Aug 29 15:54:35.746: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8533-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 29 15:54:35.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 explain e2e-test-crd-publish-openapi-8533-crds.spec'
Aug 29 15:54:35.963: INFO: stderr: ""
Aug 29 15:54:35.964: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8533-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 29 15:54:35.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 explain e2e-test-crd-publish-openapi-8533-crds.spec.bars'
Aug 29 15:54:36.162: INFO: stderr: ""
Aug 29 15:54:36.162: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8533-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 29 15:54:36.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-1425 explain e2e-test-crd-publish-openapi-8533-crds.spec.bars2'
Aug 29 15:54:36.383: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 15:54:41.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1425" for this suite.

• [SLOW TEST:14.300 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":107,"skipped":1888,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:54:41.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0829 15:54:42.268372      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 15:54:42.268: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 29 15:54:42.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2325" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":108,"skipped":1899,"failed":0}
S
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:54:42.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Aug 29 15:54:42.366: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:54:44.375: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Aug 29 15:54:44.404: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:54:46.421: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 29 15:54:46.431: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9062 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:54:46.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:54:46.432: INFO: ExecWithOptions: Clientset creation
Aug 29 15:54:46.432: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9062/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 29 15:54:46.575: INFO: Exec stderr: ""
Aug 29 15:54:46.575: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9062 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:54:46.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:54:46.579: INFO: ExecWithOptions: Clientset creation
Aug 29 15:54:46.579: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9062/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 29 15:54:46.826: INFO: Exec stderr: ""
Aug 29 15:54:46.826: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9062 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:54:46.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:54:46.828: INFO: ExecWithOptions: Clientset creation
Aug 29 15:54:46.828: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9062/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 29 15:54:47.009: INFO: Exec stderr: ""
Aug 29 15:54:47.009: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9062 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:54:47.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:54:47.017: INFO: ExecWithOptions: Clientset creation
Aug 29 15:54:47.017: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9062/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 29 15:54:47.152: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 29 15:54:47.152: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9062 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:54:47.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:54:47.155: INFO: ExecWithOptions: Clientset creation
Aug 29 15:54:47.156: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9062/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 29 15:54:47.332: INFO: Exec stderr: ""
Aug 29 15:54:47.332: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9062 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:54:47.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:54:47.332: INFO: ExecWithOptions: Clientset creation
Aug 29 15:54:47.333: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9062/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 29 15:54:47.540: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 29 15:54:47.540: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9062 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:54:47.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:54:47.541: INFO: ExecWithOptions: Clientset creation
Aug 29 15:54:47.541: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9062/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 29 15:54:47.707: INFO: Exec stderr: ""
Aug 29 15:54:47.708: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9062 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:54:47.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:54:47.709: INFO: ExecWithOptions: Clientset creation
Aug 29 15:54:47.709: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9062/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 29 15:54:47.884: INFO: Exec stderr: ""
Aug 29 15:54:47.884: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9062 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:54:47.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:54:47.885: INFO: ExecWithOptions: Clientset creation
Aug 29 15:54:47.885: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9062/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 29 15:54:48.038: INFO: Exec stderr: ""
Aug 29 15:54:48.039: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9062 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 15:54:48.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 15:54:48.039: INFO: ExecWithOptions: Clientset creation
Aug 29 15:54:48.039: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9062/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 29 15:54:48.155: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Aug 29 15:54:48.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9062" for this suite.

• [SLOW TEST:5.866 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":109,"skipped":1900,"failed":0}
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:54:48.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 29 15:56:00.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8745" for this suite.

• [SLOW TEST:72.199 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":110,"skipped":1900,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:56:00.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 29 15:56:00.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-2259 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 29 15:56:00.686: INFO: stderr: ""
Aug 29 15:56:00.686: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 29 15:56:05.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-2259 get pod e2e-test-httpd-pod -o json'
Aug 29 15:56:05.841: INFO: stderr: ""
Aug 29 15:56:05.841: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"42af4b3f04a2a17e2044eba7f0c5b179d7d33c9d91bd40a683269398b93ba5a7\",\n            \"cni.projectcalico.org/podIP\": \"172.25.1.126/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.1.126/32\"\n        },\n        \"creationTimestamp\": \"2022-08-29T15:56:00Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2259\",\n        \"resourceVersion\": \"19276\",\n        \"uid\": \"d2d19ca0-3f57-421d-acaa-13eec4fd2d6e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6zvpg\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-16-214.eu-central-1.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6zvpg\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-29T15:56:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-29T15:56:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-29T15:56:03Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-08-29T15:56:00Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://78d76a20d34c8bbc55616a69412a869e7bc50db5f296b5045129f9bf121a492c\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-08-29T15:56:02Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.16.214\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.1.126\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.1.126\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-08-29T15:56:00Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 29 15:56:05.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-2259 replace -f -'
Aug 29 15:56:06.847: INFO: stderr: ""
Aug 29 15:56:06.847: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Aug 29 15:56:06.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-2259 delete pods e2e-test-httpd-pod'
Aug 29 15:56:08.291: INFO: stderr: ""
Aug 29 15:56:08.292: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 15:56:08.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2259" for this suite.

• [SLOW TEST:7.943 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":111,"skipped":1914,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:56:08.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5152
STEP: creating service affinity-nodeport-transition in namespace services-5152
STEP: creating replication controller affinity-nodeport-transition in namespace services-5152
I0829 15:56:08.426313      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5152, replica count: 3
I0829 15:56:11.491102      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:56:11.514: INFO: Creating new exec pod
Aug 29 15:56:14.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5152 exec execpod-affinityqtpjq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Aug 29 15:56:15.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 29 15:56:15.034: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:56:15.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5152 exec execpod-affinityqtpjq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.30.113 80'
Aug 29 15:56:15.292: INFO: stderr: "+ nc -v -t -w 2 10.240.30.113 80\nConnection to 10.240.30.113 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Aug 29 15:56:15.293: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:56:15.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5152 exec execpod-affinityqtpjq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.26.197 30550'
Aug 29 15:56:15.522: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.26.197 30550\nConnection to 172.31.26.197 30550 port [tcp/*] succeeded!\n"
Aug 29 15:56:15.522: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:56:15.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5152 exec execpod-affinityqtpjq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.214 30550'
Aug 29 15:56:15.759: INFO: stderr: "+ nc -v -t -w 2 172.31.16.214 30550\n+ echo hostName\nConnection to 172.31.16.214 30550 port [tcp/*] succeeded!\n"
Aug 29 15:56:15.759: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:56:15.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5152 exec execpod-affinityqtpjq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.214:30550/ ; done'
Aug 29 15:56:16.239: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n"
Aug 29 15:56:16.240: INFO: stdout: "\naffinity-nodeport-transition-w2pt5\naffinity-nodeport-transition-lwgnt\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-w2pt5\naffinity-nodeport-transition-lwgnt\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-w2pt5\naffinity-nodeport-transition-lwgnt\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-w2pt5\naffinity-nodeport-transition-lwgnt\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-w2pt5\naffinity-nodeport-transition-lwgnt\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-w2pt5"
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-w2pt5
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-lwgnt
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-w2pt5
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-lwgnt
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-w2pt5
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-lwgnt
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-w2pt5
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-lwgnt
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-w2pt5
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-lwgnt
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.240: INFO: Received response from host: affinity-nodeport-transition-w2pt5
Aug 29 15:56:16.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5152 exec execpod-affinityqtpjq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.214:30550/ ; done'
Aug 29 15:56:16.656: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:30550/\n"
Aug 29 15:56:16.656: INFO: stdout: "\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr\naffinity-nodeport-transition-gd8dr"
Aug 29 15:56:16.656: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.656: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.656: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Received response from host: affinity-nodeport-transition-gd8dr
Aug 29 15:56:16.657: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5152, will wait for the garbage collector to delete the pods
Aug 29 15:56:16.753: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.06928ms
Aug 29 15:56:16.854: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.149949ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 15:56:19.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5152" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:11.004 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":112,"skipped":1927,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:56:19.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Aug 29 15:56:34.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1650" for this suite.
STEP: Destroying namespace "nsdeletetest-329" for this suite.
Aug 29 15:56:34.634: INFO: Namespace nsdeletetest-329 was already deleted
STEP: Destroying namespace "nsdeletetest-1178" for this suite.

• [SLOW TEST:15.325 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":113,"skipped":1940,"failed":0}
SSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:56:34.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 29 15:56:34.767: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 29 15:56:34.775: INFO: starting watch
STEP: patching
STEP: updating
Aug 29 15:56:34.805: INFO: waiting for watch events with expected annotations
Aug 29 15:56:34.805: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 29 15:56:34.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4742" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":114,"skipped":1943,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:56:34.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-8d586225-bbfb-4ed3-9f2d-dde23e002603
STEP: Creating configMap with name cm-test-opt-upd-ba93b9fe-ca69-4ee7-bad3-878cb52fa6a7
STEP: Creating the pod
Aug 29 15:56:35.006: INFO: The status of Pod pod-projected-configmaps-25906fae-7807-4ce8-93ce-fd3edaaef22e is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:56:37.022: INFO: The status of Pod pod-projected-configmaps-25906fae-7807-4ce8-93ce-fd3edaaef22e is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:56:39.015: INFO: The status of Pod pod-projected-configmaps-25906fae-7807-4ce8-93ce-fd3edaaef22e is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-8d586225-bbfb-4ed3-9f2d-dde23e002603
STEP: Updating configmap cm-test-opt-upd-ba93b9fe-ca69-4ee7-bad3-878cb52fa6a7
STEP: Creating configMap with name cm-test-opt-create-86575e7d-0576-455e-9508-94b19cc2603a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 29 15:57:52.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4079" for this suite.

• [SLOW TEST:77.232 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":115,"skipped":1946,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 15:57:52.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-3062
Aug 29 15:57:52.212: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Aug 29 15:57:54.222: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Aug 29 15:57:54.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3062 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 29 15:57:54.429: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Aug 29 15:57:54.429: INFO: stdout: "ipvs"
Aug 29 15:57:54.429: INFO: proxyMode: ipvs
Aug 29 15:57:54.445: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 29 15:57:54.451: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3062
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3062
I0829 15:57:54.479940      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3062, replica count: 3
I0829 15:57:57.534983      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 15:57:57.547: INFO: Creating new exec pod
Aug 29 15:58:00.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3062 exec execpod-affinityqkzwb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Aug 29 15:58:00.883: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Aug 29 15:58:00.883: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:58:00.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3062 exec execpod-affinityqkzwb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.27.29 80'
Aug 29 15:58:01.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.27.29 80\nConnection to 10.240.27.29 80 port [tcp/http] succeeded!\n"
Aug 29 15:58:01.222: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 15:58:01.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3062 exec execpod-affinityqkzwb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.27.29:80/ ; done'
Aug 29 15:58:01.538: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n"
Aug 29 15:58:01.538: INFO: stdout: "\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn\naffinity-clusterip-timeout-ngqsn"
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Received response from host: affinity-clusterip-timeout-ngqsn
Aug 29 15:58:01.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3062 exec execpod-affinityqkzwb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.27.29:80/'
Aug 29 15:58:01.767: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n"
Aug 29 15:58:01.767: INFO: stdout: "affinity-clusterip-timeout-ngqsn"
Aug 29 16:00:11.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3062 exec execpod-affinityqkzwb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.27.29:80/'
Aug 29 16:00:12.052: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.27.29:80/\n"
Aug 29 16:00:12.052: INFO: stdout: "affinity-clusterip-timeout-4tdjr"
Aug 29 16:00:12.052: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3062, will wait for the garbage collector to delete the pods
Aug 29 16:00:12.163: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 10.350269ms
Aug 29 16:00:12.265: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.001986ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:00:14.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3062" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:142.097 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":116,"skipped":1949,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:00:14.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:00:15.319: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:00:18.364: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:00:18.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:00:21.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6874" for this suite.
STEP: Destroying namespace "webhook-6874-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.786 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":117,"skipped":1952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:00:22.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 29 16:00:22.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:00:25.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:00:40.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6161" for this suite.

• [SLOW TEST:18.250 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":118,"skipped":1986,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:00:40.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-f25010ba-369e-4da4-8c0b-d1a8cc0f2838
STEP: Creating secret with name s-test-opt-upd-47a39735-72ec-47f8-8ad0-1c3c38c97774
STEP: Creating the pod
Aug 29 16:00:40.480: INFO: The status of Pod pod-projected-secrets-d1ad0b51-ac6e-4985-a04b-2aed71d29c0f is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:00:42.495: INFO: The status of Pod pod-projected-secrets-d1ad0b51-ac6e-4985-a04b-2aed71d29c0f is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-f25010ba-369e-4da4-8c0b-d1a8cc0f2838
STEP: Updating secret s-test-opt-upd-47a39735-72ec-47f8-8ad0-1c3c38c97774
STEP: Creating secret with name s-test-opt-create-637fa251-06bb-4c69-b76d-6a4ec128c2e5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 29 16:02:10.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4496" for this suite.

• [SLOW TEST:90.628 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":119,"skipped":1988,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:02:10.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:02:10.956: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:02:12.982: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = false)
Aug 29 16:02:14.966: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = false)
Aug 29 16:02:16.971: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = false)
Aug 29 16:02:18.972: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = false)
Aug 29 16:02:20.972: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = false)
Aug 29 16:02:22.975: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = false)
Aug 29 16:02:24.966: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = false)
Aug 29 16:02:26.972: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = false)
Aug 29 16:02:28.965: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = false)
Aug 29 16:02:30.975: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = false)
Aug 29 16:02:32.969: INFO: The status of Pod test-webserver-14a33b1c-9d86-4990-a0a7-5c3d244c1907 is Running (Ready = true)
Aug 29 16:02:32.975: INFO: Container started at 2022-08-29 16:02:11 +0000 UTC, pod became ready at 2022-08-29 16:02:31 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 29 16:02:32.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5274" for this suite.

• [SLOW TEST:22.124 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":120,"skipped":1998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:02:33.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6966, will wait for the garbage collector to delete the pods
Aug 29 16:02:37.193: INFO: Deleting Job.batch foo took: 11.61006ms
Aug 29 16:02:37.294: INFO: Terminating Job.batch foo pods took: 101.114555ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 29 16:03:09.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6966" for this suite.

• [SLOW TEST:36.010 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":121,"skipped":2029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:03:09.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:03:09.791: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:03:13.204: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:03:13.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6408-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:03:16.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5136" for this suite.
STEP: Destroying namespace "webhook-5136-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.931 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":122,"skipped":2054,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:03:16.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0829 16:03:57.241240      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 16:03:57.241: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 29 16:03:57.241: INFO: Deleting pod "simpletest.rc-2csdn" in namespace "gc-4711"
Aug 29 16:03:57.348: INFO: Deleting pod "simpletest.rc-2kmsx" in namespace "gc-4711"
Aug 29 16:03:57.401: INFO: Deleting pod "simpletest.rc-2l9j9" in namespace "gc-4711"
Aug 29 16:03:57.434: INFO: Deleting pod "simpletest.rc-2zgdm" in namespace "gc-4711"
Aug 29 16:03:57.466: INFO: Deleting pod "simpletest.rc-4tlkj" in namespace "gc-4711"
Aug 29 16:03:57.498: INFO: Deleting pod "simpletest.rc-4tpn4" in namespace "gc-4711"
Aug 29 16:03:57.542: INFO: Deleting pod "simpletest.rc-4zjbl" in namespace "gc-4711"
Aug 29 16:03:57.605: INFO: Deleting pod "simpletest.rc-5bbxk" in namespace "gc-4711"
Aug 29 16:03:57.659: INFO: Deleting pod "simpletest.rc-5qzfq" in namespace "gc-4711"
Aug 29 16:03:57.708: INFO: Deleting pod "simpletest.rc-5t98p" in namespace "gc-4711"
Aug 29 16:03:57.742: INFO: Deleting pod "simpletest.rc-5tchc" in namespace "gc-4711"
Aug 29 16:03:57.770: INFO: Deleting pod "simpletest.rc-5wlgm" in namespace "gc-4711"
Aug 29 16:03:57.842: INFO: Deleting pod "simpletest.rc-6gkwk" in namespace "gc-4711"
Aug 29 16:03:57.898: INFO: Deleting pod "simpletest.rc-6grks" in namespace "gc-4711"
Aug 29 16:03:57.932: INFO: Deleting pod "simpletest.rc-79z8l" in namespace "gc-4711"
Aug 29 16:03:57.970: INFO: Deleting pod "simpletest.rc-7fpqh" in namespace "gc-4711"
Aug 29 16:03:57.997: INFO: Deleting pod "simpletest.rc-7k22j" in namespace "gc-4711"
Aug 29 16:03:58.026: INFO: Deleting pod "simpletest.rc-7l5ns" in namespace "gc-4711"
Aug 29 16:03:58.052: INFO: Deleting pod "simpletest.rc-7t5z2" in namespace "gc-4711"
Aug 29 16:03:58.099: INFO: Deleting pod "simpletest.rc-7zxnt" in namespace "gc-4711"
Aug 29 16:03:58.146: INFO: Deleting pod "simpletest.rc-874tk" in namespace "gc-4711"
Aug 29 16:03:58.180: INFO: Deleting pod "simpletest.rc-8bzt8" in namespace "gc-4711"
Aug 29 16:03:58.227: INFO: Deleting pod "simpletest.rc-8hctb" in namespace "gc-4711"
Aug 29 16:03:58.256: INFO: Deleting pod "simpletest.rc-8lrvv" in namespace "gc-4711"
Aug 29 16:03:58.323: INFO: Deleting pod "simpletest.rc-8qhhp" in namespace "gc-4711"
Aug 29 16:03:58.376: INFO: Deleting pod "simpletest.rc-926v2" in namespace "gc-4711"
Aug 29 16:03:58.397: INFO: Deleting pod "simpletest.rc-92pb5" in namespace "gc-4711"
Aug 29 16:03:58.429: INFO: Deleting pod "simpletest.rc-9k59w" in namespace "gc-4711"
Aug 29 16:03:58.477: INFO: Deleting pod "simpletest.rc-9mzw5" in namespace "gc-4711"
Aug 29 16:03:58.523: INFO: Deleting pod "simpletest.rc-9n6fn" in namespace "gc-4711"
Aug 29 16:03:58.598: INFO: Deleting pod "simpletest.rc-9t4bl" in namespace "gc-4711"
Aug 29 16:03:58.652: INFO: Deleting pod "simpletest.rc-b6bdv" in namespace "gc-4711"
Aug 29 16:03:58.697: INFO: Deleting pod "simpletest.rc-b9gfq" in namespace "gc-4711"
Aug 29 16:03:58.735: INFO: Deleting pod "simpletest.rc-bbbx5" in namespace "gc-4711"
Aug 29 16:03:58.793: INFO: Deleting pod "simpletest.rc-bc6jc" in namespace "gc-4711"
Aug 29 16:03:58.826: INFO: Deleting pod "simpletest.rc-blbpm" in namespace "gc-4711"
Aug 29 16:03:58.855: INFO: Deleting pod "simpletest.rc-c6pbb" in namespace "gc-4711"
Aug 29 16:03:58.884: INFO: Deleting pod "simpletest.rc-c9xmr" in namespace "gc-4711"
Aug 29 16:03:58.905: INFO: Deleting pod "simpletest.rc-cbs74" in namespace "gc-4711"
Aug 29 16:03:58.929: INFO: Deleting pod "simpletest.rc-chkf8" in namespace "gc-4711"
Aug 29 16:03:58.957: INFO: Deleting pod "simpletest.rc-d5bx2" in namespace "gc-4711"
Aug 29 16:03:58.998: INFO: Deleting pod "simpletest.rc-dbzb5" in namespace "gc-4711"
Aug 29 16:03:59.021: INFO: Deleting pod "simpletest.rc-dh92k" in namespace "gc-4711"
Aug 29 16:03:59.229: INFO: Deleting pod "simpletest.rc-drn54" in namespace "gc-4711"
Aug 29 16:03:59.269: INFO: Deleting pod "simpletest.rc-dtbll" in namespace "gc-4711"
Aug 29 16:03:59.294: INFO: Deleting pod "simpletest.rc-dx47m" in namespace "gc-4711"
Aug 29 16:03:59.314: INFO: Deleting pod "simpletest.rc-dx5ft" in namespace "gc-4711"
Aug 29 16:03:59.377: INFO: Deleting pod "simpletest.rc-fbsbv" in namespace "gc-4711"
Aug 29 16:03:59.415: INFO: Deleting pod "simpletest.rc-fsmfw" in namespace "gc-4711"
Aug 29 16:03:59.460: INFO: Deleting pod "simpletest.rc-g42js" in namespace "gc-4711"
Aug 29 16:03:59.495: INFO: Deleting pod "simpletest.rc-gflvg" in namespace "gc-4711"
Aug 29 16:03:59.530: INFO: Deleting pod "simpletest.rc-gshpm" in namespace "gc-4711"
Aug 29 16:03:59.550: INFO: Deleting pod "simpletest.rc-gwcff" in namespace "gc-4711"
Aug 29 16:03:59.569: INFO: Deleting pod "simpletest.rc-gzl7m" in namespace "gc-4711"
Aug 29 16:03:59.595: INFO: Deleting pod "simpletest.rc-h6x9n" in namespace "gc-4711"
Aug 29 16:03:59.622: INFO: Deleting pod "simpletest.rc-h964h" in namespace "gc-4711"
Aug 29 16:03:59.637: INFO: Deleting pod "simpletest.rc-hd8d5" in namespace "gc-4711"
Aug 29 16:03:59.659: INFO: Deleting pod "simpletest.rc-hwrgr" in namespace "gc-4711"
Aug 29 16:03:59.694: INFO: Deleting pod "simpletest.rc-jdxtl" in namespace "gc-4711"
Aug 29 16:03:59.719: INFO: Deleting pod "simpletest.rc-k2nn6" in namespace "gc-4711"
Aug 29 16:03:59.752: INFO: Deleting pod "simpletest.rc-kchlq" in namespace "gc-4711"
Aug 29 16:03:59.786: INFO: Deleting pod "simpletest.rc-kz7c8" in namespace "gc-4711"
Aug 29 16:03:59.807: INFO: Deleting pod "simpletest.rc-l9bq7" in namespace "gc-4711"
Aug 29 16:03:59.844: INFO: Deleting pod "simpletest.rc-lhk6t" in namespace "gc-4711"
Aug 29 16:03:59.866: INFO: Deleting pod "simpletest.rc-ljtpz" in namespace "gc-4711"
Aug 29 16:03:59.881: INFO: Deleting pod "simpletest.rc-lpwp8" in namespace "gc-4711"
Aug 29 16:03:59.898: INFO: Deleting pod "simpletest.rc-m25ln" in namespace "gc-4711"
Aug 29 16:03:59.932: INFO: Deleting pod "simpletest.rc-m89kj" in namespace "gc-4711"
Aug 29 16:03:59.967: INFO: Deleting pod "simpletest.rc-mmbss" in namespace "gc-4711"
Aug 29 16:03:59.988: INFO: Deleting pod "simpletest.rc-n75zc" in namespace "gc-4711"
Aug 29 16:04:00.062: INFO: Deleting pod "simpletest.rc-n8wpj" in namespace "gc-4711"
Aug 29 16:04:00.094: INFO: Deleting pod "simpletest.rc-nl6mh" in namespace "gc-4711"
Aug 29 16:04:00.119: INFO: Deleting pod "simpletest.rc-nrtvn" in namespace "gc-4711"
Aug 29 16:04:00.156: INFO: Deleting pod "simpletest.rc-p8mct" in namespace "gc-4711"
Aug 29 16:04:00.181: INFO: Deleting pod "simpletest.rc-pghrt" in namespace "gc-4711"
Aug 29 16:04:00.215: INFO: Deleting pod "simpletest.rc-pq5g4" in namespace "gc-4711"
Aug 29 16:04:00.234: INFO: Deleting pod "simpletest.rc-q8rm6" in namespace "gc-4711"
Aug 29 16:04:00.267: INFO: Deleting pod "simpletest.rc-qrzns" in namespace "gc-4711"
Aug 29 16:04:00.286: INFO: Deleting pod "simpletest.rc-qwl6n" in namespace "gc-4711"
Aug 29 16:04:00.308: INFO: Deleting pod "simpletest.rc-rzvvx" in namespace "gc-4711"
Aug 29 16:04:00.334: INFO: Deleting pod "simpletest.rc-sbpbk" in namespace "gc-4711"
Aug 29 16:04:00.369: INFO: Deleting pod "simpletest.rc-sf92n" in namespace "gc-4711"
Aug 29 16:04:00.398: INFO: Deleting pod "simpletest.rc-sh49l" in namespace "gc-4711"
Aug 29 16:04:00.423: INFO: Deleting pod "simpletest.rc-slnzb" in namespace "gc-4711"
Aug 29 16:04:00.456: INFO: Deleting pod "simpletest.rc-spmdl" in namespace "gc-4711"
Aug 29 16:04:00.488: INFO: Deleting pod "simpletest.rc-t2mjs" in namespace "gc-4711"
Aug 29 16:04:00.515: INFO: Deleting pod "simpletest.rc-t5bws" in namespace "gc-4711"
Aug 29 16:04:00.533: INFO: Deleting pod "simpletest.rc-tggq5" in namespace "gc-4711"
Aug 29 16:04:00.556: INFO: Deleting pod "simpletest.rc-tm2j6" in namespace "gc-4711"
Aug 29 16:04:00.581: INFO: Deleting pod "simpletest.rc-vlz47" in namespace "gc-4711"
Aug 29 16:04:00.619: INFO: Deleting pod "simpletest.rc-w4jpw" in namespace "gc-4711"
Aug 29 16:04:00.643: INFO: Deleting pod "simpletest.rc-w4s9s" in namespace "gc-4711"
Aug 29 16:04:00.694: INFO: Deleting pod "simpletest.rc-wn5br" in namespace "gc-4711"
Aug 29 16:04:00.730: INFO: Deleting pod "simpletest.rc-wpdkt" in namespace "gc-4711"
Aug 29 16:04:00.752: INFO: Deleting pod "simpletest.rc-x68zs" in namespace "gc-4711"
Aug 29 16:04:00.786: INFO: Deleting pod "simpletest.rc-xblrt" in namespace "gc-4711"
Aug 29 16:04:00.837: INFO: Deleting pod "simpletest.rc-z4grv" in namespace "gc-4711"
Aug 29 16:04:00.860: INFO: Deleting pod "simpletest.rc-z9btv" in namespace "gc-4711"
Aug 29 16:04:00.913: INFO: Deleting pod "simpletest.rc-zgzjx" in namespace "gc-4711"
Aug 29 16:04:00.994: INFO: Deleting pod "simpletest.rc-zhld9" in namespace "gc-4711"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 29 16:04:01.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4711" for this suite.

• [SLOW TEST:44.079 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":123,"skipped":2070,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:04:01.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Aug 29 16:04:01.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:04:27.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8282" for this suite.

• [SLOW TEST:26.722 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":124,"skipped":2119,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:04:27.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Aug 29 16:04:27.836: INFO: Waiting up to 5m0s for pod "var-expansion-3908577b-39ca-4335-8c99-f8b627692525" in namespace "var-expansion-2914" to be "Succeeded or Failed"
Aug 29 16:04:27.846: INFO: Pod "var-expansion-3908577b-39ca-4335-8c99-f8b627692525": Phase="Pending", Reason="", readiness=false. Elapsed: 9.58719ms
Aug 29 16:04:29.861: INFO: Pod "var-expansion-3908577b-39ca-4335-8c99-f8b627692525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024953958s
Aug 29 16:04:32.293: INFO: Pod "var-expansion-3908577b-39ca-4335-8c99-f8b627692525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.456544793s
STEP: Saw pod success
Aug 29 16:04:32.293: INFO: Pod "var-expansion-3908577b-39ca-4335-8c99-f8b627692525" satisfied condition "Succeeded or Failed"
Aug 29 16:04:32.307: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod var-expansion-3908577b-39ca-4335-8c99-f8b627692525 container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:04:32.364: INFO: Waiting for pod var-expansion-3908577b-39ca-4335-8c99-f8b627692525 to disappear
Aug 29 16:04:32.371: INFO: Pod var-expansion-3908577b-39ca-4335-8c99-f8b627692525 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 29 16:04:32.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2914" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":125,"skipped":2126,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:04:32.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:04:32.958: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f164b40-6cf6-4f0d-a170-c4c417c0c460" in namespace "downward-api-5227" to be "Succeeded or Failed"
Aug 29 16:04:32.970: INFO: Pod "downwardapi-volume-9f164b40-6cf6-4f0d-a170-c4c417c0c460": Phase="Pending", Reason="", readiness=false. Elapsed: 11.629599ms
Aug 29 16:04:34.979: INFO: Pod "downwardapi-volume-9f164b40-6cf6-4f0d-a170-c4c417c0c460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020138257s
Aug 29 16:04:36.994: INFO: Pod "downwardapi-volume-9f164b40-6cf6-4f0d-a170-c4c417c0c460": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035194206s
Aug 29 16:04:39.003: INFO: Pod "downwardapi-volume-9f164b40-6cf6-4f0d-a170-c4c417c0c460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044246565s
STEP: Saw pod success
Aug 29 16:04:39.003: INFO: Pod "downwardapi-volume-9f164b40-6cf6-4f0d-a170-c4c417c0c460" satisfied condition "Succeeded or Failed"
Aug 29 16:04:39.010: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-9f164b40-6cf6-4f0d-a170-c4c417c0c460 container client-container: <nil>
STEP: delete the pod
Aug 29 16:04:39.041: INFO: Waiting for pod downwardapi-volume-9f164b40-6cf6-4f0d-a170-c4c417c0c460 to disappear
Aug 29 16:04:39.064: INFO: Pod downwardapi-volume-9f164b40-6cf6-4f0d-a170-c4c417c0c460 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 16:04:39.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5227" for this suite.

• [SLOW TEST:6.694 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":126,"skipped":2126,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:04:39.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:04:39.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-913" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":127,"skipped":2162,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:04:39.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:04:39.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:04:40.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2794" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":128,"skipped":2174,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:04:40.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Aug 29 16:04:40.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6684 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Aug 29 16:04:40.538: INFO: stderr: ""
Aug 29 16:04:40.538: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Aug 29 16:04:40.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6684 delete pods e2e-test-httpd-pod'
Aug 29 16:04:42.906: INFO: stderr: ""
Aug 29 16:04:42.906: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:04:42.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6684" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":129,"skipped":2179,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:04:42.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-3844
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 29 16:04:42.997: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 16:04:43.083: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:04:46.396: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:04:47.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:04:49.091: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:04:51.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:04:53.098: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:04:55.091: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 29 16:04:55.106: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:04:57.117: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:04:59.117: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:05:01.116: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:05:03.119: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:05:05.118: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 29 16:05:05.134: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 29 16:05:07.186: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 29 16:05:07.186: INFO: Breadth first check of 172.25.1.166 on host 172.31.16.214...
Aug 29 16:05:07.190: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.167:9080/dial?request=hostname&protocol=http&host=172.25.1.166&port=8083&tries=1'] Namespace:pod-network-test-3844 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:05:07.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:05:07.191: INFO: ExecWithOptions: Clientset creation
Aug 29 16:05:07.192: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-3844/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.1.166%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 16:05:07.315: INFO: Waiting for responses: map[]
Aug 29 16:05:07.315: INFO: reached 172.25.1.166 after 0/1 tries
Aug 29 16:05:07.315: INFO: Breadth first check of 172.25.2.181 on host 172.31.25.142...
Aug 29 16:05:07.322: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.167:9080/dial?request=hostname&protocol=http&host=172.25.2.181&port=8083&tries=1'] Namespace:pod-network-test-3844 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:05:07.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:05:07.322: INFO: ExecWithOptions: Clientset creation
Aug 29 16:05:07.323: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-3844/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.2.181%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 16:05:07.488: INFO: Waiting for responses: map[]
Aug 29 16:05:07.488: INFO: reached 172.25.2.181 after 0/1 tries
Aug 29 16:05:07.488: INFO: Breadth first check of 172.25.0.120 on host 172.31.26.197...
Aug 29 16:05:07.499: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.167:9080/dial?request=hostname&protocol=http&host=172.25.0.120&port=8083&tries=1'] Namespace:pod-network-test-3844 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:05:07.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:05:07.500: INFO: ExecWithOptions: Clientset creation
Aug 29 16:05:07.500: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-3844/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.0.120%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 16:05:07.620: INFO: Waiting for responses: map[]
Aug 29 16:05:07.620: INFO: reached 172.25.0.120 after 0/1 tries
Aug 29 16:05:07.620: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 29 16:05:07.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3844" for this suite.

• [SLOW TEST:24.724 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":130,"skipped":2179,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:05:07.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-27ec718e-d29b-413f-8e56-64760f09d5a2
STEP: Creating a pod to test consume secrets
Aug 29 16:05:07.738: INFO: Waiting up to 5m0s for pod "pod-secrets-d52dc3c5-c1f0-4703-b4fe-931ddde2b1ea" in namespace "secrets-795" to be "Succeeded or Failed"
Aug 29 16:05:07.752: INFO: Pod "pod-secrets-d52dc3c5-c1f0-4703-b4fe-931ddde2b1ea": Phase="Pending", Reason="", readiness=false. Elapsed: 14.359604ms
Aug 29 16:05:09.763: INFO: Pod "pod-secrets-d52dc3c5-c1f0-4703-b4fe-931ddde2b1ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024821394s
Aug 29 16:05:11.777: INFO: Pod "pod-secrets-d52dc3c5-c1f0-4703-b4fe-931ddde2b1ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039442193s
Aug 29 16:05:13.786: INFO: Pod "pod-secrets-d52dc3c5-c1f0-4703-b4fe-931ddde2b1ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047808325s
STEP: Saw pod success
Aug 29 16:05:13.786: INFO: Pod "pod-secrets-d52dc3c5-c1f0-4703-b4fe-931ddde2b1ea" satisfied condition "Succeeded or Failed"
Aug 29 16:05:13.794: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-secrets-d52dc3c5-c1f0-4703-b4fe-931ddde2b1ea container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:05:13.842: INFO: Waiting for pod pod-secrets-d52dc3c5-c1f0-4703-b4fe-931ddde2b1ea to disappear
Aug 29 16:05:13.848: INFO: Pod pod-secrets-d52dc3c5-c1f0-4703-b4fe-931ddde2b1ea no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 29 16:05:13.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-795" for this suite.

• [SLOW TEST:6.212 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":131,"skipped":2191,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:05:13.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Aug 29 16:05:20.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-498" for this suite.
STEP: Destroying namespace "nsdeletetest-9994" for this suite.
Aug 29 16:05:20.183: INFO: Namespace nsdeletetest-9994 was already deleted
STEP: Destroying namespace "nsdeletetest-5991" for this suite.

• [SLOW TEST:6.327 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":132,"skipped":2193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:05:20.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Aug 29 16:05:20.300: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 29 16:05:25.311: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 29 16:05:25.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4675" for this suite.

• [SLOW TEST:5.352 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":133,"skipped":2224,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:05:25.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1865
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-1865
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1865
Aug 29 16:05:25.698: INFO: Found 0 stateful pods, waiting for 1
Aug 29 16:05:35.711: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 29 16:05:35.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1865 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:05:36.249: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:05:36.249: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:05:36.249: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:05:36.262: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 29 16:05:46.634: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:05:46.634: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:05:46.679: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Aug 29 16:05:46.679: INFO: ss-0  ip-172-31-16-214.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:25 +0000 UTC  }]
Aug 29 16:05:46.679: INFO: ss-1                                                  Pending         []
Aug 29 16:05:46.679: INFO: 
Aug 29 16:05:46.679: INFO: StatefulSet ss has not reached scale 3, at 2
Aug 29 16:05:47.698: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992373029s
Aug 29 16:05:49.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973125866s
Aug 29 16:05:50.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.376157724s
Aug 29 16:05:51.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.362834085s
Aug 29 16:05:52.331: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.352732286s
Aug 29 16:05:53.342: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.340016112s
Aug 29 16:05:54.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.329430468s
Aug 29 16:05:55.359: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.321387548s
Aug 29 16:05:56.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 312.059758ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1865
Aug 29 16:05:57.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1865 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:05:57.650: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:05:57.650: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:05:57.650: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:05:57.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1865 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:05:57.982: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 29 16:05:57.982: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:05:57.982: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:05:57.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1865 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:05:58.441: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 29 16:05:58.441: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:05:58.441: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:05:58.455: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:05:58.455: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:05:58.455: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 29 16:05:58.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1865 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:05:58.660: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:05:58.660: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:05:58.660: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:05:58.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1865 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:05:59.017: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:05:59.017: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:05:59.017: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:05:59.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1865 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:05:59.248: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:05:59.248: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:05:59.248: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:05:59.248: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:05:59.255: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 29 16:06:09.276: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:06:09.276: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:06:09.276: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:06:09.310: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Aug 29 16:06:09.310: INFO: ss-0  ip-172-31-16-214.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:25 +0000 UTC  }]
Aug 29 16:06:09.310: INFO: ss-1  ip-172-31-25-142.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:46 +0000 UTC  }]
Aug 29 16:06:09.310: INFO: ss-2  ip-172-31-16-214.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:06:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:06:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:46 +0000 UTC  }]
Aug 29 16:06:09.310: INFO: 
Aug 29 16:06:09.310: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 29 16:06:10.322: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Aug 29 16:06:10.322: INFO: ss-1  ip-172-31-25-142.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:05:46 +0000 UTC  }]
Aug 29 16:06:10.322: INFO: 
Aug 29 16:06:10.322: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 29 16:06:11.334: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.977216155s
Aug 29 16:06:12.343: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.964533902s
Aug 29 16:06:13.358: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.955954249s
Aug 29 16:06:14.368: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.940831303s
Aug 29 16:06:15.374: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.931152508s
Aug 29 16:06:16.386: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.925008678s
Aug 29 16:06:17.396: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.913083905s
Aug 29 16:06:18.405: INFO: Verifying statefulset ss doesn't scale past 0 for another 902.422503ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1865
Aug 29 16:06:19.419: INFO: Scaling statefulset ss to 0
Aug 29 16:06:19.441: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 29 16:06:19.460: INFO: Deleting all statefulset in ns statefulset-1865
Aug 29 16:06:19.469: INFO: Scaling statefulset ss to 0
Aug 29 16:06:19.507: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:06:19.513: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 29 16:06:19.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1865" for this suite.

• [SLOW TEST:54.065 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":134,"skipped":2235,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:06:19.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 29 16:06:19.704: INFO: Waiting up to 5m0s for pod "downward-api-0093b19d-74b2-470b-8790-2d6af70c919c" in namespace "downward-api-1015" to be "Succeeded or Failed"
Aug 29 16:06:19.719: INFO: Pod "downward-api-0093b19d-74b2-470b-8790-2d6af70c919c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.521945ms
Aug 29 16:06:21.728: INFO: Pod "downward-api-0093b19d-74b2-470b-8790-2d6af70c919c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023500841s
Aug 29 16:06:23.741: INFO: Pod "downward-api-0093b19d-74b2-470b-8790-2d6af70c919c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037147124s
Aug 29 16:06:25.756: INFO: Pod "downward-api-0093b19d-74b2-470b-8790-2d6af70c919c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052317664s
STEP: Saw pod success
Aug 29 16:06:25.757: INFO: Pod "downward-api-0093b19d-74b2-470b-8790-2d6af70c919c" satisfied condition "Succeeded or Failed"
Aug 29 16:06:25.764: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downward-api-0093b19d-74b2-470b-8790-2d6af70c919c container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:06:25.796: INFO: Waiting for pod downward-api-0093b19d-74b2-470b-8790-2d6af70c919c to disappear
Aug 29 16:06:25.811: INFO: Pod downward-api-0093b19d-74b2-470b-8790-2d6af70c919c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 29 16:06:25.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1015" for this suite.

• [SLOW TEST:6.219 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":135,"skipped":2252,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:06:25.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9588.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9588.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9588.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9588.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:06:28.020: INFO: DNS probes using dns-test-7e6d4ca2-fe30-45f7-887f-464e709f7449 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9588.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9588.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9588.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9588.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:06:30.142: INFO: File wheezy_udp@dns-test-service-3.dns-9588.svc.cluster.local from pod  dns-9588/dns-test-e9f1cefd-2276-4472-b753-fde2b1dd8101 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 29 16:06:30.154: INFO: File jessie_udp@dns-test-service-3.dns-9588.svc.cluster.local from pod  dns-9588/dns-test-e9f1cefd-2276-4472-b753-fde2b1dd8101 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 29 16:06:30.154: INFO: Lookups using dns-9588/dns-test-e9f1cefd-2276-4472-b753-fde2b1dd8101 failed for: [wheezy_udp@dns-test-service-3.dns-9588.svc.cluster.local jessie_udp@dns-test-service-3.dns-9588.svc.cluster.local]

Aug 29 16:06:35.175: INFO: DNS probes using dns-test-e9f1cefd-2276-4472-b753-fde2b1dd8101 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9588.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9588.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9588.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9588.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:06:39.325: INFO: DNS probes using dns-test-a456013a-4d9e-4dfa-abba-e08f6431a762 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 29 16:06:39.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9588" for this suite.

• [SLOW TEST:13.574 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":136,"skipped":2292,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:06:39.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-b22b97bd-c054-431d-8700-488fb80db5da
STEP: Creating a pod to test consume secrets
Aug 29 16:06:39.514: INFO: Waiting up to 5m0s for pod "pod-secrets-ab49d0a7-3723-4bc0-b490-4b5ced29659a" in namespace "secrets-4274" to be "Succeeded or Failed"
Aug 29 16:06:39.531: INFO: Pod "pod-secrets-ab49d0a7-3723-4bc0-b490-4b5ced29659a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.321711ms
Aug 29 16:06:41.547: INFO: Pod "pod-secrets-ab49d0a7-3723-4bc0-b490-4b5ced29659a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032591555s
Aug 29 16:06:43.559: INFO: Pod "pod-secrets-ab49d0a7-3723-4bc0-b490-4b5ced29659a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045179343s
STEP: Saw pod success
Aug 29 16:06:43.559: INFO: Pod "pod-secrets-ab49d0a7-3723-4bc0-b490-4b5ced29659a" satisfied condition "Succeeded or Failed"
Aug 29 16:06:43.739: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-secrets-ab49d0a7-3723-4bc0-b490-4b5ced29659a container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:06:43.784: INFO: Waiting for pod pod-secrets-ab49d0a7-3723-4bc0-b490-4b5ced29659a to disappear
Aug 29 16:06:43.824: INFO: Pod pod-secrets-ab49d0a7-3723-4bc0-b490-4b5ced29659a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 29 16:06:43.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4274" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":137,"skipped":2293,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:06:43.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 29 16:06:44.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7787" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":138,"skipped":2309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:06:44.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-3389
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3389 to expose endpoints map[]
Aug 29 16:06:44.202: INFO: successfully validated that service endpoint-test2 in namespace services-3389 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3389
Aug 29 16:06:44.232: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:06:46.243: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3389 to expose endpoints map[pod1:[80]]
Aug 29 16:06:46.276: INFO: successfully validated that service endpoint-test2 in namespace services-3389 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Aug 29 16:06:46.276: INFO: Creating new exec pod
Aug 29 16:06:49.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3389 exec execpodbtw7n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 29 16:06:49.604: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 29 16:06:49.604: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:06:49.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3389 exec execpodbtw7n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.42 80'
Aug 29 16:06:49.845: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.26.42 80\nConnection to 10.240.26.42 80 port [tcp/http] succeeded!\n"
Aug 29 16:06:49.845: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3389
Aug 29 16:06:49.872: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:06:51.882: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3389 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 29 16:06:51.926: INFO: successfully validated that service endpoint-test2 in namespace services-3389 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Aug 29 16:06:52.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3389 exec execpodbtw7n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 29 16:06:53.193: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 29 16:06:53.193: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:06:53.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3389 exec execpodbtw7n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.42 80'
Aug 29 16:06:53.432: INFO: stderr: "+ + nc -v -t -w 2 10.240.26.42 80\necho hostName\nConnection to 10.240.26.42 80 port [tcp/http] succeeded!\n"
Aug 29 16:06:53.432: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3389
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3389 to expose endpoints map[pod2:[80]]
Aug 29 16:06:53.502: INFO: successfully validated that service endpoint-test2 in namespace services-3389 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Aug 29 16:06:54.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3389 exec execpodbtw7n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 29 16:06:54.746: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 29 16:06:54.746: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:06:54.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-3389 exec execpodbtw7n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.42 80'
Aug 29 16:06:54.973: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.26.42 80\nConnection to 10.240.26.42 80 port [tcp/http] succeeded!\n"
Aug 29 16:06:54.973: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3389
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3389 to expose endpoints map[]
Aug 29 16:06:56.056: INFO: successfully validated that service endpoint-test2 in namespace services-3389 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:06:56.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3389" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:12.059 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":139,"skipped":2343,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:06:56.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-10dcbc6a-58ef-4db1-9d79-03611103f57b
STEP: Creating a pod to test consume configMaps
Aug 29 16:06:56.275: INFO: Waiting up to 5m0s for pod "pod-configmaps-d89670f4-82e1-4cb9-9451-38c8ffb291dc" in namespace "configmap-5174" to be "Succeeded or Failed"
Aug 29 16:06:56.284: INFO: Pod "pod-configmaps-d89670f4-82e1-4cb9-9451-38c8ffb291dc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.499158ms
Aug 29 16:06:58.298: INFO: Pod "pod-configmaps-d89670f4-82e1-4cb9-9451-38c8ffb291dc": Phase="Running", Reason="", readiness=false. Elapsed: 2.022852248s
Aug 29 16:07:00.315: INFO: Pod "pod-configmaps-d89670f4-82e1-4cb9-9451-38c8ffb291dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039845982s
STEP: Saw pod success
Aug 29 16:07:00.315: INFO: Pod "pod-configmaps-d89670f4-82e1-4cb9-9451-38c8ffb291dc" satisfied condition "Succeeded or Failed"
Aug 29 16:07:00.321: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-configmaps-d89670f4-82e1-4cb9-9451-38c8ffb291dc container configmap-volume-test: <nil>
STEP: delete the pod
Aug 29 16:07:00.352: INFO: Waiting for pod pod-configmaps-d89670f4-82e1-4cb9-9451-38c8ffb291dc to disappear
Aug 29 16:07:00.357: INFO: Pod pod-configmaps-d89670f4-82e1-4cb9-9451-38c8ffb291dc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:07:00.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5174" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":140,"skipped":2369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:07:00.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 29 16:07:00.428: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 29 16:07:07.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2975" for this suite.

• [SLOW TEST:6.939 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":141,"skipped":2402,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:07:07.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 29 16:07:07.388: INFO: Waiting up to 5m0s for pod "downward-api-4320d865-75b4-47d7-933f-c79c56fee177" in namespace "downward-api-9342" to be "Succeeded or Failed"
Aug 29 16:07:07.397: INFO: Pod "downward-api-4320d865-75b4-47d7-933f-c79c56fee177": Phase="Pending", Reason="", readiness=false. Elapsed: 8.436079ms
Aug 29 16:07:09.407: INFO: Pod "downward-api-4320d865-75b4-47d7-933f-c79c56fee177": Phase="Running", Reason="", readiness=false. Elapsed: 2.019006544s
Aug 29 16:07:11.422: INFO: Pod "downward-api-4320d865-75b4-47d7-933f-c79c56fee177": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033788118s
STEP: Saw pod success
Aug 29 16:07:11.422: INFO: Pod "downward-api-4320d865-75b4-47d7-933f-c79c56fee177" satisfied condition "Succeeded or Failed"
Aug 29 16:07:11.429: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod downward-api-4320d865-75b4-47d7-933f-c79c56fee177 container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:07:11.467: INFO: Waiting for pod downward-api-4320d865-75b4-47d7-933f-c79c56fee177 to disappear
Aug 29 16:07:11.476: INFO: Pod downward-api-4320d865-75b4-47d7-933f-c79c56fee177 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 29 16:07:11.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9342" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":142,"skipped":2408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:07:11.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:07:11.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ecfd16b7-d65e-4e2c-b3c1-e2e5049d55a4" in namespace "projected-5041" to be "Succeeded or Failed"
Aug 29 16:07:11.584: INFO: Pod "downwardapi-volume-ecfd16b7-d65e-4e2c-b3c1-e2e5049d55a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.655839ms
Aug 29 16:07:13.596: INFO: Pod "downwardapi-volume-ecfd16b7-d65e-4e2c-b3c1-e2e5049d55a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021056308s
Aug 29 16:07:15.606: INFO: Pod "downwardapi-volume-ecfd16b7-d65e-4e2c-b3c1-e2e5049d55a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031768561s
STEP: Saw pod success
Aug 29 16:07:15.607: INFO: Pod "downwardapi-volume-ecfd16b7-d65e-4e2c-b3c1-e2e5049d55a4" satisfied condition "Succeeded or Failed"
Aug 29 16:07:15.614: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-ecfd16b7-d65e-4e2c-b3c1-e2e5049d55a4 container client-container: <nil>
STEP: delete the pod
Aug 29 16:07:15.645: INFO: Waiting for pod downwardapi-volume-ecfd16b7-d65e-4e2c-b3c1-e2e5049d55a4 to disappear
Aug 29 16:07:15.650: INFO: Pod downwardapi-volume-ecfd16b7-d65e-4e2c-b3c1-e2e5049d55a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 16:07:15.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5041" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":143,"skipped":2441,"failed":0}

------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:07:15.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Aug 29 16:07:15.750: INFO: Waiting up to 5m0s for pod "client-containers-3d311825-4080-4254-bb45-dd83486785a6" in namespace "containers-6" to be "Succeeded or Failed"
Aug 29 16:07:15.774: INFO: Pod "client-containers-3d311825-4080-4254-bb45-dd83486785a6": Phase="Pending", Reason="", readiness=false. Elapsed: 23.411537ms
Aug 29 16:07:17.792: INFO: Pod "client-containers-3d311825-4080-4254-bb45-dd83486785a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041288633s
Aug 29 16:07:19.804: INFO: Pod "client-containers-3d311825-4080-4254-bb45-dd83486785a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053776568s
STEP: Saw pod success
Aug 29 16:07:19.804: INFO: Pod "client-containers-3d311825-4080-4254-bb45-dd83486785a6" satisfied condition "Succeeded or Failed"
Aug 29 16:07:19.812: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod client-containers-3d311825-4080-4254-bb45-dd83486785a6 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:07:19.842: INFO: Waiting for pod client-containers-3d311825-4080-4254-bb45-dd83486785a6 to disappear
Aug 29 16:07:19.849: INFO: Pod client-containers-3d311825-4080-4254-bb45-dd83486785a6 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 29 16:07:19.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":144,"skipped":2441,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:07:19.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-9327
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 29 16:07:19.926: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 16:07:20.021: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:07:22.030: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:07:24.034: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:07:26.045: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:07:28.041: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:07:30.038: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:07:32.029: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 29 16:07:32.043: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:07:34.057: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:07:36.059: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:07:38.053: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:07:40.064: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:07:42.060: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 29 16:07:42.078: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 29 16:07:44.272: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 29 16:07:44.272: INFO: Going to poll 172.25.1.175 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 29 16:07:44.324: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.175:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9327 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:07:44.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:07:44.325: INFO: ExecWithOptions: Clientset creation
Aug 29 16:07:44.325: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9327/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.1.175%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 16:07:44.661: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 29 16:07:44.661: INFO: Going to poll 172.25.2.193 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 29 16:07:44.717: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.193:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9327 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:07:44.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:07:44.718: INFO: ExecWithOptions: Clientset creation
Aug 29 16:07:44.718: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9327/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.2.193%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 16:07:45.197: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 29 16:07:45.198: INFO: Going to poll 172.25.0.121 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 29 16:07:45.205: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.121:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9327 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:07:45.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:07:45.211: INFO: ExecWithOptions: Clientset creation
Aug 29 16:07:45.211: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9327/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.0.121%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 16:07:45.352: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 29 16:07:45.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9327" for this suite.

• [SLOW TEST:25.838 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":145,"skipped":2448,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:07:45.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 29 16:07:45.752: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 16:07:45.770: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 16:07:45.776: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-16-214.eu-central-1.compute.internal before test
Aug 29 16:07:45.787: INFO: canal-qcxrv from kube-system started at 2022-08-29 15:06:13 +0000 UTC (2 container statuses recorded)
Aug 29 16:07:45.787: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:07:45.787: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:07:45.787: INFO: envoy-agent-r297b from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.787: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:07:45.787: INFO: kube-proxy-wn284 from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.787: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:07:45.787: INFO: node-local-dns-px2xr from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.787: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:07:45.787: INFO: host-test-container-pod from pod-network-test-9327 started at 2022-08-29 16:07:42 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.787: INFO: 	Container agnhost-container ready: true, restart count 0
Aug 29 16:07:45.787: INFO: netserver-0 from pod-network-test-9327 started at 2022-08-29 16:07:19 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.787: INFO: 	Container webserver ready: true, restart count 0
Aug 29 16:07:45.788: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-942bg from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:07:45.788: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:07:45.788: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:07:45.788: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-25-142.eu-central-1.compute.internal before test
Aug 29 16:07:45.799: INFO: canal-xl6zl from kube-system started at 2022-08-29 15:07:00 +0000 UTC (2 container statuses recorded)
Aug 29 16:07:45.799: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:07:45.799: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:07:45.799: INFO: envoy-agent-sqxbm from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.799: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:07:45.799: INFO: kube-proxy-gb22k from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.799: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:07:45.799: INFO: node-local-dns-vmfhr from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.799: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:07:45.799: INFO: netserver-1 from pod-network-test-9327 started at 2022-08-29 16:07:20 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.799: INFO: 	Container webserver ready: true, restart count 0
Aug 29 16:07:45.799: INFO: test-container-pod from pod-network-test-9327 started at 2022-08-29 16:07:42 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.799: INFO: 	Container webserver ready: true, restart count 0
Aug 29 16:07:45.799: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-9j6sw from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:07:45.799: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:07:45.799: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:07:45.799: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-26-197.eu-central-1.compute.internal before test
Aug 29 16:07:45.814: INFO: calico-kube-controllers-58c7d666cf-65tgr from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.814: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 16:07:45.814: INFO: canal-kg6wt from kube-system started at 2022-08-29 15:05:48 +0000 UTC (2 container statuses recorded)
Aug 29 16:07:45.814: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:07:45.814: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:07:45.814: INFO: coredns-5fff85bf5-49vtq from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.814: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:07:45.814: INFO: coredns-5fff85bf5-5ppjp from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.814: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:07:45.814: INFO: envoy-agent-zzmwh from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.814: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:07:45.814: INFO: konnectivity-agent-6885ff4877-2bv58 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.814: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:07:45.814: INFO: konnectivity-agent-6885ff4877-n5r72 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.814: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:07:45.814: INFO: kube-proxy-9gjzc from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.814: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:07:45.814: INFO: metrics-server-6847c4d787-4wc57 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.814: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:07:45.814: INFO: metrics-server-6847c4d787-z9nwh from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.814: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:07:45.815: INFO: node-local-dns-6x48b from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.815: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:07:45.815: INFO: dashboard-metrics-scraper-8cfc65cdb-85659 from kubernetes-dashboard started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.815: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 29 16:07:45.815: INFO: dashboard-metrics-scraper-8cfc65cdb-v2g6l from kubernetes-dashboard started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.815: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 29 16:07:45.815: INFO: netserver-2 from pod-network-test-9327 started at 2022-08-29 16:07:20 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.815: INFO: 	Container webserver ready: true, restart count 0
Aug 29 16:07:45.815: INFO: sonobuoy from sonobuoy started at 2022-08-29 15:06:56 +0000 UTC (1 container statuses recorded)
Aug 29 16:07:45.815: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 16:07:45.815: INFO: sonobuoy-e2e-job-ef66887baa2a47e4 from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:07:45.815: INFO: 	Container e2e ready: true, restart count 0
Aug 29 16:07:45.815: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:07:45.815: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-4d599 from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:07:45.815: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:07:45.815: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.170fdc8863a46bfb], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 29 16:07:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7624" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":146,"skipped":2451,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:07:46.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5995
STEP: creating service affinity-nodeport in namespace services-5995
STEP: creating replication controller affinity-nodeport in namespace services-5995
I0829 16:07:46.988170      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5995, replica count: 3
I0829 16:07:50.040816      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:07:50.071: INFO: Creating new exec pod
Aug 29 16:07:53.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5995 exec execpod-affinityccm67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Aug 29 16:07:53.399: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 29 16:07:53.399: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:07:53.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5995 exec execpod-affinityccm67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.16.25 80'
Aug 29 16:07:53.615: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.16.25 80\nConnection to 10.240.16.25 80 port [tcp/http] succeeded!\n"
Aug 29 16:07:53.615: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:07:53.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5995 exec execpod-affinityccm67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.214 32394'
Aug 29 16:07:53.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.214 32394\nConnection to 172.31.16.214 32394 port [tcp/*] succeeded!\n"
Aug 29 16:07:53.828: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:07:53.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5995 exec execpod-affinityccm67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.25.142 32394'
Aug 29 16:07:54.047: INFO: stderr: "+ nc -v -t -w 2 172.31.25.142 32394\n+ echo hostName\nConnection to 172.31.25.142 32394 port [tcp/*] succeeded!\n"
Aug 29 16:07:54.047: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:07:54.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-5995 exec execpod-affinityccm67 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.214:32394/ ; done'
Aug 29 16:07:54.391: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.214:32394/\n"
Aug 29 16:07:54.391: INFO: stdout: "\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf\naffinity-nodeport-x9gzf"
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Received response from host: affinity-nodeport-x9gzf
Aug 29 16:07:54.391: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5995, will wait for the garbage collector to delete the pods
Aug 29 16:07:54.621: INFO: Deleting ReplicationController affinity-nodeport took: 9.615209ms
Aug 29 16:07:54.822: INFO: Terminating ReplicationController affinity-nodeport pods took: 201.206184ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:07:57.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5995" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.291 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":147,"skipped":2462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:07:57.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Aug 29 16:07:57.251: INFO: Waiting up to 5m0s for pod "var-expansion-d187520e-e021-4887-99d0-720f350842bf" in namespace "var-expansion-5760" to be "Succeeded or Failed"
Aug 29 16:07:57.257: INFO: Pod "var-expansion-d187520e-e021-4887-99d0-720f350842bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.989045ms
Aug 29 16:07:59.269: INFO: Pod "var-expansion-d187520e-e021-4887-99d0-720f350842bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017908369s
Aug 29 16:08:01.286: INFO: Pod "var-expansion-d187520e-e021-4887-99d0-720f350842bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034755926s
STEP: Saw pod success
Aug 29 16:08:01.286: INFO: Pod "var-expansion-d187520e-e021-4887-99d0-720f350842bf" satisfied condition "Succeeded or Failed"
Aug 29 16:08:01.294: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod var-expansion-d187520e-e021-4887-99d0-720f350842bf container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:08:01.325: INFO: Waiting for pod var-expansion-d187520e-e021-4887-99d0-720f350842bf to disappear
Aug 29 16:08:01.334: INFO: Pod var-expansion-d187520e-e021-4887-99d0-720f350842bf no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 29 16:08:01.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5760" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":148,"skipped":2494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:08:01.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 29 16:08:01.434: INFO: The status of Pod pod-update-activedeadlineseconds-875fabd3-6c38-419a-8914-e454c87c2638 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:08:03.450: INFO: The status of Pod pod-update-activedeadlineseconds-875fabd3-6c38-419a-8914-e454c87c2638 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 29 16:08:04.029: INFO: Successfully updated pod "pod-update-activedeadlineseconds-875fabd3-6c38-419a-8914-e454c87c2638"
Aug 29 16:08:04.029: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-875fabd3-6c38-419a-8914-e454c87c2638" in namespace "pods-6118" to be "terminated due to deadline exceeded"
Aug 29 16:08:04.038: INFO: Pod "pod-update-activedeadlineseconds-875fabd3-6c38-419a-8914-e454c87c2638": Phase="Running", Reason="", readiness=true. Elapsed: 9.583689ms
Aug 29 16:08:06.063: INFO: Pod "pod-update-activedeadlineseconds-875fabd3-6c38-419a-8914-e454c87c2638": Phase="Running", Reason="", readiness=true. Elapsed: 2.03485649s
Aug 29 16:08:08.080: INFO: Pod "pod-update-activedeadlineseconds-875fabd3-6c38-419a-8914-e454c87c2638": Phase="Running", Reason="", readiness=false. Elapsed: 4.051355789s
Aug 29 16:08:10.095: INFO: Pod "pod-update-activedeadlineseconds-875fabd3-6c38-419a-8914-e454c87c2638": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.066485253s
Aug 29 16:08:10.095: INFO: Pod "pod-update-activedeadlineseconds-875fabd3-6c38-419a-8914-e454c87c2638" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 29 16:08:10.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6118" for this suite.

• [SLOW TEST:8.756 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":149,"skipped":2553,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:08:10.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-b940d319-1cae-4d8f-b83d-efa26400cb3a in namespace container-probe-3091
Aug 29 16:08:12.396: INFO: Started pod liveness-b940d319-1cae-4d8f-b83d-efa26400cb3a in namespace container-probe-3091
STEP: checking the pod's current state and verifying that restartCount is present
Aug 29 16:08:12.404: INFO: Initial restart count of pod liveness-b940d319-1cae-4d8f-b83d-efa26400cb3a is 0
Aug 29 16:08:32.577: INFO: Restart count of pod container-probe-3091/liveness-b940d319-1cae-4d8f-b83d-efa26400cb3a is now 1 (20.172546375s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 29 16:08:32.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3091" for this suite.

• [SLOW TEST:22.511 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":150,"skipped":2556,"failed":0}
SSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:08:32.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 29 16:10:01.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-674" for this suite.

• [SLOW TEST:88.602 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":151,"skipped":2560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:01.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 29 16:10:01.317: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 16:10:01.333: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 16:10:01.342: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-16-214.eu-central-1.compute.internal before test
Aug 29 16:10:01.370: INFO: replace-27696489-q4t6k from cronjob-674 started at 2022-08-29 16:09:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.370: INFO: 	Container c ready: true, restart count 0
Aug 29 16:10:01.370: INFO: canal-qcxrv from kube-system started at 2022-08-29 15:06:13 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:01.370: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:10:01.370: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:10:01.370: INFO: envoy-agent-r297b from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.370: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:10:01.370: INFO: kube-proxy-wn284 from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.370: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:10:01.371: INFO: node-local-dns-px2xr from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.371: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:10:01.371: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-942bg from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:01.371: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:10:01.371: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:10:01.371: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-25-142.eu-central-1.compute.internal before test
Aug 29 16:10:01.388: INFO: replace-27696490-nskgx from cronjob-674 started at 2022-08-29 16:10:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.388: INFO: 	Container c ready: true, restart count 0
Aug 29 16:10:01.388: INFO: canal-xl6zl from kube-system started at 2022-08-29 15:07:00 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:01.388: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:10:01.388: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:10:01.388: INFO: envoy-agent-sqxbm from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.388: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:10:01.388: INFO: kube-proxy-gb22k from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.388: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:10:01.388: INFO: node-local-dns-vmfhr from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.388: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:10:01.388: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-9j6sw from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:01.388: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:10:01.388: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:10:01.388: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-26-197.eu-central-1.compute.internal before test
Aug 29 16:10:01.415: INFO: calico-kube-controllers-58c7d666cf-65tgr from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.415: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 16:10:01.415: INFO: canal-kg6wt from kube-system started at 2022-08-29 15:05:48 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:01.415: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:10:01.415: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:10:01.415: INFO: coredns-5fff85bf5-49vtq from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.415: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:10:01.415: INFO: coredns-5fff85bf5-5ppjp from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.416: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:10:01.416: INFO: envoy-agent-zzmwh from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.416: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:10:01.416: INFO: konnectivity-agent-6885ff4877-2bv58 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.416: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:10:01.416: INFO: konnectivity-agent-6885ff4877-n5r72 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.416: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:10:01.416: INFO: kube-proxy-9gjzc from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.416: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:10:01.416: INFO: metrics-server-6847c4d787-4wc57 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.416: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:10:01.416: INFO: metrics-server-6847c4d787-z9nwh from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.416: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:10:01.416: INFO: node-local-dns-6x48b from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.416: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:10:01.416: INFO: dashboard-metrics-scraper-8cfc65cdb-85659 from kubernetes-dashboard started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.416: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 29 16:10:01.417: INFO: dashboard-metrics-scraper-8cfc65cdb-v2g6l from kubernetes-dashboard started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.417: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 29 16:10:01.417: INFO: sonobuoy from sonobuoy started at 2022-08-29 15:06:56 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:01.417: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 16:10:01.417: INFO: sonobuoy-e2e-job-ef66887baa2a47e4 from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:01.417: INFO: 	Container e2e ready: true, restart count 0
Aug 29 16:10:01.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:10:01.417: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-4d599 from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:01.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:10:01.417: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node ip-172-31-16-214.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-25-142.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod replace-27696489-q4t6k requesting resource cpu=0m on Node ip-172-31-16-214.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod replace-27696490-nskgx requesting resource cpu=0m on Node ip-172-31-25-142.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod calico-kube-controllers-58c7d666cf-65tgr requesting resource cpu=0m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod canal-kg6wt requesting resource cpu=250m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod canal-qcxrv requesting resource cpu=250m on Node ip-172-31-16-214.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod canal-xl6zl requesting resource cpu=250m on Node ip-172-31-25-142.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod coredns-5fff85bf5-49vtq requesting resource cpu=50m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod coredns-5fff85bf5-5ppjp requesting resource cpu=50m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod envoy-agent-r297b requesting resource cpu=50m on Node ip-172-31-16-214.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod envoy-agent-sqxbm requesting resource cpu=50m on Node ip-172-31-25-142.eu-central-1.compute.internal
Aug 29 16:10:01.593: INFO: Pod envoy-agent-zzmwh requesting resource cpu=50m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod konnectivity-agent-6885ff4877-2bv58 requesting resource cpu=10m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod konnectivity-agent-6885ff4877-n5r72 requesting resource cpu=10m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod kube-proxy-9gjzc requesting resource cpu=75m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod kube-proxy-gb22k requesting resource cpu=75m on Node ip-172-31-25-142.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod kube-proxy-wn284 requesting resource cpu=75m on Node ip-172-31-16-214.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod metrics-server-6847c4d787-4wc57 requesting resource cpu=100m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod metrics-server-6847c4d787-z9nwh requesting resource cpu=100m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod node-local-dns-6x48b requesting resource cpu=0m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod node-local-dns-px2xr requesting resource cpu=0m on Node ip-172-31-16-214.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod node-local-dns-vmfhr requesting resource cpu=0m on Node ip-172-31-25-142.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod dashboard-metrics-scraper-8cfc65cdb-85659 requesting resource cpu=50m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod dashboard-metrics-scraper-8cfc65cdb-v2g6l requesting resource cpu=50m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.594: INFO: Pod sonobuoy-e2e-job-ef66887baa2a47e4 requesting resource cpu=0m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.595: INFO: Pod sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-4d599 requesting resource cpu=0m on Node ip-172-31-26-197.eu-central-1.compute.internal
Aug 29 16:10:01.595: INFO: Pod sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-942bg requesting resource cpu=0m on Node ip-172-31-16-214.eu-central-1.compute.internal
Aug 29 16:10:01.595: INFO: Pod sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-9j6sw requesting resource cpu=0m on Node ip-172-31-25-142.eu-central-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Aug 29 16:10:01.595: INFO: Creating a pod which consumes cpu=857m on Node ip-172-31-16-214.eu-central-1.compute.internal
Aug 29 16:10:01.614: INFO: Creating a pod which consumes cpu=857m on Node ip-172-31-25-142.eu-central-1.compute.internal
Aug 29 16:10:01.629: INFO: Creating a pod which consumes cpu=563m on Node ip-172-31-26-197.eu-central-1.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50d45001-90af-4fb9-aa17-085ce685d073.170fdca800e4b529], Reason = [Scheduled], Message = [Successfully assigned sched-pred-750/filler-pod-50d45001-90af-4fb9-aa17-085ce685d073 to ip-172-31-25-142.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50d45001-90af-4fb9-aa17-085ce685d073.170fdca82fad8520], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50d45001-90af-4fb9-aa17-085ce685d073.170fdca831d9abb7], Reason = [Created], Message = [Created container filler-pod-50d45001-90af-4fb9-aa17-085ce685d073]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50d45001-90af-4fb9-aa17-085ce685d073.170fdca83992374d], Reason = [Started], Message = [Started container filler-pod-50d45001-90af-4fb9-aa17-085ce685d073]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c5d1abd-3fb6-42d1-805a-ef225465efa9.170fdca7ffd3943e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-750/filler-pod-6c5d1abd-3fb6-42d1-805a-ef225465efa9 to ip-172-31-16-214.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c5d1abd-3fb6-42d1-805a-ef225465efa9.170fdca829c95754], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c5d1abd-3fb6-42d1-805a-ef225465efa9.170fdca82b06881b], Reason = [Created], Message = [Created container filler-pod-6c5d1abd-3fb6-42d1-805a-ef225465efa9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c5d1abd-3fb6-42d1-805a-ef225465efa9.170fdca830b2c56e], Reason = [Started], Message = [Started container filler-pod-6c5d1abd-3fb6-42d1-805a-ef225465efa9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9f1465b-5d29-4720-a9c3-3fed6d4d5eb6.170fdca8020c492d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-750/filler-pod-d9f1465b-5d29-4720-a9c3-3fed6d4d5eb6 to ip-172-31-26-197.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9f1465b-5d29-4720-a9c3-3fed6d4d5eb6.170fdca82c871f28], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9f1465b-5d29-4720-a9c3-3fed6d4d5eb6.170fdca82e4566b2], Reason = [Created], Message = [Created container filler-pod-d9f1465b-5d29-4720-a9c3-3fed6d4d5eb6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9f1465b-5d29-4720-a9c3-3fed6d4d5eb6.170fdca83448d3b4], Reason = [Started], Message = [Started container filler-pod-d9f1465b-5d29-4720-a9c3-3fed6d4d5eb6]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.170fdca87e7c524f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.]
STEP: removing the label node off the node ip-172-31-16-214.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-25-142.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-26-197.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 29 16:10:05.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-750" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":152,"skipped":2603,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:06.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-7990/configmap-test-9b1d6f17-f583-4702-8f33-59a51a5b1474
STEP: Creating a pod to test consume configMaps
Aug 29 16:10:06.783: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c85ad84-8066-4bf5-b2aa-317a57420983" in namespace "configmap-7990" to be "Succeeded or Failed"
Aug 29 16:10:06.792: INFO: Pod "pod-configmaps-2c85ad84-8066-4bf5-b2aa-317a57420983": Phase="Pending", Reason="", readiness=false. Elapsed: 8.769721ms
Aug 29 16:10:08.821: INFO: Pod "pod-configmaps-2c85ad84-8066-4bf5-b2aa-317a57420983": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038035551s
Aug 29 16:10:10.839: INFO: Pod "pod-configmaps-2c85ad84-8066-4bf5-b2aa-317a57420983": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056463761s
STEP: Saw pod success
Aug 29 16:10:10.840: INFO: Pod "pod-configmaps-2c85ad84-8066-4bf5-b2aa-317a57420983" satisfied condition "Succeeded or Failed"
Aug 29 16:10:10.847: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-configmaps-2c85ad84-8066-4bf5-b2aa-317a57420983 container env-test: <nil>
STEP: delete the pod
Aug 29 16:10:11.505: INFO: Waiting for pod pod-configmaps-2c85ad84-8066-4bf5-b2aa-317a57420983 to disappear
Aug 29 16:10:11.521: INFO: Pod pod-configmaps-2c85ad84-8066-4bf5-b2aa-317a57420983 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:10:11.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7990" for this suite.

• [SLOW TEST:5.541 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":153,"skipped":2608,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:11.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 29 16:10:22.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9392" for this suite.

• [SLOW TEST:11.375 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":154,"skipped":2624,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:22.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:10:23.030: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1a78c49-46db-4e5b-9d69-101629034a43" in namespace "downward-api-7676" to be "Succeeded or Failed"
Aug 29 16:10:23.042: INFO: Pod "downwardapi-volume-a1a78c49-46db-4e5b-9d69-101629034a43": Phase="Pending", Reason="", readiness=false. Elapsed: 11.87123ms
Aug 29 16:10:25.055: INFO: Pod "downwardapi-volume-a1a78c49-46db-4e5b-9d69-101629034a43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025227235s
Aug 29 16:10:27.066: INFO: Pod "downwardapi-volume-a1a78c49-46db-4e5b-9d69-101629034a43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035620103s
STEP: Saw pod success
Aug 29 16:10:27.066: INFO: Pod "downwardapi-volume-a1a78c49-46db-4e5b-9d69-101629034a43" satisfied condition "Succeeded or Failed"
Aug 29 16:10:27.071: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-a1a78c49-46db-4e5b-9d69-101629034a43 container client-container: <nil>
STEP: delete the pod
Aug 29 16:10:27.105: INFO: Waiting for pod downwardapi-volume-a1a78c49-46db-4e5b-9d69-101629034a43 to disappear
Aug 29 16:10:27.113: INFO: Pod downwardapi-volume-a1a78c49-46db-4e5b-9d69-101629034a43 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 16:10:27.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7676" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":155,"skipped":2628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:27.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with configMap that has name projected-configmap-test-upd-0f5d810c-e370-4f59-81a9-341bd13dab52
STEP: Creating the pod
Aug 29 16:10:27.232: INFO: The status of Pod pod-projected-configmaps-fbabbe13-95f5-4995-a3c1-2ca45a29e061 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:10:29.240: INFO: The status of Pod pod-projected-configmaps-fbabbe13-95f5-4995-a3c1-2ca45a29e061 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-0f5d810c-e370-4f59-81a9-341bd13dab52
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 29 16:10:31.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6386" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":156,"skipped":2663,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:31.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 29 16:10:31.384: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 16:10:31.493: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 16:10:31.511: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-16-214.eu-central-1.compute.internal before test
Aug 29 16:10:31.525: INFO: canal-qcxrv from kube-system started at 2022-08-29 15:06:13 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:31.525: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:10:31.525: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:10:31.525: INFO: envoy-agent-r297b from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.525: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:10:31.525: INFO: kube-proxy-wn284 from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.526: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:10:31.526: INFO: node-local-dns-px2xr from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.526: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:10:31.526: INFO: pod-projected-configmaps-fbabbe13-95f5-4995-a3c1-2ca45a29e061 from projected-6386 started at 2022-08-29 16:10:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.526: INFO: 	Container agnhost-container ready: true, restart count 0
Aug 29 16:10:31.526: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-942bg from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:31.526: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:10:31.526: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:10:31.526: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-25-142.eu-central-1.compute.internal before test
Aug 29 16:10:31.539: INFO: replace-27696490-nskgx from cronjob-674 started at 2022-08-29 16:10:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.539: INFO: 	Container c ready: true, restart count 0
Aug 29 16:10:31.539: INFO: canal-xl6zl from kube-system started at 2022-08-29 15:07:00 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:31.539: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:10:31.539: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:10:31.539: INFO: envoy-agent-sqxbm from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.539: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:10:31.539: INFO: kube-proxy-gb22k from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.539: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:10:31.539: INFO: node-local-dns-vmfhr from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.539: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:10:31.539: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-9j6sw from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:31.539: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:10:31.539: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:10:31.539: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-26-197.eu-central-1.compute.internal before test
Aug 29 16:10:31.552: INFO: calico-kube-controllers-58c7d666cf-65tgr from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 16:10:31.552: INFO: canal-kg6wt from kube-system started at 2022-08-29 15:05:48 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:10:31.552: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:10:31.552: INFO: coredns-5fff85bf5-49vtq from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:10:31.552: INFO: coredns-5fff85bf5-5ppjp from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:10:31.552: INFO: envoy-agent-zzmwh from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:10:31.552: INFO: konnectivity-agent-6885ff4877-2bv58 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:10:31.552: INFO: konnectivity-agent-6885ff4877-n5r72 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:10:31.552: INFO: kube-proxy-9gjzc from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:10:31.552: INFO: metrics-server-6847c4d787-4wc57 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:10:31.552: INFO: metrics-server-6847c4d787-z9nwh from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:10:31.552: INFO: node-local-dns-6x48b from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:10:31.552: INFO: dashboard-metrics-scraper-8cfc65cdb-85659 from kubernetes-dashboard started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 29 16:10:31.552: INFO: dashboard-metrics-scraper-8cfc65cdb-v2g6l from kubernetes-dashboard started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 29 16:10:31.552: INFO: sonobuoy from sonobuoy started at 2022-08-29 15:06:56 +0000 UTC (1 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 16:10:31.552: INFO: sonobuoy-e2e-job-ef66887baa2a47e4 from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container e2e ready: true, restart count 0
Aug 29 16:10:31.552: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:10:31.552: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-4d599 from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:10:31.552: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:10:31.552: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fc7eadbc-064b-4958-86e7-785a6108e4ca 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-fc7eadbc-064b-4958-86e7-785a6108e4ca off the node ip-172-31-16-214.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fc7eadbc-064b-4958-86e7-785a6108e4ca
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 29 16:10:37.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9249" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.415 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":157,"skipped":2669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:37.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Aug 29 16:10:37.797: INFO: created test-event-1
Aug 29 16:10:37.803: INFO: created test-event-2
Aug 29 16:10:37.809: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Aug 29 16:10:37.815: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Aug 29 16:10:37.844: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Aug 29 16:10:37.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4784" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":158,"skipped":2698,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:37.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 29 16:10:37.950: INFO: Waiting up to 5m0s for pod "pod-20a8d88f-798f-4a61-9074-3783d9d15a25" in namespace "emptydir-7185" to be "Succeeded or Failed"
Aug 29 16:10:37.955: INFO: Pod "pod-20a8d88f-798f-4a61-9074-3783d9d15a25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.879925ms
Aug 29 16:10:39.968: INFO: Pod "pod-20a8d88f-798f-4a61-9074-3783d9d15a25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017655022s
Aug 29 16:10:41.982: INFO: Pod "pod-20a8d88f-798f-4a61-9074-3783d9d15a25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032143003s
STEP: Saw pod success
Aug 29 16:10:41.982: INFO: Pod "pod-20a8d88f-798f-4a61-9074-3783d9d15a25" satisfied condition "Succeeded or Failed"
Aug 29 16:10:41.989: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-20a8d88f-798f-4a61-9074-3783d9d15a25 container test-container: <nil>
STEP: delete the pod
Aug 29 16:10:42.034: INFO: Waiting for pod pod-20a8d88f-798f-4a61-9074-3783d9d15a25 to disappear
Aug 29 16:10:42.041: INFO: Pod pod-20a8d88f-798f-4a61-9074-3783d9d15a25 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 16:10:42.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7185" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":159,"skipped":2699,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:42.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 29 16:10:42.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-800" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":160,"skipped":2720,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:42.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 29 16:10:53.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6801" for this suite.

• [SLOW TEST:11.488 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":161,"skipped":2736,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:53.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:10:54.280: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:10:57.347: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:10:57.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3585" for this suite.
STEP: Destroying namespace "webhook-3585-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":162,"skipped":2744,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:10:57.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1854
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Aug 29 16:10:57.957: INFO: Found 0 stateful pods, waiting for 3
Aug 29 16:11:07.968: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:11:07.968: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:11:07.968: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:11:07.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1854 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:11:08.290: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:11:08.290: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:11:08.290: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Aug 29 16:11:18.361: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 29 16:11:28.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1854 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:11:28.684: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:11:28.684: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:11:28.684: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Aug 29 16:11:48.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1854 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:11:49.042: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:11:49.042: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:11:49.042: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:11:59.131: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 29 16:12:09.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-1854 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:12:09.434: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:12:09.434: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:12:09.434: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 29 16:12:19.478: INFO: Deleting all statefulset in ns statefulset-1854
Aug 29 16:12:19.492: INFO: Scaling statefulset ss2 to 0
Aug 29 16:12:29.526: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:12:29.536: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 29 16:12:29.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1854" for this suite.

• [SLOW TEST:91.873 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":163,"skipped":2748,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:12:29.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8943.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8943.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8943.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8943.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:12:31.799: INFO: DNS probes using dns-8943/dns-test-e01fe6f8-3d1c-48c6-9e0b-df7aebe902ca succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 29 16:12:31.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8943" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":164,"skipped":2754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:12:31.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-dd5553e0-96c8-4b3c-86e0-cf4b491497af
Aug 29 16:12:31.939: INFO: Pod name my-hostname-basic-dd5553e0-96c8-4b3c-86e0-cf4b491497af: Found 0 pods out of 1
Aug 29 16:12:36.955: INFO: Pod name my-hostname-basic-dd5553e0-96c8-4b3c-86e0-cf4b491497af: Found 1 pods out of 1
Aug 29 16:12:36.955: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dd5553e0-96c8-4b3c-86e0-cf4b491497af" are running
Aug 29 16:12:36.969: INFO: Pod "my-hostname-basic-dd5553e0-96c8-4b3c-86e0-cf4b491497af-tngk8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 16:12:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 16:12:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 16:12:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-08-29 16:12:31 +0000 UTC Reason: Message:}])
Aug 29 16:12:36.969: INFO: Trying to dial the pod
Aug 29 16:12:42.016: INFO: Controller my-hostname-basic-dd5553e0-96c8-4b3c-86e0-cf4b491497af: Got expected result from replica 1 [my-hostname-basic-dd5553e0-96c8-4b3c-86e0-cf4b491497af-tngk8]: "my-hostname-basic-dd5553e0-96c8-4b3c-86e0-cf4b491497af-tngk8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 29 16:12:42.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5017" for this suite.

• [SLOW TEST:10.202 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":165,"skipped":2797,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:12:42.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:12:42.130: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 29 16:12:47.142: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 29 16:12:47.142: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 29 16:12:49.150: INFO: Creating deployment "test-rollover-deployment"
Aug 29 16:12:49.171: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 29 16:12:51.200: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 29 16:12:51.228: INFO: Ensure that both replica sets have 1 created replica
Aug 29 16:12:51.265: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 29 16:12:51.295: INFO: Updating deployment test-rollover-deployment
Aug 29 16:12:51.296: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 29 16:12:53.324: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 29 16:12:53.336: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 29 16:12:53.346: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:12:53.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:12:55.362: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:12:55.362: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:12:57.366: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:12:57.366: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:12:59.371: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:12:59.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:13:01.373: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:13:01.373: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:13:03.366: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 16:13:03.366: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 12, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 12, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 16:13:05.674: INFO: 
Aug 29 16:13:05.674: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 16:13:05.693: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8444  3aa198ae-2805-47ab-98b2-b3a87e8f17a1 27317 2 2022-08-29 16:12:49 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-29 16:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:13:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022dbba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-29 16:12:49 +0000 UTC,LastTransitionTime:2022-08-29 16:12:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-779c67f4f8" has successfully progressed.,LastUpdateTime:2022-08-29 16:13:03 +0000 UTC,LastTransitionTime:2022-08-29 16:12:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 16:13:05.700: INFO: New ReplicaSet "test-rollover-deployment-779c67f4f8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-779c67f4f8  deployment-8444  efd23fcb-ade7-4340-9908-0e2be76c300a 27307 2 2022-08-29 16:12:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3aa198ae-2805-47ab-98b2-b3a87e8f17a1 0xc004cc0047 0xc004cc0048}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa198ae-2805-47ab-98b2-b3a87e8f17a1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:13:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 779c67f4f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cc00f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:13:05.700: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 29 16:13:05.700: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8444  5c45634c-d46a-4f98-803c-26284b53c416 27316 2 2022-08-29 16:12:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3aa198ae-2805-47ab-98b2-b3a87e8f17a1 0xc0022dbf47 0xc0022dbf48}] []  [{e2e.test Update apps/v1 2022-08-29 16:12:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:13:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa198ae-2805-47ab-98b2-b3a87e8f17a1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:13:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00041ae38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:13:05.701: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-8444  0904bf51-72f5-4dc3-9245-a23dc80ca699 27261 2 2022-08-29 16:12:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3aa198ae-2805-47ab-98b2-b3a87e8f17a1 0xc004cc0170 0xc004cc0171}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa198ae-2805-47ab-98b2-b3a87e8f17a1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:12:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cc0218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:13:05.714: INFO: Pod "test-rollover-deployment-779c67f4f8-rjrcj" is available:
&Pod{ObjectMeta:{test-rollover-deployment-779c67f4f8-rjrcj test-rollover-deployment-779c67f4f8- deployment-8444  53909572-fd03-49d2-9c75-503d77ebd0c8 27277 0 2022-08-29 16:12:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[cni.projectcalico.org/containerID:9fe5732d9df7cb35be1014662075aab556e6538b22bec001a60f0df27e1ba40b cni.projectcalico.org/podIP:172.25.2.210/32 cni.projectcalico.org/podIPs:172.25.2.210/32] [{apps/v1 ReplicaSet test-rollover-deployment-779c67f4f8 efd23fcb-ade7-4340-9908-0e2be76c300a 0xc004cc0777 0xc004cc0778}] []  [{kube-controller-manager Update v1 2022-08-29 16:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efd23fcb-ade7-4340-9908-0e2be76c300a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:12:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:12:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dm69l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dm69l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:12:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:12:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:12:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:12:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:172.25.2.210,StartTime:2022-08-29 16:12:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:12:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://f26e254d666d02606ea642cab9f06566cd21ad773391b35c9004d812164e68d8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 29 16:13:05.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8444" for this suite.

• [SLOW TEST:23.708 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":166,"skipped":2810,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:13:05.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Aug 29 16:19:02.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7109" for this suite.

• [SLOW TEST:356.713 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":167,"skipped":2826,"failed":0}
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:19:02.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Aug 29 16:19:03.159: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:19:05.171: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Aug 29 16:19:05.242: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:19:07.253: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 29 16:19:07.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 29 16:19:07.320: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 29 16:19:09.320: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 29 16:19:09.328: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 29 16:19:11.321: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 29 16:19:11.728: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 29 16:19:11.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6013" for this suite.

• [SLOW TEST:9.281 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":168,"skipped":2831,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:19:11.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Aug 29 16:19:12.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6970" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":169,"skipped":2852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:19:12.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 29 16:19:28.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8877" for this suite.

• [SLOW TEST:16.562 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":170,"skipped":2875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:19:28.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 29 16:19:28.777: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 29 16:19:29.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8176" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":171,"skipped":2902,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:19:29.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Aug 29 16:19:30.550: INFO: Waiting up to 5m0s for pod "var-expansion-8d4e6cc2-d283-4ac7-96fe-2f7e5bfe4ee7" in namespace "var-expansion-509" to be "Succeeded or Failed"
Aug 29 16:19:30.558: INFO: Pod "var-expansion-8d4e6cc2-d283-4ac7-96fe-2f7e5bfe4ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116181ms
Aug 29 16:19:32.857: INFO: Pod "var-expansion-8d4e6cc2-d283-4ac7-96fe-2f7e5bfe4ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307274563s
Aug 29 16:19:34.885: INFO: Pod "var-expansion-8d4e6cc2-d283-4ac7-96fe-2f7e5bfe4ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.334728311s
Aug 29 16:19:36.926: INFO: Pod "var-expansion-8d4e6cc2-d283-4ac7-96fe-2f7e5bfe4ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376210414s
Aug 29 16:19:39.437: INFO: Pod "var-expansion-8d4e6cc2-d283-4ac7-96fe-2f7e5bfe4ee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.886440455s
STEP: Saw pod success
Aug 29 16:19:39.437: INFO: Pod "var-expansion-8d4e6cc2-d283-4ac7-96fe-2f7e5bfe4ee7" satisfied condition "Succeeded or Failed"
Aug 29 16:19:39.451: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod var-expansion-8d4e6cc2-d283-4ac7-96fe-2f7e5bfe4ee7 container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:19:39.498: INFO: Waiting for pod var-expansion-8d4e6cc2-d283-4ac7-96fe-2f7e5bfe4ee7 to disappear
Aug 29 16:19:39.505: INFO: Pod var-expansion-8d4e6cc2-d283-4ac7-96fe-2f7e5bfe4ee7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 29 16:19:39.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-509" for this suite.

• [SLOW TEST:9.677 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":172,"skipped":2915,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:19:39.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Aug 29 16:19:39.600: INFO: created test-pod-1
Aug 29 16:19:39.616: INFO: created test-pod-2
Aug 29 16:19:40.385: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Aug 29 16:19:40.386: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-8043' to be running and ready
Aug 29 16:19:40.421: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 16:19:40.422: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 16:19:40.422: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 16:19:40.422: INFO: 0 / 3 pods in namespace 'pods-8043' are running and ready (0 seconds elapsed)
Aug 29 16:19:40.422: INFO: expected 0 pod replicas in namespace 'pods-8043', 0 are Running and Ready.
Aug 29 16:19:40.422: INFO: POD         NODE                                            PHASE    GRACE  CONDITIONS
Aug 29 16:19:40.422: INFO: test-pod-1  ip-172-31-16-214.eu-central-1.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC  }]
Aug 29 16:19:40.422: INFO: test-pod-2  ip-172-31-25-142.eu-central-1.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC  }]
Aug 29 16:19:40.422: INFO: test-pod-3  ip-172-31-25-142.eu-central-1.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC  }]
Aug 29 16:19:40.422: INFO: 
Aug 29 16:19:42.451: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 16:19:42.451: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 16:19:42.451: INFO: 1 / 3 pods in namespace 'pods-8043' are running and ready (2 seconds elapsed)
Aug 29 16:19:42.451: INFO: expected 0 pod replicas in namespace 'pods-8043', 0 are Running and Ready.
Aug 29 16:19:42.451: INFO: POD         NODE                                            PHASE    GRACE  CONDITIONS
Aug 29 16:19:42.452: INFO: test-pod-2  ip-172-31-25-142.eu-central-1.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC  }]
Aug 29 16:19:42.452: INFO: test-pod-3  ip-172-31-25-142.eu-central-1.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-08-29 16:19:40 +0000 UTC  }]
Aug 29 16:19:42.452: INFO: 
Aug 29 16:19:44.454: INFO: 3 / 3 pods in namespace 'pods-8043' are running and ready (4 seconds elapsed)
Aug 29 16:19:44.454: INFO: expected 0 pod replicas in namespace 'pods-8043', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Aug 29 16:19:44.501: INFO: Pod quantity 3 is different from expected quantity 0
Aug 29 16:19:45.510: INFO: Pod quantity 3 is different from expected quantity 0
Aug 29 16:19:46.513: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 29 16:19:47.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8043" for this suite.

• [SLOW TEST:8.003 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":173,"skipped":2917,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:19:47.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 29 16:19:47.604: INFO: Waiting up to 5m0s for pod "downward-api-7ad1d0bf-73bb-4942-b1ed-2a9a75b47ceb" in namespace "downward-api-1742" to be "Succeeded or Failed"
Aug 29 16:19:47.626: INFO: Pod "downward-api-7ad1d0bf-73bb-4942-b1ed-2a9a75b47ceb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.238649ms
Aug 29 16:19:49.637: INFO: Pod "downward-api-7ad1d0bf-73bb-4942-b1ed-2a9a75b47ceb": Phase="Running", Reason="", readiness=true. Elapsed: 2.032656585s
Aug 29 16:19:51.650: INFO: Pod "downward-api-7ad1d0bf-73bb-4942-b1ed-2a9a75b47ceb": Phase="Running", Reason="", readiness=false. Elapsed: 4.045694438s
Aug 29 16:19:53.670: INFO: Pod "downward-api-7ad1d0bf-73bb-4942-b1ed-2a9a75b47ceb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06647485s
STEP: Saw pod success
Aug 29 16:19:53.670: INFO: Pod "downward-api-7ad1d0bf-73bb-4942-b1ed-2a9a75b47ceb" satisfied condition "Succeeded or Failed"
Aug 29 16:19:53.686: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downward-api-7ad1d0bf-73bb-4942-b1ed-2a9a75b47ceb container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:19:53.738: INFO: Waiting for pod downward-api-7ad1d0bf-73bb-4942-b1ed-2a9a75b47ceb to disappear
Aug 29 16:19:53.751: INFO: Pod downward-api-7ad1d0bf-73bb-4942-b1ed-2a9a75b47ceb no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 29 16:19:53.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1742" for this suite.

• [SLOW TEST:6.248 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":174,"skipped":2926,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:19:53.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:19:53.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a3f55bd-3b33-4369-bb76-f235d13161c7" in namespace "downward-api-4191" to be "Succeeded or Failed"
Aug 29 16:19:53.880: INFO: Pod "downwardapi-volume-5a3f55bd-3b33-4369-bb76-f235d13161c7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.348197ms
Aug 29 16:19:55.897: INFO: Pod "downwardapi-volume-5a3f55bd-3b33-4369-bb76-f235d13161c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027975965s
Aug 29 16:19:57.908: INFO: Pod "downwardapi-volume-5a3f55bd-3b33-4369-bb76-f235d13161c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03901518s
STEP: Saw pod success
Aug 29 16:19:57.908: INFO: Pod "downwardapi-volume-5a3f55bd-3b33-4369-bb76-f235d13161c7" satisfied condition "Succeeded or Failed"
Aug 29 16:19:57.914: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod downwardapi-volume-5a3f55bd-3b33-4369-bb76-f235d13161c7 container client-container: <nil>
STEP: delete the pod
Aug 29 16:19:58.010: INFO: Waiting for pod downwardapi-volume-5a3f55bd-3b33-4369-bb76-f235d13161c7 to disappear
Aug 29 16:19:58.014: INFO: Pod downwardapi-volume-5a3f55bd-3b33-4369-bb76-f235d13161c7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 16:19:58.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4191" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":175,"skipped":2942,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:19:58.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 29 16:20:02.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3014" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":176,"skipped":3030,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:20:02.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:20:03.536: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 16:20:05.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 16, 20, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 20, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 20, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 20, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:20:08.613: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:20:08.623: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9721-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:20:12.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6294" for this suite.
STEP: Destroying namespace "webhook-6294-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:10.430 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":177,"skipped":3038,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:20:12.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:20:13.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 29 16:20:13.509: INFO: The status of Pod pod-logs-websocket-87008aad-860f-4045-a757-78a27fe58d23 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:20:15.714: INFO: The status of Pod pod-logs-websocket-87008aad-860f-4045-a757-78a27fe58d23 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 29 16:20:15.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6604" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":178,"skipped":3051,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:20:15.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:20:16.059: INFO: The status of Pod busybox-readonly-fscb0f8f16-03ce-427d-b512-0062934a8580 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:20:18.074: INFO: The status of Pod busybox-readonly-fscb0f8f16-03ce-427d-b512-0062934a8580 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 29 16:20:18.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8860" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":179,"skipped":3054,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:20:18.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:20:18.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6417 create -f -'
Aug 29 16:20:19.440: INFO: stderr: ""
Aug 29 16:20:19.440: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 29 16:20:19.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6417 create -f -'
Aug 29 16:20:20.525: INFO: stderr: ""
Aug 29 16:20:20.525: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 29 16:20:21.534: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 16:20:21.534: INFO: Found 1 / 1
Aug 29 16:20:21.534: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 29 16:20:21.544: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 16:20:21.544: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 29 16:20:21.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6417 describe pod agnhost-primary-tn8pn'
Aug 29 16:20:21.661: INFO: stderr: ""
Aug 29 16:20:21.661: INFO: stdout: "Name:         agnhost-primary-tn8pn\nNamespace:    kubectl-6417\nPriority:     0\nNode:         ip-172-31-16-214.eu-central-1.compute.internal/172.31.16.214\nStart Time:   Mon, 29 Aug 2022 16:20:19 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: ae543b4d9d84ae7d5516ac24b1892bf60dca734c2299cc226513b6debe4441b7\n              cni.projectcalico.org/podIP: 172.25.1.197/32\n              cni.projectcalico.org/podIPs: 172.25.1.197/32\nStatus:       Running\nIP:           172.25.1.197\nIPs:\n  IP:           172.25.1.197\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://ceab5b59375a3b5536cab9e9c72c5ad548b54b78909bec992f0745bf3a1975ca\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 29 Aug 2022 16:20:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f8p84 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-f8p84:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6417/agnhost-primary-tn8pn to ip-172-31-16-214.eu-central-1.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Aug 29 16:20:21.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6417 describe rc agnhost-primary'
Aug 29 16:20:21.787: INFO: stderr: ""
Aug 29 16:20:21.787: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6417\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-tn8pn\n"
Aug 29 16:20:21.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6417 describe service agnhost-primary'
Aug 29 16:20:21.902: INFO: stderr: ""
Aug 29 16:20:21.903: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6417\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.240.29.58\nIPs:               10.240.29.58\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.1.197:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 29 16:20:21.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6417 describe node ip-172-31-16-214.eu-central-1.compute.internal'
Aug 29 16:20:22.080: INFO: stderr: ""
Aug 29 16:20:22.081: INFO: stdout: "Name:               ip-172-31-16-214.eu-central-1.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3a.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-16-214\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=996b087b-2636-4331-b2e2-7b877a158d9e\n                    node.kubernetes.io/instance-type=t3a.medium\n                    system/cluster=whrcmndpx8\n                    system/project=jtnngz4ghk\n                    topology.kubernetes.io/region=eu-central-1\n                    topology.kubernetes.io/zone=eu-central-1a\n                    v1.machine-controller.kubermatic.io/operating-system=ubuntu\n                    x-kubernetes.io/distribution=ubuntu\nAnnotations:        cluster.k8s.io/machine: kube-system/whrcmndpx8-worker-xt549t-58f57bf6b-fxk2q\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9e:04:da:f6:28:d5\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.16.214\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.16.214/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 29 Aug 2022 15:06:06 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-16-214.eu-central-1.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 29 Aug 2022 16:20:13 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 29 Aug 2022 15:07:30 +0000   Mon, 29 Aug 2022 15:07:30 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 29 Aug 2022 16:17:01 +0000   Mon, 29 Aug 2022 15:06:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 29 Aug 2022 16:17:01 +0000   Mon, 29 Aug 2022 15:06:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 29 Aug 2022 16:17:01 +0000   Mon, 29 Aug 2022 15:06:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 29 Aug 2022 16:17:01 +0000   Mon, 29 Aug 2022 15:07:07 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.16.214\n  ExternalIP:   3.67.170.2\n  Hostname:     ip-172-31-16-214.eu-central-1.compute.internal\n  InternalDNS:  ip-172-31-16-214.eu-central-1.compute.internal\n  ExternalDNS:  ec2-3-67-170-2.eu-central-1.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           25215872Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3969012Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       5\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         1600m\n  ephemeral-storage:           21091463949\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3457012Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       5\nSystem Info:\n  Machine ID:                 ec242b7180574a54f7dd772b12860821\n  System UUID:                ec242b71-8057-4a54-f7dd-772b12860821\n  Boot ID:                    e46f2036-53ec-4df3-88d1-554634594ff6\n  Kernel Version:             5.15.0-1017-aws\n  OS Image:                   Ubuntu 20.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.11\n  Kubelet Version:            v1.24.3\n  Kube-Proxy Version:         v1.24.3\nPodCIDR:                      172.25.1.0/24\nPodCIDRs:                     172.25.1.0/24\nProviderID:                   aws:///eu-central-1a/i-00f65f8df6cd377d7\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-qcxrv                                                250m (15%)    0 (0%)      0 (0%)           0 (0%)         74m\n  kube-system                 envoy-agent-r297b                                          50m (3%)      1 (62%)     32Mi (0%)        64Mi (1%)      74m\n  kube-system                 kube-proxy-wn284                                           75m (4%)      250m (15%)  50Mi (1%)        250Mi (7%)     74m\n  kube-system                 node-local-dns-px2xr                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\n  kubectl-6417                agnhost-primary-tn8pn                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  pods-6604                   pod-logs-websocket-87008aad-860f-4045-a757-78a27fe58d23    0 (0%)        0 (0%)      0 (0%)           0 (0%)         9s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-942bg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         375m (23%)  1250m (78%)\n  memory                      82Mi (2%)   314Mi (9%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\n  scheduling.k8s.io/foo       0           0\nEvents:                       <none>\n"
Aug 29 16:20:22.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6417 describe namespace kubectl-6417'
Aug 29 16:20:22.181: INFO: stderr: ""
Aug 29 16:20:22.181: INFO: stdout: "Name:         kubectl-6417\nLabels:       e2e-framework=kubectl\n              e2e-run=a3892898-a766-488a-a3ff-19fe219e0304\n              kubernetes.io/metadata.name=kubectl-6417\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:20:22.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6417" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":180,"skipped":3088,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:20:22.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-9093
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 29 16:20:22.251: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 16:20:22.910: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:20:24.919: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:20:26.919: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:20:28.920: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:20:30.922: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:20:32.925: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 29 16:20:34.920: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 29 16:20:34.934: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:20:36.947: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:20:38.949: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:20:40.945: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:20:42.947: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 29 16:20:44.947: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 29 16:20:44.956: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 29 16:20:47.580: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 29 16:20:47.580: INFO: Going to poll 172.25.1.198 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 29 16:20:47.586: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.198 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9093 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:20:47.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:20:47.587: INFO: ExecWithOptions: Clientset creation
Aug 29 16:20:47.588: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9093/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.1.198+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 16:20:48.755: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 29 16:20:48.755: INFO: Going to poll 172.25.2.221 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 29 16:20:48.777: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.221 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9093 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:20:48.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:20:48.778: INFO: ExecWithOptions: Clientset creation
Aug 29 16:20:48.778: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9093/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.2.221+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 16:20:49.946: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 29 16:20:49.946: INFO: Going to poll 172.25.0.124 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 29 16:20:49.961: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.124 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9093 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:20:49.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:20:49.962: INFO: ExecWithOptions: Clientset creation
Aug 29 16:20:49.963: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9093/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.0.124+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 16:20:51.114: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Aug 29 16:20:51.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9093" for this suite.

• [SLOW TEST:28.927 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":181,"skipped":3089,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:20:51.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:20:51.515: INFO: created pod
Aug 29 16:20:51.515: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4607" to be "Succeeded or Failed"
Aug 29 16:20:51.525: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039853ms
Aug 29 16:20:53.536: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0209115s
Aug 29 16:20:55.595: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079922825s
STEP: Saw pod success
Aug 29 16:20:55.595: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 29 16:21:25.596: INFO: polling logs
Aug 29 16:21:25.609: INFO: Pod logs: 
I0829 16:20:52.411497       1 log.go:195] OK: Got token
I0829 16:20:52.411769       1 log.go:195] validating with in-cluster discovery
I0829 16:20:52.412252       1 log.go:195] OK: got issuer https://whrcmndpx8.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443
I0829 16:20:52.412443       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://whrcmndpx8.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443", Subject:"system:serviceaccount:svcaccounts-4607:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1661790651, NotBefore:1661790051, IssuedAt:1661790051, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4607", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"61095f89-39ae-43f5-bf1a-be62db65c6ac"}}}
I0829 16:20:52.428248       1 log.go:195] OK: Constructed OIDC provider for issuer https://whrcmndpx8.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443
I0829 16:20:52.431462       1 log.go:195] OK: Validated signature on JWT
I0829 16:20:52.431581       1 log.go:195] OK: Got valid claims from token!
I0829 16:20:52.431615       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://whrcmndpx8.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443", Subject:"system:serviceaccount:svcaccounts-4607:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1661790651, NotBefore:1661790051, IssuedAt:1661790051, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4607", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"61095f89-39ae-43f5-bf1a-be62db65c6ac"}}}

Aug 29 16:21:25.610: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 29 16:21:25.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4607" for this suite.

• [SLOW TEST:34.506 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":182,"skipped":3105,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:21:25.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:21:25.736: INFO: Got root ca configmap in namespace "svcaccounts-7551"
Aug 29 16:21:25.754: INFO: Deleted root ca configmap in namespace "svcaccounts-7551"
STEP: waiting for a new root ca configmap created
Aug 29 16:21:26.265: INFO: Recreated root ca configmap in namespace "svcaccounts-7551"
Aug 29 16:21:26.274: INFO: Updated root ca configmap in namespace "svcaccounts-7551"
STEP: waiting for the root ca configmap reconciled
Aug 29 16:21:26.786: INFO: Reconciled root ca configmap in namespace "svcaccounts-7551"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 29 16:21:26.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7551" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":183,"skipped":3107,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:21:26.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Aug 29 16:21:27.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6343" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":184,"skipped":3134,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:21:27.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Aug 29 16:21:27.565: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:21:29.583: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Aug 29 16:21:29.611: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:21:31.617: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 29 16:21:32.430: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 29 16:21:32.444: INFO: Pod pod-with-poststart-http-hook still exists
Aug 29 16:21:34.445: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 29 16:21:34.465: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 29 16:21:34.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9839" for this suite.

• [SLOW TEST:7.040 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":185,"skipped":3152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:21:34.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 29 16:21:34.611: INFO: Waiting up to 5m0s for pod "downward-api-365f602b-9390-4f48-b219-3cd099e8351a" in namespace "downward-api-7481" to be "Succeeded or Failed"
Aug 29 16:21:34.616: INFO: Pod "downward-api-365f602b-9390-4f48-b219-3cd099e8351a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.249364ms
Aug 29 16:21:37.182: INFO: Pod "downward-api-365f602b-9390-4f48-b219-3cd099e8351a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.571369956s
Aug 29 16:21:39.194: INFO: Pod "downward-api-365f602b-9390-4f48-b219-3cd099e8351a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.583054145s
Aug 29 16:21:41.209: INFO: Pod "downward-api-365f602b-9390-4f48-b219-3cd099e8351a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.598430151s
STEP: Saw pod success
Aug 29 16:21:41.209: INFO: Pod "downward-api-365f602b-9390-4f48-b219-3cd099e8351a" satisfied condition "Succeeded or Failed"
Aug 29 16:21:41.224: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downward-api-365f602b-9390-4f48-b219-3cd099e8351a container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:21:41.256: INFO: Waiting for pod downward-api-365f602b-9390-4f48-b219-3cd099e8351a to disappear
Aug 29 16:21:41.268: INFO: Pod downward-api-365f602b-9390-4f48-b219-3cd099e8351a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 29 16:21:41.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7481" for this suite.

• [SLOW TEST:6.796 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":186,"skipped":3198,"failed":0}
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:21:41.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 29 16:21:41.332: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 29 16:21:46.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5174" for this suite.

• [SLOW TEST:5.027 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":187,"skipped":3204,"failed":0}
S
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:21:46.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:21:46.384: INFO: The status of Pod busybox-host-aliases35fefd2b-6957-409a-9790-f2d7488ba175 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:21:48.394: INFO: The status of Pod busybox-host-aliases35fefd2b-6957-409a-9790-f2d7488ba175 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 29 16:21:48.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7164" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":188,"skipped":3205,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:21:48.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-40c39b05-d177-4e56-b681-dc77cb14eb99
STEP: Creating a pod to test consume secrets
Aug 29 16:21:48.567: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9970fbe0-4be6-49d3-894b-750c5a5b0090" in namespace "projected-4351" to be "Succeeded or Failed"
Aug 29 16:21:48.574: INFO: Pod "pod-projected-secrets-9970fbe0-4be6-49d3-894b-750c5a5b0090": Phase="Pending", Reason="", readiness=false. Elapsed: 6.959757ms
Aug 29 16:21:50.756: INFO: Pod "pod-projected-secrets-9970fbe0-4be6-49d3-894b-750c5a5b0090": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188505334s
Aug 29 16:21:52.764: INFO: Pod "pod-projected-secrets-9970fbe0-4be6-49d3-894b-750c5a5b0090": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.196350133s
STEP: Saw pod success
Aug 29 16:21:52.764: INFO: Pod "pod-projected-secrets-9970fbe0-4be6-49d3-894b-750c5a5b0090" satisfied condition "Succeeded or Failed"
Aug 29 16:21:52.770: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-projected-secrets-9970fbe0-4be6-49d3-894b-750c5a5b0090 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:21:52.815: INFO: Waiting for pod pod-projected-secrets-9970fbe0-4be6-49d3-894b-750c5a5b0090 to disappear
Aug 29 16:21:52.820: INFO: Pod pod-projected-secrets-9970fbe0-4be6-49d3-894b-750c5a5b0090 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 29 16:21:52.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4351" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":189,"skipped":3213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:21:52.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 29 16:21:56.965: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 29 16:21:57.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2917" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":190,"skipped":3258,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:21:57.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:21:58.542: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:22:01.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:22:01.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8088" for this suite.
STEP: Destroying namespace "webhook-8088-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":191,"skipped":3271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:22:01.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:22:01.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5174" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":192,"skipped":3295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:22:01.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Aug 29 16:22:02.052: INFO: Waiting up to 5m0s for pod "security-context-4c02bec7-eddc-4269-88b5-42aa1832daa7" in namespace "security-context-7651" to be "Succeeded or Failed"
Aug 29 16:22:02.072: INFO: Pod "security-context-4c02bec7-eddc-4269-88b5-42aa1832daa7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.893354ms
Aug 29 16:22:04.080: INFO: Pod "security-context-4c02bec7-eddc-4269-88b5-42aa1832daa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028183116s
Aug 29 16:22:06.090: INFO: Pod "security-context-4c02bec7-eddc-4269-88b5-42aa1832daa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038237067s
STEP: Saw pod success
Aug 29 16:22:06.090: INFO: Pod "security-context-4c02bec7-eddc-4269-88b5-42aa1832daa7" satisfied condition "Succeeded or Failed"
Aug 29 16:22:06.450: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod security-context-4c02bec7-eddc-4269-88b5-42aa1832daa7 container test-container: <nil>
STEP: delete the pod
Aug 29 16:22:06.501: INFO: Waiting for pod security-context-4c02bec7-eddc-4269-88b5-42aa1832daa7 to disappear
Aug 29 16:22:06.513: INFO: Pod security-context-4c02bec7-eddc-4269-88b5-42aa1832daa7 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 29 16:22:06.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7651" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":193,"skipped":3322,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:22:06.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:22:06.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Aug 29 16:22:10.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-7775 --namespace=crd-publish-openapi-7775 create -f -'
Aug 29 16:22:11.259: INFO: stderr: ""
Aug 29 16:22:11.259: INFO: stdout: "e2e-test-crd-publish-openapi-9318-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 29 16:22:11.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-7775 --namespace=crd-publish-openapi-7775 delete e2e-test-crd-publish-openapi-9318-crds test-cr'
Aug 29 16:22:11.358: INFO: stderr: ""
Aug 29 16:22:11.358: INFO: stdout: "e2e-test-crd-publish-openapi-9318-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 29 16:22:11.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-7775 --namespace=crd-publish-openapi-7775 apply -f -'
Aug 29 16:22:11.587: INFO: stderr: ""
Aug 29 16:22:11.587: INFO: stdout: "e2e-test-crd-publish-openapi-9318-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 29 16:22:11.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-7775 --namespace=crd-publish-openapi-7775 delete e2e-test-crd-publish-openapi-9318-crds test-cr'
Aug 29 16:22:11.694: INFO: stderr: ""
Aug 29 16:22:11.694: INFO: stdout: "e2e-test-crd-publish-openapi-9318-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 29 16:22:11.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-7775 explain e2e-test-crd-publish-openapi-9318-crds'
Aug 29 16:22:12.646: INFO: stderr: ""
Aug 29 16:22:12.646: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9318-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:22:16.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7775" for this suite.

• [SLOW TEST:9.541 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":194,"skipped":3338,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:22:16.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-5034911f-f736-49d8-9db9-8ba48c42047b
STEP: Creating a pod to test consume configMaps
Aug 29 16:22:16.161: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b263e1e1-d105-4787-a4b2-864fa96237e6" in namespace "projected-4984" to be "Succeeded or Failed"
Aug 29 16:22:16.789: INFO: Pod "pod-projected-configmaps-b263e1e1-d105-4787-a4b2-864fa96237e6": Phase="Pending", Reason="", readiness=false. Elapsed: 627.963417ms
Aug 29 16:22:18.804: INFO: Pod "pod-projected-configmaps-b263e1e1-d105-4787-a4b2-864fa96237e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643052202s
Aug 29 16:22:20.820: INFO: Pod "pod-projected-configmaps-b263e1e1-d105-4787-a4b2-864fa96237e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.658542113s
STEP: Saw pod success
Aug 29 16:22:20.820: INFO: Pod "pod-projected-configmaps-b263e1e1-d105-4787-a4b2-864fa96237e6" satisfied condition "Succeeded or Failed"
Aug 29 16:22:20.833: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-projected-configmaps-b263e1e1-d105-4787-a4b2-864fa96237e6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 29 16:22:20.889: INFO: Waiting for pod pod-projected-configmaps-b263e1e1-d105-4787-a4b2-864fa96237e6 to disappear
Aug 29 16:22:20.895: INFO: Pod pod-projected-configmaps-b263e1e1-d105-4787-a4b2-864fa96237e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 29 16:22:20.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4984" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":195,"skipped":3352,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:22:20.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:22:22.585: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 16:22:25.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 16, 22, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 22, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 22, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 22, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:22:28.105: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:22:41.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6028" for this suite.
STEP: Destroying namespace "webhook-6028-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:20.617 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":196,"skipped":3360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:22:41.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-115353b0-8198-4698-851c-edf5be32d698
STEP: Creating a pod to test consume configMaps
Aug 29 16:22:41.658: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d46d0332-fd63-4d22-81a7-87c3bc3dce71" in namespace "projected-4430" to be "Succeeded or Failed"
Aug 29 16:22:41.666: INFO: Pod "pod-projected-configmaps-d46d0332-fd63-4d22-81a7-87c3bc3dce71": Phase="Pending", Reason="", readiness=false. Elapsed: 8.13748ms
Aug 29 16:22:43.679: INFO: Pod "pod-projected-configmaps-d46d0332-fd63-4d22-81a7-87c3bc3dce71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020847911s
Aug 29 16:22:45.698: INFO: Pod "pod-projected-configmaps-d46d0332-fd63-4d22-81a7-87c3bc3dce71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040237062s
STEP: Saw pod success
Aug 29 16:22:45.698: INFO: Pod "pod-projected-configmaps-d46d0332-fd63-4d22-81a7-87c3bc3dce71" satisfied condition "Succeeded or Failed"
Aug 29 16:22:45.705: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-projected-configmaps-d46d0332-fd63-4d22-81a7-87c3bc3dce71 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:22:45.741: INFO: Waiting for pod pod-projected-configmaps-d46d0332-fd63-4d22-81a7-87c3bc3dce71 to disappear
Aug 29 16:22:45.754: INFO: Pod pod-projected-configmaps-d46d0332-fd63-4d22-81a7-87c3bc3dce71 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 29 16:22:45.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4430" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":197,"skipped":3385,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:22:45.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:22:46.540: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0460c39e-38a9-4f08-9b07-8b2107a223aa" in namespace "security-context-test-4838" to be "Succeeded or Failed"
Aug 29 16:22:46.689: INFO: Pod "busybox-user-65534-0460c39e-38a9-4f08-9b07-8b2107a223aa": Phase="Pending", Reason="", readiness=false. Elapsed: 149.029008ms
Aug 29 16:22:48.706: INFO: Pod "busybox-user-65534-0460c39e-38a9-4f08-9b07-8b2107a223aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.166198743s
Aug 29 16:22:50.718: INFO: Pod "busybox-user-65534-0460c39e-38a9-4f08-9b07-8b2107a223aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.178100013s
Aug 29 16:22:50.718: INFO: Pod "busybox-user-65534-0460c39e-38a9-4f08-9b07-8b2107a223aa" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 29 16:22:50.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4838" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":198,"skipped":3412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:22:50.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Aug 29 16:22:51.183: INFO: Waiting up to 5m0s for pod "var-expansion-1fa26427-b56c-4d37-bb52-7a99223b7fc2" in namespace "var-expansion-6280" to be "Succeeded or Failed"
Aug 29 16:22:51.198: INFO: Pod "var-expansion-1fa26427-b56c-4d37-bb52-7a99223b7fc2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.199922ms
Aug 29 16:22:53.211: INFO: Pod "var-expansion-1fa26427-b56c-4d37-bb52-7a99223b7fc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026779113s
Aug 29 16:22:55.225: INFO: Pod "var-expansion-1fa26427-b56c-4d37-bb52-7a99223b7fc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041581511s
STEP: Saw pod success
Aug 29 16:22:55.225: INFO: Pod "var-expansion-1fa26427-b56c-4d37-bb52-7a99223b7fc2" satisfied condition "Succeeded or Failed"
Aug 29 16:22:55.233: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod var-expansion-1fa26427-b56c-4d37-bb52-7a99223b7fc2 container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:22:55.309: INFO: Waiting for pod var-expansion-1fa26427-b56c-4d37-bb52-7a99223b7fc2 to disappear
Aug 29 16:22:55.319: INFO: Pod var-expansion-1fa26427-b56c-4d37-bb52-7a99223b7fc2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 29 16:22:55.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6280" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":199,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:22:55.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Aug 29 16:22:55.400: INFO: PodSpec: initContainers in spec.initContainers
Aug 29 16:23:42.567: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-929e0a2b-e342-4a00-a8c7-0a0e8f423c61", GenerateName:"", Namespace:"init-container-8754", SelfLink:"", UID:"2240fad7-85b1-48fc-a143-e2d63df19857", ResourceVersion:"30489", Generation:0, CreationTimestamp:time.Date(2022, time.August, 29, 16, 22, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"400316090"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"09a8fb4fb868f3c58457a39a98441c512865238e8dc6d6e0a0ca097ef70b61d7", "cni.projectcalico.org/podIP":"172.25.2.225/32", "cni.projectcalico.org/podIPs":"172.25.2.225/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 29, 16, 22, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003be7da0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 29, 16, 22, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003be7dd0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.August, 29, 16, 22, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003be7e00), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-xpqbz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc006b64d00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xpqbz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xpqbz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xpqbz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003eddc30), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-25-142.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00261b1f0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003eddcc0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003eddce0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003eddce8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003eddcec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005c5a4d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 29, 16, 22, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 29, 16, 22, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 29, 16, 22, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.August, 29, 16, 22, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.25.142", PodIP:"172.25.2.225", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.2.225"}}, StartTime:time.Date(2022, time.August, 29, 16, 22, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00261b2d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00261b340)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://1a5062de22f40cb90837be55c24f4518642ff9af8039d552c40aef274ff7679d", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006b64d80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006b64d60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc003eddd9f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 29 16:23:42.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8754" for this suite.

• [SLOW TEST:47.297 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":200,"skipped":3461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:23:42.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4924
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4924
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4924
Aug 29 16:23:42.767: INFO: Found 0 stateful pods, waiting for 1
Aug 29 16:23:52.795: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 29 16:23:52.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-4924 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:23:53.064: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:23:53.064: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:23:53.064: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:23:53.072: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 29 16:24:03.105: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:24:03.105: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:24:03.151: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999961s
Aug 29 16:24:04.161: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990106193s
Aug 29 16:24:05.181: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.981254716s
Aug 29 16:24:06.214: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.961141726s
Aug 29 16:24:07.234: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.928129641s
Aug 29 16:24:08.256: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.908637778s
Aug 29 16:24:09.266: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.885786467s
Aug 29 16:24:10.282: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.876614733s
Aug 29 16:24:11.293: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.859333806s
Aug 29 16:24:12.308: INFO: Verifying statefulset ss doesn't scale past 1 for another 849.372329ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4924
Aug 29 16:24:13.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-4924 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:24:13.569: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:24:13.569: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:24:13.569: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:24:13.584: INFO: Found 1 stateful pods, waiting for 3
Aug 29 16:24:23.614: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:24:23.615: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:24:23.615: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 29 16:24:23.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-4924 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:24:23.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:24:23.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:24:23.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:24:23.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-4924 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:24:24.206: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:24:24.206: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:24:24.206: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:24:24.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-4924 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 16:24:24.454: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 16:24:24.454: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 16:24:24.454: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 16:24:24.454: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:24:24.461: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 29 16:24:34.510: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:24:34.510: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:24:34.510: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 16:24:34.608: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999961s
Aug 29 16:24:35.622: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988343626s
Aug 29 16:24:36.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974089093s
Aug 29 16:24:37.652: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960649585s
Aug 29 16:24:38.892: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.944412715s
Aug 29 16:24:39.908: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.704400932s
Aug 29 16:24:40.921: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.688458127s
Aug 29 16:24:41.937: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.675360094s
Aug 29 16:24:42.950: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.659315038s
Aug 29 16:24:43.967: INFO: Verifying statefulset ss doesn't scale past 3 for another 646.700821ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4924
Aug 29 16:24:44.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-4924 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:24:45.235: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:24:45.235: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:24:45.235: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:24:45.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-4924 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:24:45.509: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:24:45.509: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:24:45.509: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:24:45.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=statefulset-4924 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 16:24:45.797: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 16:24:45.797: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 16:24:45.797: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 16:24:45.797: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 29 16:24:55.878: INFO: Deleting all statefulset in ns statefulset-4924
Aug 29 16:24:55.887: INFO: Scaling statefulset ss to 0
Aug 29 16:24:55.946: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:24:55.952: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 29 16:24:55.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4924" for this suite.

• [SLOW TEST:73.353 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":201,"skipped":3513,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:24:56.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 29 16:24:56.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 29 16:25:12.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:25:15.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:25:30.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1608" for this suite.

• [SLOW TEST:35.540 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":202,"skipped":3520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:25:31.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:25:31.810: INFO: Creating deployment "webserver-deployment"
Aug 29 16:25:31.824: INFO: Waiting for observed generation 1
Aug 29 16:25:33.842: INFO: Waiting for all required pods to come up
Aug 29 16:25:33.854: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 29 16:25:35.884: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 29 16:25:35.896: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 29 16:25:36.470: INFO: Updating deployment webserver-deployment
Aug 29 16:25:36.470: INFO: Waiting for observed generation 2
Aug 29 16:25:38.513: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 29 16:25:38.521: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 29 16:25:38.527: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 29 16:25:38.547: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 29 16:25:38.547: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 29 16:25:38.553: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 29 16:25:38.566: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 29 16:25:38.566: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 29 16:25:38.585: INFO: Updating deployment webserver-deployment
Aug 29 16:25:38.585: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 29 16:25:38.600: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 29 16:25:38.646: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 16:25:38.723: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8515  8154fcc0-4bbb-41be-b4e0-b48c108da0c3 31316 3 2022-08-29 16:25:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-08-29 16:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a866d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2022-08-29 16:25:36 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-29 16:25:38 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 29 16:25:38.756: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-8515  7179a80d-8559-4ee0-aaa3-0dff32ac4b51 31294 3 2022-08-29 16:25:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8154fcc0-4bbb-41be-b4e0-b48c108da0c3 0xc004a5b467 0xc004a5b468}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8154fcc0-4bbb-41be-b4e0-b48c108da0c3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a5b528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:25:38.756: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 29 16:25:38.756: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-8515  a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 31355 3 2022-08-29 16:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8154fcc0-4bbb-41be-b4e0-b48c108da0c3 0xc004a5b347 0xc004a5b348}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8154fcc0-4bbb-41be-b4e0-b48c108da0c3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:25:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a5b3f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:25:38.776: INFO: Pod "webserver-deployment-55df494869-2c5vh" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-2c5vh webserver-deployment-55df494869- deployment-8515  bbdf2194-b408-4d76-bf0d-04f5c8704510 31207 0 2022-08-29 16:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:3a48e000be0e7bfb3a6143556bfe826e212ee9671442705f474c22ef26ab838b cni.projectcalico.org/podIP:172.25.0.126/32 cni.projectcalico.org/podIPs:172.25.0.126/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004a86c67 0xc004a86c68}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:25:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8czbc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8czbc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-197.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.197,PodIP:172.25.0.126,StartTime:2022-08-29 16:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:25:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c5fb62df66a1fdb7b87bf45383aac8200009128e7d319fe33a2efedeebd400a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.776: INFO: Pod "webserver-deployment-55df494869-5xjj5" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-5xjj5 webserver-deployment-55df494869- deployment-8515  7c2a7671-b2a0-4062-b6ae-6141dbf30120 31189 0 2022-08-29 16:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:211b5451a71053fcc80d8d46910cb24588923f7676c177375abea6fdc51da54a cni.projectcalico.org/podIP:172.25.1.216/32 cni.projectcalico.org/podIPs:172.25.1.216/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004a86ee7 0xc004a86ee8}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:25:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:25:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wnhqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wnhqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.214,PodIP:172.25.1.216,StartTime:2022-08-29 16:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:25:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://662389d8248e2962d26edf11a0c524394019b2f0d26f7afd7e52f5eaf3bd0d94,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.776: INFO: Pod "webserver-deployment-55df494869-6t2wq" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-6t2wq webserver-deployment-55df494869- deployment-8515  952e3583-d782-468f-bc9d-db14a835273b 31180 0 2022-08-29 16:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:ead2244b0b3d64ab2ddfb521d557263d5ba99b06d20148b05055dac28d331429 cni.projectcalico.org/podIP:172.25.1.214/32 cni.projectcalico.org/podIPs:172.25.1.214/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004a871b7 0xc004a871b8}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:25:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkstp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkstp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.214,PodIP:172.25.1.214,StartTime:2022-08-29 16:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:25:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8e6f14687a90d3610591d4cb5ee2205666562e982ae507df2f0a8ba5e6973a2c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.777: INFO: Pod "webserver-deployment-55df494869-76nmc" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-76nmc webserver-deployment-55df494869- deployment-8515  f5cacfdd-94ea-4834-bbf3-fd1e3add5fe4 31350 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004a874a7 0xc004a874a8}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mrh6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mrh6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-197.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.777: INFO: Pod "webserver-deployment-55df494869-79mlg" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-79mlg webserver-deployment-55df494869- deployment-8515  50b1a17b-eedc-4917-b808-7d8847a55bee 31191 0 2022-08-29 16:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:719388989de9f93527d46e1b45eea71f6ead104f912842a76c42047a24697d96 cni.projectcalico.org/podIP:172.25.2.229/32 cni.projectcalico.org/podIPs:172.25.2.229/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004a87670 0xc004a87671}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:25:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zpn9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zpn9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:172.25.2.229,StartTime:2022-08-29 16:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:25:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bdd50f109951ebe1e33fe36ff167e73ad6687522be558620e05da4ea395bfb6d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.779: INFO: Pod "webserver-deployment-55df494869-7trwh" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-7trwh webserver-deployment-55df494869- deployment-8515  37fc4209-be77-4e2c-a090-dee3ff7cd674 31204 0 2022-08-29 16:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:58214f667cecdbcf7d8760cd437df776157cb1ea3f73543db4bbce957670b962 cni.projectcalico.org/podIP:172.25.0.125/32 cni.projectcalico.org/podIPs:172.25.0.125/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004a879b7 0xc004a879b8}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:25:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-874z9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-874z9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-197.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.197,PodIP:172.25.0.125,StartTime:2022-08-29 16:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:25:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://003b59d1735ffc87027d5e055ea8272bd109daaecc28172c6dd22bb5b7470853,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.779: INFO: Pod "webserver-deployment-55df494869-8t7qg" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8t7qg webserver-deployment-55df494869- deployment-8515  82eeacef-acf4-4ef0-83e1-6694c9cce245 31353 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004a87c37 0xc004a87c38}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cllbg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cllbg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.779: INFO: Pod "webserver-deployment-55df494869-96qnp" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-96qnp webserver-deployment-55df494869- deployment-8515  ac780473-4516-4e4e-ace7-02072fad8ba0 31326 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004a87df0 0xc004a87df1}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rbx69,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbx69,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.780: INFO: Pod "webserver-deployment-55df494869-9mpg7" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-9mpg7 webserver-deployment-55df494869- deployment-8515  612080f8-8ea3-45f1-86b4-0c37659c1574 31364 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004a87fa0 0xc004a87fa1}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84ptp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84ptp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-197.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.197,PodIP:,StartTime:2022-08-29 16:25:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.781: INFO: Pod "webserver-deployment-55df494869-bwjbg" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-bwjbg webserver-deployment-55df494869- deployment-8515  978497d1-4f35-4c38-83f1-0b6c63f9de53 31347 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aac187 0xc004aac188}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hb7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hb7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.781: INFO: Pod "webserver-deployment-55df494869-czgwq" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-czgwq webserver-deployment-55df494869- deployment-8515  c75e2b24-2536-42b7-9dc4-3f50c9bd351e 31311 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aac320 0xc004aac321}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zjth9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zjth9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.781: INFO: Pod "webserver-deployment-55df494869-dnltx" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-dnltx webserver-deployment-55df494869- deployment-8515  1bf922fe-4ad4-480a-9aea-a2116c9ca29a 31329 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aac480 0xc004aac481}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4456n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4456n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.782: INFO: Pod "webserver-deployment-55df494869-m4zb2" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-m4zb2 webserver-deployment-55df494869- deployment-8515  7ea2fee8-739f-4a78-9a34-894423fef7df 31195 0 2022-08-29 16:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f2862fe8f79067d009988b66e970b69bb95ec4be0f41ecb9d82d81ab15c2941f cni.projectcalico.org/podIP:172.25.2.230/32 cni.projectcalico.org/podIPs:172.25.2.230/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aac630 0xc004aac631}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:25:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lmnv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lmnv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:172.25.2.230,StartTime:2022-08-29 16:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:25:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c2e1dc83619aa20e7dda88f5ad443a08a716fa111f6289d05f6c1e1ecca89125,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.782: INFO: Pod "webserver-deployment-55df494869-m5bpf" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-m5bpf webserver-deployment-55df494869- deployment-8515  4e54f24b-440b-4b7c-bcef-93e260321c8f 31322 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aac877 0xc004aac878}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t2khh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t2khh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-197.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.782: INFO: Pod "webserver-deployment-55df494869-mlhpl" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-mlhpl webserver-deployment-55df494869- deployment-8515  14e7cf5a-6962-459f-8511-2a206d57f2c4 31185 0 2022-08-29 16:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f7620c28b245b6664ae435b23a7d5dabfb9ea0f551cec57f6b008504bd28dad8 cni.projectcalico.org/podIP:172.25.1.213/32 cni.projectcalico.org/podIPs:172.25.1.213/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aac9f0 0xc004aac9f1}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:25:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8lrz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8lrz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.214,PodIP:172.25.1.213,StartTime:2022-08-29 16:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:25:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2faabf03c75e4c27f79311ee64ad8be3d91f90897102e88818bea16e351ea9f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.783: INFO: Pod "webserver-deployment-55df494869-msxtt" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-msxtt webserver-deployment-55df494869- deployment-8515  87b5e234-18da-4be6-99c8-6cda668325db 31346 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aacbf7 0xc004aacbf8}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nwwgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nwwgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-197.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.783: INFO: Pod "webserver-deployment-55df494869-sh6wg" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-sh6wg webserver-deployment-55df494869- deployment-8515  48042994-8269-4d2a-b306-abc952457895 31318 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aacd50 0xc004aacd51}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmjmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmjmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:,StartTime:2022-08-29 16:25:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.783: INFO: Pod "webserver-deployment-55df494869-x56d2" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-x56d2 webserver-deployment-55df494869- deployment-8515  ebe13cc8-5ec5-4633-89e3-61eb93f6f1a4 31352 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aacf07 0xc004aacf08}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gkdw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gkdw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.784: INFO: Pod "webserver-deployment-55df494869-zhzg8" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-zhzg8 webserver-deployment-55df494869- deployment-8515  f8033c01-8694-47a1-9f1c-4fe060ee08ba 31201 0 2022-08-29 16:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:50485fefff9182ed81f6681ef40eee242acf40283b0320ef7c2bdc4ba41484f1 cni.projectcalico.org/podIP:172.25.1.215/32 cni.projectcalico.org/podIPs:172.25.1.215/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aad080 0xc004aad081}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:25:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n82r2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n82r2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.214,PodIP:172.25.1.215,StartTime:2022-08-29 16:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:25:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3535bd35991014cdbe02016b14fea9bbf1aa442897f2cd218a5fa7aef801c187,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.787: INFO: Pod "webserver-deployment-55df494869-ztdcs" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-ztdcs webserver-deployment-55df494869- deployment-8515  c28c365d-2e2b-4d5c-b26d-589d64a651ff 31333 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7 0xc004aad2d7 0xc004aad2d8}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d62d34-6a14-4cb3-a3dd-0962f76a0ed7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hbdwz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hbdwz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.788: INFO: Pod "webserver-deployment-57ccb67bb8-2gh26" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-2gh26 webserver-deployment-57ccb67bb8- deployment-8515  61fe4085-2ccf-472f-ae31-dd07f47dc763 31351 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004aad470 0xc004aad471}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jm7rw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jm7rw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.788: INFO: Pod "webserver-deployment-57ccb67bb8-4qp4r" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-4qp4r webserver-deployment-57ccb67bb8- deployment-8515  219fd728-83ed-4102-b844-a86ed9fcacd2 31339 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004aad610 0xc004aad611}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ccs8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ccs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-197.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.788: INFO: Pod "webserver-deployment-57ccb67bb8-7zt79" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-7zt79 webserver-deployment-57ccb67bb8- deployment-8515  1f340d2f-c230-4435-99d5-912c450f461e 31345 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004aad7d0 0xc004aad7d1}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rxw27,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rxw27,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.789: INFO: Pod "webserver-deployment-57ccb67bb8-8f9ph" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-8f9ph webserver-deployment-57ccb67bb8- deployment-8515  955e41a0-464a-4a08-8f1b-be5ee46b7922 31284 0 2022-08-29 16:25:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:b068a879540a058709c00aa2ea95937a23f42888faa5ebee225bf1fea67a24f1 cni.projectcalico.org/podIP:172.25.2.233/32 cni.projectcalico.org/podIPs:172.25.2.233/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004aad9a0 0xc004aad9a1}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sfmhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sfmhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:,StartTime:2022-08-29 16:25:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.792: INFO: Pod "webserver-deployment-57ccb67bb8-bf9w9" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-bf9w9 webserver-deployment-57ccb67bb8- deployment-8515  42b28a8b-517e-4ee0-9e87-d5ca3693f2ff 31278 0 2022-08-29 16:25:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:900060d9356b96826eb5f4fe31eb3232235a989c6eb8bb9a653f13f58faba0b9 cni.projectcalico.org/podIP:172.25.2.232/32 cni.projectcalico.org/podIPs:172.25.2.232/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004aadc77 0xc004aadc78}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slccn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slccn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:,StartTime:2022-08-29 16:25:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.793: INFO: Pod "webserver-deployment-57ccb67bb8-c52vq" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-c52vq webserver-deployment-57ccb67bb8- deployment-8515  8017200f-4ba0-4c49-a5cc-7c21fb201ebb 31362 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004aaded7 0xc004aaded8}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h4jg6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h4jg6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.793: INFO: Pod "webserver-deployment-57ccb67bb8-d4b8z" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-d4b8z webserver-deployment-57ccb67bb8- deployment-8515  2a3b1ffa-9d20-40df-8b70-0c050434712b 31274 0 2022-08-29 16:25:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:6fdce449c980dbff04638dc7bee140c3d92da88230845e17e3e3326d32f88ad4 cni.projectcalico.org/podIP:172.25.0.127/32 cni.projectcalico.org/podIPs:172.25.0.127/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004ad20c0 0xc004ad20c1}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djlhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djlhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-197.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.197,PodIP:,StartTime:2022-08-29 16:25:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.793: INFO: Pod "webserver-deployment-57ccb67bb8-ddk94" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-ddk94 webserver-deployment-57ccb67bb8- deployment-8515  b2156eb9-cf1f-45a9-b3bd-26557075117c 31337 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004ad23f7 0xc004ad23f8}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ws5t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ws5t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-197.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.197,PodIP:,StartTime:2022-08-29 16:25:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.794: INFO: Pod "webserver-deployment-57ccb67bb8-ftdsn" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-ftdsn webserver-deployment-57ccb67bb8- deployment-8515  86f03661-884f-49ea-9fe0-b88827743731 31348 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004ad2667 0xc004ad2668}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvn74,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvn74,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-197.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.795: INFO: Pod "webserver-deployment-57ccb67bb8-jgrfp" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-jgrfp webserver-deployment-57ccb67bb8- deployment-8515  327ee539-0c54-4065-927d-844b7dbfa2a2 31356 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004ad27f0 0xc004ad27f1}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjggm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjggm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:,StartTime:2022-08-29 16:25:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.796: INFO: Pod "webserver-deployment-57ccb67bb8-kw9gf" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-kw9gf webserver-deployment-57ccb67bb8- deployment-8515  6e6be919-a388-4b90-81b0-21781e7ca201 31359 0 2022-08-29 16:25:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004ad2a57 0xc004ad2a58}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:25:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f47dq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f47dq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.214,PodIP:,StartTime:2022-08-29 16:25:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.796: INFO: Pod "webserver-deployment-57ccb67bb8-nt996" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-nt996 webserver-deployment-57ccb67bb8- deployment-8515  b64ac17a-93e1-4a80-a9fe-f691b2507edb 31280 0 2022-08-29 16:25:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:bc741bee5566fe3a2d3846c9fd991822498d5c77e6c1a9eb16be8367258c62d6 cni.projectcalico.org/podIP:172.25.1.218/32 cni.projectcalico.org/podIPs:172.25.1.218/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004ad2c87 0xc004ad2c88}] []  [{kube-controller-manager Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-08-29 16:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wvhx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wvhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.214,PodIP:,StartTime:2022-08-29 16:25:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:25:38.797: INFO: Pod "webserver-deployment-57ccb67bb8-vf4gx" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-vf4gx webserver-deployment-57ccb67bb8- deployment-8515  0ab2bb0d-2989-44c0-a96a-e0d089132783 31273 0 2022-08-29 16:25:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:a74ccb1e318f1a46bda5474cc2cfb022bd445b761218b7e30d6744a644b0ab0a cni.projectcalico.org/podIP:172.25.1.217/32 cni.projectcalico.org/podIPs:172.25.1.217/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 7179a80d-8559-4ee0-aaa3-0dff32ac4b51 0xc004ad2fd7 0xc004ad2fd8}] []  [{Go-http-client Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7179a80d-8559-4ee0-aaa3-0dff32ac4b51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:25:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zd44g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zd44g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:25:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.214,PodIP:,StartTime:2022-08-29 16:25:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 29 16:25:38.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8515" for this suite.

• [SLOW TEST:7.287 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":203,"skipped":3545,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:25:38.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Aug 29 16:25:38.904: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-2526 proxy --unix-socket=/tmp/kubectl-proxy-unix1806500428/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:25:38.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2526" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":204,"skipped":3546,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:25:38.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Aug 29 16:25:39.081: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:25:41.468: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:25:43.093: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:25:45.110: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:25:47.089: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Aug 29 16:25:47.123: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:25:49.131: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:25:51.134: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Aug 29 16:25:51.172: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 29 16:25:51.189: INFO: Pod pod-with-prestop-http-hook still exists
Aug 29 16:25:53.190: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 29 16:25:53.202: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Aug 29 16:25:53.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7244" for this suite.

• [SLOW TEST:14.264 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":205,"skipped":3549,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:25:53.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Aug 29 16:25:53.329: INFO: The status of Pod annotationupdatef3739818-30a2-428f-a66b-b6ad6e317b4d is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:25:55.339: INFO: The status of Pod annotationupdatef3739818-30a2-428f-a66b-b6ad6e317b4d is Running (Ready = true)
Aug 29 16:25:55.890: INFO: Successfully updated pod "annotationupdatef3739818-30a2-428f-a66b-b6ad6e317b4d"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 16:25:57.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-447" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":206,"skipped":3552,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:25:57.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Aug 29 16:25:58.012: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-1092 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:25:58.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1092" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":207,"skipped":3555,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:25:58.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:25:58.287: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"563b9095-675b-4e33-9cbc-a1d08eb47cb7", Controller:(*bool)(0xc004e197d6), BlockOwnerDeletion:(*bool)(0xc004e197d7)}}
Aug 29 16:25:58.305: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a056baa0-2fd7-43a2-95fc-ce1ad9a7e576", Controller:(*bool)(0xc004e19ad6), BlockOwnerDeletion:(*bool)(0xc004e19ad7)}}
Aug 29 16:25:58.319: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5a7f5dd3-9d8d-40c3-9897-7becd1c7fc88", Controller:(*bool)(0xc004e4687e), BlockOwnerDeletion:(*bool)(0xc004e4687f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Aug 29 16:26:03.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6524" for this suite.

• [SLOW TEST:5.256 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":208,"skipped":3558,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:26:03.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-d6ed9c5d-8486-44f1-8af0-d8275d79de54
STEP: Creating configMap with name cm-test-opt-upd-19f44d6e-d4b4-4a70-8c69-707e08b58d0a
STEP: Creating the pod
Aug 29 16:26:03.504: INFO: The status of Pod pod-configmaps-90f38520-1b9b-4d33-8a8e-86d2b403c99c is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:26:05.523: INFO: The status of Pod pod-configmaps-90f38520-1b9b-4d33-8a8e-86d2b403c99c is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:26:07.520: INFO: The status of Pod pod-configmaps-90f38520-1b9b-4d33-8a8e-86d2b403c99c is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-d6ed9c5d-8486-44f1-8af0-d8275d79de54
STEP: Updating configmap cm-test-opt-upd-19f44d6e-d4b4-4a70-8c69-707e08b58d0a
STEP: Creating configMap with name cm-test-opt-create-f1d71f54-f2f7-48a4-9e79-ab3f7ee19640
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:27:30.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8714" for this suite.

• [SLOW TEST:87.519 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":209,"skipped":3560,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:27:30.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:27:30.997: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6af17e7d-48fe-4aec-a45c-a28f3f1058fc" in namespace "downward-api-6361" to be "Succeeded or Failed"
Aug 29 16:27:31.015: INFO: Pod "downwardapi-volume-6af17e7d-48fe-4aec-a45c-a28f3f1058fc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.559455ms
Aug 29 16:27:33.024: INFO: Pod "downwardapi-volume-6af17e7d-48fe-4aec-a45c-a28f3f1058fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027130264s
Aug 29 16:27:35.033: INFO: Pod "downwardapi-volume-6af17e7d-48fe-4aec-a45c-a28f3f1058fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036461633s
Aug 29 16:27:37.045: INFO: Pod "downwardapi-volume-6af17e7d-48fe-4aec-a45c-a28f3f1058fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047881186s
STEP: Saw pod success
Aug 29 16:27:37.045: INFO: Pod "downwardapi-volume-6af17e7d-48fe-4aec-a45c-a28f3f1058fc" satisfied condition "Succeeded or Failed"
Aug 29 16:27:37.053: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-6af17e7d-48fe-4aec-a45c-a28f3f1058fc container client-container: <nil>
STEP: delete the pod
Aug 29 16:27:37.107: INFO: Waiting for pod downwardapi-volume-6af17e7d-48fe-4aec-a45c-a28f3f1058fc to disappear
Aug 29 16:27:37.113: INFO: Pod downwardapi-volume-6af17e7d-48fe-4aec-a45c-a28f3f1058fc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 16:27:37.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6361" for this suite.

• [SLOW TEST:6.236 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":210,"skipped":3561,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:27:37.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 29 16:27:53.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-143" for this suite.

• [SLOW TEST:16.193 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":211,"skipped":3562,"failed":0}
SSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:27:53.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-881-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 29 16:27:53.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-881" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":212,"skipped":3566,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:27:53.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:27:53.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:27:56.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-237" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":213,"skipped":3575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:27:56.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1720
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1720
STEP: creating replication controller externalsvc in namespace services-1720
I0829 16:27:57.288541      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1720, replica count: 2
I0829 16:28:00.339352      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 29 16:28:00.370: INFO: Creating new exec pod
Aug 29 16:28:04.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-1720 exec execpod9s79m -- /bin/sh -x -c nslookup nodeport-service.services-1720.svc.cluster.local'
Aug 29 16:28:04.782: INFO: stderr: "+ nslookup nodeport-service.services-1720.svc.cluster.local\n"
Aug 29 16:28:04.782: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-1720.svc.cluster.local\tcanonical name = externalsvc.services-1720.svc.cluster.local.\nName:\texternalsvc.services-1720.svc.cluster.local\nAddress: 10.240.21.65\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1720, will wait for the garbage collector to delete the pods
Aug 29 16:28:04.851: INFO: Deleting ReplicationController externalsvc took: 10.650049ms
Aug 29 16:28:04.951: INFO: Terminating ReplicationController externalsvc pods took: 100.369651ms
Aug 29 16:28:07.383: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:28:07.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1720" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.564 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":214,"skipped":3620,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:07.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-c92d5ad0-2e62-4a58-9901-31a32c1f23d6
STEP: Creating a pod to test consume configMaps
Aug 29 16:28:07.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-0834c003-858f-47b0-97ed-55962cabf065" in namespace "configmap-2714" to be "Succeeded or Failed"
Aug 29 16:28:07.565: INFO: Pod "pod-configmaps-0834c003-858f-47b0-97ed-55962cabf065": Phase="Pending", Reason="", readiness=false. Elapsed: 15.545049ms
Aug 29 16:28:09.573: INFO: Pod "pod-configmaps-0834c003-858f-47b0-97ed-55962cabf065": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023870253s
Aug 29 16:28:11.584: INFO: Pod "pod-configmaps-0834c003-858f-47b0-97ed-55962cabf065": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034942768s
STEP: Saw pod success
Aug 29 16:28:11.584: INFO: Pod "pod-configmaps-0834c003-858f-47b0-97ed-55962cabf065" satisfied condition "Succeeded or Failed"
Aug 29 16:28:11.590: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-configmaps-0834c003-858f-47b0-97ed-55962cabf065 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:28:11.624: INFO: Waiting for pod pod-configmaps-0834c003-858f-47b0-97ed-55962cabf065 to disappear
Aug 29 16:28:11.629: INFO: Pod pod-configmaps-0834c003-858f-47b0-97ed-55962cabf065 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:28:11.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2714" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":215,"skipped":3622,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:11.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-081c01b9-a186-4ba3-b501-5300df4d2453
STEP: Creating a pod to test consume configMaps
Aug 29 16:28:11.727: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b18245b-6cdc-43a9-aa7f-4ffa849a9fa9" in namespace "projected-5637" to be "Succeeded or Failed"
Aug 29 16:28:11.738: INFO: Pod "pod-projected-configmaps-4b18245b-6cdc-43a9-aa7f-4ffa849a9fa9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.945592ms
Aug 29 16:28:13.749: INFO: Pod "pod-projected-configmaps-4b18245b-6cdc-43a9-aa7f-4ffa849a9fa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022510778s
Aug 29 16:28:15.758: INFO: Pod "pod-projected-configmaps-4b18245b-6cdc-43a9-aa7f-4ffa849a9fa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031538242s
STEP: Saw pod success
Aug 29 16:28:15.758: INFO: Pod "pod-projected-configmaps-4b18245b-6cdc-43a9-aa7f-4ffa849a9fa9" satisfied condition "Succeeded or Failed"
Aug 29 16:28:15.767: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-projected-configmaps-4b18245b-6cdc-43a9-aa7f-4ffa849a9fa9 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:28:15.800: INFO: Waiting for pod pod-projected-configmaps-4b18245b-6cdc-43a9-aa7f-4ffa849a9fa9 to disappear
Aug 29 16:28:15.806: INFO: Pod pod-projected-configmaps-4b18245b-6cdc-43a9-aa7f-4ffa849a9fa9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 29 16:28:15.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5637" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":216,"skipped":3635,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:15.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-6521
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6521
STEP: Deleting pre-stop pod
Aug 29 16:28:25.010: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Aug 29 16:28:25.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6521" for this suite.

• [SLOW TEST:9.230 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":217,"skipped":3650,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:25.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 29 16:28:35.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8615" for this suite.

• [SLOW TEST:10.150 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":218,"skipped":3663,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:35.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 29 16:28:35.328: INFO: Waiting up to 5m0s for pod "pod-7fb8d49c-8274-40eb-a4d9-7d588830cafe" in namespace "emptydir-205" to be "Succeeded or Failed"
Aug 29 16:28:35.335: INFO: Pod "pod-7fb8d49c-8274-40eb-a4d9-7d588830cafe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.509601ms
Aug 29 16:28:37.349: INFO: Pod "pod-7fb8d49c-8274-40eb-a4d9-7d588830cafe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020723319s
Aug 29 16:28:39.356: INFO: Pod "pod-7fb8d49c-8274-40eb-a4d9-7d588830cafe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02801788s
Aug 29 16:28:41.366: INFO: Pod "pod-7fb8d49c-8274-40eb-a4d9-7d588830cafe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038574169s
STEP: Saw pod success
Aug 29 16:28:41.367: INFO: Pod "pod-7fb8d49c-8274-40eb-a4d9-7d588830cafe" satisfied condition "Succeeded or Failed"
Aug 29 16:28:41.373: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-7fb8d49c-8274-40eb-a4d9-7d588830cafe container test-container: <nil>
STEP: delete the pod
Aug 29 16:28:41.413: INFO: Waiting for pod pod-7fb8d49c-8274-40eb-a4d9-7d588830cafe to disappear
Aug 29 16:28:41.421: INFO: Pod pod-7fb8d49c-8274-40eb-a4d9-7d588830cafe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 16:28:41.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-205" for this suite.

• [SLOW TEST:6.235 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":219,"skipped":3664,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:41.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-ef9b6b5d-3f16-49f0-b092-04325c443caf
STEP: Creating a pod to test consume configMaps
Aug 29 16:28:41.538: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4d9b0f37-fdfc-4095-978b-30c5c9e529d6" in namespace "projected-6287" to be "Succeeded or Failed"
Aug 29 16:28:41.546: INFO: Pod "pod-projected-configmaps-4d9b0f37-fdfc-4095-978b-30c5c9e529d6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.32423ms
Aug 29 16:28:43.556: INFO: Pod "pod-projected-configmaps-4d9b0f37-fdfc-4095-978b-30c5c9e529d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018563517s
Aug 29 16:28:45.569: INFO: Pod "pod-projected-configmaps-4d9b0f37-fdfc-4095-978b-30c5c9e529d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03127638s
STEP: Saw pod success
Aug 29 16:28:45.569: INFO: Pod "pod-projected-configmaps-4d9b0f37-fdfc-4095-978b-30c5c9e529d6" satisfied condition "Succeeded or Failed"
Aug 29 16:28:45.584: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-projected-configmaps-4d9b0f37-fdfc-4095-978b-30c5c9e529d6 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:28:45.626: INFO: Waiting for pod pod-projected-configmaps-4d9b0f37-fdfc-4095-978b-30c5c9e529d6 to disappear
Aug 29 16:28:45.631: INFO: Pod pod-projected-configmaps-4d9b0f37-fdfc-4095-978b-30c5c9e529d6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 29 16:28:45.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6287" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":220,"skipped":3668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:45.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 29 16:28:48.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2805" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":221,"skipped":3707,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:48.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4834.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4834.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4834.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4834.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4834.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4834.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4834.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4834.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:28:50.780: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local from pod dns-4834/dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3: the server could not find the requested resource (get pods dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3)
Aug 29 16:28:50.792: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local from pod dns-4834/dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3: the server could not find the requested resource (get pods dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3)
Aug 29 16:28:50.803: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4834.svc.cluster.local from pod dns-4834/dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3: the server could not find the requested resource (get pods dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3)
Aug 29 16:28:50.811: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4834.svc.cluster.local from pod dns-4834/dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3: the server could not find the requested resource (get pods dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3)
Aug 29 16:28:50.836: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local from pod dns-4834/dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3: the server could not find the requested resource (get pods dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3)
Aug 29 16:28:50.849: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local from pod dns-4834/dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3: the server could not find the requested resource (get pods dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3)
Aug 29 16:28:50.862: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4834.svc.cluster.local from pod dns-4834/dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3: the server could not find the requested resource (get pods dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3)
Aug 29 16:28:50.882: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4834.svc.cluster.local from pod dns-4834/dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3: the server could not find the requested resource (get pods dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3)
Aug 29 16:28:50.883: INFO: Lookups using dns-4834/dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4834.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4834.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4834.svc.cluster.local jessie_udp@dns-test-service-2.dns-4834.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4834.svc.cluster.local]

Aug 29 16:28:55.964: INFO: DNS probes using dns-4834/dns-test-0f8b597e-2e92-4325-a99b-536ff5afacc3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 29 16:28:56.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4834" for this suite.

• [SLOW TEST:7.489 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":222,"skipped":3715,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:56.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:28:56.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:28:57.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1675" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":223,"skipped":3733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:57.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Aug 29 16:28:57.293: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5248  a8ee33c5-2c17-4ad8-ae7c-cbcc7fea9ffe 33016 0 2022-08-29 16:28:57 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-08-29 16:28:57 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fpbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fpbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:28:57.326: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:28:59.333: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Aug 29 16:28:59.333: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5248 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:28:59.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:28:59.334: INFO: ExecWithOptions: Clientset creation
Aug 29 16:28:59.334: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-5248/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Aug 29 16:28:59.490: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5248 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:28:59.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:28:59.491: INFO: ExecWithOptions: Clientset creation
Aug 29 16:28:59.491: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-5248/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 16:28:59.638: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 29 16:28:59.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5248" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":224,"skipped":3755,"failed":0}

------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:28:59.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Aug 29 16:28:59.715: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 29 16:28:59.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 create -f -'
Aug 29 16:29:00.861: INFO: stderr: ""
Aug 29 16:29:00.861: INFO: stdout: "service/agnhost-replica created\n"
Aug 29 16:29:00.861: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 29 16:29:00.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 create -f -'
Aug 29 16:29:02.532: INFO: stderr: ""
Aug 29 16:29:02.532: INFO: stdout: "service/agnhost-primary created\n"
Aug 29 16:29:02.533: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 29 16:29:02.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 create -f -'
Aug 29 16:29:02.827: INFO: stderr: ""
Aug 29 16:29:02.827: INFO: stdout: "service/frontend created\n"
Aug 29 16:29:02.827: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 29 16:29:02.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 create -f -'
Aug 29 16:29:03.104: INFO: stderr: ""
Aug 29 16:29:03.104: INFO: stdout: "deployment.apps/frontend created\n"
Aug 29 16:29:03.104: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 29 16:29:03.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 create -f -'
Aug 29 16:29:03.396: INFO: stderr: ""
Aug 29 16:29:03.396: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 29 16:29:03.396: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 29 16:29:03.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 create -f -'
Aug 29 16:29:03.628: INFO: stderr: ""
Aug 29 16:29:03.629: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Aug 29 16:29:03.629: INFO: Waiting for all frontend pods to be Running.
Aug 29 16:29:08.680: INFO: Waiting for frontend to serve content.
Aug 29 16:29:08.703: INFO: Trying to add a new entry to the guestbook.
Aug 29 16:29:08.727: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 29 16:29:08.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 delete --grace-period=0 --force -f -'
Aug 29 16:29:08.872: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:29:08.872: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Aug 29 16:29:08.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 delete --grace-period=0 --force -f -'
Aug 29 16:29:09.009: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:29:09.010: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 29 16:29:09.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 delete --grace-period=0 --force -f -'
Aug 29 16:29:09.157: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:29:09.157: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 29 16:29:09.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 delete --grace-period=0 --force -f -'
Aug 29 16:29:09.278: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:29:09.278: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 29 16:29:09.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 delete --grace-period=0 --force -f -'
Aug 29 16:29:09.436: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:29:09.436: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 29 16:29:09.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9437 delete --grace-period=0 --force -f -'
Aug 29 16:29:09.558: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:29:09.558: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:29:09.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9437" for this suite.

• [SLOW TEST:9.918 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":225,"skipped":3755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:09.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 29 16:29:13.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1415" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":226,"skipped":3784,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:13.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 29 16:29:13.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9970" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":227,"skipped":3800,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:13.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Aug 29 16:29:13.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6292 create -f -'
Aug 29 16:29:13.787: INFO: stderr: ""
Aug 29 16:29:13.787: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Aug 29 16:29:13.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6292 diff -f -'
Aug 29 16:29:14.030: INFO: rc: 1
Aug 29 16:29:14.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-6292 delete -f -'
Aug 29 16:29:14.126: INFO: stderr: ""
Aug 29 16:29:14.126: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:29:14.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6292" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":228,"skipped":3806,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:14.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:29:14.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3803b2a0-61b9-4946-a533-70014b668c26" in namespace "projected-1655" to be "Succeeded or Failed"
Aug 29 16:29:14.218: INFO: Pod "downwardapi-volume-3803b2a0-61b9-4946-a533-70014b668c26": Phase="Pending", Reason="", readiness=false. Elapsed: 10.185842ms
Aug 29 16:29:16.229: INFO: Pod "downwardapi-volume-3803b2a0-61b9-4946-a533-70014b668c26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020933099s
Aug 29 16:29:18.242: INFO: Pod "downwardapi-volume-3803b2a0-61b9-4946-a533-70014b668c26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034445543s
STEP: Saw pod success
Aug 29 16:29:18.243: INFO: Pod "downwardapi-volume-3803b2a0-61b9-4946-a533-70014b668c26" satisfied condition "Succeeded or Failed"
Aug 29 16:29:18.250: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-3803b2a0-61b9-4946-a533-70014b668c26 container client-container: <nil>
STEP: delete the pod
Aug 29 16:29:18.290: INFO: Waiting for pod downwardapi-volume-3803b2a0-61b9-4946-a533-70014b668c26 to disappear
Aug 29 16:29:18.303: INFO: Pod downwardapi-volume-3803b2a0-61b9-4946-a533-70014b668c26 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 16:29:18.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1655" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":229,"skipped":3813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:18.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Aug 29 16:29:18.447: INFO: Found Service test-service-9lnvk in namespace services-9107 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 29 16:29:18.447: INFO: Service test-service-9lnvk created
STEP: Getting /status
Aug 29 16:29:18.464: INFO: Service test-service-9lnvk has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Aug 29 16:29:18.486: INFO: observed Service test-service-9lnvk in namespace services-9107 with annotations: map[] & LoadBalancer: {[]}
Aug 29 16:29:18.486: INFO: Found Service test-service-9lnvk in namespace services-9107 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 29 16:29:18.486: INFO: Service test-service-9lnvk has service status patched
STEP: updating the ServiceStatus
Aug 29 16:29:18.527: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Aug 29 16:29:18.538: INFO: Observed Service test-service-9lnvk in namespace services-9107 with annotations: map[] & Conditions: {[]}
Aug 29 16:29:18.538: INFO: Observed event: &Service{ObjectMeta:{test-service-9lnvk  services-9107  5c9deadd-10e6-439b-b1c1-ffe766370853 33465 0 2022-08-29 16:29:18 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-08-29 16:29:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-08-29 16:29:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.240.18.78,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.240.18.78],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 29 16:29:18.538: INFO: Found Service test-service-9lnvk in namespace services-9107 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 16:29:18.538: INFO: Service test-service-9lnvk has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Aug 29 16:29:18.569: INFO: observed Service test-service-9lnvk in namespace services-9107 with labels: map[test-service-static:true]
Aug 29 16:29:18.569: INFO: observed Service test-service-9lnvk in namespace services-9107 with labels: map[test-service-static:true]
Aug 29 16:29:18.569: INFO: observed Service test-service-9lnvk in namespace services-9107 with labels: map[test-service-static:true]
Aug 29 16:29:18.569: INFO: Found Service test-service-9lnvk in namespace services-9107 with labels: map[test-service:patched test-service-static:true]
Aug 29 16:29:18.569: INFO: Service test-service-9lnvk patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Aug 29 16:29:18.609: INFO: Observed event: ADDED
Aug 29 16:29:18.609: INFO: Observed event: MODIFIED
Aug 29 16:29:18.609: INFO: Observed event: MODIFIED
Aug 29 16:29:18.618: INFO: Observed event: MODIFIED
Aug 29 16:29:18.619: INFO: Found Service test-service-9lnvk in namespace services-9107 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 29 16:29:18.619: INFO: Service test-service-9lnvk deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:29:18.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9107" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":230,"skipped":3896,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:18.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-8177/configmap-test-08245227-827e-4635-9842-782b3aabdb82
STEP: Creating a pod to test consume configMaps
Aug 29 16:29:18.748: INFO: Waiting up to 5m0s for pod "pod-configmaps-cce2f4a0-1d2c-4655-a2ed-8b04122439df" in namespace "configmap-8177" to be "Succeeded or Failed"
Aug 29 16:29:18.764: INFO: Pod "pod-configmaps-cce2f4a0-1d2c-4655-a2ed-8b04122439df": Phase="Pending", Reason="", readiness=false. Elapsed: 15.420984ms
Aug 29 16:29:20.775: INFO: Pod "pod-configmaps-cce2f4a0-1d2c-4655-a2ed-8b04122439df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026632691s
Aug 29 16:29:22.785: INFO: Pod "pod-configmaps-cce2f4a0-1d2c-4655-a2ed-8b04122439df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037067578s
STEP: Saw pod success
Aug 29 16:29:22.786: INFO: Pod "pod-configmaps-cce2f4a0-1d2c-4655-a2ed-8b04122439df" satisfied condition "Succeeded or Failed"
Aug 29 16:29:22.790: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-configmaps-cce2f4a0-1d2c-4655-a2ed-8b04122439df container env-test: <nil>
STEP: delete the pod
Aug 29 16:29:22.823: INFO: Waiting for pod pod-configmaps-cce2f4a0-1d2c-4655-a2ed-8b04122439df to disappear
Aug 29 16:29:22.831: INFO: Pod pod-configmaps-cce2f4a0-1d2c-4655-a2ed-8b04122439df no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:29:22.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8177" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":231,"skipped":3907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:22.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Aug 29 16:29:24.951: INFO: running pods: 0 < 1
Aug 29 16:29:26.959: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 29 16:29:29.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8462" for this suite.

• [SLOW TEST:6.175 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":232,"skipped":3937,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:29.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 29 16:29:29.116: INFO: Waiting up to 5m0s for pod "pod-5f1b724a-3f76-47aa-b2a6-1584c3386d36" in namespace "emptydir-2322" to be "Succeeded or Failed"
Aug 29 16:29:29.124: INFO: Pod "pod-5f1b724a-3f76-47aa-b2a6-1584c3386d36": Phase="Pending", Reason="", readiness=false. Elapsed: 7.56913ms
Aug 29 16:29:31.137: INFO: Pod "pod-5f1b724a-3f76-47aa-b2a6-1584c3386d36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020170036s
Aug 29 16:29:33.149: INFO: Pod "pod-5f1b724a-3f76-47aa-b2a6-1584c3386d36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032781034s
STEP: Saw pod success
Aug 29 16:29:33.150: INFO: Pod "pod-5f1b724a-3f76-47aa-b2a6-1584c3386d36" satisfied condition "Succeeded or Failed"
Aug 29 16:29:33.156: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-5f1b724a-3f76-47aa-b2a6-1584c3386d36 container test-container: <nil>
STEP: delete the pod
Aug 29 16:29:33.190: INFO: Waiting for pod pod-5f1b724a-3f76-47aa-b2a6-1584c3386d36 to disappear
Aug 29 16:29:33.196: INFO: Pod pod-5f1b724a-3f76-47aa-b2a6-1584c3386d36 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 16:29:33.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2322" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":233,"skipped":3989,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:33.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 29 16:29:35.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9704" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":234,"skipped":3996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:35.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:29:35.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c14eceb-cb6e-484a-9f2d-b146f7589ae4" in namespace "projected-619" to be "Succeeded or Failed"
Aug 29 16:29:35.462: INFO: Pod "downwardapi-volume-8c14eceb-cb6e-484a-9f2d-b146f7589ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.473727ms
Aug 29 16:29:37.473: INFO: Pod "downwardapi-volume-8c14eceb-cb6e-484a-9f2d-b146f7589ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021916695s
Aug 29 16:29:39.483: INFO: Pod "downwardapi-volume-8c14eceb-cb6e-484a-9f2d-b146f7589ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031729622s
Aug 29 16:29:41.490: INFO: Pod "downwardapi-volume-8c14eceb-cb6e-484a-9f2d-b146f7589ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039556256s
STEP: Saw pod success
Aug 29 16:29:41.491: INFO: Pod "downwardapi-volume-8c14eceb-cb6e-484a-9f2d-b146f7589ae4" satisfied condition "Succeeded or Failed"
Aug 29 16:29:41.499: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod downwardapi-volume-8c14eceb-cb6e-484a-9f2d-b146f7589ae4 container client-container: <nil>
STEP: delete the pod
Aug 29 16:29:41.535: INFO: Waiting for pod downwardapi-volume-8c14eceb-cb6e-484a-9f2d-b146f7589ae4 to disappear
Aug 29 16:29:41.540: INFO: Pod downwardapi-volume-8c14eceb-cb6e-484a-9f2d-b146f7589ae4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 16:29:41.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-619" for this suite.

• [SLOW TEST:6.182 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":235,"skipped":4038,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:41.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 29 16:29:41.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9441" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":236,"skipped":4048,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:41.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:29:41.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Aug 29 16:29:45.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-8173 --namespace=crd-publish-openapi-8173 create -f -'
Aug 29 16:29:45.917: INFO: stderr: ""
Aug 29 16:29:45.917: INFO: stdout: "e2e-test-crd-publish-openapi-8073-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 29 16:29:45.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-8173 --namespace=crd-publish-openapi-8173 delete e2e-test-crd-publish-openapi-8073-crds test-cr'
Aug 29 16:29:46.056: INFO: stderr: ""
Aug 29 16:29:46.056: INFO: stdout: "e2e-test-crd-publish-openapi-8073-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 29 16:29:46.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-8173 --namespace=crd-publish-openapi-8173 apply -f -'
Aug 29 16:29:46.716: INFO: stderr: ""
Aug 29 16:29:46.716: INFO: stdout: "e2e-test-crd-publish-openapi-8073-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 29 16:29:46.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-8173 --namespace=crd-publish-openapi-8173 delete e2e-test-crd-publish-openapi-8073-crds test-cr'
Aug 29 16:29:46.830: INFO: stderr: ""
Aug 29 16:29:46.830: INFO: stdout: "e2e-test-crd-publish-openapi-8073-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 29 16:29:46.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-8173 explain e2e-test-crd-publish-openapi-8073-crds'
Aug 29 16:29:47.087: INFO: stderr: ""
Aug 29 16:29:47.087: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8073-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:29:52.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8173" for this suite.

• [SLOW TEST:10.500 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":237,"skipped":4050,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:29:52.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Aug 29 16:30:12.536: INFO: EndpointSlice for Service endpointslice-419/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Aug 29 16:30:22.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-419" for this suite.

• [SLOW TEST:30.373 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":238,"skipped":4066,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:30:22.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:30:22.747: INFO: Create a RollingUpdate DaemonSet
Aug 29 16:30:22.763: INFO: Check that daemon pods launch on every node of the cluster
Aug 29 16:30:22.792: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:30:22.792: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:30:23.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:30:23.807: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:30:24.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 16:30:24.812: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Aug 29 16:30:24.812: INFO: Update the DaemonSet to trigger a rollout
Aug 29 16:30:24.843: INFO: Updating DaemonSet daemon-set
Aug 29 16:30:27.913: INFO: Roll back the DaemonSet before rollout is complete
Aug 29 16:30:27.936: INFO: Updating DaemonSet daemon-set
Aug 29 16:30:27.936: INFO: Make sure DaemonSet rollback is complete
Aug 29 16:30:27.951: INFO: Wrong image for pod: daemon-set-gxf9h. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Aug 29 16:30:27.951: INFO: Pod daemon-set-gxf9h is not available
Aug 29 16:30:32.014: INFO: Pod daemon-set-wlt8k is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2113, will wait for the garbage collector to delete the pods
Aug 29 16:30:32.293: INFO: Deleting DaemonSet.extensions daemon-set took: 9.871186ms
Aug 29 16:30:32.394: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.944667ms
Aug 29 16:30:35.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:30:35.103: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 16:30:35.108: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34155"},"items":null}

Aug 29 16:30:35.112: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34155"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 29 16:30:35.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2113" for this suite.

• [SLOW TEST:12.565 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":239,"skipped":4078,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:30:35.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:30:35.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30158122-c733-49d5-a1b2-5fba8edb73f5" in namespace "projected-6064" to be "Succeeded or Failed"
Aug 29 16:30:35.499: INFO: Pod "downwardapi-volume-30158122-c733-49d5-a1b2-5fba8edb73f5": Phase="Pending", Reason="", readiness=false. Elapsed: 253.322508ms
Aug 29 16:30:37.516: INFO: Pod "downwardapi-volume-30158122-c733-49d5-a1b2-5fba8edb73f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.269419574s
Aug 29 16:30:39.522: INFO: Pod "downwardapi-volume-30158122-c733-49d5-a1b2-5fba8edb73f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276227462s
Aug 29 16:30:41.531: INFO: Pod "downwardapi-volume-30158122-c733-49d5-a1b2-5fba8edb73f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.284502026s
STEP: Saw pod success
Aug 29 16:30:41.531: INFO: Pod "downwardapi-volume-30158122-c733-49d5-a1b2-5fba8edb73f5" satisfied condition "Succeeded or Failed"
Aug 29 16:30:41.535: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod downwardapi-volume-30158122-c733-49d5-a1b2-5fba8edb73f5 container client-container: <nil>
STEP: delete the pod
Aug 29 16:30:41.564: INFO: Waiting for pod downwardapi-volume-30158122-c733-49d5-a1b2-5fba8edb73f5 to disappear
Aug 29 16:30:41.577: INFO: Pod downwardapi-volume-30158122-c733-49d5-a1b2-5fba8edb73f5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 16:30:41.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6064" for this suite.

• [SLOW TEST:6.436 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":240,"skipped":4087,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:30:41.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-871ca1a3-700b-4d17-a15c-f144518f9e60
STEP: Creating a pod to test consume configMaps
Aug 29 16:30:41.658: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bc74a40a-a517-40c6-8655-9c4238b8b54c" in namespace "projected-3705" to be "Succeeded or Failed"
Aug 29 16:30:41.666: INFO: Pod "pod-projected-configmaps-bc74a40a-a517-40c6-8655-9c4238b8b54c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.805101ms
Aug 29 16:30:43.681: INFO: Pod "pod-projected-configmaps-bc74a40a-a517-40c6-8655-9c4238b8b54c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022304409s
Aug 29 16:30:45.690: INFO: Pod "pod-projected-configmaps-bc74a40a-a517-40c6-8655-9c4238b8b54c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031503966s
STEP: Saw pod success
Aug 29 16:30:45.690: INFO: Pod "pod-projected-configmaps-bc74a40a-a517-40c6-8655-9c4238b8b54c" satisfied condition "Succeeded or Failed"
Aug 29 16:30:45.697: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-projected-configmaps-bc74a40a-a517-40c6-8655-9c4238b8b54c container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:30:46.433: INFO: Waiting for pod pod-projected-configmaps-bc74a40a-a517-40c6-8655-9c4238b8b54c to disappear
Aug 29 16:30:46.440: INFO: Pod pod-projected-configmaps-bc74a40a-a517-40c6-8655-9c4238b8b54c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 29 16:30:46.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3705" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":241,"skipped":4090,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:30:46.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:30:46.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1108" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":242,"skipped":4099,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:30:46.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1542
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1542
I0829 16:30:46.944786      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1542, replica count: 2
I0829 16:30:49.996358      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:30:49.996: INFO: Creating new exec pod
Aug 29 16:30:53.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-1542 exec execpod9zgwx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 16:30:53.388: INFO: stderr: "+ echo+ nc -v -t -w 2 externalname-service 80\n hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 16:30:53.388: INFO: stdout: ""
Aug 29 16:30:54.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-1542 exec execpod9zgwx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 29 16:30:54.722: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 16:30:54.722: INFO: stdout: "externalname-service-8n4lr"
Aug 29 16:30:54.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-1542 exec execpod9zgwx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.29.159 80'
Aug 29 16:30:55.023: INFO: stderr: "+ nc -v -t -w 2 10.240.29.159 80\n+ echo hostName\nConnection to 10.240.29.159 80 port [tcp/http] succeeded!\n"
Aug 29 16:30:55.023: INFO: stdout: "externalname-service-stgqr"
Aug 29 16:30:55.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-1542 exec execpod9zgwx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.26.197 30666'
Aug 29 16:30:55.276: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.26.197 30666\nConnection to 172.31.26.197 30666 port [tcp/*] succeeded!\n"
Aug 29 16:30:55.276: INFO: stdout: "externalname-service-stgqr"
Aug 29 16:30:55.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-1542 exec execpod9zgwx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.214 30666'
Aug 29 16:30:55.538: INFO: stderr: "+ nc -v -t -w 2 172.31.16.214 30666\n+ echo hostName\nConnection to 172.31.16.214 30666 port [tcp/*] succeeded!\n"
Aug 29 16:30:55.538: INFO: stdout: "externalname-service-stgqr"
Aug 29 16:30:55.538: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:30:55.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1542" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.149 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":243,"skipped":4114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:30:55.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8443
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Aug 29 16:30:55.914: INFO: Found 0 stateful pods, waiting for 3
Aug 29 16:31:05.931: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:31:05.931: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:31:05.931: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Aug 29 16:31:05.978: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 29 16:31:16.345: INFO: Updating stateful set ss2
Aug 29 16:31:16.710: INFO: Waiting for Pod statefulset-8443/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Aug 29 16:31:26.800: INFO: Found 2 stateful pods, waiting for 3
Aug 29 16:31:37.147: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:31:37.147: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 16:31:37.147: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 29 16:31:37.200: INFO: Updating stateful set ss2
Aug 29 16:31:37.219: INFO: Waiting for Pod statefulset-8443/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Aug 29 16:31:47.839: INFO: Updating stateful set ss2
Aug 29 16:31:47.872: INFO: Waiting for StatefulSet statefulset-8443/ss2 to complete update
Aug 29 16:31:47.872: INFO: Waiting for Pod statefulset-8443/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 29 16:31:57.890: INFO: Deleting all statefulset in ns statefulset-8443
Aug 29 16:31:57.897: INFO: Scaling statefulset ss2 to 0
Aug 29 16:32:08.239: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:32:08.250: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 29 16:32:08.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8443" for this suite.

• [SLOW TEST:72.468 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":244,"skipped":4161,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:08.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Aug 29 16:32:08.370: INFO: Creating e2e-svc-a-k8kln
Aug 29 16:32:08.399: INFO: Creating e2e-svc-b-7x7fq
Aug 29 16:32:08.428: INFO: Creating e2e-svc-c-79fxw
STEP: deleting service collection
Aug 29 16:32:08.501: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:32:08.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8388" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":245,"skipped":4167,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:08.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Aug 29 16:32:08.655: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:32:08.657: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:32:08.657: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:32:08.657: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:32:08.689: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:32:08.689: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:32:08.729: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:32:08.729: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 16:32:09.965: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 29 16:32:09.965: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 29 16:32:10.628: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Aug 29 16:32:10.680: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Aug 29 16:32:10.687: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0
Aug 29 16:32:10.687: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0
Aug 29 16:32:10.687: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0
Aug 29 16:32:10.687: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0
Aug 29 16:32:10.688: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0
Aug 29 16:32:10.688: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0
Aug 29 16:32:10.688: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0
Aug 29 16:32:10.688: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 0
Aug 29 16:32:10.688: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:10.688: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:10.688: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:10.689: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:10.689: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:10.689: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:10.689: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:10.689: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:10.721: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:10.721: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:10.762: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:10.762: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:10.784: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:10.784: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:11.953: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:11.953: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:11.990: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
STEP: listing Deployments
Aug 29 16:32:12.004: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Aug 29 16:32:12.028: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Aug 29 16:32:12.051: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:32:12.052: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:32:12.076: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:32:12.102: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:32:12.127: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:32:14.504: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:32:14.543: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:32:14.554: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:32:14.563: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:32:14.581: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 16:32:15.996: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Aug 29 16:32:16.079: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:16.079: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:16.080: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:16.081: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:16.081: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 1
Aug 29 16:32:16.081: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:16.081: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:16.082: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:16.082: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:16.083: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 2
Aug 29 16:32:16.083: INFO: observed Deployment test-deployment in namespace deployment-7324 with ReadyReplicas 3
STEP: deleting the Deployment
Aug 29 16:32:16.102: INFO: observed event type MODIFIED
Aug 29 16:32:16.102: INFO: observed event type MODIFIED
Aug 29 16:32:16.102: INFO: observed event type MODIFIED
Aug 29 16:32:16.103: INFO: observed event type MODIFIED
Aug 29 16:32:16.103: INFO: observed event type MODIFIED
Aug 29 16:32:16.103: INFO: observed event type MODIFIED
Aug 29 16:32:16.103: INFO: observed event type MODIFIED
Aug 29 16:32:16.103: INFO: observed event type MODIFIED
Aug 29 16:32:16.103: INFO: observed event type MODIFIED
Aug 29 16:32:16.104: INFO: observed event type MODIFIED
Aug 29 16:32:16.104: INFO: observed event type MODIFIED
Aug 29 16:32:16.104: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 16:32:16.114: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 29 16:32:16.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7324" for this suite.

• [SLOW TEST:7.605 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":246,"skipped":4172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:16.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Aug 29 16:32:16.713: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Aug 29 16:32:16.794: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 29 16:32:16.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3390" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":247,"skipped":4241,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:16.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Aug 29 16:32:17.026: INFO: Waiting up to 5m0s for pod "security-context-6ee7b8e0-4c65-4914-8fb6-76305d1695eb" in namespace "security-context-1077" to be "Succeeded or Failed"
Aug 29 16:32:17.039: INFO: Pod "security-context-6ee7b8e0-4c65-4914-8fb6-76305d1695eb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.690206ms
Aug 29 16:32:19.050: INFO: Pod "security-context-6ee7b8e0-4c65-4914-8fb6-76305d1695eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024004496s
Aug 29 16:32:21.063: INFO: Pod "security-context-6ee7b8e0-4c65-4914-8fb6-76305d1695eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036968537s
STEP: Saw pod success
Aug 29 16:32:21.063: INFO: Pod "security-context-6ee7b8e0-4c65-4914-8fb6-76305d1695eb" satisfied condition "Succeeded or Failed"
Aug 29 16:32:21.069: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod security-context-6ee7b8e0-4c65-4914-8fb6-76305d1695eb container test-container: <nil>
STEP: delete the pod
Aug 29 16:32:21.119: INFO: Waiting for pod security-context-6ee7b8e0-4c65-4914-8fb6-76305d1695eb to disappear
Aug 29 16:32:21.125: INFO: Pod security-context-6ee7b8e0-4c65-4914-8fb6-76305d1695eb no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 29 16:32:21.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1077" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":248,"skipped":4248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:21.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Aug 29 16:32:21.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1593" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":249,"skipped":4270,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:21.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-d5642d82-f3e4-4f0e-9570-6b31e3afc572
STEP: Creating a pod to test consume secrets
Aug 29 16:32:21.755: INFO: Waiting up to 5m0s for pod "pod-secrets-d1b515fb-1856-4290-bf5b-255f605e0af3" in namespace "secrets-9721" to be "Succeeded or Failed"
Aug 29 16:32:21.768: INFO: Pod "pod-secrets-d1b515fb-1856-4290-bf5b-255f605e0af3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.01535ms
Aug 29 16:32:23.792: INFO: Pod "pod-secrets-d1b515fb-1856-4290-bf5b-255f605e0af3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037657705s
Aug 29 16:32:25.804: INFO: Pod "pod-secrets-d1b515fb-1856-4290-bf5b-255f605e0af3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048955829s
STEP: Saw pod success
Aug 29 16:32:25.804: INFO: Pod "pod-secrets-d1b515fb-1856-4290-bf5b-255f605e0af3" satisfied condition "Succeeded or Failed"
Aug 29 16:32:25.810: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-secrets-d1b515fb-1856-4290-bf5b-255f605e0af3 container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:32:25.853: INFO: Waiting for pod pod-secrets-d1b515fb-1856-4290-bf5b-255f605e0af3 to disappear
Aug 29 16:32:25.863: INFO: Pod pod-secrets-d1b515fb-1856-4290-bf5b-255f605e0af3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 29 16:32:25.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9721" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":250,"skipped":4271,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:25.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Aug 29 16:32:25.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:32:50.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3506" for this suite.

• [SLOW TEST:25.229 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":251,"skipped":4277,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:51.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-f3c31ee6-dda5-4709-9492-3d8769425f8c
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:32:51.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2150" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":252,"skipped":4280,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:51.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 29 16:32:51.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4461" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":253,"skipped":4297,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:51.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Aug 29 16:32:51.425: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Aug 29 16:32:51.439: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 29 16:32:51.439: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Aug 29 16:32:51.463: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 29 16:32:51.463: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Aug 29 16:32:51.501: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 29 16:32:51.501: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Aug 29 16:32:58.847: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Aug 29 16:32:58.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-471" for this suite.

• [SLOW TEST:7.796 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":254,"skipped":4309,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:32:59.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Aug 29 16:32:59.208: INFO: Creating simple deployment test-deployment-fj7gc
Aug 29 16:32:59.239: INFO: deployment "test-deployment-fj7gc" doesn't have the required revision set
STEP: Getting /status
Aug 29 16:33:01.282: INFO: Deployment test-deployment-fj7gc has Conditions: [{Available True 2022-08-29 16:33:00 +0000 UTC 2022-08-29 16:33:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-08-29 16:33:00 +0000 UTC 2022-08-29 16:32:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fj7gc-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Aug 29 16:33:01.302: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 33, 0, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 33, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 33, 0, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 32, 59, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-fj7gc-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Aug 29 16:33:01.311: INFO: Observed &Deployment event: ADDED
Aug 29 16:33:01.311: INFO: Observed Deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:32:59 +0000 UTC 2022-08-29 16:32:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fj7gc-688c4d6789"}
Aug 29 16:33:01.312: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:33:01.312: INFO: Observed Deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:32:59 +0000 UTC 2022-08-29 16:32:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fj7gc-688c4d6789"}
Aug 29 16:33:01.312: INFO: Observed Deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-29 16:32:59 +0000 UTC 2022-08-29 16:32:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 16:33:01.313: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:33:01.313: INFO: Observed Deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-29 16:32:59 +0000 UTC 2022-08-29 16:32:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 16:33:01.313: INFO: Observed Deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:32:59 +0000 UTC 2022-08-29 16:32:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-fj7gc-688c4d6789" is progressing.}
Aug 29 16:33:01.314: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:33:01.314: INFO: Observed Deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-29 16:33:00 +0000 UTC 2022-08-29 16:33:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 16:33:01.314: INFO: Observed Deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:33:00 +0000 UTC 2022-08-29 16:32:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fj7gc-688c4d6789" has successfully progressed.}
Aug 29 16:33:01.314: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:33:01.314: INFO: Observed Deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-29 16:33:00 +0000 UTC 2022-08-29 16:33:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 16:33:01.314: INFO: Observed Deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:33:00 +0000 UTC 2022-08-29 16:32:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fj7gc-688c4d6789" has successfully progressed.}
Aug 29 16:33:01.314: INFO: Found Deployment test-deployment-fj7gc in namespace deployment-5609 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 16:33:01.314: INFO: Deployment test-deployment-fj7gc has an updated status
STEP: patching the Statefulset Status
Aug 29 16:33:01.314: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 29 16:33:01.334: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Aug 29 16:33:01.340: INFO: Observed &Deployment event: ADDED
Aug 29 16:33:01.340: INFO: Observed deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:32:59 +0000 UTC 2022-08-29 16:32:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fj7gc-688c4d6789"}
Aug 29 16:33:01.341: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:33:01.341: INFO: Observed deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:32:59 +0000 UTC 2022-08-29 16:32:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fj7gc-688c4d6789"}
Aug 29 16:33:01.341: INFO: Observed deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-29 16:32:59 +0000 UTC 2022-08-29 16:32:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 16:33:01.341: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:33:01.343: INFO: Observed deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-08-29 16:32:59 +0000 UTC 2022-08-29 16:32:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 16:33:01.343: INFO: Observed deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:32:59 +0000 UTC 2022-08-29 16:32:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-fj7gc-688c4d6789" is progressing.}
Aug 29 16:33:01.344: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:33:01.344: INFO: Observed deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-29 16:33:00 +0000 UTC 2022-08-29 16:33:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 16:33:01.344: INFO: Observed deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:33:00 +0000 UTC 2022-08-29 16:32:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fj7gc-688c4d6789" has successfully progressed.}
Aug 29 16:33:01.345: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:33:01.345: INFO: Observed deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-08-29 16:33:00 +0000 UTC 2022-08-29 16:33:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 16:33:01.345: INFO: Observed deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-08-29 16:33:00 +0000 UTC 2022-08-29 16:32:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fj7gc-688c4d6789" has successfully progressed.}
Aug 29 16:33:01.345: INFO: Observed deployment test-deployment-fj7gc in namespace deployment-5609 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 16:33:01.346: INFO: Observed &Deployment event: MODIFIED
Aug 29 16:33:01.346: INFO: Found deployment test-deployment-fj7gc in namespace deployment-5609 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 29 16:33:01.346: INFO: Deployment test-deployment-fj7gc has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 16:33:01.354: INFO: Deployment "test-deployment-fj7gc":
&Deployment{ObjectMeta:{test-deployment-fj7gc  deployment-5609  7ecce66e-298a-4972-b167-82ac7140e471 35574 1 2022-08-29 16:32:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-08-29 16:32:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-08-29 16:33:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-08-29 16:33:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004254678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-fj7gc-688c4d6789",LastUpdateTime:2022-08-29 16:33:01 +0000 UTC,LastTransitionTime:2022-08-29 16:33:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 16:33:01.367: INFO: New ReplicaSet "test-deployment-fj7gc-688c4d6789" of Deployment "test-deployment-fj7gc":
&ReplicaSet{ObjectMeta:{test-deployment-fj7gc-688c4d6789  deployment-5609  8e499fcd-4923-4ffc-8d2b-66d4906e2f73 35569 1 2022-08-29 16:32:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-fj7gc 7ecce66e-298a-4972-b167-82ac7140e471 0xc004254b60 0xc004254b61}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:32:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ecce66e-298a-4972-b167-82ac7140e471\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:33:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004254c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:33:01.377: INFO: Pod "test-deployment-fj7gc-688c4d6789-n946l" is available:
&Pod{ObjectMeta:{test-deployment-fj7gc-688c4d6789-n946l test-deployment-fj7gc-688c4d6789- deployment-5609  656636f1-3a62-40a0-8013-c61eea1b65d0 35568 0 2022-08-29 16:32:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[cni.projectcalico.org/containerID:2f61e8c139e67bc6500596b974f1830ec4fdea508ff1b70fc4bce3f7e97af850 cni.projectcalico.org/podIP:172.25.2.11/32 cni.projectcalico.org/podIPs:172.25.2.11/32] [{apps/v1 ReplicaSet test-deployment-fj7gc-688c4d6789 8e499fcd-4923-4ffc-8d2b-66d4906e2f73 0xc004255087 0xc004255088}] []  [{Go-http-client Update v1 2022-08-29 16:32:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-08-29 16:32:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e499fcd-4923-4ffc-8d2b-66d4906e2f73\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:33:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmhdf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmhdf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:32:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:33:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:33:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:32:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:172.25.2.11,StartTime:2022-08-29 16:32:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:33:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8922f719408057733ff5ca103f80422a2e3ef7403ea63909ec18d976accd25b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 29 16:33:01.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5609" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":255,"skipped":4329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:33:01.409: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Aug 29 16:35:02.639: INFO: Successfully updated pod "var-expansion-012cb275-8782-473c-890c-f11dd3239092"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Aug 29 16:35:04.665: INFO: Deleting pod "var-expansion-012cb275-8782-473c-890c-f11dd3239092" in namespace "var-expansion-494"
Aug 29 16:35:04.899: INFO: Wait up to 5m0s for pod "var-expansion-012cb275-8782-473c-890c-f11dd3239092" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 29 16:35:36.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-494" for this suite.

• [SLOW TEST:155.536 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":256,"skipped":4352,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:35:36.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 29 16:35:37.680: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 16:35:37.699: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 16:35:37.704: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-16-214.eu-central-1.compute.internal before test
Aug 29 16:35:37.717: INFO: canal-qcxrv from kube-system started at 2022-08-29 15:06:13 +0000 UTC (2 container statuses recorded)
Aug 29 16:35:37.717: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:35:37.717: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:35:37.717: INFO: envoy-agent-r297b from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.717: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:35:37.717: INFO: kube-proxy-wn284 from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.717: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:35:37.717: INFO: node-local-dns-px2xr from kube-system started at 2022-08-29 15:06:13 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.717: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:35:37.717: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-942bg from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:35:37.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:35:37.717: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:35:37.717: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-25-142.eu-central-1.compute.internal before test
Aug 29 16:35:37.732: INFO: canal-xl6zl from kube-system started at 2022-08-29 15:07:00 +0000 UTC (2 container statuses recorded)
Aug 29 16:35:37.732: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:35:37.732: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:35:37.732: INFO: envoy-agent-sqxbm from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.732: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:35:37.732: INFO: kube-proxy-gb22k from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.732: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:35:37.732: INFO: node-local-dns-vmfhr from kube-system started at 2022-08-29 15:07:00 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.732: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:35:37.732: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-9j6sw from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:35:37.732: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:35:37.732: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 16:35:37.732: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-26-197.eu-central-1.compute.internal before test
Aug 29 16:35:37.748: INFO: calico-kube-controllers-58c7d666cf-65tgr from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 16:35:37.748: INFO: canal-kg6wt from kube-system started at 2022-08-29 15:05:48 +0000 UTC (2 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 16:35:37.748: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 29 16:35:37.748: INFO: coredns-5fff85bf5-49vtq from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:35:37.748: INFO: coredns-5fff85bf5-5ppjp from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container coredns ready: true, restart count 0
Aug 29 16:35:37.748: INFO: envoy-agent-zzmwh from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container envoy-agent ready: true, restart count 0
Aug 29 16:35:37.748: INFO: konnectivity-agent-6885ff4877-2bv58 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:35:37.748: INFO: konnectivity-agent-6885ff4877-n5r72 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 29 16:35:37.748: INFO: kube-proxy-9gjzc from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 16:35:37.748: INFO: metrics-server-6847c4d787-4wc57 from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:35:37.748: INFO: metrics-server-6847c4d787-z9nwh from kube-system started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container metrics-server ready: true, restart count 0
Aug 29 16:35:37.748: INFO: node-local-dns-6x48b from kube-system started at 2022-08-29 15:05:48 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container node-cache ready: true, restart count 0
Aug 29 16:35:37.748: INFO: dashboard-metrics-scraper-8cfc65cdb-85659 from kubernetes-dashboard started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 29 16:35:37.748: INFO: dashboard-metrics-scraper-8cfc65cdb-v2g6l from kubernetes-dashboard started at 2022-08-29 15:06:27 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 29 16:35:37.748: INFO: sonobuoy from sonobuoy started at 2022-08-29 15:06:56 +0000 UTC (1 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 16:35:37.748: INFO: sonobuoy-e2e-job-ef66887baa2a47e4 from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container e2e ready: true, restart count 0
Aug 29 16:35:37.748: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:35:37.748: INFO: sonobuoy-systemd-logs-daemon-set-15600321cc0c4a8e-4d599 from sonobuoy started at 2022-08-29 15:07:01 +0000 UTC (2 container statuses recorded)
Aug 29 16:35:37.748: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 16:35:37.748: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7244f2a9-880c-4a32-b7d3-a18b414094e0 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.25.142 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-7244f2a9-880c-4a32-b7d3-a18b414094e0 off the node ip-172-31-25-142.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7244f2a9-880c-4a32-b7d3-a18b414094e0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Aug 29 16:40:42.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9227" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:305.634 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":257,"skipped":4371,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:40:42.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1729
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-1729
Aug 29 16:40:42.680: INFO: Found 0 stateful pods, waiting for 1
Aug 29 16:40:52.688: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Aug 29 16:40:52.730: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Aug 29 16:40:52.750: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Aug 29 16:40:52.755: INFO: Observed &StatefulSet event: ADDED
Aug 29 16:40:52.755: INFO: Found Statefulset ss in namespace statefulset-1729 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 16:40:52.756: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Aug 29 16:40:52.756: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 29 16:40:52.768: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Aug 29 16:40:52.774: INFO: Observed &StatefulSet event: ADDED
Aug 29 16:40:52.774: INFO: Observed Statefulset ss in namespace statefulset-1729 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 16:40:52.774: INFO: Observed &StatefulSet event: MODIFIED
Aug 29 16:40:52.774: INFO: Found Statefulset ss in namespace statefulset-1729 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 29 16:40:52.775: INFO: Deleting all statefulset in ns statefulset-1729
Aug 29 16:40:52.781: INFO: Scaling statefulset ss to 0
Aug 29 16:41:02.819: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 16:41:02.827: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Aug 29 16:41:02.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1729" for this suite.

• [SLOW TEST:20.303 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":258,"skipped":4378,"failed":0}
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:41:02.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:41:02.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e47df16-a8a0-4893-9f7c-29ac86f3ca02" in namespace "downward-api-2817" to be "Succeeded or Failed"
Aug 29 16:41:03.063: INFO: Pod "downwardapi-volume-3e47df16-a8a0-4893-9f7c-29ac86f3ca02": Phase="Pending", Reason="", readiness=false. Elapsed: 101.375276ms
Aug 29 16:41:05.075: INFO: Pod "downwardapi-volume-3e47df16-a8a0-4893-9f7c-29ac86f3ca02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.113379493s
Aug 29 16:41:07.998: INFO: Pod "downwardapi-volume-3e47df16-a8a0-4893-9f7c-29ac86f3ca02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.036680313s
STEP: Saw pod success
Aug 29 16:41:07.998: INFO: Pod "downwardapi-volume-3e47df16-a8a0-4893-9f7c-29ac86f3ca02" satisfied condition "Succeeded or Failed"
Aug 29 16:41:08.008: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod downwardapi-volume-3e47df16-a8a0-4893-9f7c-29ac86f3ca02 container client-container: <nil>
STEP: delete the pod
Aug 29 16:41:08.100: INFO: Waiting for pod downwardapi-volume-3e47df16-a8a0-4893-9f7c-29ac86f3ca02 to disappear
Aug 29 16:41:08.116: INFO: Pod downwardapi-volume-3e47df16-a8a0-4893-9f7c-29ac86f3ca02 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 16:41:08.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2817" for this suite.

• [SLOW TEST:5.275 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":259,"skipped":4378,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:41:08.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 29 16:41:19.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2156" for this suite.

• [SLOW TEST:11.196 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":260,"skipped":4396,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:41:19.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Aug 29 16:41:19.457: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Aug 29 16:41:21.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-2922" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":261,"skipped":4415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:41:21.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:41:21.734: INFO: The status of Pod busybox-scheduling-00b80771-324a-46b7-8cdd-8841b268208d is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:41:23.744: INFO: The status of Pod busybox-scheduling-00b80771-324a-46b7-8cdd-8841b268208d is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Aug 29 16:41:23.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4202" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":262,"skipped":4447,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:41:23.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 29 16:41:24.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:41:24.382: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:41:25.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:41:25.416: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:41:26.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 16:41:26.406: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 29 16:41:26.465: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 16:41:26.465: INFO: Node ip-172-31-25-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:41:27.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 16:41:27.483: INFO: Node ip-172-31-25-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:41:28.487: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 16:41:28.487: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3329, will wait for the garbage collector to delete the pods
Aug 29 16:41:28.573: INFO: Deleting DaemonSet.extensions daemon-set took: 15.922724ms
Aug 29 16:41:28.673: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.448215ms
Aug 29 16:41:30.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:41:30.885: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 16:41:30.897: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37383"},"items":null}

Aug 29 16:41:30.910: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37383"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 29 16:41:30.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3329" for this suite.

• [SLOW TEST:7.080 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":263,"skipped":4451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:41:30.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-a4d5854c-32d2-437e-bd15-62222b3c3f94
STEP: Creating a pod to test consume secrets
Aug 29 16:41:31.704: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fa58acbe-bf2d-40fc-9f6d-75ac91fe0190" in namespace "projected-5861" to be "Succeeded or Failed"
Aug 29 16:41:31.720: INFO: Pod "pod-projected-secrets-fa58acbe-bf2d-40fc-9f6d-75ac91fe0190": Phase="Pending", Reason="", readiness=false. Elapsed: 16.379648ms
Aug 29 16:41:33.729: INFO: Pod "pod-projected-secrets-fa58acbe-bf2d-40fc-9f6d-75ac91fe0190": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024671297s
Aug 29 16:41:35.739: INFO: Pod "pod-projected-secrets-fa58acbe-bf2d-40fc-9f6d-75ac91fe0190": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035353569s
STEP: Saw pod success
Aug 29 16:41:35.739: INFO: Pod "pod-projected-secrets-fa58acbe-bf2d-40fc-9f6d-75ac91fe0190" satisfied condition "Succeeded or Failed"
Aug 29 16:41:35.751: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-projected-secrets-fa58acbe-bf2d-40fc-9f6d-75ac91fe0190 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:41:35.920: INFO: Waiting for pod pod-projected-secrets-fa58acbe-bf2d-40fc-9f6d-75ac91fe0190 to disappear
Aug 29 16:41:35.925: INFO: Pod pod-projected-secrets-fa58acbe-bf2d-40fc-9f6d-75ac91fe0190 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 29 16:41:35.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5861" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":264,"skipped":4490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:41:35.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Aug 29 16:42:36.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5044" for this suite.

• [SLOW TEST:60.130 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":265,"skipped":4541,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:42:36.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:42:36.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 16:42:38.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 16, 42, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 42, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 42, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 42, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:42:42.008: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Aug 29 16:42:42.082: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:42:42.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7210" for this suite.
STEP: Destroying namespace "webhook-7210-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.530 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":266,"skipped":4559,"failed":0}
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:42:42.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:42:43.469: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:42:46.545: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:42:46.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:42:49.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2593" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:8.030 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":267,"skipped":4559,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:42:50.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 29 16:42:55.108: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Aug 29 16:42:55.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2896" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":268,"skipped":4583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:42:55.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-d7da1726-b0c4-4d27-b69c-a4fd3d85d4ff
STEP: Creating a pod to test consume secrets
Aug 29 16:42:55.280: INFO: Waiting up to 5m0s for pod "pod-secrets-0e46dbc8-03d2-465f-8fac-5a2b4f6ba2a2" in namespace "secrets-4041" to be "Succeeded or Failed"
Aug 29 16:42:55.289: INFO: Pod "pod-secrets-0e46dbc8-03d2-465f-8fac-5a2b4f6ba2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.118849ms
Aug 29 16:42:57.300: INFO: Pod "pod-secrets-0e46dbc8-03d2-465f-8fac-5a2b4f6ba2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020554412s
Aug 29 16:42:59.310: INFO: Pod "pod-secrets-0e46dbc8-03d2-465f-8fac-5a2b4f6ba2a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030688829s
STEP: Saw pod success
Aug 29 16:42:59.311: INFO: Pod "pod-secrets-0e46dbc8-03d2-465f-8fac-5a2b4f6ba2a2" satisfied condition "Succeeded or Failed"
Aug 29 16:42:59.321: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-secrets-0e46dbc8-03d2-465f-8fac-5a2b4f6ba2a2 container secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:42:59.357: INFO: Waiting for pod pod-secrets-0e46dbc8-03d2-465f-8fac-5a2b4f6ba2a2 to disappear
Aug 29 16:42:59.363: INFO: Pod pod-secrets-0e46dbc8-03d2-465f-8fac-5a2b4f6ba2a2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 29 16:42:59.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4041" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":269,"skipped":4649,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:42:59.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 29 16:42:59.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5619" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":270,"skipped":4676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:42:59.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:42:59.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-220" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":271,"skipped":4707,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:42:59.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:42:59.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6056
I0829 16:42:59.922273      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6056, replica count: 1
I0829 16:43:00.973204      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0829 16:43:01.974003      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:43:02.092: INFO: Created: latency-svc-zr4t6
Aug 29 16:43:02.112: INFO: Got endpoints: latency-svc-zr4t6 [38.240965ms]
Aug 29 16:43:02.488: INFO: Created: latency-svc-p99gj
Aug 29 16:43:02.495: INFO: Created: latency-svc-q6mw8
Aug 29 16:43:02.497: INFO: Got endpoints: latency-svc-p99gj [384.368127ms]
Aug 29 16:43:02.507: INFO: Got endpoints: latency-svc-q6mw8 [393.978861ms]
Aug 29 16:43:02.529: INFO: Created: latency-svc-ntxbj
Aug 29 16:43:02.531: INFO: Got endpoints: latency-svc-ntxbj [417.706359ms]
Aug 29 16:43:02.540: INFO: Created: latency-svc-5lfjw
Aug 29 16:43:02.558: INFO: Got endpoints: latency-svc-5lfjw [445.296828ms]
Aug 29 16:43:02.561: INFO: Created: latency-svc-djqxl
Aug 29 16:43:02.561: INFO: Got endpoints: latency-svc-djqxl [447.810924ms]
Aug 29 16:43:02.573: INFO: Created: latency-svc-4jmrj
Aug 29 16:43:02.584: INFO: Got endpoints: latency-svc-4jmrj [470.38509ms]
Aug 29 16:43:02.597: INFO: Created: latency-svc-6nx4f
Aug 29 16:43:02.602: INFO: Created: latency-svc-twcc6
Aug 29 16:43:02.604: INFO: Got endpoints: latency-svc-6nx4f [488.648137ms]
Aug 29 16:43:02.612: INFO: Got endpoints: latency-svc-twcc6 [495.726014ms]
Aug 29 16:43:02.626: INFO: Created: latency-svc-qllbw
Aug 29 16:43:02.632: INFO: Created: latency-svc-tsnjs
Aug 29 16:43:02.632: INFO: Got endpoints: latency-svc-qllbw [516.176616ms]
Aug 29 16:43:02.641: INFO: Got endpoints: latency-svc-tsnjs [524.574007ms]
Aug 29 16:43:02.652: INFO: Created: latency-svc-8p9df
Aug 29 16:43:02.658: INFO: Got endpoints: latency-svc-8p9df [541.563101ms]
Aug 29 16:43:02.667: INFO: Created: latency-svc-265pm
Aug 29 16:43:02.689: INFO: Got endpoints: latency-svc-265pm [573.174154ms]
Aug 29 16:43:02.704: INFO: Created: latency-svc-cr5jg
Aug 29 16:43:02.723: INFO: Got endpoints: latency-svc-cr5jg [606.327534ms]
Aug 29 16:43:02.723: INFO: Created: latency-svc-x4sw4
Aug 29 16:43:02.737: INFO: Got endpoints: latency-svc-x4sw4 [620.908321ms]
Aug 29 16:43:02.846: INFO: Created: latency-svc-kctv5
Aug 29 16:43:02.847: INFO: Created: latency-svc-cpwqb
Aug 29 16:43:02.847: INFO: Created: latency-svc-mz8g8
Aug 29 16:43:02.847: INFO: Created: latency-svc-ccmhx
Aug 29 16:43:02.848: INFO: Created: latency-svc-5vwds
Aug 29 16:43:02.848: INFO: Created: latency-svc-v7b45
Aug 29 16:43:02.849: INFO: Created: latency-svc-8b5gs
Aug 29 16:43:02.849: INFO: Created: latency-svc-j22kl
Aug 29 16:43:02.850: INFO: Created: latency-svc-xb795
Aug 29 16:43:02.849: INFO: Created: latency-svc-c5js8
Aug 29 16:43:02.850: INFO: Created: latency-svc-xpsft
Aug 29 16:43:02.851: INFO: Created: latency-svc-95sbd
Aug 29 16:43:02.856: INFO: Got endpoints: latency-svc-kctv5 [349.268576ms]
Aug 29 16:43:02.857: INFO: Created: latency-svc-mnl7p
Aug 29 16:43:02.858: INFO: Created: latency-svc-tqndg
Aug 29 16:43:02.858: INFO: Created: latency-svc-jwr6w
Aug 29 16:43:02.863: INFO: Got endpoints: latency-svc-mz8g8 [140.512744ms]
Aug 29 16:43:02.864: INFO: Got endpoints: latency-svc-ccmhx [222.454022ms]
Aug 29 16:43:02.865: INFO: Got endpoints: latency-svc-v7b45 [127.373281ms]
Aug 29 16:43:02.867: INFO: Got endpoints: latency-svc-5vwds [254.772872ms]
Aug 29 16:43:02.875: INFO: Got endpoints: latency-svc-cpwqb [313.716552ms]
Aug 29 16:43:02.877: INFO: Got endpoints: latency-svc-8b5gs [244.510591ms]
Aug 29 16:43:02.878: INFO: Got endpoints: latency-svc-c5js8 [188.513013ms]
Aug 29 16:43:02.878: INFO: Got endpoints: latency-svc-xb795 [347.109603ms]
Aug 29 16:43:02.886: INFO: Got endpoints: latency-svc-j22kl [769.403072ms]
Aug 29 16:43:02.887: INFO: Got endpoints: latency-svc-xpsft [390.265642ms]
Aug 29 16:43:02.893: INFO: Got endpoints: latency-svc-mnl7p [307.831808ms]
Aug 29 16:43:02.900: INFO: Got endpoints: latency-svc-95sbd [341.165228ms]
Aug 29 16:43:02.900: INFO: Got endpoints: latency-svc-tqndg [241.828792ms]
Aug 29 16:43:02.904: INFO: Got endpoints: latency-svc-jwr6w [299.472977ms]
Aug 29 16:43:02.910: INFO: Created: latency-svc-h7zfr
Aug 29 16:43:02.929: INFO: Got endpoints: latency-svc-h7zfr [72.754848ms]
Aug 29 16:43:02.930: INFO: Created: latency-svc-fmhl7
Aug 29 16:43:02.934: INFO: Got endpoints: latency-svc-fmhl7 [70.707607ms]
Aug 29 16:43:02.943: INFO: Created: latency-svc-stvrj
Aug 29 16:43:02.950: INFO: Got endpoints: latency-svc-stvrj [84.941871ms]
Aug 29 16:43:02.963: INFO: Created: latency-svc-bdnbq
Aug 29 16:43:02.975: INFO: Created: latency-svc-xrtqf
Aug 29 16:43:02.979: INFO: Got endpoints: latency-svc-bdnbq [115.586263ms]
Aug 29 16:43:02.986: INFO: Got endpoints: latency-svc-xrtqf [119.057251ms]
Aug 29 16:43:02.999: INFO: Created: latency-svc-62lsc
Aug 29 16:43:03.011: INFO: Got endpoints: latency-svc-62lsc [135.378508ms]
Aug 29 16:43:03.018: INFO: Created: latency-svc-s6gp7
Aug 29 16:43:03.038: INFO: Created: latency-svc-l8mtg
Aug 29 16:43:03.042: INFO: Got endpoints: latency-svc-l8mtg [164.009478ms]
Aug 29 16:43:03.042: INFO: Got endpoints: latency-svc-s6gp7 [165.376043ms]
Aug 29 16:43:03.065: INFO: Created: latency-svc-mhkkw
Aug 29 16:43:03.072: INFO: Got endpoints: latency-svc-mhkkw [193.66611ms]
Aug 29 16:43:03.078: INFO: Created: latency-svc-2zvvx
Aug 29 16:43:03.089: INFO: Got endpoints: latency-svc-2zvvx [202.582266ms]
Aug 29 16:43:03.089: INFO: Created: latency-svc-llcn7
Aug 29 16:43:03.102: INFO: Got endpoints: latency-svc-llcn7 [214.593226ms]
Aug 29 16:43:03.113: INFO: Created: latency-svc-6689f
Aug 29 16:43:03.118: INFO: Got endpoints: latency-svc-6689f [225.489585ms]
Aug 29 16:43:03.131: INFO: Created: latency-svc-lxvt8
Aug 29 16:43:03.133: INFO: Got endpoints: latency-svc-lxvt8 [232.536991ms]
Aug 29 16:43:03.151: INFO: Created: latency-svc-rfcrr
Aug 29 16:43:03.151: INFO: Got endpoints: latency-svc-rfcrr [251.269095ms]
Aug 29 16:43:03.155: INFO: Created: latency-svc-cvf9l
Aug 29 16:43:03.159: INFO: Created: latency-svc-qqgnp
Aug 29 16:43:03.173: INFO: Created: latency-svc-zw9rf
Aug 29 16:43:03.176: INFO: Created: latency-svc-khfqx
Aug 29 16:43:03.184: INFO: Created: latency-svc-44szf
Aug 29 16:43:03.193: INFO: Created: latency-svc-l5c65
Aug 29 16:43:03.207: INFO: Got endpoints: latency-svc-cvf9l [302.797263ms]
Aug 29 16:43:03.207: INFO: Created: latency-svc-bht4s
Aug 29 16:43:03.212: INFO: Created: latency-svc-57grx
Aug 29 16:43:03.220: INFO: Created: latency-svc-nkzqk
Aug 29 16:43:03.232: INFO: Created: latency-svc-klz5n
Aug 29 16:43:03.239: INFO: Created: latency-svc-fwwzq
Aug 29 16:43:03.251: INFO: Created: latency-svc-sphmz
Aug 29 16:43:03.251: INFO: Got endpoints: latency-svc-qqgnp [321.93568ms]
Aug 29 16:43:03.259: INFO: Created: latency-svc-d6qgv
Aug 29 16:43:03.266: INFO: Created: latency-svc-5lqwr
Aug 29 16:43:03.273: INFO: Created: latency-svc-lhdfv
Aug 29 16:43:03.280: INFO: Created: latency-svc-q7t8s
Aug 29 16:43:03.286: INFO: Created: latency-svc-mxb8r
Aug 29 16:43:03.298: INFO: Got endpoints: latency-svc-zw9rf [363.155837ms]
Aug 29 16:43:03.320: INFO: Created: latency-svc-69d96
Aug 29 16:43:03.348: INFO: Got endpoints: latency-svc-khfqx [397.668981ms]
Aug 29 16:43:03.361: INFO: Created: latency-svc-6d6xm
Aug 29 16:43:03.397: INFO: Got endpoints: latency-svc-44szf [417.483956ms]
Aug 29 16:43:03.414: INFO: Created: latency-svc-dbndv
Aug 29 16:43:03.447: INFO: Got endpoints: latency-svc-l5c65 [460.725114ms]
Aug 29 16:43:03.468: INFO: Created: latency-svc-bx9mh
Aug 29 16:43:03.495: INFO: Got endpoints: latency-svc-bht4s [483.962237ms]
Aug 29 16:43:03.508: INFO: Created: latency-svc-rjgbg
Aug 29 16:43:03.546: INFO: Got endpoints: latency-svc-57grx [503.409987ms]
Aug 29 16:43:03.563: INFO: Created: latency-svc-qmsk5
Aug 29 16:43:03.594: INFO: Got endpoints: latency-svc-nkzqk [551.998835ms]
Aug 29 16:43:03.614: INFO: Created: latency-svc-rbkdk
Aug 29 16:43:03.648: INFO: Got endpoints: latency-svc-klz5n [575.624271ms]
Aug 29 16:43:03.665: INFO: Created: latency-svc-cj44c
Aug 29 16:43:03.696: INFO: Got endpoints: latency-svc-fwwzq [607.072442ms]
Aug 29 16:43:03.715: INFO: Created: latency-svc-4868t
Aug 29 16:43:03.743: INFO: Got endpoints: latency-svc-sphmz [640.911118ms]
Aug 29 16:43:03.758: INFO: Created: latency-svc-nrvsd
Aug 29 16:43:03.794: INFO: Got endpoints: latency-svc-d6qgv [675.769096ms]
Aug 29 16:43:03.812: INFO: Created: latency-svc-d8bhx
Aug 29 16:43:03.845: INFO: Got endpoints: latency-svc-5lqwr [711.887797ms]
Aug 29 16:43:03.863: INFO: Created: latency-svc-rt2xz
Aug 29 16:43:03.895: INFO: Got endpoints: latency-svc-lhdfv [742.869604ms]
Aug 29 16:43:03.914: INFO: Created: latency-svc-72ht6
Aug 29 16:43:03.946: INFO: Got endpoints: latency-svc-q7t8s [738.79115ms]
Aug 29 16:43:03.961: INFO: Created: latency-svc-wgkkr
Aug 29 16:43:03.996: INFO: Got endpoints: latency-svc-mxb8r [745.204128ms]
Aug 29 16:43:04.014: INFO: Created: latency-svc-cbn4p
Aug 29 16:43:04.045: INFO: Got endpoints: latency-svc-69d96 [746.922007ms]
Aug 29 16:43:04.144: INFO: Got endpoints: latency-svc-dbndv [746.775816ms]
Aug 29 16:43:04.194: INFO: Got endpoints: latency-svc-bx9mh [746.657735ms]
Aug 29 16:43:04.295: INFO: Got endpoints: latency-svc-qmsk5 [748.185081ms]
Aug 29 16:43:04.301: INFO: Got endpoints: latency-svc-6d6xm [952.9209ms]
Aug 29 16:43:04.311: INFO: Got endpoints: latency-svc-rjgbg [816.35165ms]
Aug 29 16:43:04.311: INFO: Created: latency-svc-tm56f
Aug 29 16:43:04.315: INFO: Created: latency-svc-l4jwm
Aug 29 16:43:04.326: INFO: Created: latency-svc-rl7t5
Aug 29 16:43:04.335: INFO: Created: latency-svc-5q64f
Aug 29 16:43:04.336: INFO: Created: latency-svc-qkrq9
Aug 29 16:43:04.343: INFO: Created: latency-svc-fqm9c
Aug 29 16:43:04.347: INFO: Got endpoints: latency-svc-rbkdk [752.503568ms]
Aug 29 16:43:04.363: INFO: Created: latency-svc-fxs69
Aug 29 16:43:04.396: INFO: Got endpoints: latency-svc-cj44c [748.299443ms]
Aug 29 16:43:04.421: INFO: Created: latency-svc-vwlth
Aug 29 16:43:04.447: INFO: Got endpoints: latency-svc-4868t [751.343735ms]
Aug 29 16:43:04.466: INFO: Created: latency-svc-n7qb5
Aug 29 16:43:04.496: INFO: Got endpoints: latency-svc-nrvsd [752.7629ms]
Aug 29 16:43:04.512: INFO: Created: latency-svc-ldrrd
Aug 29 16:43:04.547: INFO: Got endpoints: latency-svc-d8bhx [752.100084ms]
Aug 29 16:43:04.595: INFO: Got endpoints: latency-svc-rt2xz [750.171233ms]
Aug 29 16:43:04.648: INFO: Got endpoints: latency-svc-72ht6 [752.583318ms]
Aug 29 16:43:04.737: INFO: Got endpoints: latency-svc-wgkkr [790.816113ms]
Aug 29 16:43:04.739: INFO: Created: latency-svc-79fbp
Aug 29 16:43:04.746: INFO: Created: latency-svc-sgf94
Aug 29 16:43:04.753: INFO: Got endpoints: latency-svc-cbn4p [756.906135ms]
Aug 29 16:43:04.763: INFO: Created: latency-svc-vkx9w
Aug 29 16:43:04.775: INFO: Created: latency-svc-4wg6r
Aug 29 16:43:04.790: INFO: Created: latency-svc-7rbrb
Aug 29 16:43:04.804: INFO: Got endpoints: latency-svc-tm56f [758.913436ms]
Aug 29 16:43:04.826: INFO: Created: latency-svc-9kj9q
Aug 29 16:43:04.856: INFO: Got endpoints: latency-svc-l4jwm [712.18253ms]
Aug 29 16:43:04.933: INFO: Got endpoints: latency-svc-rl7t5 [738.543136ms]
Aug 29 16:43:04.952: INFO: Created: latency-svc-fsvgh
Aug 29 16:43:04.965: INFO: Got endpoints: latency-svc-5q64f [670.292546ms]
Aug 29 16:43:04.978: INFO: Created: latency-svc-t95dg
Aug 29 16:43:04.994: INFO: Created: latency-svc-rw2x4
Aug 29 16:43:04.996: INFO: Got endpoints: latency-svc-qkrq9 [695.017514ms]
Aug 29 16:43:05.025: INFO: Created: latency-svc-46jjm
Aug 29 16:43:05.048: INFO: Got endpoints: latency-svc-fqm9c [736.467174ms]
Aug 29 16:43:05.074: INFO: Created: latency-svc-rpg7b
Aug 29 16:43:05.105: INFO: Got endpoints: latency-svc-fxs69 [757.719454ms]
Aug 29 16:43:05.131: INFO: Created: latency-svc-pm6f7
Aug 29 16:43:05.145: INFO: Got endpoints: latency-svc-vwlth [748.12542ms]
Aug 29 16:43:05.203: INFO: Created: latency-svc-c24zk
Aug 29 16:43:05.215: INFO: Got endpoints: latency-svc-n7qb5 [767.676962ms]
Aug 29 16:43:05.251: INFO: Created: latency-svc-kpnz8
Aug 29 16:43:05.262: INFO: Got endpoints: latency-svc-ldrrd [765.575179ms]
Aug 29 16:43:05.288: INFO: Created: latency-svc-ksvqn
Aug 29 16:43:05.298: INFO: Got endpoints: latency-svc-79fbp [751.023671ms]
Aug 29 16:43:05.318: INFO: Created: latency-svc-45r7q
Aug 29 16:43:05.351: INFO: Got endpoints: latency-svc-sgf94 [755.425699ms]
Aug 29 16:43:05.371: INFO: Created: latency-svc-gtf8t
Aug 29 16:43:05.399: INFO: Got endpoints: latency-svc-vkx9w [750.676158ms]
Aug 29 16:43:05.417: INFO: Created: latency-svc-mkbwj
Aug 29 16:43:05.454: INFO: Got endpoints: latency-svc-4wg6r [716.692728ms]
Aug 29 16:43:05.480: INFO: Created: latency-svc-qkvfn
Aug 29 16:43:05.505: INFO: Got endpoints: latency-svc-7rbrb [751.179193ms]
Aug 29 16:43:05.524: INFO: Created: latency-svc-n7vnv
Aug 29 16:43:05.548: INFO: Got endpoints: latency-svc-9kj9q [744.279258ms]
Aug 29 16:43:05.568: INFO: Created: latency-svc-9bh6h
Aug 29 16:43:05.606: INFO: Got endpoints: latency-svc-fsvgh [749.672216ms]
Aug 29 16:43:05.632: INFO: Created: latency-svc-2tnh9
Aug 29 16:43:05.645: INFO: Got endpoints: latency-svc-t95dg [711.803327ms]
Aug 29 16:43:05.662: INFO: Created: latency-svc-xv4sd
Aug 29 16:43:05.694: INFO: Got endpoints: latency-svc-rw2x4 [728.608838ms]
Aug 29 16:43:05.746: INFO: Got endpoints: latency-svc-46jjm [750.03305ms]
Aug 29 16:43:05.845: INFO: Got endpoints: latency-svc-pm6f7 [740.127803ms]
Aug 29 16:43:05.894: INFO: Got endpoints: latency-svc-c24zk [749.451774ms]
Aug 29 16:43:05.995: INFO: Got endpoints: latency-svc-ksvqn [733.071466ms]
Aug 29 16:43:06.047: INFO: Got endpoints: latency-svc-45r7q [748.848048ms]
Aug 29 16:43:06.148: INFO: Got endpoints: latency-svc-mkbwj [748.714396ms]
Aug 29 16:43:06.194: INFO: Got endpoints: latency-svc-qkvfn [740.703589ms]
Aug 29 16:43:06.296: INFO: Got endpoints: latency-svc-9bh6h [747.568523ms]
Aug 29 16:43:06.345: INFO: Got endpoints: latency-svc-2tnh9 [738.847428ms]
Aug 29 16:43:07.245: INFO: Got endpoints: latency-svc-rpg7b [2.197188856s]
Aug 29 16:43:07.249: INFO: Got endpoints: latency-svc-kpnz8 [2.033276829s]
Aug 29 16:43:07.272: INFO: Got endpoints: latency-svc-xv4sd [1.627448669s]
Aug 29 16:43:07.273: INFO: Got endpoints: latency-svc-gtf8t [1.921889661s]
Aug 29 16:43:07.273: INFO: Got endpoints: latency-svc-n7vnv [1.768280906s]
Aug 29 16:43:07.282: INFO: Created: latency-svc-lt42d
Aug 29 16:43:07.282: INFO: Created: latency-svc-9rkxq
Aug 29 16:43:07.287: INFO: Got endpoints: latency-svc-lt42d [1.592536111s]
Aug 29 16:43:07.288: INFO: Got endpoints: latency-svc-9rkxq [1.542095184s]
Aug 29 16:43:07.299: INFO: Created: latency-svc-ssv6n
Aug 29 16:43:07.308: INFO: Got endpoints: latency-svc-ssv6n [1.462761034s]
Aug 29 16:43:07.314: INFO: Created: latency-svc-gnh7x
Aug 29 16:43:07.318: INFO: Got endpoints: latency-svc-gnh7x [1.423691681s]
Aug 29 16:43:07.341: INFO: Created: latency-svc-5f9q4
Aug 29 16:43:07.341: INFO: Got endpoints: latency-svc-5f9q4 [1.346033289s]
Aug 29 16:43:07.346: INFO: Created: latency-svc-5qp9b
Aug 29 16:43:07.348: INFO: Got endpoints: latency-svc-5qp9b [1.301208293s]
Aug 29 16:43:07.366: INFO: Created: latency-svc-gc9rn
Aug 29 16:43:07.367: INFO: Got endpoints: latency-svc-gc9rn [1.218587338s]
Aug 29 16:43:07.366: INFO: Created: latency-svc-khbvd
Aug 29 16:43:07.372: INFO: Created: latency-svc-w2cbx
Aug 29 16:43:07.372: INFO: Got endpoints: latency-svc-khbvd [1.177592303s]
Aug 29 16:43:07.376: INFO: Got endpoints: latency-svc-w2cbx [1.079606202s]
Aug 29 16:43:07.384: INFO: Created: latency-svc-vlm2j
Aug 29 16:43:07.389: INFO: Got endpoints: latency-svc-vlm2j [1.044186258s]
Aug 29 16:43:07.397: INFO: Created: latency-svc-xjwjt
Aug 29 16:43:07.413: INFO: Got endpoints: latency-svc-xjwjt [167.96724ms]
Aug 29 16:43:07.416: INFO: Created: latency-svc-hzwnz
Aug 29 16:43:07.437: INFO: Got endpoints: latency-svc-hzwnz [188.044078ms]
Aug 29 16:43:07.439: INFO: Created: latency-svc-vrcgb
Aug 29 16:43:07.453: INFO: Created: latency-svc-8m5kr
Aug 29 16:43:07.455: INFO: Got endpoints: latency-svc-vrcgb [182.566559ms]
Aug 29 16:43:07.461: INFO: Got endpoints: latency-svc-8m5kr [187.755484ms]
Aug 29 16:43:07.467: INFO: Created: latency-svc-2ddp9
Aug 29 16:43:07.477: INFO: Got endpoints: latency-svc-2ddp9 [190.199292ms]
Aug 29 16:43:07.480: INFO: Created: latency-svc-ftp4t
Aug 29 16:43:07.492: INFO: Got endpoints: latency-svc-ftp4t [218.116714ms]
Aug 29 16:43:07.493: INFO: Created: latency-svc-vfrvn
Aug 29 16:43:07.500: INFO: Got endpoints: latency-svc-vfrvn [211.615084ms]
Aug 29 16:43:07.509: INFO: Created: latency-svc-xgbx6
Aug 29 16:43:07.526: INFO: Got endpoints: latency-svc-xgbx6 [217.68231ms]
Aug 29 16:43:07.530: INFO: Created: latency-svc-2x6jb
Aug 29 16:43:07.542: INFO: Got endpoints: latency-svc-2x6jb [224.151999ms]
Aug 29 16:43:07.543: INFO: Created: latency-svc-sgxqq
Aug 29 16:43:07.556: INFO: Got endpoints: latency-svc-sgxqq [214.235152ms]
Aug 29 16:43:07.557: INFO: Created: latency-svc-zqrnb
Aug 29 16:43:07.564: INFO: Got endpoints: latency-svc-zqrnb [215.336404ms]
Aug 29 16:43:07.571: INFO: Created: latency-svc-jfffp
Aug 29 16:43:07.584: INFO: Created: latency-svc-pgzks
Aug 29 16:43:07.588: INFO: Got endpoints: latency-svc-jfffp [215.89683ms]
Aug 29 16:43:07.591: INFO: Got endpoints: latency-svc-pgzks [224.052639ms]
Aug 29 16:43:07.606: INFO: Created: latency-svc-rnf9z
Aug 29 16:43:07.616: INFO: Got endpoints: latency-svc-rnf9z [240.426746ms]
Aug 29 16:43:07.633: INFO: Created: latency-svc-7thdw
Aug 29 16:43:07.634: INFO: Created: latency-svc-q6956
Aug 29 16:43:07.657: INFO: Got endpoints: latency-svc-7thdw [267.396138ms]
Aug 29 16:43:07.661: INFO: Created: latency-svc-f6mgk
Aug 29 16:43:07.669: INFO: Created: latency-svc-n7t5m
Aug 29 16:43:07.678: INFO: Created: latency-svc-tx8q4
Aug 29 16:43:07.688: INFO: Created: latency-svc-djmdc
Aug 29 16:43:07.697: INFO: Got endpoints: latency-svc-q6956 [282.31374ms]
Aug 29 16:43:07.697: INFO: Created: latency-svc-rfjw7
Aug 29 16:43:07.704: INFO: Created: latency-svc-p4qsk
Aug 29 16:43:07.716: INFO: Created: latency-svc-4znzf
Aug 29 16:43:07.719: INFO: Created: latency-svc-kr2vb
Aug 29 16:43:07.733: INFO: Created: latency-svc-9bfjz
Aug 29 16:43:07.742: INFO: Created: latency-svc-7gqwn
Aug 29 16:43:07.752: INFO: Got endpoints: latency-svc-f6mgk [312.572747ms]
Aug 29 16:43:07.753: INFO: Created: latency-svc-5s6j4
Aug 29 16:43:07.788: INFO: Created: latency-svc-bpt2z
Aug 29 16:43:07.800: INFO: Got endpoints: latency-svc-n7t5m [344.336533ms]
Aug 29 16:43:07.804: INFO: Created: latency-svc-b5zwn
Aug 29 16:43:07.816: INFO: Created: latency-svc-fj44j
Aug 29 16:43:07.827: INFO: Created: latency-svc-w4tff
Aug 29 16:43:07.843: INFO: Created: latency-svc-wfrg4
Aug 29 16:43:07.843: INFO: Created: latency-svc-m9q25
Aug 29 16:43:07.848: INFO: Got endpoints: latency-svc-tx8q4 [387.234958ms]
Aug 29 16:43:07.865: INFO: Created: latency-svc-gphhc
Aug 29 16:43:07.897: INFO: Got endpoints: latency-svc-djmdc [419.507787ms]
Aug 29 16:43:07.914: INFO: Created: latency-svc-j2xsn
Aug 29 16:43:07.950: INFO: Got endpoints: latency-svc-rfjw7 [458.169176ms]
Aug 29 16:43:07.968: INFO: Created: latency-svc-c4mlj
Aug 29 16:43:07.998: INFO: Got endpoints: latency-svc-p4qsk [497.543612ms]
Aug 29 16:43:08.022: INFO: Created: latency-svc-6thtb
Aug 29 16:43:08.054: INFO: Got endpoints: latency-svc-4znzf [528.045854ms]
Aug 29 16:43:08.078: INFO: Created: latency-svc-f2rp6
Aug 29 16:43:08.107: INFO: Got endpoints: latency-svc-kr2vb [564.108874ms]
Aug 29 16:43:08.146: INFO: Created: latency-svc-nhnb2
Aug 29 16:43:08.166: INFO: Got endpoints: latency-svc-9bfjz [610.79469ms]
Aug 29 16:43:08.194: INFO: Created: latency-svc-grbfh
Aug 29 16:43:08.208: INFO: Got endpoints: latency-svc-7gqwn [644.190522ms]
Aug 29 16:43:08.230: INFO: Created: latency-svc-7rcv8
Aug 29 16:43:08.247: INFO: Got endpoints: latency-svc-5s6j4 [658.425856ms]
Aug 29 16:43:08.281: INFO: Created: latency-svc-k4jq5
Aug 29 16:43:08.296: INFO: Got endpoints: latency-svc-bpt2z [705.133873ms]
Aug 29 16:43:08.315: INFO: Created: latency-svc-4rh58
Aug 29 16:43:08.345: INFO: Got endpoints: latency-svc-b5zwn [728.255603ms]
Aug 29 16:43:08.376: INFO: Created: latency-svc-6bbtg
Aug 29 16:43:08.399: INFO: Got endpoints: latency-svc-fj44j [742.088614ms]
Aug 29 16:43:08.429: INFO: Created: latency-svc-jtrpk
Aug 29 16:43:08.449: INFO: Got endpoints: latency-svc-w4tff [752.639738ms]
Aug 29 16:43:08.473: INFO: Created: latency-svc-2qdkm
Aug 29 16:43:08.495: INFO: Got endpoints: latency-svc-wfrg4 [743.357667ms]
Aug 29 16:43:08.510: INFO: Created: latency-svc-plvtj
Aug 29 16:43:08.550: INFO: Got endpoints: latency-svc-m9q25 [750.05419ms]
Aug 29 16:43:08.569: INFO: Created: latency-svc-c7kw2
Aug 29 16:43:08.595: INFO: Got endpoints: latency-svc-gphhc [747.076807ms]
Aug 29 16:43:08.621: INFO: Created: latency-svc-x4ckv
Aug 29 16:43:08.647: INFO: Got endpoints: latency-svc-j2xsn [750.440623ms]
Aug 29 16:43:08.666: INFO: Created: latency-svc-vppvl
Aug 29 16:43:08.698: INFO: Got endpoints: latency-svc-c4mlj [747.545632ms]
Aug 29 16:43:08.712: INFO: Created: latency-svc-nsx9h
Aug 29 16:43:08.743: INFO: Got endpoints: latency-svc-6thtb [745.575111ms]
Aug 29 16:43:08.763: INFO: Created: latency-svc-5j4mw
Aug 29 16:43:08.793: INFO: Got endpoints: latency-svc-f2rp6 [738.995019ms]
Aug 29 16:43:08.813: INFO: Created: latency-svc-rzg7t
Aug 29 16:43:08.846: INFO: Got endpoints: latency-svc-nhnb2 [739.726538ms]
Aug 29 16:43:08.862: INFO: Created: latency-svc-vncgf
Aug 29 16:43:08.894: INFO: Got endpoints: latency-svc-grbfh [727.076611ms]
Aug 29 16:43:08.914: INFO: Created: latency-svc-c8fdq
Aug 29 16:43:08.948: INFO: Got endpoints: latency-svc-7rcv8 [739.594786ms]
Aug 29 16:43:08.966: INFO: Created: latency-svc-mtd9j
Aug 29 16:43:08.996: INFO: Got endpoints: latency-svc-k4jq5 [746.572331ms]
Aug 29 16:43:09.015: INFO: Created: latency-svc-4wp5q
Aug 29 16:43:09.045: INFO: Got endpoints: latency-svc-4rh58 [748.474062ms]
Aug 29 16:43:09.060: INFO: Created: latency-svc-lcf8x
Aug 29 16:43:09.096: INFO: Got endpoints: latency-svc-6bbtg [751.377003ms]
Aug 29 16:43:09.116: INFO: Created: latency-svc-g7sgs
Aug 29 16:43:09.148: INFO: Got endpoints: latency-svc-jtrpk [748.664444ms]
Aug 29 16:43:09.168: INFO: Created: latency-svc-rhn4h
Aug 29 16:43:09.198: INFO: Got endpoints: latency-svc-2qdkm [748.536903ms]
Aug 29 16:43:09.217: INFO: Created: latency-svc-b6nbh
Aug 29 16:43:09.248: INFO: Got endpoints: latency-svc-plvtj [752.686488ms]
Aug 29 16:43:09.272: INFO: Created: latency-svc-m7hn5
Aug 29 16:43:09.296: INFO: Got endpoints: latency-svc-c7kw2 [746.143557ms]
Aug 29 16:43:09.316: INFO: Created: latency-svc-65925
Aug 29 16:43:09.347: INFO: Got endpoints: latency-svc-x4ckv [751.99735ms]
Aug 29 16:43:09.366: INFO: Created: latency-svc-wtqnm
Aug 29 16:43:09.395: INFO: Got endpoints: latency-svc-vppvl [747.655603ms]
Aug 29 16:43:09.412: INFO: Created: latency-svc-vr7dl
Aug 29 16:43:09.448: INFO: Got endpoints: latency-svc-nsx9h [750.606405ms]
Aug 29 16:43:09.468: INFO: Created: latency-svc-2r722
Aug 29 16:43:09.496: INFO: Got endpoints: latency-svc-5j4mw [752.098052ms]
Aug 29 16:43:09.509: INFO: Created: latency-svc-qr2tj
Aug 29 16:43:09.546: INFO: Got endpoints: latency-svc-rzg7t [751.291363ms]
Aug 29 16:43:09.559: INFO: Created: latency-svc-6j8qq
Aug 29 16:43:09.596: INFO: Got endpoints: latency-svc-vncgf [749.533314ms]
Aug 29 16:43:09.613: INFO: Created: latency-svc-trzgw
Aug 29 16:43:09.644: INFO: Got endpoints: latency-svc-c8fdq [750.287632ms]
Aug 29 16:43:09.663: INFO: Created: latency-svc-q7nfj
Aug 29 16:43:09.695: INFO: Got endpoints: latency-svc-mtd9j [747.294929ms]
Aug 29 16:43:09.740: INFO: Created: latency-svc-qbwvp
Aug 29 16:43:09.745: INFO: Got endpoints: latency-svc-4wp5q [749.567754ms]
Aug 29 16:43:09.768: INFO: Created: latency-svc-4hqsb
Aug 29 16:43:09.798: INFO: Got endpoints: latency-svc-lcf8x [753.467677ms]
Aug 29 16:43:09.818: INFO: Created: latency-svc-svg8j
Aug 29 16:43:09.845: INFO: Got endpoints: latency-svc-g7sgs [748.623334ms]
Aug 29 16:43:09.862: INFO: Created: latency-svc-mk8lm
Aug 29 16:43:09.897: INFO: Got endpoints: latency-svc-rhn4h [748.490842ms]
Aug 29 16:43:09.915: INFO: Created: latency-svc-nvrgp
Aug 29 16:43:09.952: INFO: Got endpoints: latency-svc-b6nbh [753.472626ms]
Aug 29 16:43:09.965: INFO: Created: latency-svc-t54dq
Aug 29 16:43:10.000: INFO: Got endpoints: latency-svc-m7hn5 [751.575005ms]
Aug 29 16:43:10.028: INFO: Created: latency-svc-xp67r
Aug 29 16:43:10.047: INFO: Got endpoints: latency-svc-65925 [750.287982ms]
Aug 29 16:43:10.065: INFO: Created: latency-svc-fjm87
Aug 29 16:43:10.098: INFO: Got endpoints: latency-svc-wtqnm [750.878798ms]
Aug 29 16:43:10.114: INFO: Created: latency-svc-jhlps
Aug 29 16:43:10.147: INFO: Got endpoints: latency-svc-vr7dl [752.092681ms]
Aug 29 16:43:10.171: INFO: Created: latency-svc-jl4cz
Aug 29 16:43:10.203: INFO: Got endpoints: latency-svc-2r722 [754.174204ms]
Aug 29 16:43:10.225: INFO: Created: latency-svc-ntvzx
Aug 29 16:43:10.244: INFO: Got endpoints: latency-svc-qr2tj [748.423101ms]
Aug 29 16:43:10.263: INFO: Created: latency-svc-jqn45
Aug 29 16:43:10.294: INFO: Got endpoints: latency-svc-6j8qq [748.483861ms]
Aug 29 16:43:10.347: INFO: Got endpoints: latency-svc-trzgw [751.282912ms]
Aug 29 16:43:10.397: INFO: Got endpoints: latency-svc-q7nfj [753.156113ms]
Aug 29 16:43:10.448: INFO: Got endpoints: latency-svc-qbwvp [752.144141ms]
Aug 29 16:43:10.508: INFO: Got endpoints: latency-svc-4hqsb [762.1574ms]
Aug 29 16:43:10.553: INFO: Got endpoints: latency-svc-svg8j [754.258454ms]
Aug 29 16:43:10.597: INFO: Got endpoints: latency-svc-mk8lm [752.356134ms]
Aug 29 16:43:10.647: INFO: Got endpoints: latency-svc-nvrgp [750.356802ms]
Aug 29 16:43:10.701: INFO: Got endpoints: latency-svc-t54dq [749.090858ms]
Aug 29 16:43:10.746: INFO: Got endpoints: latency-svc-xp67r [746.044895ms]
Aug 29 16:43:10.797: INFO: Got endpoints: latency-svc-fjm87 [748.201939ms]
Aug 29 16:43:10.854: INFO: Got endpoints: latency-svc-jhlps [755.288965ms]
Aug 29 16:43:10.897: INFO: Got endpoints: latency-svc-jl4cz [749.22146ms]
Aug 29 16:43:10.951: INFO: Got endpoints: latency-svc-ntvzx [748.503911ms]
Aug 29 16:43:10.997: INFO: Got endpoints: latency-svc-jqn45 [752.196272ms]
Aug 29 16:43:10.997: INFO: Latencies: [70.707607ms 72.754848ms 84.941871ms 115.586263ms 119.057251ms 127.373281ms 135.378508ms 140.512744ms 164.009478ms 165.376043ms 167.96724ms 182.566559ms 187.755484ms 188.044078ms 188.513013ms 190.199292ms 193.66611ms 202.582266ms 211.615084ms 214.235152ms 214.593226ms 215.336404ms 215.89683ms 217.68231ms 218.116714ms 222.454022ms 224.052639ms 224.151999ms 225.489585ms 232.536991ms 240.426746ms 241.828792ms 244.510591ms 251.269095ms 254.772872ms 267.396138ms 282.31374ms 299.472977ms 302.797263ms 307.831808ms 312.572747ms 313.716552ms 321.93568ms 341.165228ms 344.336533ms 347.109603ms 349.268576ms 363.155837ms 384.368127ms 387.234958ms 390.265642ms 393.978861ms 397.668981ms 417.483956ms 417.706359ms 419.507787ms 445.296828ms 447.810924ms 458.169176ms 460.725114ms 470.38509ms 483.962237ms 488.648137ms 495.726014ms 497.543612ms 503.409987ms 516.176616ms 524.574007ms 528.045854ms 541.563101ms 551.998835ms 564.108874ms 573.174154ms 575.624271ms 606.327534ms 607.072442ms 610.79469ms 620.908321ms 640.911118ms 644.190522ms 658.425856ms 670.292546ms 675.769096ms 695.017514ms 705.133873ms 711.803327ms 711.887797ms 712.18253ms 716.692728ms 727.076611ms 728.255603ms 728.608838ms 733.071466ms 736.467174ms 738.543136ms 738.79115ms 738.847428ms 738.995019ms 739.594786ms 739.726538ms 740.127803ms 740.703589ms 742.088614ms 742.869604ms 743.357667ms 744.279258ms 745.204128ms 745.575111ms 746.044895ms 746.143557ms 746.572331ms 746.657735ms 746.775816ms 746.922007ms 747.076807ms 747.294929ms 747.545632ms 747.568523ms 747.655603ms 748.12542ms 748.185081ms 748.201939ms 748.299443ms 748.423101ms 748.474062ms 748.483861ms 748.490842ms 748.503911ms 748.536903ms 748.623334ms 748.664444ms 748.714396ms 748.848048ms 749.090858ms 749.22146ms 749.451774ms 749.533314ms 749.567754ms 749.672216ms 750.03305ms 750.05419ms 750.171233ms 750.287632ms 750.287982ms 750.356802ms 750.440623ms 750.606405ms 750.676158ms 750.878798ms 751.023671ms 751.179193ms 751.282912ms 751.291363ms 751.343735ms 751.377003ms 751.575005ms 751.99735ms 752.092681ms 752.098052ms 752.100084ms 752.144141ms 752.196272ms 752.356134ms 752.503568ms 752.583318ms 752.639738ms 752.686488ms 752.7629ms 753.156113ms 753.467677ms 753.472626ms 754.174204ms 754.258454ms 755.288965ms 755.425699ms 756.906135ms 757.719454ms 758.913436ms 762.1574ms 765.575179ms 767.676962ms 769.403072ms 790.816113ms 816.35165ms 952.9209ms 1.044186258s 1.079606202s 1.177592303s 1.218587338s 1.301208293s 1.346033289s 1.423691681s 1.462761034s 1.542095184s 1.592536111s 1.627448669s 1.768280906s 1.921889661s 2.033276829s 2.197188856s]
Aug 29 16:43:10.998: INFO: 50 %ile: 740.127803ms
Aug 29 16:43:10.998: INFO: 90 %ile: 767.676962ms
Aug 29 16:43:10.998: INFO: 99 %ile: 2.033276829s
Aug 29 16:43:10.998: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Aug 29 16:43:10.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6056" for this suite.

• [SLOW TEST:11.596 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":272,"skipped":4717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:43:11.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:43:11.523: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2de7d8f5-0d3c-4465-bb49-9fda1336fbdf" in namespace "projected-8637" to be "Succeeded or Failed"
Aug 29 16:43:11.532: INFO: Pod "downwardapi-volume-2de7d8f5-0d3c-4465-bb49-9fda1336fbdf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.540203ms
Aug 29 16:43:13.540: INFO: Pod "downwardapi-volume-2de7d8f5-0d3c-4465-bb49-9fda1336fbdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017097596s
Aug 29 16:43:15.550: INFO: Pod "downwardapi-volume-2de7d8f5-0d3c-4465-bb49-9fda1336fbdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026858912s
STEP: Saw pod success
Aug 29 16:43:15.550: INFO: Pod "downwardapi-volume-2de7d8f5-0d3c-4465-bb49-9fda1336fbdf" satisfied condition "Succeeded or Failed"
Aug 29 16:43:15.558: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-2de7d8f5-0d3c-4465-bb49-9fda1336fbdf container client-container: <nil>
STEP: delete the pod
Aug 29 16:43:15.596: INFO: Waiting for pod downwardapi-volume-2de7d8f5-0d3c-4465-bb49-9fda1336fbdf to disappear
Aug 29 16:43:15.602: INFO: Pod downwardapi-volume-2de7d8f5-0d3c-4465-bb49-9fda1336fbdf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 16:43:15.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8637" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":273,"skipped":4760,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:43:15.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 29 16:43:31.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7755" for this suite.

• [SLOW TEST:16.336 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":274,"skipped":4773,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:43:31.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:43:32.416: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:43:35.483: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:43:35.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:43:38.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1795" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:7.121 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":275,"skipped":4782,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:43:39.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:43:39.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5998df7c-d9fa-40cc-8c7a-25f3377c22c4" in namespace "downward-api-8543" to be "Succeeded or Failed"
Aug 29 16:43:39.234: INFO: Pod "downwardapi-volume-5998df7c-d9fa-40cc-8c7a-25f3377c22c4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.428676ms
Aug 29 16:43:41.247: INFO: Pod "downwardapi-volume-5998df7c-d9fa-40cc-8c7a-25f3377c22c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025628053s
Aug 29 16:43:43.256: INFO: Pod "downwardapi-volume-5998df7c-d9fa-40cc-8c7a-25f3377c22c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034902173s
STEP: Saw pod success
Aug 29 16:43:43.256: INFO: Pod "downwardapi-volume-5998df7c-d9fa-40cc-8c7a-25f3377c22c4" satisfied condition "Succeeded or Failed"
Aug 29 16:43:43.262: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-5998df7c-d9fa-40cc-8c7a-25f3377c22c4 container client-container: <nil>
STEP: delete the pod
Aug 29 16:43:43.299: INFO: Waiting for pod downwardapi-volume-5998df7c-d9fa-40cc-8c7a-25f3377c22c4 to disappear
Aug 29 16:43:43.307: INFO: Pod downwardapi-volume-5998df7c-d9fa-40cc-8c7a-25f3377c22c4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 16:43:43.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8543" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":276,"skipped":4787,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:43:43.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 29 16:43:43.431: INFO: Waiting up to 5m0s for pod "pod-3e940d41-3f98-4e76-848d-fe97e50d2f57" in namespace "emptydir-4584" to be "Succeeded or Failed"
Aug 29 16:43:43.455: INFO: Pod "pod-3e940d41-3f98-4e76-848d-fe97e50d2f57": Phase="Pending", Reason="", readiness=false. Elapsed: 23.487374ms
Aug 29 16:43:45.468: INFO: Pod "pod-3e940d41-3f98-4e76-848d-fe97e50d2f57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036928738s
Aug 29 16:43:47.479: INFO: Pod "pod-3e940d41-3f98-4e76-848d-fe97e50d2f57": Phase="Running", Reason="", readiness=false. Elapsed: 4.047072552s
Aug 29 16:43:49.489: INFO: Pod "pod-3e940d41-3f98-4e76-848d-fe97e50d2f57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057801794s
STEP: Saw pod success
Aug 29 16:43:49.489: INFO: Pod "pod-3e940d41-3f98-4e76-848d-fe97e50d2f57" satisfied condition "Succeeded or Failed"
Aug 29 16:43:49.496: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-3e940d41-3f98-4e76-848d-fe97e50d2f57 container test-container: <nil>
STEP: delete the pod
Aug 29 16:43:49.526: INFO: Waiting for pod pod-3e940d41-3f98-4e76-848d-fe97e50d2f57 to disappear
Aug 29 16:43:49.541: INFO: Pod pod-3e940d41-3f98-4e76-848d-fe97e50d2f57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 16:43:49.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4584" for this suite.

• [SLOW TEST:6.223 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":277,"skipped":4798,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:43:49.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:43:49.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a23c82f3-1905-45fb-b08c-dcf733253349" in namespace "downward-api-5229" to be "Succeeded or Failed"
Aug 29 16:43:49.654: INFO: Pod "downwardapi-volume-a23c82f3-1905-45fb-b08c-dcf733253349": Phase="Pending", Reason="", readiness=false. Elapsed: 9.610344ms
Aug 29 16:43:51.759: INFO: Pod "downwardapi-volume-a23c82f3-1905-45fb-b08c-dcf733253349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115538074s
Aug 29 16:43:53.767: INFO: Pod "downwardapi-volume-a23c82f3-1905-45fb-b08c-dcf733253349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.123167418s
STEP: Saw pod success
Aug 29 16:43:53.767: INFO: Pod "downwardapi-volume-a23c82f3-1905-45fb-b08c-dcf733253349" satisfied condition "Succeeded or Failed"
Aug 29 16:43:53.772: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod downwardapi-volume-a23c82f3-1905-45fb-b08c-dcf733253349 container client-container: <nil>
STEP: delete the pod
Aug 29 16:43:53.810: INFO: Waiting for pod downwardapi-volume-a23c82f3-1905-45fb-b08c-dcf733253349 to disappear
Aug 29 16:43:53.817: INFO: Pod downwardapi-volume-a23c82f3-1905-45fb-b08c-dcf733253349 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 16:43:53.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5229" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":278,"skipped":4837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:43:53.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:43:53.884: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 29 16:43:53.910: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 29 16:43:58.923: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 29 16:43:58.924: INFO: Creating deployment "test-rolling-update-deployment"
Aug 29 16:43:58.936: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 29 16:43:58.959: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 29 16:44:00.987: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 29 16:44:01.002: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 16:44:01.021: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8763  e52a00ca-b7b3-41ba-a0b2-a4f762b87c2a 40107 1 2022-08-29 16:43:58 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-08-29 16:43:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:44:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034ed138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-08-29 16:43:58 +0000 UTC,LastTransitionTime:2022-08-29 16:43:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67c8f74c6c" has successfully progressed.,LastUpdateTime:2022-08-29 16:44:00 +0000 UTC,LastTransitionTime:2022-08-29 16:43:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 16:44:01.027: INFO: New ReplicaSet "test-rolling-update-deployment-67c8f74c6c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c  deployment-8763  c1fd102b-00dc-4a33-92b3-d5c4fc343d21 40096 1 2022-08-29 16:43:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e52a00ca-b7b3-41ba-a0b2-a4f762b87c2a 0xc0023e9e27 0xc0023e9e28}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:43:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e52a00ca-b7b3-41ba-a0b2-a4f762b87c2a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:44:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67c8f74c6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023e9ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:44:01.028: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 29 16:44:01.028: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8763  a9dfe44c-f982-4513-b77b-ac1589b65c3d 40106 2 2022-08-29 16:43:53 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e52a00ca-b7b3-41ba-a0b2-a4f762b87c2a 0xc0023e9cf7 0xc0023e9cf8}] []  [{e2e.test Update apps/v1 2022-08-29 16:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:44:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e52a00ca-b7b3-41ba-a0b2-a4f762b87c2a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:44:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0023e9db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:44:01.036: INFO: Pod "test-rolling-update-deployment-67c8f74c6c-htjrt" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c-htjrt test-rolling-update-deployment-67c8f74c6c- deployment-8763  9abcd28d-bd9d-4f5c-bc4e-edf49519bde5 40095 0 2022-08-29 16:43:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[cni.projectcalico.org/containerID:82d8e625a6bd32faeb04a2469c96967ef4697f58352f45b97a8196fc027c6cff cni.projectcalico.org/podIP:172.25.2.24/32 cni.projectcalico.org/podIPs:172.25.2.24/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67c8f74c6c c1fd102b-00dc-4a33-92b3-d5c4fc343d21 0xc00163a6d7 0xc00163a6d8}] []  [{kube-controller-manager Update v1 2022-08-29 16:43:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1fd102b-00dc-4a33-92b3-d5c4fc343d21\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:43:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:44:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kq5b7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kq5b7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:43:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:44:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:44:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:43:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:172.25.2.24,StartTime:2022-08-29 16:43:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:43:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://b1ed3083b024c227d453a891840e84c6c0b051ba09695c0868c821ca785be5fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 29 16:44:01.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8763" for this suite.

• [SLOW TEST:7.219 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":279,"skipped":4875,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:44:01.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Aug 29 16:44:01.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9194" for this suite.
STEP: Destroying namespace "nspatchtest-4bd1e4ef-3b97-4d29-a4fd-1b49511ec405-2398" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":280,"skipped":4878,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:44:01.179: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 29 16:44:01.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:44:01.322: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:44:02.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 29 16:44:02.345: INFO: Node ip-172-31-16-214.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:44:03.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 16:44:03.345: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 29 16:44:03.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 16:44:03.406: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:44:04.439: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 16:44:04.444: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:44:05.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 16:44:05.424: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:44:06.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 16:44:06.429: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:44:07.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 16:44:07.443: INFO: Node ip-172-31-26-197.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Aug 29 16:44:08.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 16:44:08.997: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4014, will wait for the garbage collector to delete the pods
Aug 29 16:44:09.079: INFO: Deleting DaemonSet.extensions daemon-set took: 14.538897ms
Aug 29 16:44:09.180: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.123743ms
Aug 29 16:44:11.293: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 16:44:11.293: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 16:44:11.299: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40274"},"items":null}

Aug 29 16:44:11.303: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40274"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Aug 29 16:44:11.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4014" for this suite.

• [SLOW TEST:10.174 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":281,"skipped":4894,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:44:11.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 29 16:44:18.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2303" for this suite.

• [SLOW TEST:7.129 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":282,"skipped":4911,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:44:18.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 29 16:44:18.590: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 29 16:44:18.602: INFO: starting watch
STEP: patching
STEP: updating
Aug 29 16:44:18.634: INFO: waiting for watch events with expected annotations
Aug 29 16:44:18.634: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Aug 29 16:44:18.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3819" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":283,"skipped":4960,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:44:18.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:44:18.895: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 29 16:44:19.965: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 29 16:44:19.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3754" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":284,"skipped":4970,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:44:19.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-xdjg
STEP: Creating a pod to test atomic-volume-subpath
Aug 29 16:44:20.092: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xdjg" in namespace "subpath-4611" to be "Succeeded or Failed"
Aug 29 16:44:20.103: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.278602ms
Aug 29 16:44:22.120: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=true. Elapsed: 2.028470753s
Aug 29 16:44:24.132: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=true. Elapsed: 4.040362411s
Aug 29 16:44:26.143: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=true. Elapsed: 6.05061061s
Aug 29 16:44:28.153: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=true. Elapsed: 8.060517753s
Aug 29 16:44:30.161: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=true. Elapsed: 10.069273712s
Aug 29 16:44:32.541: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=true. Elapsed: 12.449311819s
Aug 29 16:44:34.758: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=true. Elapsed: 14.666066369s
Aug 29 16:44:36.786: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=true. Elapsed: 16.694064342s
Aug 29 16:44:38.794: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=true. Elapsed: 18.702061927s
Aug 29 16:44:40.808: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=true. Elapsed: 20.715689352s
Aug 29 16:44:42.823: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Running", Reason="", readiness=false. Elapsed: 22.730547836s
Aug 29 16:44:44.838: INFO: Pod "pod-subpath-test-configmap-xdjg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.74625955s
STEP: Saw pod success
Aug 29 16:44:44.838: INFO: Pod "pod-subpath-test-configmap-xdjg" satisfied condition "Succeeded or Failed"
Aug 29 16:44:44.845: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-subpath-test-configmap-xdjg container test-container-subpath-configmap-xdjg: <nil>
STEP: delete the pod
Aug 29 16:44:44.888: INFO: Waiting for pod pod-subpath-test-configmap-xdjg to disappear
Aug 29 16:44:44.895: INFO: Pod pod-subpath-test-configmap-xdjg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xdjg
Aug 29 16:44:44.895: INFO: Deleting pod "pod-subpath-test-configmap-xdjg" in namespace "subpath-4611"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 29 16:44:44.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4611" for this suite.

• [SLOW TEST:24.935 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":285,"skipped":4986,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:44:44.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-afec9f91-6195-45b0-85d8-7526cbcf42cd
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 29 16:44:44.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7261" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":286,"skipped":4998,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:44:44.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Aug 29 16:44:45.059: INFO: Waiting up to 5m0s for pod "client-containers-79b5549a-71ec-4370-a213-b5fefdb0e5ea" in namespace "containers-1398" to be "Succeeded or Failed"
Aug 29 16:44:45.082: INFO: Pod "client-containers-79b5549a-71ec-4370-a213-b5fefdb0e5ea": Phase="Pending", Reason="", readiness=false. Elapsed: 22.714255ms
Aug 29 16:44:47.095: INFO: Pod "client-containers-79b5549a-71ec-4370-a213-b5fefdb0e5ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.035640695s
Aug 29 16:44:49.104: INFO: Pod "client-containers-79b5549a-71ec-4370-a213-b5fefdb0e5ea": Phase="Running", Reason="", readiness=false. Elapsed: 4.044337858s
Aug 29 16:44:51.115: INFO: Pod "client-containers-79b5549a-71ec-4370-a213-b5fefdb0e5ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054994131s
STEP: Saw pod success
Aug 29 16:44:51.115: INFO: Pod "client-containers-79b5549a-71ec-4370-a213-b5fefdb0e5ea" satisfied condition "Succeeded or Failed"
Aug 29 16:44:51.120: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod client-containers-79b5549a-71ec-4370-a213-b5fefdb0e5ea container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:44:51.155: INFO: Waiting for pod client-containers-79b5549a-71ec-4370-a213-b5fefdb0e5ea to disappear
Aug 29 16:44:51.166: INFO: Pod client-containers-79b5549a-71ec-4370-a213-b5fefdb0e5ea no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Aug 29 16:44:51.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1398" for this suite.

• [SLOW TEST:6.194 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":287,"skipped":5012,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:44:51.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:44:51.862: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:44:54.911: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:44:54.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7926-crds.webhook.example.com via the AdmissionRegistration API
Aug 29 16:44:55.809: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:44:58.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3947" for this suite.
STEP: Destroying namespace "webhook-3947-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.518 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":288,"skipped":5036,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:44:58.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Aug 29 16:44:58.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9287 create -f -'
Aug 29 16:45:00.094: INFO: stderr: ""
Aug 29 16:45:00.094: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 29 16:45:01.107: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 16:45:01.107: INFO: Found 0 / 1
Aug 29 16:45:02.110: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 16:45:02.110: INFO: Found 1 / 1
Aug 29 16:45:02.110: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 29 16:45:02.120: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 16:45:02.120: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 29 16:45:02.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9287 patch pod agnhost-primary-pnc28 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 29 16:45:02.562: INFO: stderr: ""
Aug 29 16:45:02.562: INFO: stdout: "pod/agnhost-primary-pnc28 patched\n"
STEP: checking annotations
Aug 29 16:45:02.580: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 16:45:02.580: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:45:02.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9287" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":289,"skipped":5042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:02.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Aug 29 16:45:02.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5126" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":290,"skipped":5071,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:02.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-5ftgv in namespace proxy-4861
I0829 16:45:02.806853      20 runners.go:193] Created replication controller with name: proxy-service-5ftgv, namespace: proxy-4861, replica count: 1
I0829 16:45:03.858648      20 runners.go:193] proxy-service-5ftgv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0829 16:45:04.859202      20 runners.go:193] proxy-service-5ftgv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:45:04.870: INFO: setup took 2.102852248s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 29 16:45:04.899: INFO: (0) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 28.268873ms)
Aug 29 16:45:04.909: INFO: (0) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 38.773975ms)
Aug 29 16:45:04.909: INFO: (0) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 38.980959ms)
Aug 29 16:45:04.909: INFO: (0) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 38.956188ms)
Aug 29 16:45:04.909: INFO: (0) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 38.878347ms)
Aug 29 16:45:04.909: INFO: (0) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 38.947788ms)
Aug 29 16:45:04.909: INFO: (0) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 39.173453ms)
Aug 29 16:45:04.909: INFO: (0) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 38.802746ms)
Aug 29 16:45:04.912: INFO: (0) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 41.551564ms)
Aug 29 16:45:04.933: INFO: (0) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 61.99676ms)
Aug 29 16:45:04.939: INFO: (0) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 68.454873ms)
Aug 29 16:45:04.939: INFO: (0) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 68.538134ms)
Aug 29 16:45:04.942: INFO: (0) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 71.161521ms)
Aug 29 16:45:04.947: INFO: (0) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 75.913923ms)
Aug 29 16:45:04.948: INFO: (0) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 78.095541ms)
Aug 29 16:45:04.949: INFO: (0) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 78.59354ms)
Aug 29 16:45:04.968: INFO: (1) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 16.007649ms)
Aug 29 16:45:04.968: INFO: (1) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 16.67313ms)
Aug 29 16:45:04.968: INFO: (1) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 16.993006ms)
Aug 29 16:45:04.971: INFO: (1) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 19.054422ms)
Aug 29 16:45:04.971: INFO: (1) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 19.663833ms)
Aug 29 16:45:04.971: INFO: (1) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 19.430828ms)
Aug 29 16:45:04.971: INFO: (1) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 19.640243ms)
Aug 29 16:45:04.974: INFO: (1) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 21.763379ms)
Aug 29 16:45:04.974: INFO: (1) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 22.031974ms)
Aug 29 16:45:04.977: INFO: (1) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 25.013286ms)
Aug 29 16:45:04.977: INFO: (1) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 25.021046ms)
Aug 29 16:45:04.977: INFO: (1) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 25.089637ms)
Aug 29 16:45:04.977: INFO: (1) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 25.141768ms)
Aug 29 16:45:04.977: INFO: (1) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 25.395533ms)
Aug 29 16:45:04.977: INFO: (1) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 25.082787ms)
Aug 29 16:45:04.979: INFO: (1) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 26.97739ms)
Aug 29 16:45:04.991: INFO: (2) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 11.263066ms)
Aug 29 16:45:05.006: INFO: (2) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 23.722274ms)
Aug 29 16:45:05.006: INFO: (2) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 23.238184ms)
Aug 29 16:45:05.006: INFO: (2) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 21.071357ms)
Aug 29 16:45:05.006: INFO: (2) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 22.504782ms)
Aug 29 16:45:05.006: INFO: (2) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 22.96473ms)
Aug 29 16:45:05.006: INFO: (2) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 23.593651ms)
Aug 29 16:45:05.006: INFO: (2) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 25.870971ms)
Aug 29 16:45:05.006: INFO: (2) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 21.079377ms)
Aug 29 16:45:05.007: INFO: (2) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 26.634254ms)
Aug 29 16:45:05.007: INFO: (2) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 23.573921ms)
Aug 29 16:45:05.008: INFO: (2) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 23.173334ms)
Aug 29 16:45:05.008: INFO: (2) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 23.242265ms)
Aug 29 16:45:05.009: INFO: (2) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 24.451287ms)
Aug 29 16:45:05.009: INFO: (2) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 24.445286ms)
Aug 29 16:45:05.011: INFO: (2) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 25.951122ms)
Aug 29 16:45:05.042: INFO: (3) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 30.886348ms)
Aug 29 16:45:05.043: INFO: (3) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 28.638358ms)
Aug 29 16:45:05.043: INFO: (3) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 30.618913ms)
Aug 29 16:45:05.043: INFO: (3) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 28.370934ms)
Aug 29 16:45:05.043: INFO: (3) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 29.8706ms)
Aug 29 16:45:05.043: INFO: (3) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 27.289366ms)
Aug 29 16:45:05.043: INFO: (3) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 29.233069ms)
Aug 29 16:45:05.043: INFO: (3) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 26.42477ms)
Aug 29 16:45:05.047: INFO: (3) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 29.593526ms)
Aug 29 16:45:05.047: INFO: (3) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 27.195094ms)
Aug 29 16:45:05.047: INFO: (3) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 31.925247ms)
Aug 29 16:45:05.047: INFO: (3) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 28.813812ms)
Aug 29 16:45:05.047: INFO: (3) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 29.989442ms)
Aug 29 16:45:05.052: INFO: (3) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 31.776094ms)
Aug 29 16:45:05.052: INFO: (3) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 32.771751ms)
Aug 29 16:45:05.052: INFO: (3) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 32.447176ms)
Aug 29 16:45:05.088: INFO: (4) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 33.111136ms)
Aug 29 16:45:05.088: INFO: (4) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 34.068013ms)
Aug 29 16:45:05.088: INFO: (4) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 34.753965ms)
Aug 29 16:45:05.088: INFO: (4) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 32.847252ms)
Aug 29 16:45:05.088: INFO: (4) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 34.989099ms)
Aug 29 16:45:05.088: INFO: (4) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 35.946815ms)
Aug 29 16:45:05.088: INFO: (4) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 33.574085ms)
Aug 29 16:45:05.088: INFO: (4) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 36.329092ms)
Aug 29 16:45:05.088: INFO: (4) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 35.777232ms)
Aug 29 16:45:05.089: INFO: (4) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 34.4798ms)
Aug 29 16:45:05.089: INFO: (4) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 34.691114ms)
Aug 29 16:45:05.089: INFO: (4) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 33.32424ms)
Aug 29 16:45:05.092: INFO: (4) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 37.278079ms)
Aug 29 16:45:05.097: INFO: (4) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 43.826483ms)
Aug 29 16:45:05.097: INFO: (4) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 44.449034ms)
Aug 29 16:45:05.097: INFO: (4) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 42.612331ms)
Aug 29 16:45:05.140: INFO: (5) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 39.808884ms)
Aug 29 16:45:05.140: INFO: (5) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 39.814604ms)
Aug 29 16:45:05.140: INFO: (5) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 39.853744ms)
Aug 29 16:45:05.140: INFO: (5) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 40.284361ms)
Aug 29 16:45:05.140: INFO: (5) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 39.831813ms)
Aug 29 16:45:05.140: INFO: (5) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 41.064635ms)
Aug 29 16:45:05.140: INFO: (5) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 40.22214ms)
Aug 29 16:45:05.140: INFO: (5) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 38.623903ms)
Aug 29 16:45:05.141: INFO: (5) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 39.488118ms)
Aug 29 16:45:05.141: INFO: (5) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 41.387321ms)
Aug 29 16:45:05.143: INFO: (5) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 41.050765ms)
Aug 29 16:45:05.145: INFO: (5) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 42.808466ms)
Aug 29 16:45:05.145: INFO: (5) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 42.888107ms)
Aug 29 16:45:05.146: INFO: (5) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 46.453439ms)
Aug 29 16:45:05.147: INFO: (5) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 44.259181ms)
Aug 29 16:45:05.147: INFO: (5) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 46.50883ms)
Aug 29 16:45:05.168: INFO: (6) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 18.433681ms)
Aug 29 16:45:05.168: INFO: (6) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 21.581915ms)
Aug 29 16:45:05.168: INFO: (6) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 18.862798ms)
Aug 29 16:45:05.169: INFO: (6) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 18.242037ms)
Aug 29 16:45:05.169: INFO: (6) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 19.962737ms)
Aug 29 16:45:05.169: INFO: (6) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 19.241275ms)
Aug 29 16:45:05.171: INFO: (6) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 22.828567ms)
Aug 29 16:45:05.173: INFO: (6) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 25.976412ms)
Aug 29 16:45:05.174: INFO: (6) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 25.711288ms)
Aug 29 16:45:05.178: INFO: (6) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 30.754895ms)
Aug 29 16:45:05.178: INFO: (6) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 30.532611ms)
Aug 29 16:45:05.181: INFO: (6) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 33.432302ms)
Aug 29 16:45:05.181: INFO: (6) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 33.573044ms)
Aug 29 16:45:05.181: INFO: (6) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 31.05029ms)
Aug 29 16:45:05.181: INFO: (6) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 34.747075ms)
Aug 29 16:45:05.188: INFO: (6) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 40.992614ms)
Aug 29 16:45:05.231: INFO: (7) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 39.726032ms)
Aug 29 16:45:05.241: INFO: (7) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 49.958749ms)
Aug 29 16:45:05.242: INFO: (7) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 53.332038ms)
Aug 29 16:45:05.242: INFO: (7) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 52.946101ms)
Aug 29 16:45:05.242: INFO: (7) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 52.388272ms)
Aug 29 16:45:05.242: INFO: (7) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 51.292392ms)
Aug 29 16:45:05.242: INFO: (7) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 53.501842ms)
Aug 29 16:45:05.243: INFO: (7) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 51.432845ms)
Aug 29 16:45:05.243: INFO: (7) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 54.750253ms)
Aug 29 16:45:05.243: INFO: (7) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 53.025293ms)
Aug 29 16:45:05.243: INFO: (7) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 51.7307ms)
Aug 29 16:45:05.243: INFO: (7) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 51.864013ms)
Aug 29 16:45:05.243: INFO: (7) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 51.819912ms)
Aug 29 16:45:05.243: INFO: (7) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 52.710487ms)
Aug 29 16:45:05.243: INFO: (7) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 55.270632ms)
Aug 29 16:45:05.244: INFO: (7) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 52.557334ms)
Aug 29 16:45:05.264: INFO: (8) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 19.084903ms)
Aug 29 16:45:05.264: INFO: (8) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 15.914408ms)
Aug 29 16:45:05.264: INFO: (8) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 16.679131ms)
Aug 29 16:45:05.266: INFO: (8) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 19.066573ms)
Aug 29 16:45:05.271: INFO: (8) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 21.849291ms)
Aug 29 16:45:05.271: INFO: (8) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 23.598982ms)
Aug 29 16:45:05.275: INFO: (8) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 26.903299ms)
Aug 29 16:45:05.275: INFO: (8) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 26.9996ms)
Aug 29 16:45:05.275: INFO: (8) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 26.352039ms)
Aug 29 16:45:05.276: INFO: (8) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 26.688515ms)
Aug 29 16:45:05.281: INFO: (8) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 31.497959ms)
Aug 29 16:45:05.281: INFO: (8) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 31.496319ms)
Aug 29 16:45:05.281: INFO: (8) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 31.57041ms)
Aug 29 16:45:05.281: INFO: (8) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 31.678322ms)
Aug 29 16:45:05.284: INFO: (8) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 34.922848ms)
Aug 29 16:45:05.284: INFO: (8) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 35.282794ms)
Aug 29 16:45:05.311: INFO: (9) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 26.598113ms)
Aug 29 16:45:05.311: INFO: (9) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 26.892138ms)
Aug 29 16:45:05.311: INFO: (9) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 25.934081ms)
Aug 29 16:45:05.312: INFO: (9) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 26.189766ms)
Aug 29 16:45:05.312: INFO: (9) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 27.60234ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 29.615725ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 30.170115ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 29.029056ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 30.523921ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 30.161655ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 31.143242ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 30.659943ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 30.574133ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 30.414689ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 29.562644ms)
Aug 29 16:45:05.315: INFO: (9) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 30.195846ms)
Aug 29 16:45:05.333: INFO: (10) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 14.291099ms)
Aug 29 16:45:05.337: INFO: (10) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 17.685169ms)
Aug 29 16:45:05.343: INFO: (10) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 22.91797ms)
Aug 29 16:45:05.349: INFO: (10) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 29.296411ms)
Aug 29 16:45:05.350: INFO: (10) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 30.272007ms)
Aug 29 16:45:05.352: INFO: (10) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 32.378034ms)
Aug 29 16:45:05.355: INFO: (10) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 34.594862ms)
Aug 29 16:45:05.360: INFO: (10) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 39.834784ms)
Aug 29 16:45:05.361: INFO: (10) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 40.689808ms)
Aug 29 16:45:05.361: INFO: (10) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 40.954353ms)
Aug 29 16:45:05.362: INFO: (10) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 40.994894ms)
Aug 29 16:45:05.362: INFO: (10) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 41.083555ms)
Aug 29 16:45:05.362: INFO: (10) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 40.75182ms)
Aug 29 16:45:05.362: INFO: (10) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 40.984224ms)
Aug 29 16:45:05.362: INFO: (10) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 41.397641ms)
Aug 29 16:45:05.363: INFO: (10) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 43.085481ms)
Aug 29 16:45:05.385: INFO: (11) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 18.677115ms)
Aug 29 16:45:05.385: INFO: (11) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 19.51582ms)
Aug 29 16:45:05.385: INFO: (11) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 20.777311ms)
Aug 29 16:45:05.385: INFO: (11) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 21.411942ms)
Aug 29 16:45:05.385: INFO: (11) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 20.460796ms)
Aug 29 16:45:05.385: INFO: (11) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 21.127087ms)
Aug 29 16:45:05.387: INFO: (11) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 21.144268ms)
Aug 29 16:45:05.389: INFO: (11) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 25.394472ms)
Aug 29 16:45:05.390: INFO: (11) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 23.007991ms)
Aug 29 16:45:05.390: INFO: (11) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 22.756596ms)
Aug 29 16:45:05.390: INFO: (11) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 22.352569ms)
Aug 29 16:45:05.390: INFO: (11) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 22.40631ms)
Aug 29 16:45:05.390: INFO: (11) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 23.51918ms)
Aug 29 16:45:05.390: INFO: (11) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 22.632514ms)
Aug 29 16:45:05.390: INFO: (11) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 22.514632ms)
Aug 29 16:45:05.390: INFO: (11) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 24.537167ms)
Aug 29 16:45:05.415: INFO: (12) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 21.81939ms)
Aug 29 16:45:05.415: INFO: (12) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 24.400515ms)
Aug 29 16:45:05.415: INFO: (12) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 21.017787ms)
Aug 29 16:45:05.415: INFO: (12) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 23.824635ms)
Aug 29 16:45:05.415: INFO: (12) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 21.830851ms)
Aug 29 16:45:05.415: INFO: (12) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 22.832348ms)
Aug 29 16:45:05.416: INFO: (12) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 23.5067ms)
Aug 29 16:45:05.416: INFO: (12) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 19.572361ms)
Aug 29 16:45:05.416: INFO: (12) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 18.811037ms)
Aug 29 16:45:05.416: INFO: (12) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 19.992038ms)
Aug 29 16:45:05.416: INFO: (12) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 19.478589ms)
Aug 29 16:45:05.417: INFO: (12) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 21.159258ms)
Aug 29 16:45:05.430: INFO: (12) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 33.996132ms)
Aug 29 16:45:05.435: INFO: (12) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 37.93374ms)
Aug 29 16:45:05.435: INFO: (12) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 40.654568ms)
Aug 29 16:45:05.435: INFO: (12) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 38.853617ms)
Aug 29 16:45:05.450: INFO: (13) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 14.487693ms)
Aug 29 16:45:05.455: INFO: (13) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 18.889219ms)
Aug 29 16:45:05.455: INFO: (13) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 18.613044ms)
Aug 29 16:45:05.455: INFO: (13) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 18.815027ms)
Aug 29 16:45:05.458: INFO: (13) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 21.771509ms)
Aug 29 16:45:05.460: INFO: (13) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 22.460171ms)
Aug 29 16:45:05.460: INFO: (13) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 23.178324ms)
Aug 29 16:45:05.460: INFO: (13) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 23.54197ms)
Aug 29 16:45:05.460: INFO: (13) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 23.815044ms)
Aug 29 16:45:05.461: INFO: (13) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 25.2563ms)
Aug 29 16:45:05.461: INFO: (13) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 22.815217ms)
Aug 29 16:45:05.461: INFO: (13) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 22.268687ms)
Aug 29 16:45:05.461: INFO: (13) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 24.67736ms)
Aug 29 16:45:05.463: INFO: (13) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 24.274772ms)
Aug 29 16:45:05.463: INFO: (13) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 27.050481ms)
Aug 29 16:45:05.463: INFO: (13) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 25.28755ms)
Aug 29 16:45:05.479: INFO: (14) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 15.320807ms)
Aug 29 16:45:05.479: INFO: (14) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 14.212968ms)
Aug 29 16:45:05.479: INFO: (14) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 14.796478ms)
Aug 29 16:45:05.481: INFO: (14) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 15.656813ms)
Aug 29 16:45:05.481: INFO: (14) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 15.748925ms)
Aug 29 16:45:05.481: INFO: (14) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 16.213843ms)
Aug 29 16:45:05.482: INFO: (14) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 16.371286ms)
Aug 29 16:45:05.486: INFO: (14) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 20.255603ms)
Aug 29 16:45:05.489: INFO: (14) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 23.56573ms)
Aug 29 16:45:05.489: INFO: (14) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 23.790664ms)
Aug 29 16:45:05.489: INFO: (14) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 24.13553ms)
Aug 29 16:45:05.489: INFO: (14) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 24.055219ms)
Aug 29 16:45:05.489: INFO: (14) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 24.07798ms)
Aug 29 16:45:05.491: INFO: (14) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 25.469514ms)
Aug 29 16:45:05.498: INFO: (14) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 32.735101ms)
Aug 29 16:45:05.499: INFO: (14) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 33.506314ms)
Aug 29 16:45:05.511: INFO: (15) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 12.783432ms)
Aug 29 16:45:05.521: INFO: (15) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 21.322441ms)
Aug 29 16:45:05.521: INFO: (15) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 22.272767ms)
Aug 29 16:45:05.529: INFO: (15) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 28.782371ms)
Aug 29 16:45:05.530: INFO: (15) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 30.809046ms)
Aug 29 16:45:05.530: INFO: (15) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 30.195535ms)
Aug 29 16:45:05.530: INFO: (15) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 29.256409ms)
Aug 29 16:45:05.530: INFO: (15) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 29.86408ms)
Aug 29 16:45:05.530: INFO: (15) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 30.344638ms)
Aug 29 16:45:05.530: INFO: (15) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 30.44275ms)
Aug 29 16:45:05.530: INFO: (15) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 30.183455ms)
Aug 29 16:45:05.530: INFO: (15) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 30.024652ms)
Aug 29 16:45:05.530: INFO: (15) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 29.536584ms)
Aug 29 16:45:05.530: INFO: (15) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 30.144105ms)
Aug 29 16:45:05.540: INFO: (15) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 40.328673ms)
Aug 29 16:45:05.540: INFO: (15) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 39.109141ms)
Aug 29 16:45:05.562: INFO: (16) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 16.012439ms)
Aug 29 16:45:05.567: INFO: (16) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 19.382418ms)
Aug 29 16:45:05.567: INFO: (16) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 23.197834ms)
Aug 29 16:45:05.567: INFO: (16) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 22.956659ms)
Aug 29 16:45:05.568: INFO: (16) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 20.891544ms)
Aug 29 16:45:05.568: INFO: (16) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 23.54275ms)
Aug 29 16:45:05.570: INFO: (16) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 21.782759ms)
Aug 29 16:45:05.571: INFO: (16) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 24.033308ms)
Aug 29 16:45:05.572: INFO: (16) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 24.230692ms)
Aug 29 16:45:05.572: INFO: (16) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 26.39731ms)
Aug 29 16:45:05.572: INFO: (16) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 23.740193ms)
Aug 29 16:45:05.567: INFO: (16) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 20.386035ms)
Aug 29 16:45:05.573: INFO: (16) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 29.30075ms)
Aug 29 16:45:05.573: INFO: (16) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 30.080974ms)
Aug 29 16:45:05.574: INFO: (16) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 26.773126ms)
Aug 29 16:45:05.574: INFO: (16) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 27.967736ms)
Aug 29 16:45:05.604: INFO: (17) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 27.096831ms)
Aug 29 16:45:05.604: INFO: (17) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 24.337083ms)
Aug 29 16:45:05.604: INFO: (17) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 29.637326ms)
Aug 29 16:45:05.604: INFO: (17) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 24.70108ms)
Aug 29 16:45:05.604: INFO: (17) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 24.798191ms)
Aug 29 16:45:05.604: INFO: (17) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 24.892103ms)
Aug 29 16:45:05.604: INFO: (17) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 29.973522ms)
Aug 29 16:45:05.604: INFO: (17) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 24.922684ms)
Aug 29 16:45:05.605: INFO: (17) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 29.105727ms)
Aug 29 16:45:05.605: INFO: (17) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 28.841602ms)
Aug 29 16:45:05.605: INFO: (17) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 25.826979ms)
Aug 29 16:45:05.605: INFO: (17) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 30.43563ms)
Aug 29 16:45:05.605: INFO: (17) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 28.20225ms)
Aug 29 16:45:05.611: INFO: (17) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 31.766113ms)
Aug 29 16:45:05.611: INFO: (17) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 31.430958ms)
Aug 29 16:45:05.611: INFO: (17) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 32.844002ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 25.127298ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 28.18225ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 27.120602ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 25.714008ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 26.590693ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 26.226576ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 25.485924ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 24.72443ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 23.331066ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 24.213302ms)
Aug 29 16:45:05.639: INFO: (18) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 24.321053ms)
Aug 29 16:45:05.647: INFO: (18) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 32.964704ms)
Aug 29 16:45:05.648: INFO: (18) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 32.506106ms)
Aug 29 16:45:05.648: INFO: (18) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 32.628928ms)
Aug 29 16:45:05.648: INFO: (18) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 33.010484ms)
Aug 29 16:45:05.648: INFO: (18) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 31.792324ms)
Aug 29 16:45:05.666: INFO: (19) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">test<... (200; 18.009144ms)
Aug 29 16:45:05.667: INFO: (19) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn/proxy/rewriteme">test</a> (200; 17.378582ms)
Aug 29 16:45:05.667: INFO: (19) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:443/proxy/tlsrewritem... (200; 18.694576ms)
Aug 29 16:45:05.672: INFO: (19) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:460/proxy/: tls baz (200; 22.457901ms)
Aug 29 16:45:05.672: INFO: (19) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/: <a href="/api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:1080/proxy/rewriteme">... (200; 23.224154ms)
Aug 29 16:45:05.677: INFO: (19) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 27.789664ms)
Aug 29 16:45:05.678: INFO: (19) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 25.85656ms)
Aug 29 16:45:05.678: INFO: (19) /api/v1/namespaces/proxy-4861/pods/http:proxy-service-5ftgv-5zwfn:160/proxy/: foo (200; 25.706057ms)
Aug 29 16:45:05.678: INFO: (19) /api/v1/namespaces/proxy-4861/pods/https:proxy-service-5ftgv-5zwfn:462/proxy/: tls qux (200; 25.588645ms)
Aug 29 16:45:05.678: INFO: (19) /api/v1/namespaces/proxy-4861/pods/proxy-service-5ftgv-5zwfn:162/proxy/: bar (200; 25.691647ms)
Aug 29 16:45:05.678: INFO: (19) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname2/proxy/: tls qux (200; 25.783379ms)
Aug 29 16:45:05.678: INFO: (19) /api/v1/namespaces/proxy-4861/services/https:proxy-service-5ftgv:tlsportname1/proxy/: tls baz (200; 28.821032ms)
Aug 29 16:45:05.682: INFO: (19) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname1/proxy/: foo (200; 32.195771ms)
Aug 29 16:45:05.682: INFO: (19) /api/v1/namespaces/proxy-4861/services/http:proxy-service-5ftgv:portname2/proxy/: bar (200; 30.397369ms)
Aug 29 16:45:05.682: INFO: (19) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname1/proxy/: foo (200; 32.081739ms)
Aug 29 16:45:05.682: INFO: (19) /api/v1/namespaces/proxy-4861/services/proxy-service-5ftgv:portname2/proxy/: bar (200; 32.028438ms)
STEP: deleting ReplicationController proxy-service-5ftgv in namespace proxy-4861, will wait for the garbage collector to delete the pods
Aug 29 16:45:05.758: INFO: Deleting ReplicationController proxy-service-5ftgv took: 11.376099ms
Aug 29 16:45:05.858: INFO: Terminating ReplicationController proxy-service-5ftgv pods took: 100.421008ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Aug 29 16:45:09.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4861" for this suite.

• [SLOW TEST:6.361 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":291,"skipped":5084,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:09.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Aug 29 16:45:09.175: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 29 16:45:14.188: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Aug 29 16:45:14.204: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Aug 29 16:45:14.229: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Aug 29 16:45:14.238: INFO: Observed &ReplicaSet event: ADDED
Aug 29 16:45:14.239: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 16:45:14.239: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 16:45:14.240: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 16:45:14.240: INFO: Found replicaset test-rs in namespace replicaset-4357 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 16:45:14.240: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Aug 29 16:45:14.241: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 29 16:45:14.251: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Aug 29 16:45:14.258: INFO: Observed &ReplicaSet event: ADDED
Aug 29 16:45:14.258: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 16:45:14.259: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 16:45:14.260: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 16:45:14.260: INFO: Observed replicaset test-rs in namespace replicaset-4357 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 16:45:14.260: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 16:45:14.260: INFO: Found replicaset test-rs in namespace replicaset-4357 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 29 16:45:14.260: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 29 16:45:14.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4357" for this suite.

• [SLOW TEST:5.197 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":292,"skipped":5092,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:14.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:45:14.356: INFO: Creating pod...
Aug 29 16:45:16.394: INFO: Creating service...
Aug 29 16:45:16.409: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/pods/agnhost/proxy/some/path/with/DELETE
Aug 29 16:45:16.452: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 29 16:45:16.453: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/pods/agnhost/proxy/some/path/with/GET
Aug 29 16:45:16.476: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 29 16:45:16.476: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/pods/agnhost/proxy/some/path/with/HEAD
Aug 29 16:45:16.490: INFO: http.Client request:HEAD | StatusCode:200
Aug 29 16:45:16.490: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 29 16:45:16.501: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 29 16:45:16.501: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/pods/agnhost/proxy/some/path/with/PATCH
Aug 29 16:45:16.514: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 29 16:45:16.514: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/pods/agnhost/proxy/some/path/with/POST
Aug 29 16:45:16.531: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 29 16:45:16.532: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/pods/agnhost/proxy/some/path/with/PUT
Aug 29 16:45:16.542: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 29 16:45:16.542: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/services/test-service/proxy/some/path/with/DELETE
Aug 29 16:45:16.560: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 29 16:45:16.560: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/services/test-service/proxy/some/path/with/GET
Aug 29 16:45:16.573: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 29 16:45:16.573: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/services/test-service/proxy/some/path/with/HEAD
Aug 29 16:45:16.596: INFO: http.Client request:HEAD | StatusCode:200
Aug 29 16:45:16.596: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/services/test-service/proxy/some/path/with/OPTIONS
Aug 29 16:45:16.612: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 29 16:45:16.612: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/services/test-service/proxy/some/path/with/PATCH
Aug 29 16:45:16.628: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 29 16:45:16.629: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/services/test-service/proxy/some/path/with/POST
Aug 29 16:45:16.643: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 29 16:45:16.643: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-1933/services/test-service/proxy/some/path/with/PUT
Aug 29 16:45:16.663: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Aug 29 16:45:16.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1933" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":293,"skipped":5112,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:16.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 29 16:45:16.796: INFO: Waiting up to 5m0s for pod "pod-9d2c5791-9fbd-47a4-b751-312d3238aade" in namespace "emptydir-8759" to be "Succeeded or Failed"
Aug 29 16:45:16.809: INFO: Pod "pod-9d2c5791-9fbd-47a4-b751-312d3238aade": Phase="Pending", Reason="", readiness=false. Elapsed: 12.043678ms
Aug 29 16:45:18.820: INFO: Pod "pod-9d2c5791-9fbd-47a4-b751-312d3238aade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023185843s
Aug 29 16:45:20.832: INFO: Pod "pod-9d2c5791-9fbd-47a4-b751-312d3238aade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03594023s
STEP: Saw pod success
Aug 29 16:45:20.833: INFO: Pod "pod-9d2c5791-9fbd-47a4-b751-312d3238aade" satisfied condition "Succeeded or Failed"
Aug 29 16:45:20.841: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-9d2c5791-9fbd-47a4-b751-312d3238aade container test-container: <nil>
STEP: delete the pod
Aug 29 16:45:20.876: INFO: Waiting for pod pod-9d2c5791-9fbd-47a4-b751-312d3238aade to disappear
Aug 29 16:45:20.881: INFO: Pod pod-9d2c5791-9fbd-47a4-b751-312d3238aade no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 16:45:20.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8759" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":294,"skipped":5145,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:20.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Aug 29 16:45:21.634: INFO: Waiting up to 5m0s for pod "test-pod-694f47d0-8df4-4134-aea3-8d84b40c9953" in namespace "svcaccounts-6713" to be "Succeeded or Failed"
Aug 29 16:45:21.643: INFO: Pod "test-pod-694f47d0-8df4-4134-aea3-8d84b40c9953": Phase="Pending", Reason="", readiness=false. Elapsed: 8.75022ms
Aug 29 16:45:23.956: INFO: Pod "test-pod-694f47d0-8df4-4134-aea3-8d84b40c9953": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32187754s
Aug 29 16:45:25.976: INFO: Pod "test-pod-694f47d0-8df4-4134-aea3-8d84b40c9953": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.341578369s
STEP: Saw pod success
Aug 29 16:45:25.976: INFO: Pod "test-pod-694f47d0-8df4-4134-aea3-8d84b40c9953" satisfied condition "Succeeded or Failed"
Aug 29 16:45:26.007: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod test-pod-694f47d0-8df4-4134-aea3-8d84b40c9953 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:45:26.065: INFO: Waiting for pod test-pod-694f47d0-8df4-4134-aea3-8d84b40c9953 to disappear
Aug 29 16:45:26.072: INFO: Pod test-pod-694f47d0-8df4-4134-aea3-8d84b40c9953 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 29 16:45:26.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6713" for this suite.

• [SLOW TEST:5.192 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":295,"skipped":5162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:26.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Aug 29 16:45:26.254: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Aug 29 16:45:28.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7119" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":296,"skipped":5195,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:28.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:45:28.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5225d124-e66a-4087-9200-8534a1a80421" in namespace "downward-api-9778" to be "Succeeded or Failed"
Aug 29 16:45:28.385: INFO: Pod "downwardapi-volume-5225d124-e66a-4087-9200-8534a1a80421": Phase="Pending", Reason="", readiness=false. Elapsed: 10.628362ms
Aug 29 16:45:30.405: INFO: Pod "downwardapi-volume-5225d124-e66a-4087-9200-8534a1a80421": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029806629s
Aug 29 16:45:32.424: INFO: Pod "downwardapi-volume-5225d124-e66a-4087-9200-8534a1a80421": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049343454s
Aug 29 16:45:34.445: INFO: Pod "downwardapi-volume-5225d124-e66a-4087-9200-8534a1a80421": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069710499s
STEP: Saw pod success
Aug 29 16:45:34.445: INFO: Pod "downwardapi-volume-5225d124-e66a-4087-9200-8534a1a80421" satisfied condition "Succeeded or Failed"
Aug 29 16:45:34.453: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-5225d124-e66a-4087-9200-8534a1a80421 container client-container: <nil>
STEP: delete the pod
Aug 29 16:45:34.539: INFO: Waiting for pod downwardapi-volume-5225d124-e66a-4087-9200-8534a1a80421 to disappear
Aug 29 16:45:34.560: INFO: Pod downwardapi-volume-5225d124-e66a-4087-9200-8534a1a80421 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 16:45:34.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9778" for this suite.

• [SLOW TEST:7.411 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":297,"skipped":5205,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:35.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:45:35.877: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c0964aa5-ccb3-41e1-b5d5-ca270b7084a0" in namespace "projected-520" to be "Succeeded or Failed"
Aug 29 16:45:35.886: INFO: Pod "downwardapi-volume-c0964aa5-ccb3-41e1-b5d5-ca270b7084a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.506915ms
Aug 29 16:45:37.906: INFO: Pod "downwardapi-volume-c0964aa5-ccb3-41e1-b5d5-ca270b7084a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028435875s
Aug 29 16:45:39.921: INFO: Pod "downwardapi-volume-c0964aa5-ccb3-41e1-b5d5-ca270b7084a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043686781s
Aug 29 16:45:41.934: INFO: Pod "downwardapi-volume-c0964aa5-ccb3-41e1-b5d5-ca270b7084a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056172861s
STEP: Saw pod success
Aug 29 16:45:41.934: INFO: Pod "downwardapi-volume-c0964aa5-ccb3-41e1-b5d5-ca270b7084a0" satisfied condition "Succeeded or Failed"
Aug 29 16:45:41.940: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-c0964aa5-ccb3-41e1-b5d5-ca270b7084a0 container client-container: <nil>
STEP: delete the pod
Aug 29 16:45:42.704: INFO: Waiting for pod downwardapi-volume-c0964aa5-ccb3-41e1-b5d5-ca270b7084a0 to disappear
Aug 29 16:45:42.710: INFO: Pod downwardapi-volume-c0964aa5-ccb3-41e1-b5d5-ca270b7084a0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 16:45:42.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-520" for this suite.

• [SLOW TEST:7.018 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":298,"skipped":5205,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:42.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 29 16:45:42.801: INFO: Waiting up to 5m0s for pod "pod-f9e67f46-8de3-43b0-9006-9529da3a6e4d" in namespace "emptydir-2897" to be "Succeeded or Failed"
Aug 29 16:45:42.809: INFO: Pod "pod-f9e67f46-8de3-43b0-9006-9529da3a6e4d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.205099ms
Aug 29 16:45:44.820: INFO: Pod "pod-f9e67f46-8de3-43b0-9006-9529da3a6e4d": Phase="Running", Reason="", readiness=false. Elapsed: 2.01855465s
Aug 29 16:45:46.834: INFO: Pod "pod-f9e67f46-8de3-43b0-9006-9529da3a6e4d": Phase="Running", Reason="", readiness=false. Elapsed: 4.032954484s
Aug 29 16:45:48.842: INFO: Pod "pod-f9e67f46-8de3-43b0-9006-9529da3a6e4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041159932s
STEP: Saw pod success
Aug 29 16:45:48.843: INFO: Pod "pod-f9e67f46-8de3-43b0-9006-9529da3a6e4d" satisfied condition "Succeeded or Failed"
Aug 29 16:45:48.848: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-f9e67f46-8de3-43b0-9006-9529da3a6e4d container test-container: <nil>
STEP: delete the pod
Aug 29 16:45:48.880: INFO: Waiting for pod pod-f9e67f46-8de3-43b0-9006-9529da3a6e4d to disappear
Aug 29 16:45:48.885: INFO: Pod pod-f9e67f46-8de3-43b0-9006-9529da3a6e4d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 16:45:48.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2897" for this suite.

• [SLOW TEST:6.178 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":299,"skipped":5226,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:48.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-515ab836-16a3-4ff8-822f-ca534d8af061
STEP: Creating secret with name s-test-opt-upd-21ef0246-a6f3-4738-ad52-84f5ce948830
STEP: Creating the pod
Aug 29 16:45:49.010: INFO: The status of Pod pod-secrets-3c87912c-5f43-4e59-9952-d325d3f5d7ee is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:45:51.030: INFO: The status of Pod pod-secrets-3c87912c-5f43-4e59-9952-d325d3f5d7ee is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-515ab836-16a3-4ff8-822f-ca534d8af061
STEP: Updating secret s-test-opt-upd-21ef0246-a6f3-4738-ad52-84f5ce948830
STEP: Creating secret with name s-test-opt-create-283e7714-3ac0-46c0-96b3-31d8d46d26d2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Aug 29 16:45:53.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2645" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":300,"skipped":5230,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:53.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:45:53.495: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 29 16:45:55.827: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 16:45:55.868: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4918  d13b35e4-e621-4907-9700-8cfcc8be8f7c 41375 1 2022-08-29 16:45:55 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-08-29 16:45:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e31df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 29 16:45:55.878: INFO: New ReplicaSet "test-cleanup-deployment-6755c7b765" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-6755c7b765  deployment-4918  6e402dc3-3870-4509-a045-64e74b7b0739 41377 1 2022-08-29 16:45:55 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment d13b35e4-e621-4907-9700-8cfcc8be8f7c 0xc000fa6847 0xc000fa6848}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:45:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d13b35e4-e621-4907-9700-8cfcc8be8f7c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6755c7b765,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000fa6978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:45:55.879: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 29 16:45:55.879: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4918  0549d59b-928a-4057-a4ba-22be02a6f61e 41376 1 2022-08-29 16:45:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d13b35e4-e621-4907-9700-8cfcc8be8f7c 0xc000fa63e7 0xc000fa63e8}] []  [{e2e.test Update apps/v1 2022-08-29 16:45:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:45:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-08-29 16:45:55 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d13b35e4-e621-4907-9700-8cfcc8be8f7c\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000fa66a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:45:55.900: INFO: Pod "test-cleanup-controller-c8gcg" is available:
&Pod{ObjectMeta:{test-cleanup-controller-c8gcg test-cleanup-controller- deployment-4918  d630e8f5-7a65-4594-ad58-ae26ef0a5474 41369 0 2022-08-29 16:45:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:61183c8e9b5e4d5e8bc8115a974627d55ac280a0c14f9d418320cbc9d12d118f cni.projectcalico.org/podIP:172.25.2.34/32 cni.projectcalico.org/podIPs:172.25.2.34/32] [{apps/v1 ReplicaSet test-cleanup-controller 0549d59b-928a-4057-a4ba-22be02a6f61e 0xc000fa7f87 0xc000fa7f88}] []  [{kube-controller-manager Update v1 2022-08-29 16:45:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0549d59b-928a-4057-a4ba-22be02a6f61e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:45:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:45:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4c8l8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c8l8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:45:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:45:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:45:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:45:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:172.25.2.34,StartTime:2022-08-29 16:45:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:45:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a7f6406a3e2b5f52822033055d4f1707cb28b3c9427852fc63b0170e065b9d7a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:45:55.901: INFO: Pod "test-cleanup-deployment-6755c7b765-jnfg4" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-6755c7b765-jnfg4 test-cleanup-deployment-6755c7b765- deployment-4918  af3dce9e-6d12-47f5-857e-53da517bc794 41383 0 2022-08-29 16:45:55 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-6755c7b765 6e402dc3-3870-4509-a045-64e74b7b0739 0xc0018621a7 0xc0018621a8}] []  [{kube-controller-manager Update v1 2022-08-29 16:45:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e402dc3-3870-4509-a045-64e74b7b0739\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kngn4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kngn4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:45:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 29 16:45:55.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4918" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":301,"skipped":5240,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:45:55.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-69ea84a9-ef8a-4b50-bd13-19743093ed34
STEP: Creating a pod to test consume configMaps
Aug 29 16:45:56.016: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b1c932f-ca5a-49f2-a0d1-8b62a9fa2b18" in namespace "configmap-8819" to be "Succeeded or Failed"
Aug 29 16:45:56.047: INFO: Pod "pod-configmaps-7b1c932f-ca5a-49f2-a0d1-8b62a9fa2b18": Phase="Pending", Reason="", readiness=false. Elapsed: 30.90728ms
Aug 29 16:45:58.056: INFO: Pod "pod-configmaps-7b1c932f-ca5a-49f2-a0d1-8b62a9fa2b18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040066513s
Aug 29 16:46:00.070: INFO: Pod "pod-configmaps-7b1c932f-ca5a-49f2-a0d1-8b62a9fa2b18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054048532s
STEP: Saw pod success
Aug 29 16:46:00.070: INFO: Pod "pod-configmaps-7b1c932f-ca5a-49f2-a0d1-8b62a9fa2b18" satisfied condition "Succeeded or Failed"
Aug 29 16:46:00.085: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-configmaps-7b1c932f-ca5a-49f2-a0d1-8b62a9fa2b18 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:46:00.134: INFO: Waiting for pod pod-configmaps-7b1c932f-ca5a-49f2-a0d1-8b62a9fa2b18 to disappear
Aug 29 16:46:00.141: INFO: Pod pod-configmaps-7b1c932f-ca5a-49f2-a0d1-8b62a9fa2b18 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:46:00.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8819" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":302,"skipped":5247,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:46:00.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-86be4201-e80c-44fb-9111-08def4113218
STEP: Creating a pod to test consume configMaps
Aug 29 16:46:00.244: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b5be2f7f-826c-4ee3-8e0c-be9f30a7a3bc" in namespace "projected-6877" to be "Succeeded or Failed"
Aug 29 16:46:00.257: INFO: Pod "pod-projected-configmaps-b5be2f7f-826c-4ee3-8e0c-be9f30a7a3bc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.177491ms
Aug 29 16:46:02.285: INFO: Pod "pod-projected-configmaps-b5be2f7f-826c-4ee3-8e0c-be9f30a7a3bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041411043s
Aug 29 16:46:04.697: INFO: Pod "pod-projected-configmaps-b5be2f7f-826c-4ee3-8e0c-be9f30a7a3bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.452687357s
Aug 29 16:46:06.964: INFO: Pod "pod-projected-configmaps-b5be2f7f-826c-4ee3-8e0c-be9f30a7a3bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.72038433s
STEP: Saw pod success
Aug 29 16:46:06.964: INFO: Pod "pod-projected-configmaps-b5be2f7f-826c-4ee3-8e0c-be9f30a7a3bc" satisfied condition "Succeeded or Failed"
Aug 29 16:46:06.973: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-projected-configmaps-b5be2f7f-826c-4ee3-8e0c-be9f30a7a3bc container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:46:07.006: INFO: Waiting for pod pod-projected-configmaps-b5be2f7f-826c-4ee3-8e0c-be9f30a7a3bc to disappear
Aug 29 16:46:07.037: INFO: Pod pod-projected-configmaps-b5be2f7f-826c-4ee3-8e0c-be9f30a7a3bc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Aug 29 16:46:07.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6877" for this suite.

• [SLOW TEST:7.293 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":303,"skipped":5254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:46:07.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:46:07.538: INFO: Creating simple deployment test-new-deployment
Aug 29 16:46:07.567: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 16:46:09.695: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5124  bab89ffb-6ab2-4a17-83c5-2bc829abc01e 41571 3 2022-08-29 16:46:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-08-29 16:46:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023e8e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2022-08-29 16:46:09 +0000 UTC,LastTransitionTime:2022-08-29 16:46:07 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-08-29 16:46:09 +0000 UTC,LastTransitionTime:2022-08-29 16:46:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 16:46:09.702: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-5124  5eae1ab5-184e-4c51-b259-3af8133f13fd 41569 3 2022-08-29 16:46:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment bab89ffb-6ab2-4a17-83c5-2bc829abc01e 0xc0034ece37 0xc0034ece38}] []  [{kube-controller-manager Update apps/v1 2022-08-29 16:46:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bab89ffb-6ab2-4a17-83c5-2bc829abc01e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-08-29 16:46:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034ecec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 16:46:09.716: INFO: Pod "test-new-deployment-55df494869-cqhn4" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-cqhn4 test-new-deployment-55df494869- deployment-5124  f208dfbc-65d2-408c-89ff-fad70be58550 41577 0 2022-08-29 16:46:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet test-new-deployment-55df494869 5eae1ab5-184e-4c51-b259-3af8133f13fd 0xc0034ed267 0xc0034ed268}] []  [{kube-controller-manager Update v1 2022-08-29 16:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5eae1ab5-184e-4c51-b259-3af8133f13fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-twwmg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-twwmg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:46:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:46:09.716: INFO: Pod "test-new-deployment-55df494869-d5j95" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-d5j95 test-new-deployment-55df494869- deployment-5124  3f040c3c-88a5-4e93-bdf0-eca0c4568218 41575 0 2022-08-29 16:46:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet test-new-deployment-55df494869 5eae1ab5-184e-4c51-b259-3af8133f13fd 0xc0034ed3c0 0xc0034ed3c1}] []  [{kube-controller-manager Update v1 2022-08-29 16:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5eae1ab5-184e-4c51-b259-3af8133f13fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8n49m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8n49m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:46:09.717: INFO: Pod "test-new-deployment-55df494869-l77cf" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-l77cf test-new-deployment-55df494869- deployment-5124  3eb5cb49-847c-42c0-adbb-1884b561c696 41572 0 2022-08-29 16:46:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet test-new-deployment-55df494869 5eae1ab5-184e-4c51-b259-3af8133f13fd 0xc0034ed507 0xc0034ed508}] []  [{kube-controller-manager Update v1 2022-08-29 16:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5eae1ab5-184e-4c51-b259-3af8133f13fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-08-29 16:46:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6ffp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6ffp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:46:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:46:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:46:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:46:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.142,PodIP:,StartTime:2022-08-29 16:46:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 16:46:09.717: INFO: Pod "test-new-deployment-55df494869-p8j6f" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-p8j6f test-new-deployment-55df494869- deployment-5124  3c252ce5-38a2-4374-beb3-4fb6b0b327fb 41553 0 2022-08-29 16:46:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:ed3e6cfad3d6932ba44a18f6d08b7f81abf7500e3bdb3f3e2c6bf38d39c8511e cni.projectcalico.org/podIP:172.25.1.12/32 cni.projectcalico.org/podIPs:172.25.1.12/32] [{apps/v1 ReplicaSet test-new-deployment-55df494869 5eae1ab5-184e-4c51-b259-3af8133f13fd 0xc0034ed6e7 0xc0034ed6e8}] []  [{kube-controller-manager Update v1 2022-08-29 16:46:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5eae1ab5-184e-4c51-b259-3af8133f13fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-08-29 16:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-08-29 16:46:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6l4r4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6l4r4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-214.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:46:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:46:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:46:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-08-29 16:46:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.214,PodIP:172.25.1.12,StartTime:2022-08-29 16:46:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-08-29 16:46:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f63e95ab8bddf46b6e0cbad78618a45c7e5c6a43e7b6aaaedf75f061be1e384b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Aug 29 16:46:09.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5124" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":304,"skipped":5364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:46:09.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-7929
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7929 to expose endpoints map[]
Aug 29 16:46:09.868: INFO: successfully validated that service multi-endpoint-test in namespace services-7929 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7929
Aug 29 16:46:09.893: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:46:11.910: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7929 to expose endpoints map[pod1:[100]]
Aug 29 16:46:11.943: INFO: successfully validated that service multi-endpoint-test in namespace services-7929 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7929
Aug 29 16:46:11.967: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:46:13.983: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7929 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 29 16:46:14.037: INFO: successfully validated that service multi-endpoint-test in namespace services-7929 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Aug 29 16:46:14.038: INFO: Creating new exec pod
Aug 29 16:46:17.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-7929 exec execpod6snn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Aug 29 16:46:17.665: INFO: stderr: "+ + nc -vecho -t hostName -w\n 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 29 16:46:17.666: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:46:17.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-7929 exec execpod6snn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.47 80'
Aug 29 16:46:18.197: INFO: stderr: "+ nc -v -t -w 2 10.240.26.47 80\nConnection to 10.240.26.47 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Aug 29 16:46:18.197: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:46:18.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-7929 exec execpod6snn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Aug 29 16:46:18.429: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 29 16:46:18.429: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:46:18.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-7929 exec execpod6snn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.47 81'
Aug 29 16:46:18.665: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.26.47 81\nConnection to 10.240.26.47 81 port [tcp/*] succeeded!\n"
Aug 29 16:46:18.665: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7929
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7929 to expose endpoints map[pod2:[101]]
Aug 29 16:46:18.738: INFO: successfully validated that service multi-endpoint-test in namespace services-7929 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7929
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7929 to expose endpoints map[]
Aug 29 16:46:18.788: INFO: successfully validated that service multi-endpoint-test in namespace services-7929 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:46:18.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7929" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.090 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":305,"skipped":5394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:46:18.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Aug 29 16:46:18.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 create -f -'
Aug 29 16:46:20.162: INFO: stderr: ""
Aug 29 16:46:20.162: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 29 16:46:20.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:46:20.278: INFO: stderr: ""
Aug 29 16:46:20.278: INFO: stdout: "update-demo-nautilus-gb8mq update-demo-nautilus-lsmx9 "
Aug 29 16:46:20.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-gb8mq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:46:20.381: INFO: stderr: ""
Aug 29 16:46:20.381: INFO: stdout: ""
Aug 29 16:46:20.381: INFO: update-demo-nautilus-gb8mq is created but not running
Aug 29 16:46:25.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:46:25.552: INFO: stderr: ""
Aug 29 16:46:25.552: INFO: stdout: "update-demo-nautilus-gb8mq update-demo-nautilus-lsmx9 "
Aug 29 16:46:25.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-gb8mq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:46:25.756: INFO: stderr: ""
Aug 29 16:46:25.756: INFO: stdout: "true"
Aug 29 16:46:25.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-gb8mq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 16:46:25.895: INFO: stderr: ""
Aug 29 16:46:25.895: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 16:46:25.895: INFO: validating pod update-demo-nautilus-gb8mq
Aug 29 16:46:25.914: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 16:46:25.914: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 16:46:25.914: INFO: update-demo-nautilus-gb8mq is verified up and running
Aug 29 16:46:25.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-lsmx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:46:26.020: INFO: stderr: ""
Aug 29 16:46:26.020: INFO: stdout: "true"
Aug 29 16:46:26.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-lsmx9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 16:46:26.112: INFO: stderr: ""
Aug 29 16:46:26.112: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 16:46:26.112: INFO: validating pod update-demo-nautilus-lsmx9
Aug 29 16:46:26.137: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 16:46:26.137: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 16:46:26.137: INFO: update-demo-nautilus-lsmx9 is verified up and running
STEP: scaling down the replication controller
Aug 29 16:46:26.139: INFO: scanned /root for discovery docs: <nil>
Aug 29 16:46:26.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 29 16:46:27.540: INFO: stderr: ""
Aug 29 16:46:27.540: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 29 16:46:27.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:46:27.645: INFO: stderr: ""
Aug 29 16:46:27.645: INFO: stdout: "update-demo-nautilus-gb8mq update-demo-nautilus-lsmx9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 29 16:46:32.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:46:32.763: INFO: stderr: ""
Aug 29 16:46:32.763: INFO: stdout: "update-demo-nautilus-gb8mq "
Aug 29 16:46:32.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-gb8mq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:46:32.876: INFO: stderr: ""
Aug 29 16:46:32.876: INFO: stdout: "true"
Aug 29 16:46:32.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-gb8mq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 16:46:32.982: INFO: stderr: ""
Aug 29 16:46:32.982: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 16:46:32.982: INFO: validating pod update-demo-nautilus-gb8mq
Aug 29 16:46:32.993: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 16:46:32.993: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 16:46:32.993: INFO: update-demo-nautilus-gb8mq is verified up and running
STEP: scaling up the replication controller
Aug 29 16:46:32.995: INFO: scanned /root for discovery docs: <nil>
Aug 29 16:46:32.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 29 16:46:34.562: INFO: stderr: ""
Aug 29 16:46:34.562: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 29 16:46:34.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:46:34.675: INFO: stderr: ""
Aug 29 16:46:34.675: INFO: stdout: "update-demo-nautilus-72xv9 update-demo-nautilus-gb8mq "
Aug 29 16:46:34.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-72xv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:46:34.783: INFO: stderr: ""
Aug 29 16:46:34.784: INFO: stdout: ""
Aug 29 16:46:34.784: INFO: update-demo-nautilus-72xv9 is created but not running
Aug 29 16:46:39.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 16:46:39.947: INFO: stderr: ""
Aug 29 16:46:39.947: INFO: stdout: "update-demo-nautilus-72xv9 update-demo-nautilus-gb8mq "
Aug 29 16:46:39.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-72xv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:46:40.048: INFO: stderr: ""
Aug 29 16:46:40.048: INFO: stdout: "true"
Aug 29 16:46:40.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-72xv9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 16:46:40.143: INFO: stderr: ""
Aug 29 16:46:40.143: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 16:46:40.143: INFO: validating pod update-demo-nautilus-72xv9
Aug 29 16:46:40.157: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 16:46:40.157: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 16:46:40.157: INFO: update-demo-nautilus-72xv9 is verified up and running
Aug 29 16:46:40.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-gb8mq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 16:46:40.238: INFO: stderr: ""
Aug 29 16:46:40.238: INFO: stdout: "true"
Aug 29 16:46:40.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods update-demo-nautilus-gb8mq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 16:46:40.327: INFO: stderr: ""
Aug 29 16:46:40.327: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Aug 29 16:46:40.327: INFO: validating pod update-demo-nautilus-gb8mq
Aug 29 16:46:40.345: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 16:46:40.345: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 16:46:40.345: INFO: update-demo-nautilus-gb8mq is verified up and running
STEP: using delete to clean up resources
Aug 29 16:46:40.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 delete --grace-period=0 --force -f -'
Aug 29 16:46:40.434: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:46:40.434: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 29 16:46:40.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get rc,svc -l name=update-demo --no-headers'
Aug 29 16:46:40.895: INFO: stderr: "No resources found in kubectl-9310 namespace.\n"
Aug 29 16:46:40.895: INFO: stdout: ""
Aug 29 16:46:40.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-9310 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 29 16:46:41.073: INFO: stderr: ""
Aug 29 16:46:41.073: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:46:41.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9310" for this suite.

• [SLOW TEST:22.244 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":306,"skipped":5417,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:46:41.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:46:41.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Aug 29 16:46:45.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-4149 --namespace=crd-publish-openapi-4149 create -f -'
Aug 29 16:46:46.959: INFO: stderr: ""
Aug 29 16:46:46.959: INFO: stdout: "e2e-test-crd-publish-openapi-8749-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 29 16:46:46.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-4149 --namespace=crd-publish-openapi-4149 delete e2e-test-crd-publish-openapi-8749-crds test-cr'
Aug 29 16:46:47.130: INFO: stderr: ""
Aug 29 16:46:47.130: INFO: stdout: "e2e-test-crd-publish-openapi-8749-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 29 16:46:47.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-4149 --namespace=crd-publish-openapi-4149 apply -f -'
Aug 29 16:46:47.377: INFO: stderr: ""
Aug 29 16:46:47.377: INFO: stdout: "e2e-test-crd-publish-openapi-8749-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 29 16:46:47.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-4149 --namespace=crd-publish-openapi-4149 delete e2e-test-crd-publish-openapi-8749-crds test-cr'
Aug 29 16:46:47.486: INFO: stderr: ""
Aug 29 16:46:47.486: INFO: stdout: "e2e-test-crd-publish-openapi-8749-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 29 16:46:47.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=crd-publish-openapi-4149 explain e2e-test-crd-publish-openapi-8749-crds'
Aug 29 16:46:47.715: INFO: stderr: ""
Aug 29 16:46:47.715: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8749-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:46:50.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4149" for this suite.

• [SLOW TEST:9.837 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":307,"skipped":5435,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:46:50.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-2379
STEP: creating replication controller nodeport-test in namespace services-2379
I0829 16:46:51.640647      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2379, replica count: 2
I0829 16:46:54.692736      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:46:54.692: INFO: Creating new exec pod
Aug 29 16:46:59.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-2379 exec execpodlw84b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 29 16:47:00.059: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 29 16:47:00.059: INFO: stdout: ""
Aug 29 16:47:01.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-2379 exec execpodlw84b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 29 16:47:01.339: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 29 16:47:01.339: INFO: stdout: "nodeport-test-lndlk"
Aug 29 16:47:01.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-2379 exec execpodlw84b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.27.165 80'
Aug 29 16:47:01.573: INFO: stderr: "+ nc -v -t -w 2 10.240.27.165 80\nConnection to 10.240.27.165 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Aug 29 16:47:01.573: INFO: stdout: "nodeport-test-6hv42"
Aug 29 16:47:01.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-2379 exec execpodlw84b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.26.197 31969'
Aug 29 16:47:01.839: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.26.197 31969\nConnection to 172.31.26.197 31969 port [tcp/*] succeeded!\n"
Aug 29 16:47:01.839: INFO: stdout: "nodeport-test-6hv42"
Aug 29 16:47:01.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-2379 exec execpodlw84b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.214 31969'
Aug 29 16:47:02.111: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 172.31.16.214 31969\nConnection to 172.31.16.214 31969 port [tcp/*] succeeded!\n"
Aug 29 16:47:02.111: INFO: stdout: ""
Aug 29 16:47:03.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-2379 exec execpodlw84b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.214 31969'
Aug 29 16:47:03.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.214 31969\nConnection to 172.31.16.214 31969 port [tcp/*] succeeded!\n"
Aug 29 16:47:03.382: INFO: stdout: "nodeport-test-lndlk"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:47:03.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2379" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:12.477 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":308,"skipped":5469,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:03.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Aug 29 16:47:03.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-4868 create -f -'
Aug 29 16:47:04.210: INFO: stderr: ""
Aug 29 16:47:04.210: INFO: stdout: "pod/pause created\n"
Aug 29 16:47:04.210: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 29 16:47:04.210: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4868" to be "running and ready"
Aug 29 16:47:04.220: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.559274ms
Aug 29 16:47:06.232: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.022355579s
Aug 29 16:47:06.232: INFO: Pod "pause" satisfied condition "running and ready"
Aug 29 16:47:06.232: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 29 16:47:06.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-4868 label pods pause testing-label=testing-label-value'
Aug 29 16:47:06.340: INFO: stderr: ""
Aug 29 16:47:06.340: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 29 16:47:06.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-4868 get pod pause -L testing-label'
Aug 29 16:47:06.442: INFO: stderr: ""
Aug 29 16:47:06.442: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 29 16:47:06.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-4868 label pods pause testing-label-'
Aug 29 16:47:06.577: INFO: stderr: ""
Aug 29 16:47:06.577: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 29 16:47:06.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-4868 get pod pause -L testing-label'
Aug 29 16:47:06.680: INFO: stderr: ""
Aug 29 16:47:06.680: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Aug 29 16:47:06.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-4868 delete --grace-period=0 --force -f -'
Aug 29 16:47:06.794: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 16:47:06.794: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 29 16:47:06.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-4868 get rc,svc -l name=pause --no-headers'
Aug 29 16:47:06.914: INFO: stderr: "No resources found in kubectl-4868 namespace.\n"
Aug 29 16:47:06.914: INFO: stdout: ""
Aug 29 16:47:06.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-4868 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 29 16:47:07.010: INFO: stderr: ""
Aug 29 16:47:07.010: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:47:07.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4868" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":309,"skipped":5476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:07.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 29 16:47:07.217: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6672  6104daed-3d11-41da-b554-6654330556db 42074 0 2022-08-29 16:47:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:47:07.217: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6672  6104daed-3d11-41da-b554-6654330556db 42075 0 2022-08-29 16:47:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:47:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:47:07.217: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6672  6104daed-3d11-41da-b554-6654330556db 42077 0 2022-08-29 16:47:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:47:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 29 16:47:17.306: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6672  6104daed-3d11-41da-b554-6654330556db 42160 0 2022-08-29 16:47:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:47:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:47:17.307: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6672  6104daed-3d11-41da-b554-6654330556db 42161 0 2022-08-29 16:47:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:47:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:47:17.307: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6672  6104daed-3d11-41da-b554-6654330556db 42162 0 2022-08-29 16:47:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-08-29 16:47:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 29 16:47:17.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6672" for this suite.

• [SLOW TEST:10.286 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":310,"skipped":5518,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:17.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Aug 29 16:47:17.395: INFO: Waiting up to 5m0s for pod "downward-api-4fe27098-ff76-44d7-b691-9e01956b54df" in namespace "downward-api-4954" to be "Succeeded or Failed"
Aug 29 16:47:17.402: INFO: Pod "downward-api-4fe27098-ff76-44d7-b691-9e01956b54df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.337962ms
Aug 29 16:47:19.419: INFO: Pod "downward-api-4fe27098-ff76-44d7-b691-9e01956b54df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023841045s
Aug 29 16:47:21.431: INFO: Pod "downward-api-4fe27098-ff76-44d7-b691-9e01956b54df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035822812s
STEP: Saw pod success
Aug 29 16:47:21.431: INFO: Pod "downward-api-4fe27098-ff76-44d7-b691-9e01956b54df" satisfied condition "Succeeded or Failed"
Aug 29 16:47:21.438: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod downward-api-4fe27098-ff76-44d7-b691-9e01956b54df container dapi-container: <nil>
STEP: delete the pod
Aug 29 16:47:21.500: INFO: Waiting for pod downward-api-4fe27098-ff76-44d7-b691-9e01956b54df to disappear
Aug 29 16:47:21.506: INFO: Pod downward-api-4fe27098-ff76-44d7-b691-9e01956b54df no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Aug 29 16:47:21.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4954" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":311,"skipped":5536,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:21.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:47:22.080: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:47:25.135: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:47:25.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2229" for this suite.
STEP: Destroying namespace "webhook-2229-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":312,"skipped":5543,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:25.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:47:25.559: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b4c3c24c-ba4a-4bed-8f23-133ed6a4e1f3" in namespace "security-context-test-9990" to be "Succeeded or Failed"
Aug 29 16:47:25.565: INFO: Pod "busybox-privileged-false-b4c3c24c-ba4a-4bed-8f23-133ed6a4e1f3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.425173ms
Aug 29 16:47:27.579: INFO: Pod "busybox-privileged-false-b4c3c24c-ba4a-4bed-8f23-133ed6a4e1f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020318988s
Aug 29 16:47:29.590: INFO: Pod "busybox-privileged-false-b4c3c24c-ba4a-4bed-8f23-133ed6a4e1f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031329228s
Aug 29 16:47:29.590: INFO: Pod "busybox-privileged-false-b4c3c24c-ba4a-4bed-8f23-133ed6a4e1f3" satisfied condition "Succeeded or Failed"
Aug 29 16:47:29.606: INFO: Got logs for pod "busybox-privileged-false-b4c3c24c-ba4a-4bed-8f23-133ed6a4e1f3": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 29 16:47:29.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9990" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":313,"skipped":5550,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:29.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 29 16:47:30.042: INFO: Waiting up to 5m0s for pod "pod-9b01120d-1c40-4860-ba8f-6f36f25dc5e5" in namespace "emptydir-383" to be "Succeeded or Failed"
Aug 29 16:47:30.049: INFO: Pod "pod-9b01120d-1c40-4860-ba8f-6f36f25dc5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.696026ms
Aug 29 16:47:32.067: INFO: Pod "pod-9b01120d-1c40-4860-ba8f-6f36f25dc5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024733938s
Aug 29 16:47:34.086: INFO: Pod "pod-9b01120d-1c40-4860-ba8f-6f36f25dc5e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043545315s
STEP: Saw pod success
Aug 29 16:47:34.087: INFO: Pod "pod-9b01120d-1c40-4860-ba8f-6f36f25dc5e5" satisfied condition "Succeeded or Failed"
Aug 29 16:47:34.098: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-9b01120d-1c40-4860-ba8f-6f36f25dc5e5 container test-container: <nil>
STEP: delete the pod
Aug 29 16:47:34.172: INFO: Waiting for pod pod-9b01120d-1c40-4860-ba8f-6f36f25dc5e5 to disappear
Aug 29 16:47:34.183: INFO: Pod pod-9b01120d-1c40-4860-ba8f-6f36f25dc5e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 16:47:34.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-383" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":314,"skipped":5563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:34.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
STEP: Replace a pod template
Aug 29 16:47:34.330: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Aug 29 16:47:34.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5619" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":315,"skipped":5622,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:34.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Aug 29 16:47:34.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-7159 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 29 16:47:34.866: INFO: stderr: ""
Aug 29 16:47:34.866: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Aug 29 16:47:34.866: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 29 16:47:34.866: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7159" to be "running and ready, or succeeded"
Aug 29 16:47:34.873: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074907ms
Aug 29 16:47:36.884: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.017845354s
Aug 29 16:47:36.884: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 29 16:47:36.884: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 29 16:47:36.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-7159 logs logs-generator logs-generator'
Aug 29 16:47:36.988: INFO: stderr: ""
Aug 29 16:47:36.988: INFO: stdout: "I0829 16:47:35.661602       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/v2fg 311\nI0829 16:47:35.862060       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/qzf8 451\nI0829 16:47:36.062470       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/vgc 201\nI0829 16:47:36.261793       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/8x8 358\nI0829 16:47:36.462142       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/bnsw 308\nI0829 16:47:36.662594       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/mx8 444\nI0829 16:47:36.862059       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/j8ms 224\n"
STEP: limiting log lines
Aug 29 16:47:36.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-7159 logs logs-generator logs-generator --tail=1'
Aug 29 16:47:37.097: INFO: stderr: ""
Aug 29 16:47:37.097: INFO: stdout: "I0829 16:47:37.062494       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/rms 352\n"
Aug 29 16:47:37.097: INFO: got output "I0829 16:47:37.062494       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/rms 352\n"
STEP: limiting log bytes
Aug 29 16:47:37.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-7159 logs logs-generator logs-generator --limit-bytes=1'
Aug 29 16:47:37.219: INFO: stderr: ""
Aug 29 16:47:37.219: INFO: stdout: "I"
Aug 29 16:47:37.219: INFO: got output "I"
STEP: exposing timestamps
Aug 29 16:47:37.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-7159 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 29 16:47:37.343: INFO: stderr: ""
Aug 29 16:47:37.343: INFO: stdout: "2022-08-29T16:47:37.262018812Z I0829 16:47:37.261836       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/gr6d 500\n"
Aug 29 16:47:37.343: INFO: got output "2022-08-29T16:47:37.262018812Z I0829 16:47:37.261836       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/gr6d 500\n"
STEP: restricting to a time range
Aug 29 16:47:39.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-7159 logs logs-generator logs-generator --since=1s'
Aug 29 16:47:39.963: INFO: stderr: ""
Aug 29 16:47:39.963: INFO: stdout: "I0829 16:47:39.062150       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/k8xb 426\nI0829 16:47:39.262530       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/fjk 443\nI0829 16:47:39.461671       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/ltg8 514\nI0829 16:47:39.662015       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/blg 459\nI0829 16:47:39.862628       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/5cz 474\n"
Aug 29 16:47:39.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-7159 logs logs-generator logs-generator --since=24h'
Aug 29 16:47:40.090: INFO: stderr: ""
Aug 29 16:47:40.091: INFO: stdout: "I0829 16:47:35.661602       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/v2fg 311\nI0829 16:47:35.862060       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/qzf8 451\nI0829 16:47:36.062470       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/vgc 201\nI0829 16:47:36.261793       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/8x8 358\nI0829 16:47:36.462142       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/bnsw 308\nI0829 16:47:36.662594       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/mx8 444\nI0829 16:47:36.862059       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/j8ms 224\nI0829 16:47:37.062494       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/rms 352\nI0829 16:47:37.261836       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/gr6d 500\nI0829 16:47:37.462289       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/8vv 211\nI0829 16:47:37.662692       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/sf8z 223\nI0829 16:47:37.865104       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/q7gq 339\nI0829 16:47:38.062462       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/klj 219\nI0829 16:47:38.261753       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/jhd 301\nI0829 16:47:38.462530       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/cvs 410\nI0829 16:47:38.661851       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/v24 217\nI0829 16:47:38.865881       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/kdx 223\nI0829 16:47:39.062150       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/k8xb 426\nI0829 16:47:39.262530       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/fjk 443\nI0829 16:47:39.461671       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/ltg8 514\nI0829 16:47:39.662015       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/blg 459\nI0829 16:47:39.862628       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/5cz 474\nI0829 16:47:40.062039       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/x8h 418\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Aug 29 16:47:40.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-7159 delete pod logs-generator'
Aug 29 16:47:40.986: INFO: stderr: ""
Aug 29 16:47:40.987: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:47:40.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7159" for this suite.

• [SLOW TEST:7.390 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":316,"skipped":5624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:41.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Aug 29 16:47:41.834: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:47:43.853: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.26.197 on the node which pod1 resides and expect scheduled
Aug 29 16:47:43.882: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:47:46.459: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.26.197 but use UDP protocol on the node which pod2 resides
Aug 29 16:47:46.480: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:47:48.495: INFO: The status of Pod pod3 is Running (Ready = true)
Aug 29 16:47:48.516: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:47:50.527: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Aug 29 16:47:50.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.26.197 http://127.0.0.1:54323/hostname] Namespace:hostport-5234 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:47:50.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:47:50.534: INFO: ExecWithOptions: Clientset creation
Aug 29 16:47:50.534: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-5234/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.26.197+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.26.197, port: 54323
Aug 29 16:47:50.849: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.26.197:54323/hostname] Namespace:hostport-5234 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:47:50.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:47:50.849: INFO: ExecWithOptions: Clientset creation
Aug 29 16:47:50.849: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-5234/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.26.197%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.26.197, port: 54323 UDP
Aug 29 16:47:51.049: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.26.197 54323] Namespace:hostport-5234 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:47:51.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:47:51.050: INFO: ExecWithOptions: Clientset creation
Aug 29 16:47:51.050: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-5234/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+172.31.26.197+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Aug 29 16:47:56.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5234" for this suite.

• [SLOW TEST:14.503 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":317,"skipped":5647,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:56.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:47:56.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-2286 version'
Aug 29 16:47:56.418: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 29 16:47:56.418: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.3\", GitCommit:\"aef86a93758dc3cb2c658dd9657ab4ad4afc21cb\", GitTreeState:\"clean\", BuildDate:\"2022-07-13T14:30:46Z\", GoVersion:\"go1.18.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.3\", GitCommit:\"aef86a93758dc3cb2c658dd9657ab4ad4afc21cb\", GitTreeState:\"clean\", BuildDate:\"2022-07-13T14:23:26Z\", GoVersion:\"go1.18.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:47:56.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2286" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":318,"skipped":5655,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:47:56.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 29 16:47:56.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:47:59.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:48:15.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-737" for this suite.

• [SLOW TEST:19.500 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":319,"skipped":5695,"failed":0}
SSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:48:15.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 29 16:48:16.040: INFO: starting watch
STEP: patching
STEP: updating
Aug 29 16:48:16.057: INFO: waiting for watch events with expected annotations
Aug 29 16:48:16.058: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Aug 29 16:48:16.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2489" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":320,"skipped":5703,"failed":0}
SSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:48:16.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:48:16.208: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-249ef891-d698-4506-b519-6d4ed57f556e" in namespace "security-context-test-3896" to be "Succeeded or Failed"
Aug 29 16:48:16.215: INFO: Pod "busybox-readonly-false-249ef891-d698-4506-b519-6d4ed57f556e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.643993ms
Aug 29 16:48:18.234: INFO: Pod "busybox-readonly-false-249ef891-d698-4506-b519-6d4ed57f556e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025393877s
Aug 29 16:48:20.247: INFO: Pod "busybox-readonly-false-249ef891-d698-4506-b519-6d4ed57f556e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038460232s
Aug 29 16:48:20.247: INFO: Pod "busybox-readonly-false-249ef891-d698-4506-b519-6d4ed57f556e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Aug 29 16:48:20.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3896" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":321,"skipped":5707,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:48:20.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Aug 29 16:48:48.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8203" for this suite.

• [SLOW TEST:28.247 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":322,"skipped":5771,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:48:48.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Aug 29 16:48:50.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6519" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":323,"skipped":5783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:48:50.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8607
STEP: creating service affinity-clusterip-transition in namespace services-8607
STEP: creating replication controller affinity-clusterip-transition in namespace services-8607
I0829 16:48:50.788508      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8607, replica count: 3
I0829 16:48:53.840671      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:48:53.865: INFO: Creating new exec pod
Aug 29 16:48:56.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-8607 exec execpod-affinitylzbxl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Aug 29 16:48:57.183: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 29 16:48:57.183: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:48:57.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-8607 exec execpod-affinitylzbxl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.29.131 80'
Aug 29 16:48:57.444: INFO: stderr: "+ nc -v -t -w 2 10.240.29.131 80\nConnection to 10.240.29.131 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Aug 29 16:48:57.444: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:48:57.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-8607 exec execpod-affinitylzbxl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.29.131:80/ ; done'
Aug 29 16:48:57.931: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n"
Aug 29 16:48:57.931: INFO: stdout: "\naffinity-clusterip-transition-zm8dx\naffinity-clusterip-transition-6cwbh\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-zm8dx\naffinity-clusterip-transition-6cwbh\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-zm8dx\naffinity-clusterip-transition-6cwbh\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-zm8dx\naffinity-clusterip-transition-6cwbh\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-zm8dx\naffinity-clusterip-transition-6cwbh\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-zm8dx"
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-zm8dx
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-6cwbh
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-zm8dx
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-6cwbh
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-zm8dx
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-6cwbh
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-zm8dx
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-6cwbh
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-zm8dx
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-6cwbh
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:57.931: INFO: Received response from host: affinity-clusterip-transition-zm8dx
Aug 29 16:48:57.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-8607 exec execpod-affinitylzbxl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.29.131:80/ ; done'
Aug 29 16:48:58.316: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.131:80/\n"
Aug 29 16:48:58.316: INFO: stdout: "\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p\naffinity-clusterip-transition-d9f6p"
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Received response from host: affinity-clusterip-transition-d9f6p
Aug 29 16:48:58.316: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8607, will wait for the garbage collector to delete the pods
Aug 29 16:48:58.406: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.947581ms
Aug 29 16:48:58.507: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.031325ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:49:00.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8607" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.701 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":324,"skipped":5828,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:00.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Aug 29 16:49:00.452: INFO: Major version: 1
STEP: Confirm minor version
Aug 29 16:49:00.453: INFO: cleanMinorVersion: 24
Aug 29 16:49:00.453: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Aug 29 16:49:00.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8544" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":325,"skipped":5841,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:00.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Aug 29 16:49:00.555: INFO: The status of Pod pod-update-4c817c52-2709-4509-bd89-2dbe41981f4e is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:49:02.563: INFO: The status of Pod pod-update-4c817c52-2709-4509-bd89-2dbe41981f4e is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 29 16:49:03.100: INFO: Successfully updated pod "pod-update-4c817c52-2709-4509-bd89-2dbe41981f4e"
STEP: verifying the updated pod is in kubernetes
Aug 29 16:49:03.115: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 29 16:49:03.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5979" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":326,"skipped":5859,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:03.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 29 16:49:03.206: INFO: Waiting up to 5m0s for pod "pod-86cb4da9-9601-436c-9012-8311850327ac" in namespace "emptydir-9352" to be "Succeeded or Failed"
Aug 29 16:49:03.215: INFO: Pod "pod-86cb4da9-9601-436c-9012-8311850327ac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.422557ms
Aug 29 16:49:05.224: INFO: Pod "pod-86cb4da9-9601-436c-9012-8311850327ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017772257s
Aug 29 16:49:07.236: INFO: Pod "pod-86cb4da9-9601-436c-9012-8311850327ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029601742s
Aug 29 16:49:09.258: INFO: Pod "pod-86cb4da9-9601-436c-9012-8311850327ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052151802s
STEP: Saw pod success
Aug 29 16:49:09.258: INFO: Pod "pod-86cb4da9-9601-436c-9012-8311850327ac" satisfied condition "Succeeded or Failed"
Aug 29 16:49:09.265: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-86cb4da9-9601-436c-9012-8311850327ac container test-container: <nil>
STEP: delete the pod
Aug 29 16:49:09.291: INFO: Waiting for pod pod-86cb4da9-9601-436c-9012-8311850327ac to disappear
Aug 29 16:49:09.314: INFO: Pod pod-86cb4da9-9601-436c-9012-8311850327ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Aug 29 16:49:09.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9352" for this suite.

• [SLOW TEST:6.194 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":327,"skipped":5882,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:09.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:49:09.417: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f25d3e8c-b935-4d5b-8cad-28449b194282" in namespace "downward-api-4942" to be "Succeeded or Failed"
Aug 29 16:49:09.427: INFO: Pod "downwardapi-volume-f25d3e8c-b935-4d5b-8cad-28449b194282": Phase="Pending", Reason="", readiness=false. Elapsed: 9.752467ms
Aug 29 16:49:11.438: INFO: Pod "downwardapi-volume-f25d3e8c-b935-4d5b-8cad-28449b194282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021167477s
Aug 29 16:49:13.455: INFO: Pod "downwardapi-volume-f25d3e8c-b935-4d5b-8cad-28449b194282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03727778s
STEP: Saw pod success
Aug 29 16:49:13.455: INFO: Pod "downwardapi-volume-f25d3e8c-b935-4d5b-8cad-28449b194282" satisfied condition "Succeeded or Failed"
Aug 29 16:49:13.461: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod downwardapi-volume-f25d3e8c-b935-4d5b-8cad-28449b194282 container client-container: <nil>
STEP: delete the pod
Aug 29 16:49:13.508: INFO: Waiting for pod downwardapi-volume-f25d3e8c-b935-4d5b-8cad-28449b194282 to disappear
Aug 29 16:49:13.514: INFO: Pod downwardapi-volume-f25d3e8c-b935-4d5b-8cad-28449b194282 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Aug 29 16:49:13.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4942" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":328,"skipped":5940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:13.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 29 16:49:14.095: INFO: starting watch
STEP: patching
STEP: updating
Aug 29 16:49:14.120: INFO: waiting for watch events with expected annotations
Aug 29 16:49:14.120: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:49:14.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2041" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":329,"skipped":5970,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:14.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Aug 29 16:49:14.378: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:49:16.388: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Aug 29 16:49:17.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8318" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":330,"skipped":6078,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:17.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Aug 29 16:49:17.536: INFO: The status of Pod labelsupdateb9fd0ebd-35fa-4d35-ab05-797df85bfa6d is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:49:19.545: INFO: The status of Pod labelsupdateb9fd0ebd-35fa-4d35-ab05-797df85bfa6d is Running (Ready = true)
Aug 29 16:49:20.091: INFO: Successfully updated pod "labelsupdateb9fd0ebd-35fa-4d35-ab05-797df85bfa6d"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 16:49:24.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3684" for this suite.

• [SLOW TEST:6.712 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":331,"skipped":6081,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:24.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Aug 29 16:49:24.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1fcbbac2-db5d-4747-a807-c8572d70ffa9" in namespace "projected-689" to be "Succeeded or Failed"
Aug 29 16:49:24.233: INFO: Pod "downwardapi-volume-1fcbbac2-db5d-4747-a807-c8572d70ffa9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.249063ms
Aug 29 16:49:26.252: INFO: Pod "downwardapi-volume-1fcbbac2-db5d-4747-a807-c8572d70ffa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027057107s
Aug 29 16:49:28.262: INFO: Pod "downwardapi-volume-1fcbbac2-db5d-4747-a807-c8572d70ffa9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037502619s
Aug 29 16:49:30.275: INFO: Pod "downwardapi-volume-1fcbbac2-db5d-4747-a807-c8572d70ffa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05054402s
STEP: Saw pod success
Aug 29 16:49:30.275: INFO: Pod "downwardapi-volume-1fcbbac2-db5d-4747-a807-c8572d70ffa9" satisfied condition "Succeeded or Failed"
Aug 29 16:49:30.284: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod downwardapi-volume-1fcbbac2-db5d-4747-a807-c8572d70ffa9 container client-container: <nil>
STEP: delete the pod
Aug 29 16:49:30.313: INFO: Waiting for pod downwardapi-volume-1fcbbac2-db5d-4747-a807-c8572d70ffa9 to disappear
Aug 29 16:49:30.319: INFO: Pod downwardapi-volume-1fcbbac2-db5d-4747-a807-c8572d70ffa9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Aug 29 16:49:30.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-689" for this suite.

• [SLOW TEST:6.192 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":332,"skipped":6087,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:30.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-g9bc
STEP: Creating a pod to test atomic-volume-subpath
Aug 29 16:49:30.428: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-g9bc" in namespace "subpath-4818" to be "Succeeded or Failed"
Aug 29 16:49:30.441: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.632689ms
Aug 29 16:49:32.453: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.024541984s
Aug 29 16:49:34.462: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=true. Elapsed: 4.033392845s
Aug 29 16:49:36.474: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=true. Elapsed: 6.045797097s
Aug 29 16:49:38.483: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=true. Elapsed: 8.054655548s
Aug 29 16:49:40.497: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=true. Elapsed: 10.068407384s
Aug 29 16:49:42.511: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=true. Elapsed: 12.082326486s
Aug 29 16:49:44.521: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=true. Elapsed: 14.092181557s
Aug 29 16:49:46.531: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=true. Elapsed: 16.102918487s
Aug 29 16:49:48.539: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=true. Elapsed: 18.110933968s
Aug 29 16:49:50.556: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=true. Elapsed: 20.127972467s
Aug 29 16:49:52.569: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Running", Reason="", readiness=false. Elapsed: 22.140609212s
Aug 29 16:49:54.579: INFO: Pod "pod-subpath-test-downwardapi-g9bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.150375447s
STEP: Saw pod success
Aug 29 16:49:54.579: INFO: Pod "pod-subpath-test-downwardapi-g9bc" satisfied condition "Succeeded or Failed"
Aug 29 16:49:54.586: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-g9bc container test-container-subpath-downwardapi-g9bc: <nil>
STEP: delete the pod
Aug 29 16:49:54.635: INFO: Waiting for pod pod-subpath-test-downwardapi-g9bc to disappear
Aug 29 16:49:54.641: INFO: Pod pod-subpath-test-downwardapi-g9bc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-g9bc
Aug 29 16:49:54.641: INFO: Deleting pod "pod-subpath-test-downwardapi-g9bc" in namespace "subpath-4818"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Aug 29 16:49:54.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4818" for this suite.

• [SLOW TEST:24.314 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":333,"skipped":6091,"failed":0}
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:54.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Aug 29 16:49:54.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1395" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":334,"skipped":6091,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:54.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-1619b617-910c-4008-bd1a-f44a25303f6b
STEP: Creating a pod to test consume configMaps
Aug 29 16:49:54.899: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d13d8c4-11bd-4771-a418-428538b67865" in namespace "configmap-3570" to be "Succeeded or Failed"
Aug 29 16:49:54.914: INFO: Pod "pod-configmaps-7d13d8c4-11bd-4771-a418-428538b67865": Phase="Pending", Reason="", readiness=false. Elapsed: 14.942931ms
Aug 29 16:49:56.923: INFO: Pod "pod-configmaps-7d13d8c4-11bd-4771-a418-428538b67865": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023689828s
Aug 29 16:49:58.930: INFO: Pod "pod-configmaps-7d13d8c4-11bd-4771-a418-428538b67865": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031240098s
STEP: Saw pod success
Aug 29 16:49:58.930: INFO: Pod "pod-configmaps-7d13d8c4-11bd-4771-a418-428538b67865" satisfied condition "Succeeded or Failed"
Aug 29 16:49:58.937: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod pod-configmaps-7d13d8c4-11bd-4771-a418-428538b67865 container agnhost-container: <nil>
STEP: delete the pod
Aug 29 16:49:58.976: INFO: Waiting for pod pod-configmaps-7d13d8c4-11bd-4771-a418-428538b67865 to disappear
Aug 29 16:49:58.983: INFO: Pod pod-configmaps-7d13d8c4-11bd-4771-a418-428538b67865 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:49:58.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3570" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":335,"skipped":6106,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:49:59.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 29 16:49:59.092: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 16:50:59.190: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Aug 29 16:50:59.228: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 29 16:50:59.238: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 29 16:50:59.265: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 29 16:50:59.283: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 29 16:50:59.316: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 29 16:50:59.330: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Aug 29 16:51:07.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9487" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:68.728 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":336,"skipped":6112,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:07.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-7668
STEP: creating service affinity-clusterip in namespace services-7668
STEP: creating replication controller affinity-clusterip in namespace services-7668
I0829 16:51:07.838548      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7668, replica count: 3
I0829 16:51:10.889750      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 16:51:10.914: INFO: Creating new exec pod
Aug 29 16:51:14.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-7668 exec execpod-affinitytp67x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Aug 29 16:51:14.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 29 16:51:14.965: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:51:14.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-7668 exec execpod-affinitytp67x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.22.121 80'
Aug 29 16:51:15.182: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.22.121 80\nConnection to 10.240.22.121 80 port [tcp/http] succeeded!\n"
Aug 29 16:51:15.182: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 29 16:51:15.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-7668 exec execpod-affinitytp67x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.22.121:80/ ; done'
Aug 29 16:51:15.504: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.22.121:80/\n"
Aug 29 16:51:15.504: INFO: stdout: "\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp\naffinity-clusterip-8gkbp"
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Received response from host: affinity-clusterip-8gkbp
Aug 29 16:51:15.504: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7668, will wait for the garbage collector to delete the pods
Aug 29 16:51:15.603: INFO: Deleting ReplicationController affinity-clusterip took: 12.066223ms
Aug 29 16:51:15.704: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.029122ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:51:17.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7668" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.009 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":337,"skipped":6215,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:17.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 29 16:51:19.935: INFO: DNS probes using dns-8945/dns-test-2b3c8cdb-22c9-4cf0-a9d4-dc74856ae610 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Aug 29 16:51:19.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8945" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":338,"skipped":6235,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:19.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4814
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4814
STEP: creating replication controller externalsvc in namespace services-4814
I0829 16:51:20.096949      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4814, replica count: 2
I0829 16:51:23.147905      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 29 16:51:23.284: INFO: Creating new exec pod
Aug 29 16:51:25.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=services-4814 exec execpodlngdj -- /bin/sh -x -c nslookup clusterip-service.services-4814.svc.cluster.local'
Aug 29 16:51:25.906: INFO: stderr: "+ nslookup clusterip-service.services-4814.svc.cluster.local\n"
Aug 29 16:51:25.906: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-4814.svc.cluster.local\tcanonical name = externalsvc.services-4814.svc.cluster.local.\nName:\texternalsvc.services-4814.svc.cluster.local\nAddress: 10.240.30.159\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4814, will wait for the garbage collector to delete the pods
Aug 29 16:51:25.980: INFO: Deleting ReplicationController externalsvc took: 14.263733ms
Aug 29 16:51:26.080: INFO: Terminating ReplicationController externalsvc pods took: 100.148434ms
Aug 29 16:51:27.825: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Aug 29 16:51:27.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4814" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:7.986 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":339,"skipped":6242,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:27.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:51:28.083: INFO: The status of Pod server-envvars-0a608d3b-c198-4bac-af44-498c59fc8dca is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:51:30.098: INFO: The status of Pod server-envvars-0a608d3b-c198-4bac-af44-498c59fc8dca is Running (Ready = true)
Aug 29 16:51:30.159: INFO: Waiting up to 5m0s for pod "client-envvars-c4577865-2a90-4375-9e00-4560a8ee3318" in namespace "pods-4106" to be "Succeeded or Failed"
Aug 29 16:51:30.173: INFO: Pod "client-envvars-c4577865-2a90-4375-9e00-4560a8ee3318": Phase="Pending", Reason="", readiness=false. Elapsed: 13.205058ms
Aug 29 16:51:32.186: INFO: Pod "client-envvars-c4577865-2a90-4375-9e00-4560a8ee3318": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026298558s
Aug 29 16:51:34.195: INFO: Pod "client-envvars-c4577865-2a90-4375-9e00-4560a8ee3318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035942716s
STEP: Saw pod success
Aug 29 16:51:34.196: INFO: Pod "client-envvars-c4577865-2a90-4375-9e00-4560a8ee3318" satisfied condition "Succeeded or Failed"
Aug 29 16:51:34.203: INFO: Trying to get logs from node ip-172-31-25-142.eu-central-1.compute.internal pod client-envvars-c4577865-2a90-4375-9e00-4560a8ee3318 container env3cont: <nil>
STEP: delete the pod
Aug 29 16:51:34.541: INFO: Waiting for pod client-envvars-c4577865-2a90-4375-9e00-4560a8ee3318 to disappear
Aug 29 16:51:34.548: INFO: Pod client-envvars-c4577865-2a90-4375-9e00-4560a8ee3318 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 29 16:51:34.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4106" for this suite.

• [SLOW TEST:6.602 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":340,"skipped":6251,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:34.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:51:34.641: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 29 16:51:39.652: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Aug 29 16:51:39.677: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Aug 29 16:51:39.723: INFO: observed ReplicaSet test-rs in namespace replicaset-4804 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 16:51:39.746: INFO: observed ReplicaSet test-rs in namespace replicaset-4804 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 16:51:39.748: INFO: observed ReplicaSet test-rs in namespace replicaset-4804 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 16:51:41.636: INFO: observed ReplicaSet test-rs in namespace replicaset-4804 with ReadyReplicas 2, AvailableReplicas 2
Aug 29 16:51:41.729: INFO: observed Replicaset test-rs in namespace replicaset-4804 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 29 16:51:41.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4804" for this suite.

• [SLOW TEST:7.175 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":341,"skipped":6284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:41.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Aug 29 16:51:41.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3298" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":342,"skipped":6314,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:41.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 29 16:51:41.874: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Aug 29 16:51:47.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9009" for this suite.

• [SLOW TEST:5.852 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":343,"skipped":6322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:47.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-13922951-dcc6-47c0-ae63-a55bc3004ec1
STEP: Creating a pod to test consume secrets
Aug 29 16:51:47.831: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7a75487e-f930-44c5-b794-041146a49927" in namespace "projected-4847" to be "Succeeded or Failed"
Aug 29 16:51:47.839: INFO: Pod "pod-projected-secrets-7a75487e-f930-44c5-b794-041146a49927": Phase="Pending", Reason="", readiness=false. Elapsed: 7.681049ms
Aug 29 16:51:49.849: INFO: Pod "pod-projected-secrets-7a75487e-f930-44c5-b794-041146a49927": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01749764s
Aug 29 16:51:51.859: INFO: Pod "pod-projected-secrets-7a75487e-f930-44c5-b794-041146a49927": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028195892s
STEP: Saw pod success
Aug 29 16:51:51.860: INFO: Pod "pod-projected-secrets-7a75487e-f930-44c5-b794-041146a49927" satisfied condition "Succeeded or Failed"
Aug 29 16:51:51.868: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-projected-secrets-7a75487e-f930-44c5-b794-041146a49927 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 29 16:51:51.900: INFO: Waiting for pod pod-projected-secrets-7a75487e-f930-44c5-b794-041146a49927 to disappear
Aug 29 16:51:51.909: INFO: Pod pod-projected-secrets-7a75487e-f930-44c5-b794-041146a49927 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Aug 29 16:51:51.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4847" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":344,"skipped":6350,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:51.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Aug 29 16:51:52.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3634" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":345,"skipped":6365,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:52.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-37905ddc-d4ef-4b07-ac6d-c294ffcb9372
STEP: Creating the pod
Aug 29 16:51:52.130: INFO: The status of Pod pod-configmaps-7e411b6f-3a36-471b-a469-b3c31c748156 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:51:54.142: INFO: The status of Pod pod-configmaps-7e411b6f-3a36-471b-a469-b3c31c748156 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-37905ddc-d4ef-4b07-ac6d-c294ffcb9372
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Aug 29 16:51:56.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-621" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":346,"skipped":6370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:56.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Aug 29 16:51:56.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6282" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":347,"skipped":6399,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:56.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 29 16:51:57.364: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8358  3909ad6a-aa64-44bf-bfeb-d41aeb638355 44592 0 2022-08-29 16:51:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-08-29 16:51:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 16:51:57.364: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8358  3909ad6a-aa64-44bf-bfeb-d41aeb638355 44594 0 2022-08-29 16:51:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-08-29 16:51:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Aug 29 16:51:57.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8358" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":348,"skipped":6405,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:51:57.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Aug 29 16:51:59.485: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9753 pod-service-account-630a90ad-6c6f-478b-b84f-6b9ced12b0e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 29 16:51:59.810: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9753 pod-service-account-630a90ad-6c6f-478b-b84f-6b9ced12b0e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 29 16:52:00.078: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9753 pod-service-account-630a90ad-6c6f-478b-b84f-6b9ced12b0e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 29 16:52:00.289: INFO: Got root ca configmap in namespace "svcaccounts-9753"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Aug 29 16:52:00.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9753" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":349,"skipped":6423,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:52:00.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Aug 29 16:52:04.634: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5976 PodName:var-expansion-80a93e50-736b-4e0c-8109-60b1e69d2684 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:52:04.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:52:04.635: INFO: ExecWithOptions: Clientset creation
Aug 29 16:52:04.635: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-5976/pods/var-expansion-80a93e50-736b-4e0c-8109-60b1e69d2684/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Aug 29 16:52:04.808: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5976 PodName:var-expansion-80a93e50-736b-4e0c-8109-60b1e69d2684 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 16:52:04.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
Aug 29 16:52:04.809: INFO: ExecWithOptions: Clientset creation
Aug 29 16:52:04.809: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-5976/pods/var-expansion-80a93e50-736b-4e0c-8109-60b1e69d2684/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Aug 29 16:52:06.430: INFO: Successfully updated pod "var-expansion-80a93e50-736b-4e0c-8109-60b1e69d2684"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Aug 29 16:52:06.441: INFO: Deleting pod "var-expansion-80a93e50-736b-4e0c-8109-60b1e69d2684" in namespace "var-expansion-5976"
Aug 29 16:52:06.461: INFO: Wait up to 5m0s for pod "var-expansion-80a93e50-736b-4e0c-8109-60b1e69d2684" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Aug 29 16:52:38.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5976" for this suite.

• [SLOW TEST:37.975 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":350,"skipped":6429,"failed":0}
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:52:38.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Aug 29 16:52:38.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1073880741 --namespace=kubectl-2783 cluster-info'
Aug 29 16:52:38.658: INFO: stderr: ""
Aug 29 16:52:38.658: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Aug 29 16:52:38.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2783" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":351,"skipped":6429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:52:38.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-ed1fb660-948f-4add-834a-c28543eaf17b
STEP: Creating a pod to test consume secrets
Aug 29 16:52:38.769: INFO: Waiting up to 5m0s for pod "pod-secrets-4ef3f5ea-8b5d-4013-a6f3-1725de0bbc76" in namespace "secrets-6367" to be "Succeeded or Failed"
Aug 29 16:52:38.783: INFO: Pod "pod-secrets-4ef3f5ea-8b5d-4013-a6f3-1725de0bbc76": Phase="Pending", Reason="", readiness=false. Elapsed: 13.78418ms
Aug 29 16:52:40.794: INFO: Pod "pod-secrets-4ef3f5ea-8b5d-4013-a6f3-1725de0bbc76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025020303s
Aug 29 16:52:42.809: INFO: Pod "pod-secrets-4ef3f5ea-8b5d-4013-a6f3-1725de0bbc76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039455572s
STEP: Saw pod success
Aug 29 16:52:42.810: INFO: Pod "pod-secrets-4ef3f5ea-8b5d-4013-a6f3-1725de0bbc76" satisfied condition "Succeeded or Failed"
Aug 29 16:52:42.817: INFO: Trying to get logs from node ip-172-31-16-214.eu-central-1.compute.internal pod pod-secrets-4ef3f5ea-8b5d-4013-a6f3-1725de0bbc76 container secret-env-test: <nil>
STEP: delete the pod
Aug 29 16:52:43.099: INFO: Waiting for pod pod-secrets-4ef3f5ea-8b5d-4013-a6f3-1725de0bbc76 to disappear
Aug 29 16:52:43.104: INFO: Pod pod-secrets-4ef3f5ea-8b5d-4013-a6f3-1725de0bbc76 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Aug 29 16:52:43.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6367" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":352,"skipped":6474,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:52:43.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:52:43.730: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 16:52:45.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.August, 29, 16, 52, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 52, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.August, 29, 16, 52, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.August, 29, 16, 52, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:52:48.797: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:52:49.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-427" for this suite.
STEP: Destroying namespace "webhook-427-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.742 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":353,"skipped":6504,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:52:49.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Aug 29 16:52:49.988: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 29 16:52:54.999: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Aug 29 16:52:55.009: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Aug 29 16:52:55.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2581" for this suite.

• [SLOW TEST:5.220 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":354,"skipped":6523,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:52:55.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 29 16:52:55.907: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 29 16:52:58.946: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Aug 29 16:52:59.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8158" for this suite.
STEP: Destroying namespace "webhook-8158-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":355,"skipped":6524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Aug 29 16:52:59.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1073880741
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Aug 29 16:52:59.562: INFO: The status of Pod pod-secrets-141ee37a-74d4-436b-9d9d-3fb39f2c9a5a is Pending, waiting for it to be Running (with Ready = true)
Aug 29 16:53:01.571: INFO: The status of Pod pod-secrets-141ee37a-74d4-436b-9d9d-3fb39f2c9a5a is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Aug 29 16:53:01.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6671" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":356,"skipped":6549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSAug 29 16:53:01.647: INFO: Running AfterSuite actions on all nodes
Aug 29 16:53:01.647: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Aug 29 16:53:01.647: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Aug 29 16:53:01.647: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Aug 29 16:53:01.647: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Aug 29 16:53:01.647: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Aug 29 16:53:01.647: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Aug 29 16:53:01.647: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Aug 29 16:53:01.647: INFO: Running AfterSuite actions on node 1
Aug 29 16:53:01.647: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6615,"failed":0}

Ran 356 of 6971 Specs in 6340.313 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6615 Skipped
PASS

Ginkgo ran 1 suite in 1h45m43.311206467s
Test Suite Passed
